{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"readme/","title":"Readme","text":""},{"location":"readme/#documentation-local-setup-guide","title":"\ud83d\udcda Documentation - Local Setup Guide","text":"<p>This guide will help you set up and run the documentation site locally using MkDocs.</p>"},{"location":"readme/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<ul> <li>Ensure you have an active Conda environment set up.</li> </ul>"},{"location":"readme/#install-dependencies","title":"\ud83d\udce6 Install Dependencies","text":"<p>To install the documentation dependencies locally, run:</p> <pre><code>pip install -e \".[docs]\"\n</code></pre> <p>This installs all required packages needed to build and serve the documentation.</p>"},{"location":"readme/#run-documentation-locally","title":"\ud83d\ude80 Run Documentation Locally","text":"<p>Once the installation is complete, start the MkDocs development server with:</p> <pre><code>mkdocs serve\n</code></pre> <p>You can now view the documentation in your browser at:</p> <p>\ud83d\udc49 http://127.0.0.1:8000/pages/</p>"},{"location":"readme/#components-library","title":"\ud83e\udde9 Components Library","text":"<p>Below are examples of commonly used components in the documentation:</p>"},{"location":"readme/#note","title":"\ud83d\udd14 Note","text":"<pre><code>&gt; **Note**  \n&gt; This is a note block to highlight important information.\n</code></pre> <p>Rendered Output:</p> <pre><code>&gt; This is a note block to highlight important information.\n</code></pre>"},{"location":"readme/#code-block","title":"\ud83d\udcbb Code Block","text":"<pre><code>def hello_world():\n    print(\"Hello, world!\")\n</code></pre> <p>Rendered Output:</p> <pre><code>def hello_world():\n    print(\"Hello, world!\")\n</code></pre>"},{"location":"api/kedro.logging/","title":"kedro.logging","text":"Name Type Description <code>kedro.logging.RichHandler</code> Class A logging handler for rendering rich output."},{"location":"api/kedro.logging/#kedro.logging","title":"kedro.logging","text":""},{"location":"api/kedro.logging/#kedro.logging.RichHandler","title":"kedro.logging.RichHandler","text":"<pre><code>RichHandler(*args, **kwargs)\n</code></pre> <p>               Bases: <code>RichHandler</code></p> <p>Identical to rich's logging handler but with a few extra behaviours: * warnings issued by the <code>warnings</code> module are redirected to logging * pretty printing is enabled on the Python REPL (including IPython and Jupyter) * all tracebacks are handled by rich when rich_tracebacks=True * constructor's arguments are mapped and passed to <code>rich.traceback.install</code></p> <p>The list of available options of <code>RichHandler</code> can be found here: https://rich.readthedocs.io/en/stable/reference/logging.html#rich.logging.RichHandler</p> <p>The list of available options of <code>rich.traceback.install</code> can be found here: https://rich.readthedocs.io/en/stable/reference/traceback.html#rich.traceback.install</p> Source code in <code>kedro/logging.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any):\n    if \"markup\" not in kwargs:\n        kwargs.update({\"markup\": True})  # Set markup as default\n    super().__init__(*args, **kwargs)\n    logging.captureWarnings(True)\n    rich.pretty.install()\n\n    # We suppress click here to hide tracebacks related to it conversely,\n    # kedro is not suppressed to show its tracebacks for easier debugging.\n    # sys.executable is used to get the kedro executable path to hide the\n    # top level traceback.\n\n    traceback_install_kwargs = {\n        \"suppress\": [click, str(Path(sys.executable).parent)]\n    }\n\n    # Mapping arguments from RichHandler's Constructor to rich.traceback.install\n    prefix = \"tracebacks_\"\n    for key, value in kwargs.items():\n        if key.startswith(prefix):\n            key_prefix_removed = key[len(prefix) :]\n            if key_prefix_removed == \"suppress\":\n                traceback_install_kwargs[key_prefix_removed].extend(value)\n            else:\n                traceback_install_kwargs[key_prefix_removed] = value\n\n    if self.rich_tracebacks and not _is_databricks():\n        # Rich traceback handling does not work on databricks. Hopefully this will be\n        # fixed on their side at some point, but until then we disable it.\n        # See https://github.com/Textualize/rich/issues/2455\n        rich.traceback.install(**traceback_install_kwargs)  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/kedro.logging/#kedro.logging.RichHandler.emit","title":"emit","text":"<pre><code>emit(record)\n</code></pre> Source code in <code>kedro/logging.py</code> <pre><code>def emit(self, record: LogRecord) -&gt; None:\n    args = record.args\n    if not args or not hasattr(record, \"rich_format\"):\n        return super().emit(record)\n\n    else:\n        # LogRecord is shared among all handler, created a new instance so it does not affect the rest.\n        updated_record = copy.copy(record)\n        format_args = record.rich_format  # type: ignore\n\n        if format_args and isinstance(format_args, list):\n            new_args = list(args)\n            # Add markup to the message\n            # if the list is shorter than the arg list, keep it unchanged\n            for i, color in enumerate(format_args):\n                new_args[i] = _format_rich(str(new_args[i]), color)\n            updated_record.args = tuple(new_args)\n            super().emit(updated_record)\n        else:\n            raise TypeError(\"rich_format only accept non-empty list as an argument\")\n</code></pre>"},{"location":"api/kedro.utils/","title":"kedro.framework.cli.utils","text":"Name Type Description <code>load_obj</code> Function Load an object from a dotted path. <code>random_string</code> Function Generate a random string of a given length."},{"location":"api/kedro.utils/#kedro.framework.cli.utils","title":"kedro.framework.cli.utils","text":"<p>Utilities for use with click.</p>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.command_with_verbosity","title":"kedro.framework.cli.utils.command_with_verbosity","text":"<pre><code>command_with_verbosity(group, *args, **kwargs)\n</code></pre> <p>Custom command decorator with verbose flag added.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def command_with_verbosity(group: click.core.Group, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Custom command decorator with verbose flag added.\"\"\"\n\n    def decorator(func: Any) -&gt; Any:\n        func = _click_verbose(func)\n        func = group.command(*args, **kwargs)(func)\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.env_option","title":"kedro.framework.cli.utils.env_option","text":"<pre><code>env_option(func_=None, **kwargs)\n</code></pre> <p>Add <code>--env</code> CLI option to a function.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def env_option(func_: Any | None = None, **kwargs: Any) -&gt; Any:\n    \"\"\"Add `--env` CLI option to a function.\"\"\"\n    default_args = {\"type\": str, \"default\": None, \"help\": ENV_HELP}\n    kwargs = {**default_args, **kwargs}\n    opt = click.option(\"--env\", \"-e\", **kwargs)\n    return opt(func_) if func_ else opt\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.find_stylesheets","title":"kedro.framework.cli.utils.find_stylesheets","text":"<pre><code>find_stylesheets()\n</code></pre> <p>Fetch all stylesheets used in the official Kedro documentation</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def find_stylesheets() -&gt; Iterable[str]:  # pragma: no cover\n    # TODO: Deprecate this function in favour of kedro-sphinx-theme\n    \"\"\"Fetch all stylesheets used in the official Kedro documentation\"\"\"\n    css_path = Path(__file__).resolve().parents[1] / \"html\" / \"_static\" / \"css\"\n    return (str(css_path / \"copybutton.css\"),)\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.forward_command","title":"kedro.framework.cli.utils.forward_command","text":"<pre><code>forward_command(group, name=None, forward_help=False)\n</code></pre> <p>A command that receives the rest of the command line as 'args'.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def forward_command(\n    group: Any, name: str | None = None, forward_help: bool = False\n) -&gt; Any:\n    \"\"\"A command that receives the rest of the command line as 'args'.\"\"\"\n\n    def wrapit(func: Any) -&gt; Any:\n        func = click.argument(\"args\", nargs=-1, type=click.UNPROCESSED)(func)\n        func = command_with_verbosity(\n            group,\n            name=name,\n            context_settings={\n                \"ignore_unknown_options\": True,\n                \"help_option_names\": [] if forward_help else [\"-h\", \"--help\"],\n            },\n        )(func)\n        return func\n\n    return wrapit\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.get_pkg_version","title":"kedro.framework.cli.utils.get_pkg_version","text":"<pre><code>get_pkg_version(reqs_path, package_name)\n</code></pre> <p>Get package version from requirements.txt.</p> <p>Parameters:</p> <ul> <li> <code>reqs_path</code>               (<code>str | Path</code>)           \u2013            <p>Path to requirements.txt file.</p> </li> <li> <code>package_name</code>               (<code>str</code>)           \u2013            <p>Package to search for.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Package and its version as specified in requirements.txt.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KedroCliError</code>             \u2013            <p>If the file specified in <code>reqs_path</code> does not exist or <code>package_name</code> was not found in that file.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def get_pkg_version(reqs_path: (str | Path), package_name: str) -&gt; str:\n    \"\"\"Get package version from requirements.txt.\n\n    Args:\n        reqs_path: Path to requirements.txt file.\n        package_name: Package to search for.\n\n    Returns:\n        Package and its version as specified in requirements.txt.\n\n    Raises:\n        KedroCliError: If the file specified in ``reqs_path`` does not exist\n            or ``package_name`` was not found in that file.\n    \"\"\"\n    warnings.warn(\n        \"`get_pkg_version()` has been deprecated and will be removed in Kedro 0.20.0\",\n        KedroDeprecationWarning,\n    )\n    reqs_path = Path(reqs_path).absolute()\n    if not reqs_path.is_file():\n        raise KedroCliError(f\"Given path '{reqs_path}' is not a regular file.\")\n\n    pattern = re.compile(package_name + r\"([^\\w]|$)\")\n    with reqs_path.open(\"r\", encoding=\"utf-8\") as reqs_file:\n        for req_line in reqs_file:\n            req_line = req_line.strip()  # noqa: PLW2901\n            if pattern.search(req_line):\n                return req_line\n\n    raise KedroCliError(f\"Cannot find '{package_name}' package in '{reqs_path}'.\")\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.python_call","title":"kedro.framework.cli.utils.python_call","text":"<pre><code>python_call(module, arguments, **kwargs)\n</code></pre> <p>Run a subprocess command that invokes a Python module.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def python_call(\n    module: str, arguments: Iterable[str], **kwargs: Any\n) -&gt; None:  # pragma: no cover\n    \"\"\"Run a subprocess command that invokes a Python module.\"\"\"\n    call([sys.executable, \"-m\", module, *list(arguments)], **kwargs)\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.split_string","title":"kedro.framework.cli.utils.split_string","text":"<pre><code>split_string(ctx, param, value)\n</code></pre> <p>Split string by comma.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def split_string(ctx: click.Context, param: Any, value: str) -&gt; list[str]:\n    \"\"\"Split string by comma.\"\"\"\n    return [item.strip() for item in value.split(\",\") if item.strip()]\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.CommandCollection","title":"kedro.framework.cli.utils.CommandCollection","text":"<pre><code>CommandCollection(*groups)\n</code></pre> <p>               Bases: <code>CommandCollection</code></p> <p>Modified from the Click one to still run the source groups function.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def __init__(self, *groups: tuple[str, Sequence[click.MultiCommand]]):\n    self.groups = [\n        (title, self._merge_same_name_collections(cli_list))\n        for title, cli_list in groups\n    ]\n    sources = list(chain.from_iterable(cli_list for _, cli_list in self.groups))\n    help_texts = [\n        cli.help\n        for cli_collection in sources\n        for cli in cli_collection.sources\n        if cli.help\n    ]\n    super().__init__(\n        sources=sources,  # type: ignore[arg-type]\n        help=\"\\n\\n\".join(help_texts),\n        context_settings=CONTEXT_SETTINGS,\n    )\n    self.params = sources[0].params\n    self.callback = sources[0].callback\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.CommandCollection.callback","title":"callback  <code>instance-attribute</code>","text":"<pre><code>callback = callback\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.CommandCollection.groups","title":"groups  <code>instance-attribute</code>","text":"<pre><code>groups = [(title, _merge_same_name_collections(cli_list)) for (title, cli_list) in groups]\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.CommandCollection.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params = params\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.CommandCollection._merge_same_name_collections","title":"_merge_same_name_collections  <code>staticmethod</code>","text":"<pre><code>_merge_same_name_collections(groups)\n</code></pre> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>@staticmethod\ndef _merge_same_name_collections(\n    groups: Sequence[click.MultiCommand],\n) -&gt; list[click.CommandCollection]:\n    named_groups: defaultdict[str, list[click.MultiCommand]] = defaultdict(list)\n    helps: defaultdict[str, list] = defaultdict(list)\n    for group in groups:\n        named_groups[group.name].append(group)  # type: ignore[index]\n        if group.help:\n            helps[group.name].append(group.help)  # type: ignore[index]\n    return [\n        click.CommandCollection(\n            name=group_name,\n            sources=cli_list,\n            help=\"\\n\\n\".join(helps[group_name]),\n            callback=cli_list[0].callback,\n            params=cli_list[0].params,\n        )\n        for group_name, cli_list in named_groups.items()\n        if cli_list\n    ]\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.CommandCollection.format_commands","title":"format_commands","text":"<pre><code>format_commands(ctx, formatter)\n</code></pre> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def format_commands(\n    self, ctx: click.core.Context, formatter: click.formatting.HelpFormatter\n) -&gt; None:\n    for title, cli in self.groups:\n        for group in cli:\n            if group.sources:\n                formatter.write(\n                    click.style(f\"\\n{title} from {group.name}\", fg=\"green\")\n                )\n                group.format_commands(ctx, formatter)\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.CommandCollection.resolve_command","title":"resolve_command","text":"<pre><code>resolve_command(ctx, args)\n</code></pre> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def resolve_command(\n    self, ctx: click.core.Context, args: list\n) -&gt; tuple[str | None, click.Command | None, list[str]]:\n    try:\n        return super().resolve_command(ctx, args)\n    except click.exceptions.UsageError as exc:\n        original_command_name = click.utils.make_str(args[0])\n        existing_command_names = self.list_commands(ctx)\n        exc.message += _suggest_cli_command(\n            original_command_name, existing_command_names\n        )\n        raise\n</code></pre>"},{"location":"api/kedro.utils/#kedro.framework.cli.utils.KedroCliError","title":"kedro.framework.cli.utils.KedroCliError","text":"<p>               Bases: <code>ClickException</code></p> <p>Exceptions generated from the Kedro CLI.</p> <p>Users should pass an appropriate message at the constructor.</p>"},{"location":"api/config/kedro.config.AbstractConfigLoader/","title":"AbstractConfigLoader","text":""},{"location":"api/config/kedro.config.AbstractConfigLoader/#kedro.config.AbstractConfigLoader","title":"kedro.config.AbstractConfigLoader","text":"<pre><code>AbstractConfigLoader(conf_source, env=None, runtime_params=None, **kwargs)\n</code></pre> <p>               Bases: <code>UserDict</code></p> <p><code>AbstractConfigLoader</code> is the abstract base class     for all <code>ConfigLoader</code> implementations. All user-defined <code>ConfigLoader</code> implementations should inherit     from <code>AbstractConfigLoader</code> and implement all relevant abstract methods.</p> Source code in <code>kedro/config/abstract_config.py</code> <pre><code>def __init__(\n    self,\n    conf_source: str,\n    env: str | None = None,\n    runtime_params: dict[str, Any] | None = None,\n    **kwargs: Any,\n):\n    super().__init__()\n    self.conf_source = conf_source\n    self.env = env\n    self.runtime_params = runtime_params or {}\n</code></pre>"},{"location":"api/config/kedro.config.AbstractConfigLoader/#kedro.config.AbstractConfigLoader.conf_source","title":"conf_source  <code>instance-attribute</code>","text":"<pre><code>conf_source = conf_source\n</code></pre>"},{"location":"api/config/kedro.config.AbstractConfigLoader/#kedro.config.AbstractConfigLoader.env","title":"env  <code>instance-attribute</code>","text":"<pre><code>env = env\n</code></pre>"},{"location":"api/config/kedro.config.AbstractConfigLoader/#kedro.config.AbstractConfigLoader.runtime_params","title":"runtime_params  <code>instance-attribute</code>","text":"<pre><code>runtime_params = runtime_params or {}\n</code></pre>"},{"location":"api/config/kedro.config.AbstractConfigLoader/#kedro.config.AbstractConfigLoader.get","title":"get","text":"<pre><code>get(key, default=None)\n</code></pre> <p>D.get(k[,d]) -&gt; D[k] if k in D, else d.  d defaults to None.</p> Source code in <code>kedro/config/abstract_config.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n    \"D.get(k[,d]) -&gt; D[k] if k in D, else d.  d defaults to None.\"\n    try:\n        return self[key]\n    except KeyError:\n        return default\n</code></pre>"},{"location":"api/config/kedro.config.MissingConfigException/","title":"MissingConfigException","text":""},{"location":"api/config/kedro.config.MissingConfigException/#kedro.config.MissingConfigException","title":"kedro.config.MissingConfigException","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when no configuration files can be found within a config path</p>"},{"location":"api/config/kedro.config.OmegaConfigLoader/","title":"OmegaConfigLoader","text":""},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader","title":"kedro.config.OmegaConfigLoader","text":"<pre><code>OmegaConfigLoader(conf_source, env=None, runtime_params=None, *, config_patterns=None, base_env=None, default_run_env=None, custom_resolvers=None, merge_strategy=None)\n</code></pre> <p>               Bases: <code>AbstractConfigLoader</code></p> <p>Recursively scan directories (config paths) contained in <code>conf_source</code> for configuration files with a <code>yaml</code>, <code>yml</code> or <code>json</code> extension, load and merge them through <code>OmegaConf</code> (https://omegaconf.readthedocs.io/) and return them in the form of a config dictionary.</p> <p>The first processed config path is the <code>base</code> directory inside <code>conf_source</code>. The optional <code>env</code> argument can be used to specify a subdirectory of <code>conf_source</code> to process as a config path after <code>base</code>.</p> <p>When the same top-level key appears in any two config files located in the same (sub)directory, a <code>ValueError</code> is raised.</p> <p>When the same key appears in any two config files located in different (sub)directories, the last processed config path takes precedence and overrides this key and any sub-keys.</p> <p>You can access the different configurations as follows: ::</p> <pre><code>&gt;&gt;&gt; import logging.config\n&gt;&gt;&gt; from kedro.config import OmegaConfigLoader\n&gt;&gt;&gt; from kedro.framework.project import settings\n&gt;&gt;&gt;\n&gt;&gt;&gt; conf_path = str(project_path / settings.CONF_SOURCE)\n&gt;&gt;&gt; conf_loader = OmegaConfigLoader(conf_source=conf_path, env=\"local\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; conf_catalog = conf_loader[\"catalog\"]\n&gt;&gt;&gt; conf_params = conf_loader[\"parameters\"]\n</code></pre> <p><code>OmegaConf</code> supports variable interpolation in configuration https://omegaconf.readthedocs.io/en/2.2_branch/usage.html#merging-configurations. It is recommended to use this instead of yaml anchors with the <code>OmegaConfigLoader</code>.</p> <p>This version of the <code>OmegaConfigLoader</code> does not support any of the built-in <code>OmegaConf</code> resolvers. Support for resolvers might be added in future versions.</p> <p>To use this class, change the setting for the <code>CONFIG_LOADER_CLASS</code> constant in <code>settings.py</code>.</p> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; # in settings.py\n&gt;&gt;&gt; from kedro.config import OmegaConfigLoader\n&gt;&gt;&gt;\n&gt;&gt;&gt; CONFIG_LOADER_CLASS = OmegaConfigLoader\n</code></pre> <p>Parameters:</p> <ul> <li> <code>conf_source</code>               (<code>str</code>)           \u2013            <p>Path to use as root directory for loading configuration. This can be a local filesystem path or a remote URL with protocol (e.g., s3://, gs://, etc.)</p> </li> <li> <code>env</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Environment that will take precedence over base.</p> </li> <li> <code>runtime_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra parameters passed to a Kedro run.</p> </li> <li> <code>config_patterns</code>               (<code>dict[str, list[str]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Regex patterns that specify the naming convention for configuration files so they can be loaded. Can be customised by supplying config_patterns as in <code>CONFIG_LOADER_ARGS</code> in <code>settings.py</code>.</p> </li> <li> <code>base_env</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the base environment. When the <code>OmegaConfigLoader</code> is used directly this defaults to <code>None</code>. Otherwise, the value will come from the <code>CONFIG_LOADER_ARGS</code> in the project settings, where base_env defaults to <code>\"base\"</code>. This is used in the <code>conf_paths</code> property method to construct the configuration paths.</p> </li> <li> <code>default_run_env</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the default run environment. When the <code>OmegaConfigLoader</code> is used directly this defaults to <code>None</code>. Otherwise, the value will come from the <code>CONFIG_LOADER_ARGS</code> in the project settings, where default_run_env defaults to <code>\"local\"</code>. Can be overridden by supplying the <code>env</code> argument.</p> </li> <li> <code>custom_resolvers</code>               (<code>dict[str, Callable] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of custom resolvers to be registered. For more information, see here: https://omegaconf.readthedocs.io/en/2.3_branch/custom_resolvers.html#custom-resolvers</p> </li> <li> <code>merge_strategy</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary that specifies the merging strategy for each configuration type. The accepted merging strategies are <code>soft</code> and <code>destructive</code>. Defaults to <code>destructive</code>.</p> </li> </ul> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    conf_source: str,\n    env: str | None = None,\n    runtime_params: dict[str, Any] | None = None,\n    *,\n    config_patterns: dict[str, list[str]] | None = None,\n    base_env: str | None = None,\n    default_run_env: str | None = None,\n    custom_resolvers: dict[str, Callable] | None = None,\n    merge_strategy: dict[str, str] | None = None,\n):\n    \"\"\"Instantiates a ``OmegaConfigLoader``.\n\n    Args:\n        conf_source: Path to use as root directory for loading configuration.\n            This can be a local filesystem path or a remote URL with protocol\n            (e.g., s3://, gs://, etc.)\n        env: Environment that will take precedence over base.\n        runtime_params: Extra parameters passed to a Kedro run.\n        config_patterns: Regex patterns that specify the naming convention for configuration\n            files so they can be loaded. Can be customised by supplying config_patterns as\n            in `CONFIG_LOADER_ARGS` in `settings.py`.\n        base_env: Name of the base environment. When the ``OmegaConfigLoader`` is used directly\n            this defaults to `None`. Otherwise, the value will come from the `CONFIG_LOADER_ARGS` in the project\n            settings, where base_env defaults to `\"base\"`.\n            This is used in the `conf_paths` property method to construct\n            the configuration paths.\n        default_run_env: Name of the default run environment. When the ``OmegaConfigLoader`` is used directly\n            this defaults to `None`. Otherwise, the value will come from the `CONFIG_LOADER_ARGS` in the project\n            settings, where default_run_env defaults to `\"local\"`.\n            Can be overridden by supplying the `env` argument.\n        custom_resolvers: A dictionary of custom resolvers to be registered. For more information,\n            see here: https://omegaconf.readthedocs.io/en/2.3_branch/custom_resolvers.html#custom-resolvers\n        merge_strategy: A dictionary that specifies the merging strategy for each configuration type.\n            The accepted merging strategies are `soft` and `destructive`. Defaults to `destructive`.\n    \"\"\"\n    self.base_env = base_env or \"\"\n    self.default_run_env = default_run_env or \"\"\n    self.merge_strategy = merge_strategy or {}\n    self._globals_oc: DictConfig | None = None\n    self._runtime_params_oc: DictConfig | None = None\n    self.config_patterns = {\n        \"catalog\": [\"catalog*\", \"catalog*/**\", \"**/catalog*\"],\n        \"parameters\": [\"parameters*\", \"parameters*/**\", \"**/parameters*\"],\n        \"credentials\": [\"credentials*\", \"credentials*/**\", \"**/credentials*\"],\n        \"globals\": [\"globals.yml\"],\n    }\n    self.config_patterns.update(config_patterns or {})\n\n    # Deactivate oc.env built-in resolver for OmegaConf\n    OmegaConf.clear_resolver(\"oc.env\")\n    # Register user provided custom resolvers\n    self._custom_resolvers = custom_resolvers\n    if custom_resolvers:\n        self._register_new_resolvers(custom_resolvers)\n    # Register globals resolver\n    self._register_globals_resolver()\n\n    # Setup file system and protocol\n    self._fs, self._protocol = self._initialise_filesystem_and_protocol(conf_source)\n\n    # Store remote root path if using cloud protocol\n    if self._protocol in CLOUD_PROTOCOLS or self._protocol in HTTP_PROTOCOLS:\n        options = _parse_filepath(conf_source)\n        self._remote_root_path = options[\"path\"].rstrip(\"/\")\n\n    super().__init__(\n        conf_source=conf_source,\n        env=env,\n        runtime_params=runtime_params,\n    )\n    try:\n        self._globals = self[\"globals\"]\n    except MissingConfigException:\n        self._globals = {}\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._custom_resolvers","title":"_custom_resolvers  <code>instance-attribute</code>","text":"<pre><code>_custom_resolvers = custom_resolvers\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._globals","title":"_globals  <code>instance-attribute</code>","text":"<pre><code>_globals = self['globals']\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._globals_oc","title":"_globals_oc  <code>instance-attribute</code>","text":"<pre><code>_globals_oc = None\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._remote_root_path","title":"_remote_root_path  <code>instance-attribute</code>","text":"<pre><code>_remote_root_path = rstrip('/')\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._runtime_params_oc","title":"_runtime_params_oc  <code>instance-attribute</code>","text":"<pre><code>_runtime_params_oc = None\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader.base_env","title":"base_env  <code>instance-attribute</code>","text":"<pre><code>base_env = base_env or ''\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader.config_patterns","title":"config_patterns  <code>instance-attribute</code>","text":"<pre><code>config_patterns = {'catalog': ['catalog*', 'catalog*/**', '**/catalog*'], 'parameters': ['parameters*', 'parameters*/**', '**/parameters*'], 'credentials': ['credentials*', 'credentials*/**', '**/credentials*'], 'globals': ['globals.yml']}\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader.default_run_env","title":"default_run_env  <code>instance-attribute</code>","text":"<pre><code>default_run_env = default_run_env or ''\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader.merge_strategy","title":"merge_strategy  <code>instance-attribute</code>","text":"<pre><code>merge_strategy = merge_strategy or {}\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key)\n</code></pre> <p>Get configuration files by key, load and merge them, and return them in the form of a config dictionary.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>Key of the configuration type to fetch.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If key provided isn't present in the config_patterns of this <code>OmegaConfigLoader</code> instance.</p> </li> <li> <code>MissingConfigException</code>             \u2013            <p>If no configuration files exist matching the patterns mapped to the provided key.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>A Python dictionary with the combined configuration from all configuration files.</p> </li> </ul> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def __getitem__(self, key: str) -&gt; dict[str, Any]:  # noqa: PLR0912\n    \"\"\"Get configuration files by key, load and merge them, and\n    return them in the form of a config dictionary.\n\n    Args:\n        key: Key of the configuration type to fetch.\n\n    Raises:\n        KeyError: If key provided isn't present in the config_patterns of this\n           ``OmegaConfigLoader`` instance.\n        MissingConfigException: If no configuration files exist matching the patterns\n            mapped to the provided key.\n\n    Returns:\n        Dict[str, Any]:  A Python dictionary with the combined\n           configuration from all configuration files.\n    \"\"\"\n    # Allow bypassing of loading config from patterns if a key and value have been set\n    # explicitly on the ``OmegaConfigLoader`` instance.\n\n    # Re-register runtime params resolver incase it was previously deactivated\n    self._register_runtime_params_resolver()\n\n    if key in self:\n        return super().__getitem__(key)  # type: ignore[no-any-return]\n\n    if key not in self.config_patterns:\n        raise KeyError(\n            f\"No config patterns were found for '{key}' in your config loader\"\n        )\n    patterns = [*self.config_patterns[key]]\n\n    if key == \"globals\":\n        # \"runtime_params\" resolver is not allowed in globals.\n        OmegaConf.clear_resolver(\"runtime_params\")\n\n    read_environment_variables = key == \"credentials\"\n\n    processed_files: set[Path] = set()\n    # Load base env config\n    # Handle remote paths\n    if self._protocol in CLOUD_PROTOCOLS or self._protocol in HTTP_PROTOCOLS:\n        base_path = f\"{self._remote_root_path}/{self.base_env}\"\n    elif self._protocol == \"file\":\n        base_path = str(Path(self.conf_source) / self.base_env)\n    else:\n        base_path = str(Path(self._fs.ls(\"\", detail=False)[-1]) / self.base_env)\n    try:\n        base_config = self.load_and_merge_dir_config(  # type: ignore[no-untyped-call]\n            base_path, patterns, key, processed_files, read_environment_variables\n        )\n    except UnsupportedInterpolationType as exc:\n        if \"runtime_params\" in str(exc):\n            raise UnsupportedInterpolationType(\n                \"The `runtime_params:` resolver is not supported for globals.\"\n            )\n        else:\n            raise exc\n\n    config = base_config\n\n    # Load chosen env config\n    run_env = self.env or self.default_run_env\n\n    # Return if chosen env config is the same as base config to avoid loading the same config twice\n    if run_env == self.base_env:\n        return config  # type: ignore[no-any-return]\n\n    # Handle remote paths\n    if self._protocol in CLOUD_PROTOCOLS or self._protocol in HTTP_PROTOCOLS:\n        env_path = f\"{self._remote_root_path}/{run_env}\"\n    elif self._protocol == \"file\":\n        env_path = str(Path(self.conf_source) / run_env)\n    else:\n        env_path = str(Path(self._fs.ls(\"\", detail=False)[-1]) / run_env)\n    try:\n        env_config = self.load_and_merge_dir_config(  # type: ignore[no-untyped-call]\n            env_path, patterns, key, processed_files, read_environment_variables\n        )\n    except UnsupportedInterpolationType as exc:\n        if \"runtime_params\" in str(exc):\n            raise UnsupportedInterpolationType(\n                \"The `runtime_params:` resolver is not supported for globals.\"\n            )\n        else:\n            raise exc\n\n    resulting_config = self._merge_configs(config, env_config, key, env_path)\n\n    if not processed_files and key != \"globals\":\n        raise MissingConfigException(\n            f\"No files of YAML or JSON format found in {base_path} or {env_path} matching\"\n            f\" the glob pattern(s): {[*self.config_patterns[key]]}\"\n        )\n\n    return resulting_config  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def __repr__(self) -&gt; str:  # pragma: no cover\n    return (\n        f\"OmegaConfigLoader(conf_source={self.conf_source}, env={self.env}, \"\n        f\"runtime_params={self.runtime_params}, \"\n        f\"config_patterns={self.config_patterns}, \"\n        f\"base_env={self.base_env}), \"\n        f\"default_run_env={self.default_run_env}), \"\n        f\"custom_resolvers={self._custom_resolvers}), \"\n        f\"merge_strategy={self.merge_strategy})\"\n    )\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key, value)\n</code></pre> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def __setitem__(self, key: str, value: Any) -&gt; None:\n    if key == \"globals\":\n        # Update the cached value at self._globals since it is used by the globals resolver\n        self._globals = value\n    super().__setitem__(key, value)\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._check_duplicates","title":"_check_duplicates","text":"<pre><code>_check_duplicates(key, config_per_file)\n</code></pre> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def _check_duplicates(self, key: str, config_per_file: dict[Path, Any]) -&gt; None:\n    if key == \"parameters\":\n        seen_files_to_keys = {\n            file: self._get_all_keys(OmegaConf.to_container(config, resolve=False))\n            for file, config in config_per_file.items()\n        }\n    else:\n        seen_files_to_keys = {\n            file: set(config.keys()) for file, config in config_per_file.items()\n        }\n\n    duplicates = []\n\n    filepaths = list(seen_files_to_keys.keys())\n    for i, filepath1 in enumerate(filepaths, 1):\n        config1 = seen_files_to_keys[filepath1]\n        for filepath2 in filepaths[i:]:\n            config2 = seen_files_to_keys[filepath2]\n\n            combined_keys = config1 &amp; config2\n            overlapping_keys = {\n                key for key in combined_keys if not key.startswith(\"_\")\n            }\n\n            if overlapping_keys:\n                sorted_keys = \", \".join(sorted(overlapping_keys))\n                if len(sorted_keys) &gt; 100:  # noqa: PLR2004\n                    sorted_keys = sorted_keys[:100] + \"...\"\n                duplicates.append(\n                    f\"Duplicate keys found in {filepath1} and {filepath2}: {sorted_keys}\"\n                )\n\n    if duplicates:\n        dup_str = \"\\n\".join(duplicates)\n        raise ValueError(f\"{dup_str}\")\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._destructive_merge","title":"_destructive_merge  <code>staticmethod</code>","text":"<pre><code>_destructive_merge(config, env_config, env_path)\n</code></pre> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>@staticmethod\ndef _destructive_merge(\n    config: dict[str, Any], env_config: dict[str, Any], env_path: str\n) -&gt; dict[str, Any]:\n    # Destructively merge the two env dirs. The chosen env will override base.\n    common_keys = config.keys() &amp; env_config.keys()\n    if common_keys:\n        sorted_keys = \", \".join(sorted(common_keys))\n        msg = (\n            \"Config from path '%s' will override the following \"\n            \"existing top-level config keys: %s\"\n        )\n        _config_logger.debug(msg, env_path, sorted_keys)\n\n    config.update(env_config)\n    return config\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._get_all_keys","title":"_get_all_keys","text":"<pre><code>_get_all_keys(cfg, parent_key='')\n</code></pre> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def _get_all_keys(self, cfg: Any, parent_key: str = \"\") -&gt; set[str]:\n    keys: set[str] = set()\n\n    for key, value in cfg.items():\n        full_key = f\"{parent_key}.{key}\" if parent_key else key\n        if isinstance(value, dict):\n            keys.update(self._get_all_keys(value, full_key))\n        else:\n            keys.add(full_key)\n    return keys\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._get_globals_value","title":"_get_globals_value","text":"<pre><code>_get_globals_value(variable, default_value=_NO_VALUE)\n</code></pre> <p>Return the globals values to the resolver</p> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def _get_globals_value(self, variable: str, default_value: Any = _NO_VALUE) -&gt; Any:\n    \"\"\"Return the globals values to the resolver\"\"\"\n    if variable.startswith(\"_\"):\n        raise InterpolationResolutionError(\n            \"Keys starting with '_' are not supported for globals.\"\n        )\n\n    if not self._globals_oc:\n        self._globals_oc = OmegaConf.create(self._globals)\n\n    interpolated_value = OmegaConf.select(\n        self._globals_oc, variable, default=default_value\n    )\n    if interpolated_value != _NO_VALUE:\n        return interpolated_value\n    else:\n        raise InterpolationResolutionError(\n            f\"Globals key '{variable}' not found and no default value provided.\"\n        )\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._get_runtime_value","title":"_get_runtime_value","text":"<pre><code>_get_runtime_value(variable, default_value=_NO_VALUE)\n</code></pre> <p>Return the runtime params values to the resolver</p> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def _get_runtime_value(self, variable: str, default_value: Any = _NO_VALUE) -&gt; Any:\n    \"\"\"Return the runtime params values to the resolver\"\"\"\n    if not self._runtime_params_oc:\n        self._runtime_params_oc = OmegaConf.create(self.runtime_params)\n\n    interpolated_value = OmegaConf.select(\n        self._runtime_params_oc, variable, default=default_value\n    )\n    if interpolated_value != _NO_VALUE:\n        return interpolated_value\n    else:\n        raise InterpolationResolutionError(\n            f\"Runtime parameter '{variable}' not found and no default value provided.\"\n        )\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._initialise_filesystem_and_protocol","title":"_initialise_filesystem_and_protocol  <code>staticmethod</code>","text":"<pre><code>_initialise_filesystem_and_protocol(conf_source)\n</code></pre> <p>Set up the file system based on the file type or protocol detected in conf_source.</p> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>@staticmethod\ndef _initialise_filesystem_and_protocol(\n    conf_source: str,\n) -&gt; tuple[fsspec.AbstractFileSystem, str]:\n    \"\"\"Set up the file system based on the file type or protocol detected in conf_source.\"\"\"\n    # Force string for regex\n    conf_source = str(conf_source)\n    # Check if it's an archive file\n    file_mimetype, _ = mimetypes.guess_type(conf_source)\n    if file_mimetype == \"application/x-tar\":\n        return fsspec.filesystem(protocol=\"tar\", fo=conf_source), \"tar\"\n    elif file_mimetype in (\n        \"application/zip\",\n        \"application/x-zip-compressed\",\n        \"application/zip-compressed\",\n    ):\n        return fsspec.filesystem(protocol=\"zip\", fo=conf_source), \"zip\"\n\n    # Parse to check for protocol\n    options = _parse_filepath(conf_source)\n    protocol = options[\"protocol\"]\n\n    # Create and return the appropriate filesystem\n    if protocol in HTTP_PROTOCOLS or protocol in CLOUD_PROTOCOLS:\n        # For HTTP and cloud storage protocols, create the appropriate filesystem\n        return fsspec.filesystem(protocol=protocol), protocol\n    else:\n        # Default to local filesystem\n        return fsspec.filesystem(protocol=\"file\", fo=conf_source), \"file\"\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._is_hidden","title":"_is_hidden","text":"<pre><code>_is_hidden(path_str)\n</code></pre> <p>Check if path contains any hidden directory or is a hidden file</p> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def _is_hidden(self, path_str: str) -&gt; bool:\n    \"\"\"Check if path contains any hidden directory or is a hidden file\"\"\"\n    path = Path(path_str)\n    conf_path = Path(self.conf_source).resolve().as_posix()\n    if self._protocol == \"file\":\n        path = path.resolve()\n    posix_path = path.as_posix()\n    if posix_path.startswith(conf_path):\n        posix_path = posix_path.replace(conf_path, \"\")\n    parts = posix_path.split(self._fs.sep)  # filesystem specific separator\n    HIDDEN = \".\"\n    # Check if any component (folder or file) starts with a dot (.)\n    return any(part.startswith(HIDDEN) for part in parts)\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._is_valid_config_path","title":"_is_valid_config_path","text":"<pre><code>_is_valid_config_path(path)\n</code></pre> <p>Check if given path is a file path and file type is yaml or json.</p> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def _is_valid_config_path(self, path: Path) -&gt; bool:\n    \"\"\"Check if given path is a file path and file type is yaml or json.\"\"\"\n    posix_path = path.as_posix()\n    return self._fs.isfile(str(posix_path)) and path.suffix in [\n        \".yml\",\n        \".yaml\",\n        \".json\",\n    ]\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._merge_configs","title":"_merge_configs","text":"<pre><code>_merge_configs(config, env_config, key, env_path)\n</code></pre> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def _merge_configs(\n    self,\n    config: dict[str, Any],\n    env_config: dict[str, Any],\n    key: str,\n    env_path: str,\n) -&gt; Any:\n    merging_strategy = self.merge_strategy.get(key, \"destructive\")\n    try:\n        strategy = MergeStrategies[merging_strategy.upper()]\n\n        # Get the corresponding merge function and call it\n        merge_function_name = MERGING_IMPLEMENTATIONS[strategy]\n        merge_function = getattr(self, merge_function_name)\n        return merge_function(config, env_config, env_path)\n    except KeyError:\n        allowed_strategies = [strategy.name.lower() for strategy in MergeStrategies]\n        raise ValueError(\n            f\"Merging strategy {merging_strategy} not supported. The accepted merging \"\n            f\"strategies are {allowed_strategies}.\"\n        )\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._register_globals_resolver","title":"_register_globals_resolver","text":"<pre><code>_register_globals_resolver()\n</code></pre> <p>Register the globals resolver</p> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def _register_globals_resolver(self) -&gt; None:\n    \"\"\"Register the globals resolver\"\"\"\n    OmegaConf.register_new_resolver(\n        \"globals\",\n        self._get_globals_value,\n        replace=True,\n    )\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._register_new_resolvers","title":"_register_new_resolvers  <code>staticmethod</code>","text":"<pre><code>_register_new_resolvers(resolvers)\n</code></pre> <p>Register custom resolvers</p> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>@staticmethod\ndef _register_new_resolvers(resolvers: dict[str, Callable]) -&gt; None:\n    \"\"\"Register custom resolvers\"\"\"\n    for name, resolver in resolvers.items():\n        if not OmegaConf.has_resolver(name):\n            msg = f\"Registering new custom resolver: {name}\"\n            _config_logger.debug(msg)\n            OmegaConf.register_new_resolver(name=name, resolver=resolver)\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._register_runtime_params_resolver","title":"_register_runtime_params_resolver","text":"<pre><code>_register_runtime_params_resolver()\n</code></pre> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def _register_runtime_params_resolver(self) -&gt; None:\n    OmegaConf.register_new_resolver(\n        \"runtime_params\",\n        self._get_runtime_value,\n        replace=True,\n    )\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._resolve_environment_variables","title":"_resolve_environment_variables  <code>staticmethod</code>","text":"<pre><code>_resolve_environment_variables(config)\n</code></pre> <p>Use the <code>oc.env</code> resolver to read environment variables and replace them in-place, clearing the resolver after the operation is complete if it was not registered beforehand.</p> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>@staticmethod\ndef _resolve_environment_variables(config: DictConfig) -&gt; None:\n    \"\"\"Use the ``oc.env`` resolver to read environment variables and replace\n    them in-place, clearing the resolver after the operation is complete if\n    it was not registered beforehand.\n\n    Arguments:\n        config {Dict[str, Any]} -- The configuration dictionary to resolve.\n    \"\"\"\n    if not OmegaConf.has_resolver(\"oc.env\"):\n        OmegaConf.register_new_resolver(\"oc.env\", oc.env)\n        OmegaConf.resolve(config)\n        OmegaConf.clear_resolver(\"oc.env\")\n    else:\n        OmegaConf.resolve(config)\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader._soft_merge","title":"_soft_merge  <code>staticmethod</code>","text":"<pre><code>_soft_merge(config, env_config, env_path=None)\n</code></pre> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>@staticmethod\ndef _soft_merge(\n    config: dict[str, Any], env_config: dict[str, Any], env_path: str | None = None\n) -&gt; Any:\n    # Soft merge the two env dirs. The chosen env will override base if keys clash.\n    return OmegaConf.to_container(OmegaConf.merge(config, env_config))\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader.keys","title":"keys","text":"<pre><code>keys()\n</code></pre> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>def keys(self) -&gt; KeysView:\n    return KeysView(self.config_patterns)\n</code></pre>"},{"location":"api/config/kedro.config.OmegaConfigLoader/#kedro.config.OmegaConfigLoader.load_and_merge_dir_config","title":"load_and_merge_dir_config","text":"<pre><code>load_and_merge_dir_config(conf_path, patterns, key, processed_files, read_environment_variables=False)\n</code></pre> <p>Recursively load and merge all configuration files in a directory using OmegaConf, which satisfy a given list of glob patterns from a specific path.</p> <p>Parameters:</p> <ul> <li> <code>conf_path</code>               (<code>str</code>)           \u2013            <p>Path to configuration directory.</p> </li> <li> <code>patterns</code>               (<code>Iterable[str]</code>)           \u2013            <p>List of glob patterns to match the filenames against.</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>Key of the configuration type to fetch.</p> </li> <li> <code>processed_files</code>               (<code>set</code>)           \u2013            <p>Set of files read for a given configuration type.</p> </li> <li> <code>read_environment_variables</code>               (<code>bool | None</code>, default:                   <code>False</code> )           \u2013            <p>Whether to resolve environment variables.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MissingConfigException</code>             \u2013            <p>If configuration path doesn't exist or isn't valid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If two or more configuration files contain the same key(s).</p> </li> <li> <code>ParserError</code>             \u2013            <p>If config file contains invalid YAML or JSON syntax.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Resulting configuration dictionary.</p> </li> </ul> Source code in <code>kedro/config/omegaconf_config.py</code> <pre><code>@typing.no_type_check\ndef load_and_merge_dir_config(\n    self,\n    conf_path: str,\n    patterns: Iterable[str],\n    key: str,\n    processed_files: set,\n    read_environment_variables: bool | None = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Recursively load and merge all configuration files in a directory using OmegaConf,\n    which satisfy a given list of glob patterns from a specific path.\n\n    Args:\n        conf_path: Path to configuration directory.\n        patterns: List of glob patterns to match the filenames against.\n        key: Key of the configuration type to fetch.\n        processed_files: Set of files read for a given configuration type.\n        read_environment_variables: Whether to resolve environment variables.\n\n    Raises:\n        MissingConfigException: If configuration path doesn't exist or isn't valid.\n        ValueError: If two or more configuration files contain the same key(s).\n        ParserError: If config file contains invalid YAML or JSON syntax.\n\n    Returns:\n        Resulting configuration dictionary.\n\n    \"\"\"\n    # Handle directory existence check for remote paths\n    if self._protocol in CLOUD_PROTOCOLS or self._protocol in HTTP_PROTOCOLS:\n        try:\n            # Check directory existence in remote paths\n            self._fs.ls(conf_path)\n        except Exception as exc:\n            raise MissingConfigException(\n                f\"Given configuration path either does not exist \"\n                f\"or is not a valid directory: {conf_path}. Error: {exc!s}\"\n            )\n    # Original check for local paths\n    elif not self._fs.isdir(Path(conf_path).as_posix()):\n        raise MissingConfigException(\n            f\"Given configuration path either does not exist \"\n            f\"or is not a valid directory: {conf_path}\"\n        )\n\n    paths = []\n    for pattern in patterns:\n        for each in self._fs.glob(Path(f\"{conf_path!s}/{pattern}\").as_posix()):\n            if not self._is_hidden(each):\n                paths.append(Path(each))\n\n    deduplicated_paths = set(paths)\n    config_files_filtered = [\n        path for path in deduplicated_paths if self._is_valid_config_path(path)\n    ]\n\n    config_per_file = {}\n    for config_filepath in config_files_filtered:\n        try:\n            with self._fs.open(str(config_filepath.as_posix())) as open_config:\n                # As fsspec doesn't allow the file to be read as StringIO,\n                # this is a workaround to read it as a binary file and decode it back to utf8.\n                tmp_fo = io.StringIO(open_config.read().decode(\"utf8\"))\n                config = OmegaConf.load(tmp_fo)\n                processed_files.add(config_filepath)\n            if read_environment_variables:\n                self._resolve_environment_variables(config)\n            config_per_file[config_filepath] = config\n        except (ParserError, ScannerError) as exc:\n            line = exc.problem_mark.line\n            cursor = exc.problem_mark.column\n            raise ParserError(\n                f\"Invalid YAML or JSON file {Path(config_filepath).as_posix()},\"\n                f\" unable to read line {line}, position {cursor}.\"\n            ) from exc\n\n    aggregate_config = config_per_file.values()\n    self._check_duplicates(key, config_per_file)\n\n    if not aggregate_config:\n        return {}\n\n    if key == \"parameters\":\n        # Merge with runtime parameters only for \"parameters\"\n        return OmegaConf.to_container(\n            OmegaConf.merge(*aggregate_config, self.runtime_params), resolve=True\n        )\n\n    merged_config_container = OmegaConf.to_container(\n        OmegaConf.merge(*aggregate_config), resolve=True\n    )\n    return {\n        k: v for k, v in merged_config_container.items() if not k.startswith(\"_\")\n    }\n</code></pre>"},{"location":"api/config/kedro.config/","title":"kedro.config","text":"Name Type Description <code>kedro.config.AbstractConfigLoader</code> Class Abstract base class for config loaders. <code>kedro.config.OmegaConfigLoader</code> Class Config loader that uses OmegaConf. <code>kedro.config.MissingConfigException</code> Exception Raised when a required config is missing."},{"location":"api/config/kedro.config/#kedro.config","title":"kedro.config","text":"<p><code>kedro.config</code> provides functionality for loading Kedro configuration from different file formats.</p>"},{"location":"api/framework/kedro.framework.cli/","title":"CLI","text":"Submodule Description <code>kedro.framework.cli.catalog</code> A collection of CLI commands for working with Kedro catalog. <code>kedro.framework.cli.cli</code> <code>kedro</code> is a CLI for managing Kedro projects. <code>kedro.framework.cli.hooks</code> Provides primitives to use hooks to extend KedroCLI's behavior. <code>kedro.framework.cli.jupyter</code> A collection of helper functions to integrate with Jupyter/IPython. <code>kedro.framework.cli.micropkg</code> A collection of CLI commands for working with Kedro micro-packages. <code>kedro.framework.cli.pipeline</code> A collection of CLI commands for working with Kedro pipelines. <code>kedro.framework.cli.project</code> A collection of CLI commands for working with Kedro projects. <code>kedro.framework.cli.registry</code> A collection of CLI commands for working with registered Kedro pipelines. <code>kedro.framework.cli.starters</code> <code>kedro</code> is a CLI for managing Kedro projects. <code>kedro.framework.cli.utils</code> Utilities for use with click."},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli","title":"kedro.framework.cli","text":"<p><code>kedro.framework.cli</code> implements commands available from Kedro's CLI.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog","title":"kedro.framework.cli.catalog","text":"<p>A collection of CLI commands for working with Kedro catalog.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.pipelines","title":"pipelines  <code>module-attribute</code>","text":"<pre><code>pipelines = _ProjectPipelines()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.settings","title":"settings  <code>module-attribute</code>","text":"<pre><code>settings = _ProjectSettings()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.AbstractDataset","title":"AbstractDataset","text":"<p>               Bases: <code>ABC</code>, <code>Generic[_DI, _DO]</code></p> <p><code>AbstractDataset</code> is the base class for all dataset implementations.</p> <p>All dataset implementations should extend this abstract class and implement the methods marked as abstract. If a specific dataset implementation cannot be used in conjunction with the <code>ParallelRunner</code>, such user-defined dataset should have the attribute <code>_SINGLE_PROCESS = True</code>. Example: ::</p> <pre><code>&gt;&gt;&gt; from pathlib import Path, PurePosixPath\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from kedro.io import AbstractDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; class MyOwnDataset(AbstractDataset[pd.DataFrame, pd.DataFrame]):\n&gt;&gt;&gt;     def __init__(self, filepath, param1, param2=True):\n&gt;&gt;&gt;         self._filepath = PurePosixPath(filepath)\n&gt;&gt;&gt;         self._param1 = param1\n&gt;&gt;&gt;         self._param2 = param2\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def load(self) -&gt; pd.DataFrame:\n&gt;&gt;&gt;         return pd.read_csv(self._filepath)\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def save(self, df: pd.DataFrame) -&gt; None:\n&gt;&gt;&gt;         df.to_csv(str(self._filepath))\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _exists(self) -&gt; bool:\n&gt;&gt;&gt;         return Path(self._filepath.as_posix()).exists()\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _describe(self):\n&gt;&gt;&gt;         return dict(param1=self._param1, param2=self._param2)\n</code></pre> <p>Example catalog.yml specification: ::</p> <pre><code>my_dataset:\n    type: &lt;path-to-my-own-dataset&gt;.MyOwnDataset\n    filepath: data/01_raw/my_data.csv\n    param1: &lt;param1-value&gt; # param1 is a required argument\n    # param2 will be True by default\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.AbstractDataset.__init_subclass__","title":"__init_subclass__","text":"<pre><code>__init_subclass__(**kwargs)\n</code></pre> <p>Customizes the behavior of subclasses of AbstractDataset during their creation. This method is automatically invoked when a subclass of AbstractDataset is defined.</p> <p>Decorates the <code>load</code> and <code>save</code> methods provided by the class. If <code>_load</code> or <code>_save</code> are defined, alias them as a prerequisite.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def __init_subclass__(cls, **kwargs: Any) -&gt; None:\n    \"\"\"Customizes the behavior of subclasses of AbstractDataset during\n    their creation. This method is automatically invoked when a subclass\n    of AbstractDataset is defined.\n\n    Decorates the `load` and `save` methods provided by the class.\n    If `_load` or `_save` are defined, alias them as a prerequisite.\n    \"\"\"\n\n    # Save the original __init__ method of the subclass\n    init_func: Callable = cls.__init__\n\n    @wraps(init_func)\n    def new_init(self, *args, **kwargs) -&gt; None:  # type: ignore[no-untyped-def]\n        \"\"\"Executes the original __init__, then save the arguments used\n        to initialize the instance.\n        \"\"\"\n        # Call the original __init__ method\n        init_func(self, *args, **kwargs)\n        # Capture and save the arguments passed to the original __init__\n        self._init_args = getcallargs(init_func, self, *args, **kwargs)\n\n    # Replace the subclass's __init__ with the new_init\n    # A hook for subclasses to capture initialization arguments and save them\n    # in the AbstractDataset._init_args field\n    cls.__init__ = new_init  # type: ignore[method-assign]\n\n    super().__init_subclass__(**kwargs)\n\n    if hasattr(cls, \"_load\") and not cls._load.__qualname__.startswith(\"Abstract\"):\n        cls.load = cls._load  # type: ignore[method-assign]\n\n    if hasattr(cls, \"_save\") and not cls._save.__qualname__.startswith(\"Abstract\"):\n        cls.save = cls._save  # type: ignore[method-assign]\n\n    if hasattr(cls, \"load\") and not cls.load.__qualname__.startswith(\"Abstract\"):\n        cls.load = cls._load_wrapper(  # type: ignore[assignment]\n            cls.load\n            if not getattr(cls.load, \"__loadwrapped__\", False)\n            else cls.load.__wrapped__  # type: ignore[attr-defined]\n        )\n\n    if hasattr(cls, \"save\") and not cls.save.__qualname__.startswith(\"Abstract\"):\n        cls.save = cls._save_wrapper(  # type: ignore[assignment]\n            cls.save\n            if not getattr(cls.save, \"__savewrapped__\", False)\n            else cls.save.__wrapped__  # type: ignore[attr-defined]\n        )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.AbstractDataset.exists","title":"exists","text":"<pre><code>exists()\n</code></pre> <p>Checks whether a dataset's output already exists by calling the provided _exists() method.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>Flag indicating whether the output already exists.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>when underlying exists method raises error.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Checks whether a dataset's output already exists by calling\n    the provided _exists() method.\n\n    Returns:\n        Flag indicating whether the output already exists.\n\n    Raises:\n        DatasetError: when underlying exists method raises error.\n\n    \"\"\"\n    try:\n        self._logger.debug(\"Checking whether target of %s exists\", str(self))\n        return self._exists()\n    except Exception as exc:\n        message = f\"Failed during exists check for dataset {self!s}.\\n{exc!s}\"\n        raise DatasetError(message) from exc\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.AbstractDataset.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(name, config, load_version=None, save_version=None)\n</code></pre> <p>Create a dataset instance using the configuration provided.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Data set name.</p> </li> <li> <code>config</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Data set config dictionary.</p> </li> <li> <code>load_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Version string to be used for <code>load</code> operation if the dataset is versioned. Has no effect on the dataset if versioning was not enabled.</p> </li> <li> <code>save_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Version string to be used for <code>save</code> operation if the dataset is versioned. Has no effect on the dataset if versioning was not enabled.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractDataset</code>           \u2013            <p>An instance of an <code>AbstractDataset</code> subclass.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>When the function fails to create the dataset from its config.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>@classmethod\ndef from_config(\n    cls: type,\n    name: str,\n    config: dict[str, Any],\n    load_version: str | None = None,\n    save_version: str | None = None,\n) -&gt; AbstractDataset:\n    \"\"\"Create a dataset instance using the configuration provided.\n\n    Args:\n        name: Data set name.\n        config: Data set config dictionary.\n        load_version: Version string to be used for ``load`` operation if\n            the dataset is versioned. Has no effect on the dataset\n            if versioning was not enabled.\n        save_version: Version string to be used for ``save`` operation if\n            the dataset is versioned. Has no effect on the dataset\n            if versioning was not enabled.\n\n    Returns:\n        An instance of an ``AbstractDataset`` subclass.\n\n    Raises:\n        DatasetError: When the function fails to create the dataset\n            from its config.\n\n    \"\"\"\n    try:\n        class_obj, config = parse_dataset_definition(\n            config, load_version, save_version\n        )\n    except Exception as exc:\n        raise DatasetError(\n            f\"An exception occurred when parsing config \"\n            f\"for dataset '{name}':\\n{exc!s}\"\n        ) from exc\n\n    try:\n        dataset = class_obj(**config)\n    except TypeError as err:\n        raise DatasetError(\n            f\"\\n{err}.\\nDataset '{name}' must only contain arguments valid for the \"\n            f\"constructor of '{class_obj.__module__}.{class_obj.__qualname__}'.\"\n        ) from err\n    except Exception as err:\n        raise DatasetError(\n            f\"\\n{err}.\\nFailed to instantiate dataset '{name}' \"\n            f\"of type '{class_obj.__module__}.{class_obj.__qualname__}'.\"\n        ) from err\n    return dataset\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.AbstractDataset.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load()\n</code></pre> <p>Loads data by delegation to the provided load method.</p> <p>Returns:</p> <ul> <li> <code>_DO</code>           \u2013            <p>Data returned by the provided load method.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>When underlying load method raises error.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>@abc.abstractmethod\ndef load(self) -&gt; _DO:\n    \"\"\"Loads data by delegation to the provided load method.\n\n    Returns:\n        Data returned by the provided load method.\n\n    Raises:\n        DatasetError: When underlying load method raises error.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"'{self.__class__.__name__}' is a subclass of AbstractDataset and \"\n        f\"it must implement the 'load' method\"\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.AbstractDataset.release","title":"release","text":"<pre><code>release()\n</code></pre> <p>Release any cached data.</p> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>when underlying release method raises error.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>def release(self) -&gt; None:\n    \"\"\"Release any cached data.\n\n    Raises:\n        DatasetError: when underlying release method raises error.\n\n    \"\"\"\n    try:\n        self._logger.debug(\"Releasing %s\", str(self))\n        self._release()\n    except Exception as exc:\n        message = f\"Failed during release for dataset {self!s}.\\n{exc!s}\"\n        raise DatasetError(message) from exc\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.AbstractDataset.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(data)\n</code></pre> <p>Saves data by delegation to the provided save method.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>_DI</code>)           \u2013            <p>the value to be saved by provided save method.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>when underlying save method raises error.</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>when save method got file instead of dir, on Windows.</p> </li> <li> <code>NotADirectoryError</code>             \u2013            <p>when save method got file instead of dir, on Unix.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>@abc.abstractmethod\ndef save(self, data: _DI) -&gt; None:\n    \"\"\"Saves data by delegation to the provided save method.\n\n    Args:\n        data: the value to be saved by provided save method.\n\n    Raises:\n        DatasetError: when underlying save method raises error.\n        FileNotFoundError: when save method got file instead of dir, on Windows.\n        NotADirectoryError: when save method got file instead of dir, on Unix.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"'{self.__class__.__name__}' is a subclass of AbstractDataset and \"\n        f\"it must implement the 'save' method\"\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.AbstractDataset.to_config","title":"to_config","text":"<pre><code>to_config()\n</code></pre> <p>Converts the dataset instance into a dictionary-based configuration for serialization. Ensures that any subclass-specific details are handled, with additional logic for versioning and caching implemented for <code>CachedDataset</code>.</p> <p>Adds a key for the dataset's type using its module and class name and includes the initialization arguments.</p> <p>For <code>CachedDataset</code> it extracts the underlying dataset's configuration, handles the <code>versioned</code> flag and removes unnecessary metadata. It also ensures the embedded dataset's configuration is appropriately flattened or transformed.</p> <p>If the dataset has a version key, it sets the <code>versioned</code> flag in the configuration.</p> <p>Removes the <code>metadata</code> key from the configuration if present.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>A dictionary containing the dataset's type and initialization arguments.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>def to_config(self) -&gt; dict[str, Any]:\n    \"\"\"Converts the dataset instance into a dictionary-based configuration for\n    serialization. Ensures that any subclass-specific details are handled, with\n    additional logic for versioning and caching implemented for `CachedDataset`.\n\n    Adds a key for the dataset's type using its module and class name and\n    includes the initialization arguments.\n\n    For `CachedDataset` it extracts the underlying dataset's configuration,\n    handles the `versioned` flag and removes unnecessary metadata. It also\n    ensures the embedded dataset's configuration is appropriately flattened\n    or transformed.\n\n    If the dataset has a version key, it sets the `versioned` flag in the\n    configuration.\n\n    Removes the `metadata` key from the configuration if present.\n\n    Returns:\n        A dictionary containing the dataset's type and initialization arguments.\n    \"\"\"\n    return_config: dict[str, Any] = {\n        f\"{TYPE_KEY}\": f\"{type(self).__module__}.{type(self).__name__}\"\n    }\n\n    if self._init_args:  # type: ignore[attr-defined]\n        self._init_args.pop(\"self\", None)  # type: ignore[attr-defined]\n        return_config.update(self._init_args)  # type: ignore[attr-defined]\n\n    if type(self).__name__ == \"CachedDataset\":\n        cached_ds = return_config.pop(\"dataset\")\n        cached_ds_return_config: dict[str, Any] = {}\n        if isinstance(cached_ds, dict):\n            cached_ds_return_config = cached_ds\n        elif isinstance(cached_ds, AbstractDataset):\n            cached_ds_return_config = cached_ds.to_config()\n        if VERSIONED_FLAG_KEY in cached_ds_return_config:\n            return_config[VERSIONED_FLAG_KEY] = cached_ds_return_config.pop(\n                VERSIONED_FLAG_KEY\n            )\n        # Pop metadata from configuration\n        cached_ds_return_config.pop(\"metadata\", None)\n        return_config[\"dataset\"] = cached_ds_return_config\n\n    # Set `versioned` key if version present in the dataset\n    if return_config.pop(VERSION_KEY, None):\n        return_config[VERSIONED_FLAG_KEY] = True\n\n    # Pop metadata from configuration\n    return_config.pop(\"metadata\", None)\n\n    return return_config\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog","title":"DataCatalog","text":"<pre><code>DataCatalog(datasets=None, feed_dict=None, dataset_patterns=None, load_versions=None, save_version=None, default_pattern=None, config_resolver=None)\n</code></pre> <p><code>DataCatalog</code> stores instances of <code>AbstractDataset</code> implementations to provide <code>load</code> and <code>save</code> capabilities from anywhere in the program. To use a <code>DataCatalog</code>, you need to instantiate it with a dictionary of datasets. Then it will act as a single point of reference for your calls, relaying load and save functions to the underlying datasets.</p> <p>anywhere in the program. To use a <code>DataCatalog</code>, you need to instantiate it with a dictionary of datasets. Then it will act as a single point of reference for your calls, relaying load and save functions to the underlying datasets.</p> <p>Parameters:</p> <ul> <li> <code>datasets</code>               (<code>dict[str, AbstractDataset] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of dataset names and dataset instances.</p> </li> <li> <code>feed_dict</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>A feed dict with data to be added in memory.</p> </li> <li> <code>dataset_patterns</code>               (<code>Patterns | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of dataset factory patterns and corresponding dataset configuration. When fetched from catalog configuration these patterns will be sorted by: 1. Decreasing specificity (number of characters outside the curly brackets) 2. Decreasing number of placeholders (number of curly bracket pairs) 3. Alphabetically A pattern of specificity 0 is a catch-all pattern and will overwrite the default pattern provided through the runners if it comes before \"default\" in the alphabet. Such an overwriting pattern will emit a warning. The <code>\"{default}\"</code> name will not emit a warning.</p> </li> <li> <code>load_versions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A mapping between dataset names and versions to load. Has no effect on datasets without enabled versioning.</p> </li> <li> <code>save_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Version string to be used for <code>save</code> operations by all datasets with enabled versioning. It must: a) be a case-insensitive string that conforms with operating system filename limitations, b) always return the latest version when sorted in lexicographical order.</p> </li> <li> <code>default_pattern</code>               (<code>Patterns | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of the default catch-all pattern that overrides the default pattern provided through the runners.</p> </li> <li> <code>config_resolver</code>               (<code>CatalogConfigResolver | None</code>, default:                   <code>None</code> )           \u2013            <p>An instance of CatalogConfigResolver to resolve dataset patterns and configurations.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt; cars = CSVDataset(filepath=\"cars.csv\",\n&gt;&gt;&gt;                   load_args=None,\n&gt;&gt;&gt;                   save_args={\"index\": False})\n&gt;&gt;&gt; catalog = DataCatalog(datasets={'cars': cars})\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    datasets: dict[str, AbstractDataset] | None = None,\n    feed_dict: dict[str, Any] | None = None,\n    dataset_patterns: Patterns | None = None,  # Kept for interface compatibility\n    load_versions: dict[str, str] | None = None,\n    save_version: str | None = None,\n    default_pattern: Patterns | None = None,  # Kept for interface compatibility\n    config_resolver: CatalogConfigResolver | None = None,\n) -&gt; None:\n    \"\"\"``DataCatalog`` stores instances of ``AbstractDataset``\n    implementations to provide ``load`` and ``save`` capabilities from\n    anywhere in the program. To use a ``DataCatalog``, you need to\n    instantiate it with a dictionary of datasets. Then it will act as a\n    single point of reference for your calls, relaying load and save\n    functions to the underlying datasets.\n\n    Args:\n        datasets: A dictionary of dataset names and dataset instances.\n        feed_dict: A feed dict with data to be added in memory.\n        dataset_patterns: A dictionary of dataset factory patterns\n            and corresponding dataset configuration. When fetched from catalog configuration\n            these patterns will be sorted by:\n            1. Decreasing specificity (number of characters outside the curly brackets)\n            2. Decreasing number of placeholders (number of curly bracket pairs)\n            3. Alphabetically\n            A pattern of specificity 0 is a catch-all pattern and will overwrite the default\n            pattern provided through the runners if it comes before \"default\" in the alphabet.\n            Such an overwriting pattern will emit a warning. The `\"{default}\"` name will\n            not emit a warning.\n        load_versions: A mapping between dataset names and versions\n            to load. Has no effect on datasets without enabled versioning.\n        save_version: Version string to be used for ``save`` operations\n            by all datasets with enabled versioning. It must: a) be a\n            case-insensitive string that conforms with operating system\n            filename limitations, b) always return the latest version when\n            sorted in lexicographical order.\n        default_pattern: A dictionary of the default catch-all pattern that overrides the default\n            pattern provided through the runners.\n        config_resolver: An instance of CatalogConfigResolver to resolve dataset patterns and configurations.\n\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; cars = CSVDataset(filepath=\"cars.csv\",\n        &gt;&gt;&gt;                   load_args=None,\n        &gt;&gt;&gt;                   save_args={\"index\": False})\n        &gt;&gt;&gt; catalog = DataCatalog(datasets={'cars': cars})\n    \"\"\"\n    warnings.warn(\n        \"`DataCatalog` has been deprecated and will be replaced by `KedroDataCatalog`, in Kedro 1.0.0.\"\n        \"Currently some `KedroDataCatalog` APIs have been retained for compatibility with `DataCatalog`, including \"\n        \"the `datasets` property and the `get_datasets`, `_get_datasets`, `add`,` list`, `add_feed_dict`, \"\n        \"and `shallow_copy` methods. These will be removed or replaced with updated alternatives in Kedro 1.0.0. \"\n        \"For more details, refer to the documentation: \"\n        \"https://docs.kedro.org/en/stable/data/index.html#kedrodatacatalog-experimental-feature\",\n        KedroDeprecationWarning,\n    )\n    self._config_resolver = config_resolver or CatalogConfigResolver()\n    # Kept to avoid breaking changes\n    if not config_resolver:\n        self._config_resolver._dataset_patterns = dataset_patterns or {}\n        self._config_resolver._default_pattern = default_pattern or {}\n\n    self._load_versions, self._save_version = _validate_versions(\n        datasets, load_versions or {}, save_version\n    )\n\n    self._datasets: dict[str, AbstractDataset] = {}\n    self.datasets: _FrozenDatasets | None = None\n\n    self.add_all(datasets or {})\n\n    self._use_rich_markup = _has_rich_handler()\n\n    if feed_dict:\n        self.add_feed_dict(feed_dict)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog.__contains__","title":"__contains__","text":"<pre><code>__contains__(dataset_name)\n</code></pre> <p>Check if an item is in the catalog as a materialised dataset or pattern</p> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def __contains__(self, dataset_name: str) -&gt; bool:\n    \"\"\"Check if an item is in the catalog as a materialised dataset or pattern\"\"\"\n    return (\n        dataset_name in self._datasets\n        or self._config_resolver.match_pattern(dataset_name) is not None\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog.add","title":"add","text":"<pre><code>add(dataset_name, dataset, replace=False)\n</code></pre> <p>Adds a new <code>AbstractDataset</code> object to the <code>DataCatalog</code>.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>A unique dataset name which has not been registered yet.</p> </li> <li> <code>dataset</code>               (<code>AbstractDataset</code>)           \u2013            <p>A dataset object to be associated with the given data set name.</p> </li> <li> <code>replace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Specifies whether to replace an existing dataset with the same name is allowed.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetAlreadyExistsError</code>             \u2013            <p>When a dataset with the same name has already been registered.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt; catalog = DataCatalog(datasets={\n&gt;&gt;&gt;                   'cars': CSVDataset(filepath=\"cars.csv\")\n&gt;&gt;&gt;                  })\n&gt;&gt;&gt;\n&gt;&gt;&gt; catalog.add(\"boats\", CSVDataset(filepath=\"boats.csv\"))\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def add(\n    self,\n    dataset_name: str,\n    dataset: AbstractDataset,\n    replace: bool = False,\n) -&gt; None:\n    \"\"\"Adds a new ``AbstractDataset`` object to the ``DataCatalog``.\n\n    Args:\n        dataset_name: A unique dataset name which has not been\n            registered yet.\n        dataset: A dataset object to be associated with the given data\n            set name.\n        replace: Specifies whether to replace an existing dataset\n            with the same name is allowed.\n\n    Raises:\n        DatasetAlreadyExistsError: When a dataset with the same name\n            has already been registered.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; catalog = DataCatalog(datasets={\n        &gt;&gt;&gt;                   'cars': CSVDataset(filepath=\"cars.csv\")\n        &gt;&gt;&gt;                  })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; catalog.add(\"boats\", CSVDataset(filepath=\"boats.csv\"))\n    \"\"\"\n    if dataset_name in self._datasets:\n        if replace:\n            self._logger.warning(\"Replacing dataset '%s'\", dataset_name)\n        else:\n            raise DatasetAlreadyExistsError(\n                f\"Dataset '{dataset_name}' has already been registered\"\n            )\n    self._load_versions, self._save_version = _validate_versions(\n        {dataset_name: dataset}, self._load_versions, self._save_version\n    )\n    self._datasets[dataset_name] = dataset\n    self.datasets = _FrozenDatasets(self.datasets, {dataset_name: dataset})\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog.add_all","title":"add_all","text":"<pre><code>add_all(datasets, replace=False)\n</code></pre> <p>Adds a group of new datasets to the <code>DataCatalog</code>.</p> <p>Parameters:</p> <ul> <li> <code>datasets</code>               (<code>dict[str, AbstractDataset]</code>)           \u2013            <p>A dictionary of dataset names and dataset instances.</p> </li> <li> <code>replace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Specifies whether to replace an existing dataset with the same name is allowed.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetAlreadyExistsError</code>             \u2013            <p>When a dataset with the same name has already been registered.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset, ParquetDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt; catalog = DataCatalog(datasets={\n&gt;&gt;&gt;                   \"cars\": CSVDataset(filepath=\"cars.csv\")\n&gt;&gt;&gt;                  })\n&gt;&gt;&gt; additional = {\n&gt;&gt;&gt;     \"planes\": ParquetDataset(\"planes.parq\"),\n&gt;&gt;&gt;     \"boats\": CSVDataset(filepath=\"boats.csv\")\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; catalog.add_all(additional)\n&gt;&gt;&gt;\n&gt;&gt;&gt; assert catalog.list() == [\"cars\", \"planes\", \"boats\"]\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def add_all(\n    self,\n    datasets: dict[str, AbstractDataset],\n    replace: bool = False,\n) -&gt; None:\n    \"\"\"Adds a group of new datasets to the ``DataCatalog``.\n\n    Args:\n        datasets: A dictionary of dataset names and dataset\n            instances.\n        replace: Specifies whether to replace an existing dataset\n            with the same name is allowed.\n\n    Raises:\n        DatasetAlreadyExistsError: When a dataset with the same name\n            has already been registered.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset, ParquetDataset\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; catalog = DataCatalog(datasets={\n        &gt;&gt;&gt;                   \"cars\": CSVDataset(filepath=\"cars.csv\")\n        &gt;&gt;&gt;                  })\n        &gt;&gt;&gt; additional = {\n        &gt;&gt;&gt;     \"planes\": ParquetDataset(\"planes.parq\"),\n        &gt;&gt;&gt;     \"boats\": CSVDataset(filepath=\"boats.csv\")\n        &gt;&gt;&gt; }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; catalog.add_all(additional)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; assert catalog.list() == [\"cars\", \"planes\", \"boats\"]\n    \"\"\"\n    for ds_name, ds in datasets.items():\n        self.add(ds_name, ds, replace)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog.add_feed_dict","title":"add_feed_dict","text":"<pre><code>add_feed_dict(feed_dict, replace=False)\n</code></pre> <p>Add datasets to the <code>DataCatalog</code> using the data provided through the <code>feed_dict</code>.</p> <p><code>feed_dict</code> is a dictionary where the keys represent dataset names and the values can either be raw data or Kedro datasets - instances of classes that inherit from <code>AbstractDataset</code>. If raw data is provided, it will be automatically wrapped in a <code>MemoryDataset</code> before being added to the <code>DataCatalog</code>.</p> <p>Parameters:</p> <ul> <li> <code>feed_dict</code>               (<code>dict[str, Any]</code>)           \u2013            <p>A dictionary with data to be added to the <code>DataCatalog</code>. Keys are dataset names and values can be raw data or instances of classes that inherit from <code>AbstractDataset</code>.</p> </li> <li> <code>replace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Specifies whether to replace an existing dataset with the same name in the <code>DataCatalog</code>.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; df = pd.DataFrame({\"col1\": [1, 2],\n&gt;&gt;&gt;                    \"col2\": [4, 5],\n&gt;&gt;&gt;                    \"col3\": [5, 6]})\n&gt;&gt;&gt;\n&gt;&gt;&gt; catalog = DataCatalog()\n&gt;&gt;&gt; catalog.add_feed_dict({\n&gt;&gt;&gt;     \"data_df\": df\n&gt;&gt;&gt; }, replace=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; assert catalog.load(\"data_df\").equals(df)\n&gt;&gt;&gt;\n&gt;&gt;&gt; csv_dataset = CSVDataset(filepath=\"test.csv\")\n&gt;&gt;&gt; csv_dataset.save(df)\n&gt;&gt;&gt; catalog.add_feed_dict({\"data_csv_dataset\": csv_dataset})\n&gt;&gt;&gt;\n&gt;&gt;&gt; assert catalog.load(\"data_csv_dataset\").equals(df)\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def add_feed_dict(self, feed_dict: dict[str, Any], replace: bool = False) -&gt; None:\n    \"\"\"Add datasets to the ``DataCatalog`` using the data provided through the `feed_dict`.\n\n    `feed_dict` is a dictionary where the keys represent dataset names and the values can either be raw data or\n    Kedro datasets - instances of classes that inherit from ``AbstractDataset``. If raw data is provided,\n    it will be automatically wrapped in a ``MemoryDataset`` before being added to the ``DataCatalog``.\n\n    Args:\n        feed_dict: A dictionary with data to be added to the ``DataCatalog``. Keys are dataset names and\n            values can be raw data or instances of classes that inherit from ``AbstractDataset``.\n        replace: Specifies whether to replace an existing dataset with the same name in the ``DataCatalog``.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df = pd.DataFrame({\"col1\": [1, 2],\n        &gt;&gt;&gt;                    \"col2\": [4, 5],\n        &gt;&gt;&gt;                    \"col3\": [5, 6]})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; catalog = DataCatalog()\n        &gt;&gt;&gt; catalog.add_feed_dict({\n        &gt;&gt;&gt;     \"data_df\": df\n        &gt;&gt;&gt; }, replace=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; assert catalog.load(\"data_df\").equals(df)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; csv_dataset = CSVDataset(filepath=\"test.csv\")\n        &gt;&gt;&gt; csv_dataset.save(df)\n        &gt;&gt;&gt; catalog.add_feed_dict({\"data_csv_dataset\": csv_dataset})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; assert catalog.load(\"data_csv_dataset\").equals(df)\n    \"\"\"\n    for ds_name, ds_data in feed_dict.items():\n        dataset = (\n            ds_data\n            if isinstance(ds_data, AbstractDataset)\n            else MemoryDataset(data=ds_data)  # type: ignore[abstract]\n        )\n        self.add(ds_name, dataset, replace)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog.confirm","title":"confirm","text":"<pre><code>confirm(name)\n</code></pre> <p>Confirm a dataset by its name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of the dataset.</p> </li> </ul> <p>Raises:     DatasetError: When the dataset does not have <code>confirm</code> method.</p> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def confirm(self, name: str) -&gt; None:\n    \"\"\"Confirm a dataset by its name.\n\n    Args:\n        name: Name of the dataset.\n    Raises:\n        DatasetError: When the dataset does not have `confirm` method.\n\n    \"\"\"\n    self._logger.info(\"Confirming dataset '%s'\", name)\n    dataset = self._get_dataset(name)\n\n    if hasattr(dataset, \"confirm\"):\n        dataset.confirm()\n    else:\n        raise DatasetError(f\"Dataset '{name}' does not have 'confirm' method\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog.exists","title":"exists","text":"<pre><code>exists(name)\n</code></pre> <p>Checks whether registered dataset exists by calling its <code>exists()</code> method. Raises a warning and returns False if <code>exists()</code> is not implemented.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>A dataset to be checked.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>Whether the dataset output exists.</p> </li> </ul> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def exists(self, name: str) -&gt; bool:\n    \"\"\"Checks whether registered dataset exists by calling its `exists()`\n    method. Raises a warning and returns False if `exists()` is not\n    implemented.\n\n    Args:\n        name: A dataset to be checked.\n\n    Returns:\n        Whether the dataset output exists.\n\n    \"\"\"\n    try:\n        dataset = self._get_dataset(name)\n    except DatasetNotFoundError:\n        return False\n    return dataset.exists()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(catalog, credentials=None, load_versions=None, save_version=None)\n</code></pre> <p>Create a <code>DataCatalog</code> instance from configuration. This is a factory method used to provide developers with a way to instantiate <code>DataCatalog</code> with configuration parsed from configuration files.</p> <p>Parameters:</p> <ul> <li> <code>catalog</code>               (<code>dict[str, dict[str, Any]] | None</code>)           \u2013            <p>A dictionary whose keys are the dataset names and the values are dictionaries with the constructor arguments for classes implementing <code>AbstractDataset</code>. The dataset class to be loaded is specified with the key <code>type</code> and their fully qualified class name. All <code>kedro.io</code> dataset can be specified by their class name only, i.e. their module name can be omitted.</p> </li> <li> <code>credentials</code>               (<code>dict[str, dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary containing credentials for different datasets. Use the <code>credentials</code> key in a <code>AbstractDataset</code> to refer to the appropriate credentials as shown in the example below.</p> </li> <li> <code>load_versions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A mapping between dataset names and versions to load. Has no effect on datasets without enabled versioning.</p> </li> <li> <code>save_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Version string to be used for <code>save</code> operations by all datasets with enabled versioning. It must: a) be a case-insensitive string that conforms with operating system filename limitations, b) always return the latest version when sorted in lexicographical order.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataCatalog</code>           \u2013            <p>An instantiated <code>DataCatalog</code> containing all specified</p> </li> <li> <code>DataCatalog</code>           \u2013            <p>datasets, created and ready to use.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>When the method fails to create any of the data sets from their config.</p> </li> <li> <code>DatasetNotFoundError</code>             \u2013            <p>When <code>load_versions</code> refers to a dataset that doesn't exist in the catalog.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; config = {\n&gt;&gt;&gt;     \"cars\": {\n&gt;&gt;&gt;         \"type\": \"pandas.CSVDataset\",\n&gt;&gt;&gt;         \"filepath\": \"cars.csv\",\n&gt;&gt;&gt;         \"save_args\": {\n&gt;&gt;&gt;             \"index\": False\n&gt;&gt;&gt;         }\n&gt;&gt;&gt;     },\n&gt;&gt;&gt;     \"boats\": {\n&gt;&gt;&gt;         \"type\": \"pandas.CSVDataset\",\n&gt;&gt;&gt;         \"filepath\": \"s3://aws-bucket-name/boats.csv\",\n&gt;&gt;&gt;         \"credentials\": \"boats_credentials\",\n&gt;&gt;&gt;         \"save_args\": {\n&gt;&gt;&gt;             \"index\": False\n&gt;&gt;&gt;         }\n&gt;&gt;&gt;     }\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; credentials = {\n&gt;&gt;&gt;     \"boats_credentials\": {\n&gt;&gt;&gt;         \"client_kwargs\": {\n&gt;&gt;&gt;             \"aws_access_key_id\": \"&lt;your key id&gt;\",\n&gt;&gt;&gt;             \"aws_secret_access_key\": \"&lt;your secret&gt;\"\n&gt;&gt;&gt;         }\n&gt;&gt;&gt;      }\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; catalog = DataCatalog.from_config(config, credentials)\n&gt;&gt;&gt;\n&gt;&gt;&gt; df = catalog.load(\"cars\")\n&gt;&gt;&gt; catalog.save(\"boats\", df)\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    catalog: dict[str, dict[str, Any]] | None,\n    credentials: dict[str, dict[str, Any]] | None = None,\n    load_versions: dict[str, str] | None = None,\n    save_version: str | None = None,\n) -&gt; DataCatalog:\n    \"\"\"Create a ``DataCatalog`` instance from configuration. This is a\n    factory method used to provide developers with a way to instantiate\n    ``DataCatalog`` with configuration parsed from configuration files.\n\n    Args:\n        catalog: A dictionary whose keys are the dataset names and\n            the values are dictionaries with the constructor arguments\n            for classes implementing ``AbstractDataset``. The dataset\n            class to be loaded is specified with the key ``type`` and their\n            fully qualified class name. All ``kedro.io`` dataset can be\n            specified by their class name only, i.e. their module name\n            can be omitted.\n        credentials: A dictionary containing credentials for different\n            datasets. Use the ``credentials`` key in a ``AbstractDataset``\n            to refer to the appropriate credentials as shown in the example\n            below.\n        load_versions: A mapping between dataset names and versions\n            to load. Has no effect on datasets without enabled versioning.\n        save_version: Version string to be used for ``save`` operations\n            by all datasets with enabled versioning. It must: a) be a\n            case-insensitive string that conforms with operating system\n            filename limitations, b) always return the latest version when\n            sorted in lexicographical order.\n\n    Returns:\n        An instantiated ``DataCatalog`` containing all specified\n        datasets, created and ready to use.\n\n    Raises:\n        DatasetError: When the method fails to create any of the data\n            sets from their config.\n        DatasetNotFoundError: When `load_versions` refers to a dataset that doesn't\n            exist in the catalog.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; config = {\n        &gt;&gt;&gt;     \"cars\": {\n        &gt;&gt;&gt;         \"type\": \"pandas.CSVDataset\",\n        &gt;&gt;&gt;         \"filepath\": \"cars.csv\",\n        &gt;&gt;&gt;         \"save_args\": {\n        &gt;&gt;&gt;             \"index\": False\n        &gt;&gt;&gt;         }\n        &gt;&gt;&gt;     },\n        &gt;&gt;&gt;     \"boats\": {\n        &gt;&gt;&gt;         \"type\": \"pandas.CSVDataset\",\n        &gt;&gt;&gt;         \"filepath\": \"s3://aws-bucket-name/boats.csv\",\n        &gt;&gt;&gt;         \"credentials\": \"boats_credentials\",\n        &gt;&gt;&gt;         \"save_args\": {\n        &gt;&gt;&gt;             \"index\": False\n        &gt;&gt;&gt;         }\n        &gt;&gt;&gt;     }\n        &gt;&gt;&gt; }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; credentials = {\n        &gt;&gt;&gt;     \"boats_credentials\": {\n        &gt;&gt;&gt;         \"client_kwargs\": {\n        &gt;&gt;&gt;             \"aws_access_key_id\": \"&lt;your key id&gt;\",\n        &gt;&gt;&gt;             \"aws_secret_access_key\": \"&lt;your secret&gt;\"\n        &gt;&gt;&gt;         }\n        &gt;&gt;&gt;      }\n        &gt;&gt;&gt; }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; catalog = DataCatalog.from_config(config, credentials)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df = catalog.load(\"cars\")\n        &gt;&gt;&gt; catalog.save(\"boats\", df)\n    \"\"\"\n    catalog = catalog or {}\n    datasets = {}\n    config_resolver = CatalogConfigResolver(catalog, credentials)\n    save_version = save_version or generate_timestamp()\n    load_versions = load_versions or {}\n\n    for ds_name in catalog:\n        if not config_resolver.is_pattern(ds_name):\n            datasets[ds_name] = AbstractDataset.from_config(\n                ds_name,\n                config_resolver.config.get(ds_name, {}),\n                load_versions.get(ds_name),\n                save_version,\n            )\n\n    missing_keys = [\n        ds_name\n        for ds_name in load_versions\n        if not (\n            ds_name in config_resolver.config\n            or config_resolver.match_pattern(ds_name)\n        )\n    ]\n    if missing_keys:\n        raise DatasetNotFoundError(\n            f\"'load_versions' keys [{', '.join(sorted(missing_keys))}] \"\n            f\"are not found in the catalog.\"\n        )\n\n    return cls(\n        datasets=datasets,\n        dataset_patterns=config_resolver._dataset_patterns,\n        load_versions=load_versions,\n        save_version=save_version,\n        default_pattern=config_resolver._default_pattern,\n        config_resolver=config_resolver,\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog.list","title":"list","text":"<pre><code>list(regex_search=None)\n</code></pre> <p>List of all dataset names registered in the catalog. This can be filtered by providing an optional regular expression which will only return matching keys.</p> <p>Parameters:</p> <ul> <li> <code>regex_search</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional regular expression which can be provided to limit the datasets returned by a particular pattern.</p> </li> </ul> <p>Returns:     A list of dataset names available which match the     <code>regex_search</code> criteria (if provided). All dataset names are returned     by default.</p> <p>Raises:</p> <ul> <li> <code>SyntaxError</code>             \u2013            <p>When an invalid regex filter is provided.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; catalog = DataCatalog()\n&gt;&gt;&gt; # get datasets where the substring 'raw' is present\n&gt;&gt;&gt; raw_data = catalog.list(regex_search='raw')\n&gt;&gt;&gt; # get datasets which start with 'prm' or 'feat'\n&gt;&gt;&gt; feat_eng_data = catalog.list(regex_search='^(prm|feat)')\n&gt;&gt;&gt; # get datasets which end with 'time_series'\n&gt;&gt;&gt; models = catalog.list(regex_search='.+time_series$')\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def list(self, regex_search: str | None = None) -&gt; list[str]:\n    \"\"\"\n    List of all dataset names registered in the catalog.\n    This can be filtered by providing an optional regular expression\n    which will only return matching keys.\n\n    Args:\n        regex_search: An optional regular expression which can be provided\n            to limit the datasets returned by a particular pattern.\n    Returns:\n        A list of dataset names available which match the\n        `regex_search` criteria (if provided). All dataset names are returned\n        by default.\n\n    Raises:\n        SyntaxError: When an invalid regex filter is provided.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; catalog = DataCatalog()\n        &gt;&gt;&gt; # get datasets where the substring 'raw' is present\n        &gt;&gt;&gt; raw_data = catalog.list(regex_search='raw')\n        &gt;&gt;&gt; # get datasets which start with 'prm' or 'feat'\n        &gt;&gt;&gt; feat_eng_data = catalog.list(regex_search='^(prm|feat)')\n        &gt;&gt;&gt; # get datasets which end with 'time_series'\n        &gt;&gt;&gt; models = catalog.list(regex_search='.+time_series$')\n    \"\"\"\n\n    if regex_search is None:\n        return list(self._datasets.keys())\n\n    if not regex_search.strip():\n        self._logger.warning(\"The empty string will not match any datasets\")\n        return []\n\n    try:\n        pattern = re.compile(regex_search, flags=re.IGNORECASE)\n\n    except re.error as exc:\n        raise SyntaxError(\n            f\"Invalid regular expression provided: '{regex_search}'\"\n        ) from exc\n    return [ds_name for ds_name in self._datasets if pattern.search(ds_name)]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog.load","title":"load","text":"<pre><code>load(name, version=None)\n</code></pre> <p>Loads a registered dataset.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>A dataset to be loaded.</p> </li> <li> <code>version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional argument for concrete data version to be loaded. Works only with versioned datasets.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The loaded data as configured.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetNotFoundError</code>             \u2013            <p>When a dataset with the given name has not yet been registered.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro.io import DataCatalog\n&gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt; cars = CSVDataset(filepath=\"cars.csv\",\n&gt;&gt;&gt;                   load_args=None,\n&gt;&gt;&gt;                   save_args={\"index\": False})\n&gt;&gt;&gt; catalog = DataCatalog(datasets={'cars': cars})\n&gt;&gt;&gt;\n&gt;&gt;&gt; df = catalog.load(\"cars\")\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def load(self, name: str, version: str | None = None) -&gt; Any:\n    \"\"\"Loads a registered dataset.\n\n    Args:\n        name: A dataset to be loaded.\n        version: Optional argument for concrete data version to be loaded.\n            Works only with versioned datasets.\n\n    Returns:\n        The loaded data as configured.\n\n    Raises:\n        DatasetNotFoundError: When a dataset with the given name\n            has not yet been registered.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; from kedro.io import DataCatalog\n        &gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; cars = CSVDataset(filepath=\"cars.csv\",\n        &gt;&gt;&gt;                   load_args=None,\n        &gt;&gt;&gt;                   save_args={\"index\": False})\n        &gt;&gt;&gt; catalog = DataCatalog(datasets={'cars': cars})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df = catalog.load(\"cars\")\n    \"\"\"\n    load_version = Version(version, None) if version else None\n    dataset = self._get_dataset(name, version=load_version)\n\n    self._logger.info(\n        \"Loading data from %s (%s)...\",\n        name,\n        type(dataset).__name__,\n        extra={\"rich_format\": [\"dark_orange\"]},\n    )\n\n    result = dataset.load()\n\n    return result\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog.release","title":"release","text":"<pre><code>release(name)\n</code></pre> <p>Release any cached data associated with a dataset</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>A dataset to be checked.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetNotFoundError</code>             \u2013            <p>When a dataset with the given name has not yet been registered.</p> </li> </ul> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def release(self, name: str) -&gt; None:\n    \"\"\"Release any cached data associated with a dataset\n\n    Args:\n        name: A dataset to be checked.\n\n    Raises:\n        DatasetNotFoundError: When a dataset with the given name\n            has not yet been registered.\n    \"\"\"\n    dataset = self._get_dataset(name)\n    dataset.release()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog.save","title":"save","text":"<pre><code>save(name, data)\n</code></pre> <p>Save data to a registered dataset.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>A dataset to be saved to.</p> </li> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>A data object to be saved as configured in the registered dataset.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetNotFoundError</code>             \u2013            <p>When a dataset with the given name has not yet been registered.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt; cars = CSVDataset(filepath=\"cars.csv\",\n&gt;&gt;&gt;                   load_args=None,\n&gt;&gt;&gt;                   save_args={\"index\": False})\n&gt;&gt;&gt; catalog = DataCatalog(datasets={'cars': cars})\n&gt;&gt;&gt;\n&gt;&gt;&gt; df = pd.DataFrame({'col1': [1, 2],\n&gt;&gt;&gt;                    'col2': [4, 5],\n&gt;&gt;&gt;                    'col3': [5, 6]})\n&gt;&gt;&gt; catalog.save(\"cars\", df)\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def save(self, name: str, data: Any) -&gt; None:\n    \"\"\"Save data to a registered dataset.\n\n    Args:\n        name: A dataset to be saved to.\n        data: A data object to be saved as configured in the registered\n            dataset.\n\n    Raises:\n        DatasetNotFoundError: When a dataset with the given name\n            has not yet been registered.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; cars = CSVDataset(filepath=\"cars.csv\",\n        &gt;&gt;&gt;                   load_args=None,\n        &gt;&gt;&gt;                   save_args={\"index\": False})\n        &gt;&gt;&gt; catalog = DataCatalog(datasets={'cars': cars})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df = pd.DataFrame({'col1': [1, 2],\n        &gt;&gt;&gt;                    'col2': [4, 5],\n        &gt;&gt;&gt;                    'col3': [5, 6]})\n        &gt;&gt;&gt; catalog.save(\"cars\", df)\n    \"\"\"\n    dataset = self._get_dataset(name)\n\n    self._logger.info(\n        \"Saving data to %s (%s)...\",\n        name,\n        type(dataset).__name__,\n        extra={\"rich_format\": [\"dark_orange\"]},\n    )\n\n    dataset.save(data)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.DataCatalog.shallow_copy","title":"shallow_copy","text":"<pre><code>shallow_copy(extra_dataset_patterns=None)\n</code></pre> <p>Returns a shallow copy of the current object.</p> <p>Returns:</p> <ul> <li> <code>DataCatalog</code>           \u2013            <p>Copy of the current object.</p> </li> </ul> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def shallow_copy(\n    self, extra_dataset_patterns: Patterns | None = None\n) -&gt; DataCatalog:\n    \"\"\"Returns a shallow copy of the current object.\n\n    Returns:\n        Copy of the current object.\n    \"\"\"\n    if extra_dataset_patterns:\n        self._config_resolver.add_runtime_patterns(extra_dataset_patterns)\n    return self.__class__(\n        datasets=self._datasets,\n        dataset_patterns=self._config_resolver._dataset_patterns,\n        default_pattern=self._config_resolver._default_pattern,\n        load_versions=self._load_versions,\n        save_version=self._save_version,\n        config_resolver=self._config_resolver,\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.KedroCliError","title":"KedroCliError","text":"<p>               Bases: <code>ClickException</code></p> <p>Exceptions generated from the Kedro CLI.</p> <p>Users should pass an appropriate message at the constructor.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.KedroSession","title":"KedroSession","text":"<pre><code>KedroSession(session_id, package_name=None, project_path=None, save_on_close=False, conf_source=None)\n</code></pre> <p><code>KedroSession</code> is the object that is responsible for managing the lifecycle of a Kedro run. Use <code>KedroSession.create()</code> as a context manager to construct a new KedroSession with session data provided (see the example below).</p> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro.framework.session import KedroSession\n&gt;&gt;&gt; from kedro.framework.startup import bootstrap_project\n&gt;&gt;&gt; from pathlib import Path\n\n&gt;&gt;&gt; # If you are creating a session outside of a Kedro project (i.e. not using\n&gt;&gt;&gt; # `kedro run` or `kedro jupyter`), you need to run `bootstrap_project` to\n&gt;&gt;&gt; # let Kedro find your configuration.\n&gt;&gt;&gt; bootstrap_project(Path(\"&lt;project_root&gt;\"))\n&gt;&gt;&gt; with KedroSession.create() as session:\n&gt;&gt;&gt;     session.run()\n</code></pre> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    package_name: str | None = None,\n    project_path: Path | str | None = None,\n    save_on_close: bool = False,\n    conf_source: str | None = None,\n):\n    self._project_path = Path(\n        project_path or _find_kedro_project(Path.cwd()) or Path.cwd()\n    ).resolve()\n    self.session_id = session_id\n    self.save_on_close = save_on_close\n    self._package_name = package_name\n    self._store = self._init_store()\n    self._run_called = False\n\n    hook_manager = _create_hook_manager()\n    _register_hooks(hook_manager, settings.HOOKS)\n    _register_hooks_entry_points(hook_manager, settings.DISABLE_HOOKS_FOR_PLUGINS)\n    self._hook_manager = hook_manager\n\n    self._conf_source = conf_source or str(\n        self._project_path / settings.CONF_SOURCE\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.KedroSession.store","title":"store  <code>property</code>","text":"<pre><code>store\n</code></pre> <p>Return a copy of internal store.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.KedroSession.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the current session and save its store to disk if <code>save_on_close</code> attribute is True.</p> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the current session and save its store to disk\n    if `save_on_close` attribute is True.\n    \"\"\"\n    if self.save_on_close:\n        self._store.save()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.KedroSession.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(project_path=None, save_on_close=True, env=None, extra_params=None, conf_source=None)\n</code></pre> <p>Create a new instance of <code>KedroSession</code> with the session data.</p> <p>Parameters:</p> <ul> <li> <code>project_path</code>               (<code>Path | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the project root directory. Default is current working directory Path.cwd().</p> </li> <li> <code>save_on_close</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether or not to save the session when it's closed.</p> </li> <li> <code>conf_source</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to a directory containing configuration</p> </li> <li> <code>env</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Environment for the KedroContext.</p> </li> <li> <code>extra_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary containing extra project parameters for underlying KedroContext. If specified, will update (and therefore take precedence over) the parameters retrieved from the project configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>KedroSession</code>           \u2013            <p>A new <code>KedroSession</code> instance.</p> </li> </ul> Source code in <code>kedro/framework/session/session.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    project_path: Path | str | None = None,\n    save_on_close: bool = True,\n    env: str | None = None,\n    extra_params: dict[str, Any] | None = None,\n    conf_source: str | None = None,\n) -&gt; KedroSession:\n    \"\"\"Create a new instance of ``KedroSession`` with the session data.\n\n    Args:\n        project_path: Path to the project root directory. Default is\n            current working directory Path.cwd().\n        save_on_close: Whether or not to save the session when it's closed.\n        conf_source: Path to a directory containing configuration\n        env: Environment for the KedroContext.\n        extra_params: Optional dictionary containing extra project parameters\n            for underlying KedroContext. If specified, will update (and therefore\n            take precedence over) the parameters retrieved from the project\n            configuration.\n\n    Returns:\n        A new ``KedroSession`` instance.\n    \"\"\"\n    validate_settings()\n\n    session = cls(\n        project_path=project_path,\n        session_id=generate_timestamp(),\n        save_on_close=save_on_close,\n        conf_source=conf_source,\n    )\n\n    # have to explicitly type session_data otherwise mypy will complain\n    # possibly related to this: https://github.com/python/mypy/issues/1430\n    session_data: dict[str, Any] = {\n        \"project_path\": session._project_path,\n        \"session_id\": session.session_id,\n    }\n\n    ctx = click.get_current_context(silent=True)\n    if ctx:\n        session_data[\"cli\"] = _jsonify_cli_context(ctx)\n\n    env = env or os.getenv(\"KEDRO_ENV\")\n    if env:\n        session_data[\"env\"] = env\n\n    if extra_params:\n        session_data[\"extra_params\"] = extra_params\n\n    try:\n        session_data[\"username\"] = getpass.getuser()\n    except Exception as exc:\n        logging.getLogger(__name__).debug(\n            \"Unable to get username. Full exception: %s\", exc\n        )\n\n    session_data.update(**_describe_git(session._project_path))\n    session._store.update(session_data)\n\n    return session\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.KedroSession.load_context","title":"load_context","text":"<pre><code>load_context()\n</code></pre> <p>An instance of the project context.</p> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def load_context(self) -&gt; KedroContext:\n    \"\"\"An instance of the project context.\"\"\"\n    env = self.store.get(\"env\")\n    extra_params = self.store.get(\"extra_params\")\n    config_loader = self._get_config_loader()\n    context_class = settings.CONTEXT_CLASS\n    context = context_class(\n        package_name=self._package_name,\n        project_path=self._project_path,\n        config_loader=config_loader,\n        env=env,\n        extra_params=extra_params,\n        hook_manager=self._hook_manager,\n    )\n    self._hook_manager.hook.after_context_created(context=context)\n\n    return context  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.KedroSession.run","title":"run","text":"<pre><code>run(pipeline_name=None, tags=None, runner=None, node_names=None, from_nodes=None, to_nodes=None, from_inputs=None, to_outputs=None, load_versions=None, namespace=None)\n</code></pre> <p>Runs the pipeline with a specified runner.</p> <p>Parameters:</p> <ul> <li> <code>pipeline_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the pipeline that is being run.</p> </li> <li> <code>tags</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of node tags which should be used to filter the nodes of the <code>Pipeline</code>. If specified, only the nodes containing any of these tags will be run.</p> </li> <li> <code>runner</code>               (<code>AbstractRunner | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional parameter specifying the runner that you want to run the pipeline with.</p> </li> <li> <code>node_names</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of node names which should be used to filter the nodes of the <code>Pipeline</code>. If specified, only the nodes with these names will be run.</p> </li> <li> <code>from_nodes</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of node names which should be used as a starting point of the new <code>Pipeline</code>.</p> </li> <li> <code>to_nodes</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of node names which should be used as an end point of the new <code>Pipeline</code>.</p> </li> <li> <code>from_inputs</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of input datasets which should be used as a starting point of the new <code>Pipeline</code>.</p> </li> <li> <code>to_outputs</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of output datasets which should be used as an end point of the new <code>Pipeline</code>.</p> </li> <li> <code>load_versions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional flag to specify a particular dataset version timestamp to load.</p> </li> <li> <code>namespace</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The namespace of the nodes that is being run.</p> </li> </ul> <p>Raises:     ValueError: If the named or <code>__default__</code> pipeline is not         defined by <code>register_pipelines</code>.     Exception: Any uncaught exception during the run will be re-raised         after being passed to <code>on_pipeline_error</code> hook.     KedroSessionError: If more than one run is attempted to be executed during         a single session. Returns:     Any node outputs that cannot be processed by the <code>DataCatalog</code>.     These are returned in a dictionary, where the keys are defined     by the node outputs.</p> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def run(  # noqa: PLR0913\n    self,\n    pipeline_name: str | None = None,\n    tags: Iterable[str] | None = None,\n    runner: AbstractRunner | None = None,\n    node_names: Iterable[str] | None = None,\n    from_nodes: Iterable[str] | None = None,\n    to_nodes: Iterable[str] | None = None,\n    from_inputs: Iterable[str] | None = None,\n    to_outputs: Iterable[str] | None = None,\n    load_versions: dict[str, str] | None = None,\n    namespace: str | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Runs the pipeline with a specified runner.\n\n    Args:\n        pipeline_name: Name of the pipeline that is being run.\n        tags: An optional list of node tags which should be used to\n            filter the nodes of the ``Pipeline``. If specified, only the nodes\n            containing *any* of these tags will be run.\n        runner: An optional parameter specifying the runner that you want to run\n            the pipeline with.\n        node_names: An optional list of node names which should be used to\n            filter the nodes of the ``Pipeline``. If specified, only the nodes\n            with these names will be run.\n        from_nodes: An optional list of node names which should be used as a\n            starting point of the new ``Pipeline``.\n        to_nodes: An optional list of node names which should be used as an\n            end point of the new ``Pipeline``.\n        from_inputs: An optional list of input datasets which should be\n            used as a starting point of the new ``Pipeline``.\n        to_outputs: An optional list of output datasets which should be\n            used as an end point of the new ``Pipeline``.\n        load_versions: An optional flag to specify a particular dataset\n            version timestamp to load.\n        namespace: The namespace of the nodes that is being run.\n    Raises:\n        ValueError: If the named or `__default__` pipeline is not\n            defined by `register_pipelines`.\n        Exception: Any uncaught exception during the run will be re-raised\n            after being passed to ``on_pipeline_error`` hook.\n        KedroSessionError: If more than one run is attempted to be executed during\n            a single session.\n    Returns:\n        Any node outputs that cannot be processed by the ``DataCatalog``.\n        These are returned in a dictionary, where the keys are defined\n        by the node outputs.\n    \"\"\"\n    # Report project name\n    self._logger.info(\"Kedro project %s\", self._project_path.name)\n\n    if self._run_called:\n        raise KedroSessionError(\n            \"A run has already been completed as part of the\"\n            \" active KedroSession. KedroSession has a 1-1 mapping with\"\n            \" runs, and thus only one run should be executed per session.\"\n        )\n\n    session_id = self.store[\"session_id\"]\n    save_version = session_id\n    extra_params = self.store.get(\"extra_params\") or {}\n    context = self.load_context()\n\n    name = pipeline_name or \"__default__\"\n\n    try:\n        pipeline = pipelines[name]\n    except KeyError as exc:\n        raise ValueError(\n            f\"Failed to find the pipeline named '{name}'. \"\n            f\"It needs to be generated and returned \"\n            f\"by the 'register_pipelines' function.\"\n        ) from exc\n\n    filtered_pipeline = pipeline.filter(\n        tags=tags,\n        from_nodes=from_nodes,\n        to_nodes=to_nodes,\n        node_names=node_names,\n        from_inputs=from_inputs,\n        to_outputs=to_outputs,\n        node_namespace=namespace,\n    )\n\n    record_data = {\n        \"session_id\": session_id,\n        \"project_path\": self._project_path.as_posix(),\n        \"env\": context.env,\n        \"kedro_version\": kedro_version,\n        \"tags\": tags,\n        \"from_nodes\": from_nodes,\n        \"to_nodes\": to_nodes,\n        \"node_names\": node_names,\n        \"from_inputs\": from_inputs,\n        \"to_outputs\": to_outputs,\n        \"load_versions\": load_versions,\n        \"extra_params\": extra_params,\n        \"pipeline_name\": pipeline_name,\n        \"namespace\": namespace,\n        \"runner\": getattr(runner, \"__name__\", str(runner)),\n    }\n\n    catalog = context._get_catalog(\n        save_version=save_version,\n        load_versions=load_versions,\n    )\n\n    # Run the runner\n    hook_manager = self._hook_manager\n    runner = runner or SequentialRunner()\n    if not isinstance(runner, AbstractRunner):\n        raise KedroSessionError(\n            \"KedroSession expect an instance of Runner instead of a class.\"\n            \"Have you forgotten the `()` at the end of the statement?\"\n        )\n    hook_manager.hook.before_pipeline_run(\n        run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\n    )\n    try:\n        run_result = runner.run(\n            filtered_pipeline, catalog, hook_manager, session_id\n        )\n        self._run_called = True\n    except Exception as error:\n        hook_manager.hook.on_pipeline_error(\n            error=error,\n            run_params=record_data,\n            pipeline=filtered_pipeline,\n            catalog=catalog,\n        )\n        raise\n\n    hook_manager.hook.after_pipeline_run(\n        run_params=record_data,\n        run_result=run_result,\n        pipeline=filtered_pipeline,\n        catalog=catalog,\n    )\n    return run_result\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.ProjectMetadata","title":"ProjectMetadata","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Structure holding project metadata derived from <code>pyproject.toml</code></p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog._LazyDataset","title":"_LazyDataset","text":"<pre><code>_LazyDataset(name, config, load_version=None, save_version=None)\n</code></pre> <p>A helper class to store AbstractDataset configuration and materialize dataset object.</p> Source code in <code>kedro/io/kedro_data_catalog.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    config: dict[str, Any],\n    load_version: str | None = None,\n    save_version: str | None = None,\n):\n    self.name = name\n    self.config = config\n    self.load_version = load_version\n    self.save_version = save_version\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog._add_missing_datasets_to_catalog","title":"_add_missing_datasets_to_catalog","text":"<pre><code>_add_missing_datasets_to_catalog(missing_ds, catalog_path)\n</code></pre> Source code in <code>kedro/framework/cli/catalog.py</code> <pre><code>def _add_missing_datasets_to_catalog(missing_ds: list[str], catalog_path: Path) -&gt; None:\n    if catalog_path.is_file():\n        catalog_config = yaml.safe_load(catalog_path.read_text()) or {}\n    else:\n        catalog_config = {}\n\n    for ds_name in missing_ds:\n        catalog_config[ds_name] = {\"type\": \"MemoryDataset\"}\n\n    # Create only `catalog` folder under existing environment\n    # (all parent folders must exist).\n    catalog_path.parent.mkdir(exist_ok=True)\n    with catalog_path.open(mode=\"w\") as catalog_file:\n        yaml.safe_dump(catalog_config, catalog_file, default_flow_style=False)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog._create_session","title":"_create_session","text":"<pre><code>_create_session(package_name, **kwargs)\n</code></pre> Source code in <code>kedro/framework/cli/catalog.py</code> <pre><code>def _create_session(package_name: str, **kwargs: Any) -&gt; KedroSession:\n    kwargs.setdefault(\"save_on_close\", False)\n    return KedroSession.create(**kwargs)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog._map_type_to_datasets","title":"_map_type_to_datasets","text":"<pre><code>_map_type_to_datasets(datasets, datasets_meta)\n</code></pre> <p>Build dictionary with a dataset type as a key and list of datasets of the specific type as a value.</p> Source code in <code>kedro/framework/cli/catalog.py</code> <pre><code>def _map_type_to_datasets(\n    datasets: set[str], datasets_meta: dict[str, AbstractDataset]\n) -&gt; dict:\n    \"\"\"Build dictionary with a dataset type as a key and list of\n    datasets of the specific type as a value.\n    \"\"\"\n    mapping = defaultdict(list)  # type: ignore[var-annotated]\n    for dataset_name in filterfalse(is_parameter, datasets):\n        if isinstance(datasets_meta[dataset_name], _LazyDataset):\n            ds_type = str(datasets_meta[dataset_name]).split(\".\")[-1]\n        else:\n            ds_type = datasets_meta[dataset_name].__class__.__name__\n        if dataset_name not in mapping[ds_type]:\n            mapping[ds_type].append(dataset_name)\n    return mapping\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.catalog","title":"catalog","text":"<pre><code>catalog()\n</code></pre> <p>Commands for working with catalog.</p> Source code in <code>kedro/framework/cli/catalog.py</code> <pre><code>@catalog_cli.group()\ndef catalog() -&gt; None:\n    \"\"\"Commands for working with catalog.\"\"\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.catalog_cli","title":"catalog_cli","text":"<pre><code>catalog_cli()\n</code></pre> Source code in <code>kedro/framework/cli/catalog.py</code> <pre><code>@click.group(name=\"Kedro\")\ndef catalog_cli() -&gt; None:  # pragma: no cover\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.create_catalog","title":"create_catalog","text":"<pre><code>create_catalog(metadata, pipeline_name, env)\n</code></pre> <p>Create Data Catalog YAML configuration with missing datasets.</p> <p>Add <code>MemoryDataset</code> datasets to Data Catalog YAML configuration file for each dataset in a registered pipeline if it is missing from the <code>DataCatalog</code>.</p> <p>The catalog configuration will be saved to <code>&lt;conf_source&gt;/&lt;env&gt;/catalog_&lt;pipeline_name&gt;.yml</code> file.</p> Source code in <code>kedro/framework/cli/catalog.py</code> <pre><code>@catalog.command(\"create\")\n@env_option(help=\"Environment to create Data Catalog YAML file in. Defaults to `base`.\")\n@click.option(\n    \"--pipeline\",\n    \"-p\",\n    \"pipeline_name\",\n    type=str,\n    required=True,\n    help=\"Name of a pipeline.\",\n)\n@click.pass_obj\ndef create_catalog(metadata: ProjectMetadata, pipeline_name: str, env: str) -&gt; None:\n    \"\"\"Create Data Catalog YAML configuration with missing datasets.\n\n    Add ``MemoryDataset`` datasets to Data Catalog YAML configuration\n    file for each dataset in a registered pipeline if it is missing from\n    the ``DataCatalog``.\n\n    The catalog configuration will be saved to\n    `&lt;conf_source&gt;/&lt;env&gt;/catalog_&lt;pipeline_name&gt;.yml` file.\n    \"\"\"\n    env = env or \"base\"\n    session = _create_session(metadata.package_name, env=env)\n    context = session.load_context()\n\n    pipeline = pipelines.get(pipeline_name)\n\n    if not pipeline:\n        existing_pipelines = \", \".join(sorted(pipelines.keys()))\n        raise KedroCliError(\n            f\"'{pipeline_name}' pipeline not found! Existing pipelines: {existing_pipelines}\"\n        )\n\n    pipeline_datasets = set(filterfalse(is_parameter, pipeline.datasets()))\n\n    catalog_datasets = set(filterfalse(is_parameter, context.catalog.list()))\n\n    # Datasets that are missing in Data Catalog\n    missing_ds = sorted(pipeline_datasets - catalog_datasets)\n    if missing_ds:\n        catalog_path = (\n            context.project_path\n            / settings.CONF_SOURCE\n            / env\n            / f\"catalog_{pipeline_name}.yml\"\n        )\n        _add_missing_datasets_to_catalog(missing_ds, catalog_path)\n        click.echo(f\"Data Catalog YAML configuration was created: {catalog_path}\")\n    else:\n        click.echo(\"All datasets are already configured.\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.env_option","title":"env_option","text":"<pre><code>env_option(func_=None, **kwargs)\n</code></pre> <p>Add <code>--env</code> CLI option to a function.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def env_option(func_: Any | None = None, **kwargs: Any) -&gt; Any:\n    \"\"\"Add `--env` CLI option to a function.\"\"\"\n    default_args = {\"type\": str, \"default\": None, \"help\": ENV_HELP}\n    kwargs = {**default_args, **kwargs}\n    opt = click.option(\"--env\", \"-e\", **kwargs)\n    return opt(func_) if func_ else opt\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.is_parameter","title":"is_parameter","text":"<pre><code>is_parameter(dataset_name)\n</code></pre> <p>Check if dataset is a parameter.</p> Source code in <code>kedro/framework/cli/catalog.py</code> <pre><code>def is_parameter(dataset_name: str) -&gt; bool:\n    # TODO: when breaking change move it to kedro/io/core.py\n    \"\"\"Check if dataset is a parameter.\"\"\"\n    return dataset_name.startswith(\"params:\") or dataset_name == \"parameters\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.list_datasets","title":"list_datasets","text":"<pre><code>list_datasets(metadata, pipeline, env)\n</code></pre> <p>Show datasets per type.</p> Source code in <code>kedro/framework/cli/catalog.py</code> <pre><code>@catalog.command(\"list\")\n@env_option\n@click.option(\n    \"--pipeline\",\n    \"-p\",\n    type=str,\n    default=\"\",\n    help=\"Name of the modular pipeline to run. If not set, \"\n    \"the project pipeline is run by default.\",\n    callback=split_string,\n)\n@click.pass_obj\ndef list_datasets(metadata: ProjectMetadata, pipeline: str, env: str) -&gt; None:\n    \"\"\"Show datasets per type.\"\"\"\n    title = \"Datasets in '{}' pipeline\"\n    not_mentioned = \"Datasets not mentioned in pipeline\"\n    mentioned = \"Datasets mentioned in pipeline\"\n    factories = \"Datasets generated from factories\"\n\n    session = _create_session(metadata.package_name, env=env)\n    context = session.load_context()\n    try:\n        data_catalog = context.catalog\n        datasets_meta = data_catalog._datasets\n        catalog_ds = set(data_catalog.list())\n    except Exception as exc:\n        raise KedroCliError(\n            f\"Unable to instantiate Kedro Catalog.\\nError: {exc}\"\n        ) from exc\n\n    target_pipelines = pipeline or pipelines.keys()\n\n    result = {}\n    for pipe in target_pipelines:\n        pl_obj = pipelines.get(pipe)\n        if pl_obj:\n            pipeline_ds = pl_obj.datasets()\n        else:\n            existing_pls = \", \".join(sorted(pipelines.keys()))\n            raise KedroCliError(\n                f\"'{pipe}' pipeline not found! Existing pipelines: {existing_pls}\"\n            )\n\n        unused_ds = catalog_ds - pipeline_ds\n        default_ds = pipeline_ds - catalog_ds\n        used_ds = catalog_ds - unused_ds\n\n        # resolve any factory datasets in the pipeline\n        factory_ds_by_type = defaultdict(list)\n\n        for ds_name in default_ds:\n            if data_catalog.config_resolver.match_pattern(ds_name):\n                ds_config = data_catalog.config_resolver.resolve_pattern(ds_name)\n                factory_ds_by_type[ds_config.get(\"type\", \"DefaultDataset\")].append(\n                    ds_name\n                )\n\n        default_ds = default_ds - set(chain.from_iterable(factory_ds_by_type.values()))\n\n        unused_by_type = _map_type_to_datasets(unused_ds, datasets_meta)\n        used_by_type = _map_type_to_datasets(used_ds, datasets_meta)\n\n        if default_ds:\n            used_by_type[\"DefaultDataset\"].extend(default_ds)\n\n        data = (\n            (mentioned, dict(used_by_type)),\n            (factories, dict(factory_ds_by_type)),\n            (not_mentioned, dict(unused_by_type)),\n        )\n        result[title.format(pipe)] = {key: value for key, value in data if value}\n    secho(yaml.dump(result))\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.rank_catalog_factories","title":"rank_catalog_factories","text":"<pre><code>rank_catalog_factories(metadata, env)\n</code></pre> <p>List all dataset factories in the catalog, ranked by priority by which they are matched.</p> Source code in <code>kedro/framework/cli/catalog.py</code> <pre><code>@catalog.command(\"rank\")\n@env_option\n@click.pass_obj\ndef rank_catalog_factories(metadata: ProjectMetadata, env: str) -&gt; None:\n    \"\"\"List all dataset factories in the catalog, ranked by priority by which they are matched.\"\"\"\n    session = _create_session(metadata.package_name, env=env)\n    context = session.load_context()\n\n    catalog_factories = context.catalog.config_resolver.list_patterns()\n    if catalog_factories:\n        click.echo(yaml.dump(catalog_factories))\n    else:\n        click.echo(\"There are no dataset factories in the catalog.\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.resolve_patterns","title":"resolve_patterns","text":"<pre><code>resolve_patterns(metadata, env)\n</code></pre> <p>Resolve catalog factories against pipeline datasets. Note that this command is runner agnostic and thus won't take into account any default dataset creation defined in the runner.</p> Source code in <code>kedro/framework/cli/catalog.py</code> <pre><code>@catalog.command(\"resolve\")\n@env_option\n@click.pass_obj\ndef resolve_patterns(metadata: ProjectMetadata, env: str) -&gt; None:\n    \"\"\"Resolve catalog factories against pipeline datasets. Note that this command is runner\n    agnostic and thus won't take into account any default dataset creation defined in the runner.\"\"\"\n\n    session = _create_session(metadata.package_name, env=env)\n    context = session.load_context()\n\n    catalog_config = context.config_loader[\"catalog\"]\n    credentials_config = context._get_config_credentials()\n    data_catalog = DataCatalog.from_config(\n        catalog=catalog_config, credentials=credentials_config\n    )\n\n    explicit_datasets = {\n        ds_name: ds_config\n        for ds_name, ds_config in catalog_config.items()\n        if not data_catalog.config_resolver.is_pattern(ds_name)\n    }\n\n    target_pipelines = pipelines.keys()\n    pipeline_datasets = set()\n\n    for pipe in target_pipelines:\n        pl_obj = pipelines.get(pipe)\n        if pl_obj:\n            pipeline_datasets.update(pl_obj.datasets())\n\n    for ds_name in pipeline_datasets:\n        if ds_name in explicit_datasets or is_parameter(ds_name):\n            continue\n\n        ds_config = data_catalog.config_resolver.resolve_pattern(ds_name)\n\n        # Exclude MemoryDatasets not set in the catalog explicitly\n        if ds_config:\n            explicit_datasets[ds_name] = ds_config\n\n    secho(yaml.dump(explicit_datasets))\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.catalog.split_string","title":"split_string","text":"<pre><code>split_string(ctx, param, value)\n</code></pre> <p>Split string by comma.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def split_string(ctx: click.Context, param: Any, value: str) -&gt; list[str]:\n    \"\"\"Split string by comma.\"\"\"\n    return [item.strip() for item in value.split(\",\") if item.strip()]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli","title":"kedro.framework.cli.cli","text":"<p>kedro is a CLI for managing Kedro projects.</p> <p>This module implements commands available from the kedro CLI.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.BRIGHT_BLACK","title":"BRIGHT_BLACK  <code>module-attribute</code>","text":"<pre><code>BRIGHT_BLACK = (128, 128, 128)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.CONTEXT_SETTINGS","title":"CONTEXT_SETTINGS  <code>module-attribute</code>","text":"<pre><code>CONTEXT_SETTINGS = {'help_option_names': ['-h', '--help']}\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.ENTRY_POINT_GROUPS","title":"ENTRY_POINT_GROUPS  <code>module-attribute</code>","text":"<pre><code>ENTRY_POINT_GROUPS = {'global': 'kedro.global_commands', 'project': 'kedro.project_commands', 'init': 'kedro.init', 'line_magic': 'kedro.line_magic', 'hooks': 'kedro.hooks', 'cli_hooks': 'kedro.cli_hooks', 'starters': 'kedro.starters'}\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.LOGGING","title":"LOGGING  <code>module-attribute</code>","text":"<pre><code>LOGGING = _ProjectLogging()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.LOGO","title":"LOGO  <code>module-attribute</code>","text":"<pre><code>LOGO = f'\n _            _\n| | _____  __| |_ __ ___\n| |/ / _ \\/ _` | '__/ _ \\\n|   &lt;  __/ (_| | | | (_) |\n|_|\\_\\___|\\__,_|_|  \\___/\nv{__version__}\n'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.ORANGE","title":"ORANGE  <code>module-attribute</code>","text":"<pre><code>ORANGE = (255, 175, 0)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.version","title":"version  <code>module-attribute</code>","text":"<pre><code>version = '0.19.12'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.CommandCollection","title":"CommandCollection","text":"<pre><code>CommandCollection(*groups)\n</code></pre> <p>               Bases: <code>CommandCollection</code></p> <p>Modified from the Click one to still run the source groups function.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def __init__(self, *groups: tuple[str, Sequence[click.MultiCommand]]):\n    self.groups = [\n        (title, self._merge_same_name_collections(cli_list))\n        for title, cli_list in groups\n    ]\n    sources = list(chain.from_iterable(cli_list for _, cli_list in self.groups))\n    help_texts = [\n        cli.help\n        for cli_collection in sources\n        for cli in cli_collection.sources\n        if cli.help\n    ]\n    super().__init__(\n        sources=sources,  # type: ignore[arg-type]\n        help=\"\\n\\n\".join(help_texts),\n        context_settings=CONTEXT_SETTINGS,\n    )\n    self.params = sources[0].params\n    self.callback = sources[0].callback\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.KedroCLI","title":"KedroCLI","text":"<pre><code>KedroCLI(project_path)\n</code></pre> <p>               Bases: <code>CommandCollection</code></p> <p>A CommandCollection class to encapsulate the KedroCLI command loading.</p> Source code in <code>kedro/framework/cli/cli.py</code> <pre><code>def __init__(self, project_path: Path):\n    self._metadata = None  # running in package mode\n    if _is_project(project_path):\n        self._metadata = bootstrap_project(project_path)\n    self._cli_hook_manager = get_cli_hook_manager()\n\n    super().__init__(\n        (\"Global commands\", self.global_groups),\n        (\"Project specific commands\", self.project_groups),\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.KedroCLI.global_groups","title":"global_groups  <code>property</code>","text":"<pre><code>global_groups\n</code></pre> <p>Property which loads all global command groups from plugins and combines them with the built-in ones (eventually overriding the built-in ones if they are redefined by plugins).</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.KedroCLI.project_groups","title":"project_groups  <code>property</code>","text":"<pre><code>project_groups\n</code></pre> <p>Property which loads all project command groups from the project and the plugins, then combines them with the built-in ones. Built-in commands can be overridden by plugins, which can be overridden by a custom project cli.py. See https://docs.kedro.org/en/stable/extend_kedro/common_use_cases.html#use-case-3-how-to-add-or-modify-cli-commands on how to add this.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.KedroCliError","title":"KedroCliError","text":"<p>               Bases: <code>ClickException</code></p> <p>Exceptions generated from the Kedro CLI.</p> <p>Users should pass an appropriate message at the constructor.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.LazyGroup","title":"LazyGroup","text":"<pre><code>LazyGroup(*args, lazy_subcommands=None, **kwargs)\n</code></pre> <p>               Bases: <code>Group</code></p> <p>A click Group that supports lazy loading of subcommands.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def __init__(\n    self,\n    *args: Any,\n    lazy_subcommands: dict[str, str] | None = None,\n    **kwargs: Any,\n):\n    super().__init__(*args, **kwargs)\n    # lazy_subcommands is a map of the form:\n    #\n    #   {command-name} -&gt; {module-name}.{command-object-name}\n    #\n    self.lazy_subcommands = lazy_subcommands or {}\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli._find_kedro_project","title":"_find_kedro_project","text":"<pre><code>_find_kedro_project(current_dir)\n</code></pre> <p>Given a path, find a Kedro project associated with it.</p> Can be <ul> <li>Itself, if a path is a root directory of a Kedro project.</li> <li>One of its parents, if self is not a Kedro project but one of the parent path is.</li> <li>None, if neither self nor any parent path is a Kedro project.</li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Kedro project associated with a given path,</p> </li> <li> <code>Any</code>           \u2013            <p>or None if no relevant Kedro project is found.</p> </li> </ul> Source code in <code>kedro/utils.py</code> <pre><code>def _find_kedro_project(current_dir: Path) -&gt; Any:  # pragma: no cover\n    \"\"\"Given a path, find a Kedro project associated with it.\n\n    Can be:\n        - Itself, if a path is a root directory of a Kedro project.\n        - One of its parents, if self is not a Kedro project but one of the parent path is.\n        - None, if neither self nor any parent path is a Kedro project.\n\n    Returns:\n        Kedro project associated with a given path,\n        or None if no relevant Kedro project is found.\n    \"\"\"\n    paths_to_check = [current_dir, *list(current_dir.parents)]\n    for parent_dir in paths_to_check:\n        if _is_project(parent_dir):\n            return parent_dir\n    return None\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli._get_entry_points","title":"_get_entry_points","text":"<pre><code>_get_entry_points(name)\n</code></pre> <p>Get all kedro related entry points</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _get_entry_points(name: str) -&gt; Any:\n    \"\"\"Get all kedro related entry points\"\"\"\n    return importlib_metadata.entry_points().select(  # type: ignore[no-untyped-call]\n        group=ENTRY_POINT_GROUPS[name]\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli._init_plugins","title":"_init_plugins","text":"<pre><code>_init_plugins()\n</code></pre> Source code in <code>kedro/framework/cli/cli.py</code> <pre><code>def _init_plugins() -&gt; None:\n    init_hooks = load_entry_points(\"init\")\n    for init_hook in init_hooks:\n        init_hook()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli._is_project","title":"_is_project","text":"<pre><code>_is_project(project_path)\n</code></pre> <p>Evaluate if a given path is a root directory of a Kedro project or not.</p> <p>Parameters:</p> <ul> <li> <code>project_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to be tested for being a root of a Kedro project.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if a given path is a root directory of a Kedro project, otherwise False.</p> </li> </ul> Source code in <code>kedro/utils.py</code> <pre><code>def _is_project(project_path: Union[str, Path]) -&gt; bool:\n    \"\"\"Evaluate if a given path is a root directory of a Kedro project or not.\n\n    Args:\n        project_path: Path to be tested for being a root of a Kedro project.\n\n    Returns:\n        True if a given path is a root directory of a Kedro project, otherwise False.\n    \"\"\"\n    metadata_file = Path(project_path).expanduser().resolve() / _PYPROJECT\n    if not metadata_file.is_file():\n        return False\n\n    try:\n        return \"[tool.kedro]\" in metadata_file.read_text(encoding=\"utf-8\")\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.bootstrap_project","title":"bootstrap_project","text":"<pre><code>bootstrap_project(project_path)\n</code></pre> <p>Run setup required at the beginning of the workflow when running in project mode, and return project metadata.</p> Source code in <code>kedro/framework/startup.py</code> <pre><code>def bootstrap_project(project_path: str | Path) -&gt; ProjectMetadata:\n    \"\"\"Run setup required at the beginning of the workflow\n    when running in project mode, and return project metadata.\n    \"\"\"\n\n    project_path = Path(project_path).expanduser().resolve()\n    metadata = _get_project_metadata(project_path)\n    _add_src_to_path(metadata.source_dir, project_path)\n    configure_project(metadata.package_name)\n    return metadata\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.cli","title":"cli","text":"<pre><code>cli()\n</code></pre> <p>Kedro is a CLI for creating and using Kedro projects. For more information, type <code>kedro info</code>.</p> <p>NOTE: If a command from a plugin conflicts with a built-in command from Kedro, the command from the plugin will take precedence.</p> Source code in <code>kedro/framework/cli/cli.py</code> <pre><code>@click.group(context_settings=CONTEXT_SETTINGS, name=\"Kedro\")\n@click.version_option(version, \"--version\", \"-V\", help=\"Show version and exit\")\ndef cli() -&gt; None:  # pragma: no cover\n    \"\"\"Kedro is a CLI for creating and using Kedro projects. For more\n    information, type ``kedro info``.\n\n    NOTE: If a command from a plugin conflicts with a built-in command from Kedro,\n    the command from the plugin will take precedence.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.get_cli_hook_manager","title":"get_cli_hook_manager","text":"<pre><code>get_cli_hook_manager()\n</code></pre> <p>Create or return the global _hook_manager singleton instance.</p> Source code in <code>kedro/framework/cli/hooks/manager.py</code> <pre><code>def get_cli_hook_manager() -&gt; PluginManager:\n    \"\"\"Create or return the global _hook_manager singleton instance.\"\"\"\n    global _cli_hook_manager  # noqa: PLW0603\n    if _cli_hook_manager is None:\n        _cli_hook_manager = CLIHooksManager()\n    _cli_hook_manager.trace.root.setwriter(logger.debug)\n    _cli_hook_manager.enable_tracing()\n    return _cli_hook_manager\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.global_commands","title":"global_commands","text":"<pre><code>global_commands()\n</code></pre> Source code in <code>kedro/framework/cli/cli.py</code> <pre><code>@click.group(\n    context_settings=CONTEXT_SETTINGS,\n    name=\"Kedro\",\n    cls=LazyGroup,\n    lazy_subcommands={\n        \"new\": \"kedro.framework.cli.starters.new\",\n        \"starter\": \"kedro.framework.cli.starters.starter\",\n    },\n)\ndef global_commands() -&gt; None:\n    pass  # pragma: no cover\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.info","title":"info","text":"<pre><code>info()\n</code></pre> <p>Get more information about kedro.</p> Source code in <code>kedro/framework/cli/cli.py</code> <pre><code>@cli.command()\ndef info() -&gt; None:\n    \"\"\"Get more information about kedro.\"\"\"\n    click.secho(LOGO, fg=\"green\")\n    click.echo(\n        \"Kedro is a Python framework for\\n\"\n        \"creating reproducible, maintainable\\n\"\n        \"and modular data science code.\"\n    )\n\n    plugin_versions = {}\n    plugin_entry_points = defaultdict(set)\n    for plugin_entry_point in ENTRY_POINT_GROUPS:\n        for entry_point in _get_entry_points(plugin_entry_point):\n            module_name = entry_point.module.split(\".\")[0]\n            plugin_versions[module_name] = entry_point.dist.version\n            plugin_entry_points[module_name].add(plugin_entry_point)\n\n    click.echo()\n    if plugin_versions:\n        click.echo(\"Installed plugins:\")\n        for plugin_name, plugin_version in sorted(plugin_versions.items()):\n            entrypoints_str = \",\".join(sorted(plugin_entry_points[plugin_name]))\n            click.echo(\n                f\"{plugin_name}: {plugin_version} (entry points:{entrypoints_str})\"\n            )\n    else:  # pragma: no cover\n        click.echo(\"No plugins installed\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.load_entry_points","title":"load_entry_points","text":"<pre><code>load_entry_points(name)\n</code></pre> <p>Load package entry point commands.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The key value specified in ENTRY_POINT_GROUPS.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KedroCliError</code>             \u2013            <p>If loading an entry point failed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[MultiCommand]</code>           \u2013            <p>List of entry point commands.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def load_entry_points(name: str) -&gt; Sequence[click.MultiCommand]:\n    \"\"\"Load package entry point commands.\n\n    Args:\n        name: The key value specified in ENTRY_POINT_GROUPS.\n\n    Raises:\n        KedroCliError: If loading an entry point failed.\n\n    Returns:\n        List of entry point commands.\n\n    \"\"\"\n\n    entry_point_commands = []\n    for entry_point in _get_entry_points(name):\n        loaded_entry_point = _safe_load_entry_point(entry_point)\n        if loaded_entry_point:\n            entry_point_commands.append(loaded_entry_point)\n    return entry_point_commands\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Main entry point. Look for a <code>cli.py</code>, and, if found, add its commands to <code>kedro</code>'s before invoking the CLI.</p> Source code in <code>kedro/framework/cli/cli.py</code> <pre><code>def main() -&gt; None:  # pragma: no cover\n    \"\"\"Main entry point. Look for a ``cli.py``, and, if found, add its\n    commands to `kedro`'s before invoking the CLI.\n    \"\"\"\n    _init_plugins()\n    cli_collection = KedroCLI(\n        project_path=_find_kedro_project(Path.cwd()) or Path.cwd()\n    )\n    cli_collection()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.cli.project_commands","title":"project_commands","text":"<pre><code>project_commands()\n</code></pre> Source code in <code>kedro/framework/cli/cli.py</code> <pre><code>@click.group(\n    context_settings=CONTEXT_SETTINGS,\n    cls=LazyGroup,\n    name=\"Kedro\",\n    lazy_subcommands={\n        \"registry\": \"kedro.framework.cli.registry.registry\",\n        \"catalog\": \"kedro.framework.cli.catalog.catalog\",\n        \"ipython\": \"kedro.framework.cli.project.ipython\",\n        \"run\": \"kedro.framework.cli.project.run\",\n        \"micropkg\": \"kedro.framework.cli.micropkg.micropkg\",\n        \"package\": \"kedro.framework.cli.project.package\",\n        \"jupyter\": \"kedro.framework.cli.jupyter.jupyter\",\n        \"pipeline\": \"kedro.framework.cli.pipeline.pipeline\",\n    },\n)\ndef project_commands() -&gt; None:\n    pass  # pragma: no cover\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.hooks","title":"kedro.framework.cli.hooks","text":"<p><code>kedro.framework.cli.hooks</code> provides primitives to use hooks to extend KedroCLI's behaviour</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.hooks.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['CLIHooksManager', 'cli_hook_impl', 'get_cli_hook_manager']\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.hooks.cli_hook_impl","title":"cli_hook_impl  <code>module-attribute</code>","text":"<pre><code>cli_hook_impl = HookimplMarker(CLI_HOOK_NAMESPACE)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.hooks.CLIHooksManager","title":"CLIHooksManager","text":"<pre><code>CLIHooksManager()\n</code></pre> <p>               Bases: <code>PluginManager</code></p> <p>Hooks manager to manage CLI hooks</p> Source code in <code>kedro/framework/cli/hooks/manager.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__(CLI_HOOK_NAMESPACE)\n    self.add_hookspecs(CLICommandSpecs)\n    self._register_cli_hooks()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.hooks.get_cli_hook_manager","title":"get_cli_hook_manager","text":"<pre><code>get_cli_hook_manager()\n</code></pre> <p>Create or return the global _hook_manager singleton instance.</p> Source code in <code>kedro/framework/cli/hooks/manager.py</code> <pre><code>def get_cli_hook_manager() -&gt; PluginManager:\n    \"\"\"Create or return the global _hook_manager singleton instance.\"\"\"\n    global _cli_hook_manager  # noqa: PLW0603\n    if _cli_hook_manager is None:\n        _cli_hook_manager = CLIHooksManager()\n    _cli_hook_manager.trace.root.setwriter(logger.debug)\n    _cli_hook_manager.enable_tracing()\n    return _cli_hook_manager\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter","title":"kedro.framework.cli.jupyter","text":"<p>A collection of helper functions to integrate with Jupyter/IPython and CLI commands for working with Kedro catalog.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter.KedroCliError","title":"KedroCliError","text":"<p>               Bases: <code>ClickException</code></p> <p>Exceptions generated from the Kedro CLI.</p> <p>Users should pass an appropriate message at the constructor.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter.ProjectMetadata","title":"ProjectMetadata","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Structure holding project metadata derived from <code>pyproject.toml</code></p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter._check_module_importable","title":"_check_module_importable","text":"<pre><code>_check_module_importable(module_name)\n</code></pre> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _check_module_importable(module_name: str) -&gt; None:\n    try:\n        import_module(module_name)\n    except ImportError as exc:\n        raise KedroCliError(\n            f\"Module '{module_name}' not found. Make sure to install required project \"\n            f\"dependencies by running the 'pip install -r requirements.txt' command first.\"\n        ) from exc\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter._create_kernel","title":"_create_kernel","text":"<pre><code>_create_kernel(kernel_name, display_name)\n</code></pre> <p>Creates an IPython kernel for the kedro project. If one with the same kernel_name exists already it will be replaced.</p> <p>Installs the default IPython kernel (which points towards <code>sys.executable</code>) and customises it to make the launch command load the kedro extension. This is equivalent to the method recommended for creating a custom IPython kernel on the CLI: https://ipython.readthedocs.io/en/stable/install/kernel_install.html.</p> <p>On linux this creates a directory ~/.local/share/jupyter/kernels/{kernel_name} containing kernel.json, logo-32x32.png, logo-64x64.png and logo-svg.svg. An example kernel.json looks as follows:</p> <p>{  \"argv\": [   \"/Users/antony_milne/miniconda3/envs/spaceflights/bin/python\",   \"-m\",   \"ipykernel_launcher\",   \"-f\",   \"{connection_file}\",   \"--ext\",   \"kedro.ipython\"  ],  \"display_name\": \"Kedro (spaceflights)\",  \"language\": \"python\",  \"metadata\": {   \"debugger\": false  } }</p> <p>Parameters:</p> <ul> <li> <code>kernel_name</code>               (<code>str</code>)           \u2013            <p>Name of the kernel to create.</p> </li> <li> <code>display_name</code>               (<code>str</code>)           \u2013            <p>Kernel name as it is displayed in the UI.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>String of the path of the created kernel.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KedroCliError</code>             \u2013            <p>When kernel cannot be setup.</p> </li> </ul> Source code in <code>kedro/framework/cli/jupyter.py</code> <pre><code>def _create_kernel(kernel_name: str, display_name: str) -&gt; str:\n    \"\"\"Creates an IPython kernel for the kedro project. If one with the same kernel_name\n    exists already it will be replaced.\n\n    Installs the default IPython kernel (which points towards `sys.executable`)\n    and customises it to make the launch command load the kedro extension.\n    This is equivalent to the method recommended for creating a custom IPython kernel\n    on the CLI: https://ipython.readthedocs.io/en/stable/install/kernel_install.html.\n\n    On linux this creates a directory ~/.local/share/jupyter/kernels/{kernel_name}\n    containing kernel.json, logo-32x32.png, logo-64x64.png and logo-svg.svg. An example kernel.json\n    looks as follows:\n\n    {\n     \"argv\": [\n      \"/Users/antony_milne/miniconda3/envs/spaceflights/bin/python\",\n      \"-m\",\n      \"ipykernel_launcher\",\n      \"-f\",\n      \"{connection_file}\",\n      \"--ext\",\n      \"kedro.ipython\"\n     ],\n     \"display_name\": \"Kedro (spaceflights)\",\n     \"language\": \"python\",\n     \"metadata\": {\n      \"debugger\": false\n     }\n    }\n\n    Args:\n        kernel_name: Name of the kernel to create.\n        display_name: Kernel name as it is displayed in the UI.\n\n    Returns:\n        String of the path of the created kernel.\n\n    Raises:\n        KedroCliError: When kernel cannot be setup.\n    \"\"\"\n    # These packages are required by jupyter lab and notebook, which we have already\n    # checked are importable, so we don't run _check_module_importable on them.\n    from ipykernel.kernelspec import install\n\n    try:\n        # Install with user=True rather than system-wide to minimise footprint and\n        # ensure that we have permissions to write there. Under the hood this calls\n        # jupyter_client.KernelSpecManager.install_kernel_spec, which automatically\n        # removes an old kernel spec if it already exists.\n        kernel_path = install(\n            user=True,\n            kernel_name=kernel_name,\n            display_name=display_name,\n        )\n\n        kernel_json = Path(kernel_path) / \"kernel.json\"\n        kernel_spec = json.loads(kernel_json.read_text(encoding=\"utf-8\"))\n        kernel_spec[\"argv\"].extend([\"--ext\", \"kedro.ipython\"])\n        # indent=1 is to match the default ipykernel style (see\n        # ipykernel.write_kernel_spec).\n        kernel_json.write_text(json.dumps(kernel_spec, indent=1), encoding=\"utf-8\")\n\n        kedro_ipython_dir = Path(__file__).parents[2] / \"ipython\"\n        shutil.copy(kedro_ipython_dir / \"logo-32x32.png\", kernel_path)\n        shutil.copy(kedro_ipython_dir / \"logo-64x64.png\", kernel_path)\n        shutil.copy(kedro_ipython_dir / \"logo-svg.svg\", kernel_path)\n    except Exception as exc:\n        raise KedroCliError(\n            f\"Cannot setup kedro kernel for Jupyter.\\nError: {exc}\"\n        ) from exc\n    return kernel_path\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter.env_option","title":"env_option","text":"<pre><code>env_option(func_=None, **kwargs)\n</code></pre> <p>Add <code>--env</code> CLI option to a function.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def env_option(func_: Any | None = None, **kwargs: Any) -&gt; Any:\n    \"\"\"Add `--env` CLI option to a function.\"\"\"\n    default_args = {\"type\": str, \"default\": None, \"help\": ENV_HELP}\n    kwargs = {**default_args, **kwargs}\n    opt = click.option(\"--env\", \"-e\", **kwargs)\n    return opt(func_) if func_ else opt\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter.forward_command","title":"forward_command","text":"<pre><code>forward_command(group, name=None, forward_help=False)\n</code></pre> <p>A command that receives the rest of the command line as 'args'.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def forward_command(\n    group: Any, name: str | None = None, forward_help: bool = False\n) -&gt; Any:\n    \"\"\"A command that receives the rest of the command line as 'args'.\"\"\"\n\n    def wrapit(func: Any) -&gt; Any:\n        func = click.argument(\"args\", nargs=-1, type=click.UNPROCESSED)(func)\n        func = command_with_verbosity(\n            group,\n            name=name,\n            context_settings={\n                \"ignore_unknown_options\": True,\n                \"help_option_names\": [] if forward_help else [\"-h\", \"--help\"],\n            },\n        )(func)\n        return func\n\n    return wrapit\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter.jupyter","title":"jupyter","text":"<pre><code>jupyter()\n</code></pre> <p>Open Jupyter Notebook / Lab with project specific variables loaded.</p> Source code in <code>kedro/framework/cli/jupyter.py</code> <pre><code>@jupyter_cli.group()\ndef jupyter() -&gt; None:\n    \"\"\"Open Jupyter Notebook / Lab with project specific variables loaded.\"\"\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter.jupyter_cli","title":"jupyter_cli","text":"<pre><code>jupyter_cli()\n</code></pre> Source code in <code>kedro/framework/cli/jupyter.py</code> <pre><code>@click.group(name=\"Kedro\")\ndef jupyter_cli() -&gt; None:  # pragma: no cover\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter.jupyter_lab","title":"jupyter_lab","text":"<pre><code>jupyter_lab(metadata, /, env, args, **kwargs)\n</code></pre> <p>Open Jupyter Lab with project specific variables loaded.</p> Source code in <code>kedro/framework/cli/jupyter.py</code> <pre><code>@forward_command(jupyter, \"lab\", forward_help=True)\n@env_option\n@click.pass_obj  # this will pass the metadata as first argument\ndef jupyter_lab(\n    metadata: ProjectMetadata,\n    /,\n    env: str,\n    args: Any,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Open Jupyter Lab with project specific variables loaded.\"\"\"\n    _check_module_importable(\"jupyterlab\")\n    validate_settings()\n\n    kernel_name = f\"kedro_{metadata.package_name}\"\n    _create_kernel(kernel_name, f\"Kedro ({metadata.package_name})\")\n\n    if env:\n        os.environ[\"KEDRO_ENV\"] = env\n\n    python_call(\n        \"jupyter\",\n        [\"lab\", f\"--MultiKernelManager.default_kernel_name={kernel_name}\", *list(args)],\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter.jupyter_notebook","title":"jupyter_notebook","text":"<pre><code>jupyter_notebook(metadata, /, env, args, **kwargs)\n</code></pre> <p>Open Jupyter Notebook with project specific variables loaded.</p> Source code in <code>kedro/framework/cli/jupyter.py</code> <pre><code>@forward_command(jupyter, \"notebook\", forward_help=True)\n@env_option\n@click.pass_obj  # this will pass the metadata as first argument\ndef jupyter_notebook(\n    metadata: ProjectMetadata,\n    /,\n    env: str,\n    args: Any,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Open Jupyter Notebook with project specific variables loaded.\"\"\"\n    _check_module_importable(\"notebook\")\n    validate_settings()\n\n    kernel_name = f\"kedro_{metadata.package_name}\"\n    _create_kernel(kernel_name, f\"Kedro ({metadata.package_name})\")\n\n    if env:\n        os.environ[\"KEDRO_ENV\"] = env\n\n    python_call(\n        \"jupyter\",\n        [\n            \"notebook\",\n            f\"--MultiKernelManager.default_kernel_name={kernel_name}\",\n            *list(args),\n        ],\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter.python_call","title":"python_call","text":"<pre><code>python_call(module, arguments, **kwargs)\n</code></pre> <p>Run a subprocess command that invokes a Python module.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def python_call(\n    module: str, arguments: Iterable[str], **kwargs: Any\n) -&gt; None:  # pragma: no cover\n    \"\"\"Run a subprocess command that invokes a Python module.\"\"\"\n    call([sys.executable, \"-m\", module, *list(arguments)], **kwargs)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter.setup","title":"setup","text":"<pre><code>setup(metadata, /, args, **kwargs)\n</code></pre> <p>Initialise the Jupyter Kernel for a kedro project.</p> Source code in <code>kedro/framework/cli/jupyter.py</code> <pre><code>@forward_command(jupyter, \"setup\", forward_help=True)\n@click.pass_obj  # this will pass the metadata as first argument\ndef setup(metadata: ProjectMetadata, /, args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialise the Jupyter Kernel for a kedro project.\"\"\"\n    _check_module_importable(\"ipykernel\")\n    validate_settings()\n\n    kernel_name = f\"kedro_{metadata.package_name}\"\n    kernel_path = _create_kernel(kernel_name, f\"Kedro ({metadata.package_name})\")\n    click.secho(f\"\\nThe kernel has been created successfully at {kernel_path}\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.jupyter.validate_settings","title":"validate_settings","text":"<pre><code>validate_settings()\n</code></pre> <p>Eagerly validate that the settings module is importable if it exists. This is desirable to surface any syntax or import errors early. In particular, without eagerly importing the settings module, dynaconf would silence any import error (e.g. missing dependency, missing/mislabelled pipeline), and users would instead get a cryptic error message <code>Expected an instance of `ConfigLoader`, got `NoneType` instead</code>. More info on the dynaconf issue: https://github.com/dynaconf/dynaconf/issues/460</p> Source code in <code>kedro/framework/project/__init__.py</code> <pre><code>def validate_settings() -&gt; None:\n    \"\"\"Eagerly validate that the settings module is importable if it exists. This is desirable to\n    surface any syntax or import errors early. In particular, without eagerly importing\n    the settings module, dynaconf would silence any import error (e.g. missing\n    dependency, missing/mislabelled pipeline), and users would instead get a cryptic\n    error message ``Expected an instance of `ConfigLoader`, got `NoneType` instead``.\n    More info on the dynaconf issue: https://github.com/dynaconf/dynaconf/issues/460\n    \"\"\"\n    if PACKAGE_NAME is None:\n        raise ValueError(\n            \"Package name not found. Make sure you have configured the project using \"\n            \"'bootstrap_project'. This should happen automatically if you are using \"\n            \"Kedro command line interface.\"\n        )\n    # Check if file exists, if it does, validate it.\n    if importlib.util.find_spec(f\"{PACKAGE_NAME}.settings\") is not None:\n        importlib.import_module(f\"{PACKAGE_NAME}.settings\")\n    else:\n        logger = logging.getLogger(__name__)\n        logger.warning(\"No 'settings.py' found, defaults will be used.\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg","title":"kedro.framework.cli.micropkg","text":"<p>A collection of CLI commands for working with Kedro micro-packages.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._PYPROJECT_TOML_TEMPLATE","title":"_PYPROJECT_TOML_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>_PYPROJECT_TOML_TEMPLATE = '\\n[build-system]\\nrequires = [\"setuptools\"]\\nbuild-backend = \"setuptools.build_meta\"\\n\\n[project]\\nname = \"{name}\"\\nversion = \"{version}\"\\ndescription = \"Micro-package `{name}`\"\\ndependencies = {install_requires}\\n\\n[tool.setuptools.packages]\\nfind = {{}}\\n'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg.KedroCliError","title":"KedroCliError","text":"<p>               Bases: <code>ClickException</code></p> <p>Exceptions generated from the Kedro CLI.</p> <p>Users should pass an appropriate message at the constructor.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg.ProjectMetadata","title":"ProjectMetadata","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Structure holding project metadata derived from <code>pyproject.toml</code></p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._EquivalentRequirement","title":"_EquivalentRequirement","text":"<p>               Bases: <code>Requirement</code></p> <p>Parse a requirement according to PEP 508.</p> <p>This class overrides eq to be backwards compatible with pkg_resources.Requirement while making str and hash use the non-canonicalized name as agreed in https://github.com/pypa/packaging/issues/644,</p> <p>Implementation taken from https://github.com/pypa/packaging/pull/696/</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._append_package_reqs","title":"_append_package_reqs","text":"<pre><code>_append_package_reqs(requirements_txt, package_reqs, package_name)\n</code></pre> <p>Appends micro-package requirements to project level requirements.txt</p> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _append_package_reqs(\n    requirements_txt: Path, package_reqs: list[str], package_name: str\n) -&gt; None:\n    \"\"\"Appends micro-package requirements to project level requirements.txt\"\"\"\n    incoming_reqs = _safe_parse_requirements(package_reqs)\n    if requirements_txt.is_file():\n        existing_reqs = _safe_parse_requirements(requirements_txt.read_text())\n        reqs_to_add = set(incoming_reqs) - set(existing_reqs)\n        if not reqs_to_add:\n            return\n\n        sorted_reqs = sorted(str(req) for req in reqs_to_add)\n        sep = \"\\n\"\n        with open(requirements_txt, \"a\", encoding=\"utf-8\") as file:\n            file.write(\n                f\"\\n\\n# Additional requirements from micro-package `{package_name}`:\\n\"\n            )\n            file.write(sep.join(sorted_reqs))\n        click.secho(\n            f\"Added the following requirements from micro-package '{package_name}' to \"\n            f\"requirements.txt:\\n{sep.join(sorted_reqs)}\"\n        )\n    else:\n        click.secho(\n            \"No project requirements.txt found. Copying contents from project requirements.txt...\"\n        )\n        sorted_reqs = sorted(str(req) for req in incoming_reqs)\n        sep = \"\\n\"\n        with open(requirements_txt, \"a\", encoding=\"utf-8\") as file:\n            file.write(sep.join(sorted_reqs))\n\n    click.secho(\n        \"Use 'pip-compile requirements.txt --output-file requirements.lock' to compile \"\n        \"and 'pip install -r requirements.lock' to install the updated list of requirements.\"\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._assert_pkg_name_ok","title":"_assert_pkg_name_ok","text":"<pre><code>_assert_pkg_name_ok(pkg_name)\n</code></pre> <p>Check that python package name is in line with PEP8 requirements.</p> <p>Parameters:</p> <ul> <li> <code>pkg_name</code>               (<code>str</code>)           \u2013            <p>Candidate Python package name.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KedroCliError</code>             \u2013            <p>If package name violates the requirements.</p> </li> </ul> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _assert_pkg_name_ok(pkg_name: str) -&gt; None:\n    \"\"\"Check that python package name is in line with PEP8 requirements.\n\n    Args:\n        pkg_name: Candidate Python package name.\n\n    Raises:\n        KedroCliError: If package name violates the requirements.\n    \"\"\"\n\n    base_message = f\"'{pkg_name}' is not a valid Python package name.\"\n    if not re.match(r\"^[a-zA-Z_]\", pkg_name):\n        message = base_message + \" It must start with a letter or underscore.\"\n        raise KedroCliError(message)\n    if len(pkg_name) &lt; 2:  # noqa: PLR2004\n        message = base_message + \" It must be at least 2 characters long.\"\n        raise KedroCliError(message)\n    if not re.match(r\"^\\w+$\", pkg_name[1:]):\n        message = (\n            base_message + \" It must contain only letters, digits, and/or underscores.\"\n        )\n        raise KedroCliError(message)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._check_module_path","title":"_check_module_path","text":"<pre><code>_check_module_path(ctx, param, value)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _check_module_path(ctx: click.core.Context, param: Any, value: str) -&gt; str:\n    if value and not re.match(r\"^[\\w.]+$\", value):\n        message = (\n            \"The micro-package location you provided is not a valid Python module path\"\n        )\n        raise KedroCliError(message)\n    return value\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._check_pipeline_name","title":"_check_pipeline_name","text":"<pre><code>_check_pipeline_name(ctx, param, value)\n</code></pre> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _check_pipeline_name(ctx: click.Context, param: Any, value: str) -&gt; str:\n    if value:\n        _assert_pkg_name_ok(value)\n    return value\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._clean_pycache","title":"_clean_pycache","text":"<pre><code>_clean_pycache(path)\n</code></pre> <p>Recursively clean all pycache folders from <code>path</code>.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>Existing local directory to clean pycache folders from.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _clean_pycache(path: Path) -&gt; None:\n    \"\"\"Recursively clean all __pycache__ folders from `path`.\n\n    Args:\n        path: Existing local directory to clean __pycache__ folders from.\n    \"\"\"\n    to_delete = [each.resolve() for each in path.rglob(\"__pycache__\")]\n\n    for each in to_delete:\n        shutil.rmtree(each, ignore_errors=True)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._create_nested_package","title":"_create_nested_package","text":"<pre><code>_create_nested_package(project, package_path)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _create_nested_package(project: Project, package_path: Path) -&gt; Path:\n    # fails if parts of the path exists already\n    packages = package_path.parts\n    parent = generate.create_package(project, packages[0])\n    nested_path = Path(project.address) / packages[0]\n    for package in packages[1:]:\n        parent = generate.create_package(project, package, sourcefolder=parent)\n        nested_path = nested_path / package\n    return nested_path\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._drop_comment","title":"_drop_comment","text":"<pre><code>_drop_comment(line)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _drop_comment(line: str) -&gt; str:\n    # https://github.com/pypa/setuptools/blob/b545fc7/\\\n    # pkg_resources/_vendor/jaraco/text/__init__.py#L554-L566\n    return line.partition(\" #\")[0]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._find_config_files","title":"_find_config_files","text":"<pre><code>_find_config_files(source_config_dir, glob_patterns)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _find_config_files(\n    source_config_dir: Path, glob_patterns: list[str]\n) -&gt; list[tuple[Path, str]]:\n    config_files: list[tuple[Path, str]] = []\n\n    if source_config_dir.is_dir():\n        config_files = [\n            (path, path.parent.relative_to(source_config_dir).as_posix())\n            for glob_pattern in glob_patterns\n            for path in source_config_dir.glob(glob_pattern)\n            if path.is_file()\n        ]\n\n    return config_files\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._generate_manifest_file","title":"_generate_manifest_file","text":"<pre><code>_generate_manifest_file(output_dir)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _generate_manifest_file(output_dir: Path) -&gt; None:\n    manifest_file = output_dir / \"MANIFEST.in\"\n    manifest_file.write_text(\n        \"\"\"\n        global-include README.md\n        global-include config/parameters*\n        global-include config/**/parameters*\n        global-include config/parameters*/**\n        global-include config/parameters*/**/*\n        \"\"\"\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._generate_pyproject_file","title":"_generate_pyproject_file","text":"<pre><code>_generate_pyproject_file(package_name, version, install_requires, output_dir)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _generate_pyproject_file(\n    package_name: str, version: str, install_requires: list[str], output_dir: Path\n) -&gt; Path:\n    pyproject_file = output_dir / \"pyproject.toml\"\n\n    pyproject_file_context = {\n        \"name\": package_name,\n        \"version\": version,\n        \"install_requires\": install_requires,\n    }\n\n    pyproject_file.write_text(_PYPROJECT_TOML_TEMPLATE.format(**pyproject_file_context))\n    return pyproject_file\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._generate_sdist_file","title":"_generate_sdist_file","text":"<pre><code>_generate_sdist_file(micropkg_name, destination, source_paths, version, metadata, alias=None)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _generate_sdist_file(  # noqa: PLR0913,too-many-locals\n    micropkg_name: str,\n    destination: Path,\n    source_paths: tuple[Path, Path, list[tuple[Path, str]]],\n    version: str,\n    metadata: ProjectMetadata,\n    alias: str | None = None,\n) -&gt; None:\n    package_name = alias or micropkg_name\n    package_source, tests_source, conf_source = source_paths\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        temp_dir_path = Path(temp_dir).resolve()\n\n        project = Project(temp_dir_path)  # project where to do refactoring\n        _refactor_code_for_package(\n            project,\n            package_source,\n            tests_source,\n            alias,\n            metadata,\n        )\n        project.close()\n\n        # Copy &amp; \"refactor\" config\n        _, _, conf_target = _get_package_artifacts(temp_dir_path, package_name)\n        _sync_path_list(conf_source, conf_target)\n        if conf_target.is_dir() and alias:\n            _rename_files(conf_target, micropkg_name, alias)\n\n        # Build a pyproject.toml on the fly\n        try:\n            install_requires = _make_install_requires(\n                package_source / \"requirements.txt\"\n            )\n        except Exception as exc:\n            click.secho(\"FAILED\", fg=\"red\")\n            cls = exc.__class__\n            raise KedroCliError(f\"{cls.__module__}.{cls.__qualname__}: {exc}\") from exc\n\n        _generate_manifest_file(temp_dir_path)\n        _generate_pyproject_file(package_name, version, install_requires, temp_dir_path)\n\n        package_file = destination / _get_sdist_name(name=package_name, version=version)\n\n        if package_file.is_file():\n            click.secho(\n                f\"Package file {package_file} will be overwritten!\", fg=\"yellow\"\n            )\n\n        # python -m build --outdir &lt;destination&gt;\n        call(\n            [\n                sys.executable,\n                \"-m\",\n                \"build\",\n                \"--sdist\",\n                \"--outdir\",\n                str(destination),\n            ],\n            cwd=temp_dir,\n        )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._get_all_library_reqs","title":"_get_all_library_reqs","text":"<pre><code>_get_all_library_reqs(metadata)\n</code></pre> <p>Get all library requirements from metadata, leaving markers intact.</p> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _get_all_library_reqs(metadata: PackageMetadata) -&gt; list[str]:\n    \"\"\"Get all library requirements from metadata, leaving markers intact.\"\"\"\n    # See https://discuss.python.org/t/\\\n    # programmatically-getting-non-optional-requirements-of-current-directory/26963/2\n    return [\n        str(_EquivalentRequirement(dep_str))\n        for dep_str in metadata.get_all(\"Requires-Dist\", [])\n    ]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._get_artifacts_to_package","title":"_get_artifacts_to_package","text":"<pre><code>_get_artifacts_to_package(project_metadata, module_path, env)\n</code></pre> <p>From existing project, returns in order: source_path, tests_path, config_paths</p> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _get_artifacts_to_package(\n    project_metadata: ProjectMetadata, module_path: str, env: str\n) -&gt; tuple[Path, Path, Path]:\n    \"\"\"From existing project, returns in order: source_path, tests_path, config_paths\"\"\"\n    package_dir = project_metadata.source_dir / project_metadata.package_name\n    project_root = project_metadata.project_path\n    project_conf_path = project_metadata.project_path / settings.CONF_SOURCE\n    artifacts = (\n        Path(package_dir, *module_path.split(\".\")),\n        Path(project_root, \"tests\", *module_path.split(\".\")),\n        project_conf_path / env,\n    )\n    return artifacts\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._get_default_version","title":"_get_default_version","text":"<pre><code>_get_default_version(metadata, micropkg_module_path)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _get_default_version(metadata: ProjectMetadata, micropkg_module_path: str) -&gt; str:\n    # default to micropkg package version\n    try:\n        micropkg_module = import_module(\n            f\"{metadata.package_name}.{micropkg_module_path}\"\n        )\n        return micropkg_module.__version__  # type: ignore[no-any-return]\n    except (AttributeError, ModuleNotFoundError):\n        logger.warning(\n            \"Micropackage version not found in '%s.%s', will take the top-level one in '%s'\",\n            metadata.package_name,\n            micropkg_module_path,\n            metadata.package_name,\n        )\n        # if micropkg version doesn't exist, take the project one\n        project_module = import_module(f\"{metadata.package_name}\")\n        return project_module.__version__  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._get_fsspec_filesystem","title":"_get_fsspec_filesystem","text":"<pre><code>_get_fsspec_filesystem(location, fs_args)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _get_fsspec_filesystem(location: str, fs_args: str | None) -&gt; Any:\n    import fsspec\n\n    from kedro.io.core import get_protocol_and_path\n\n    protocol, _ = get_protocol_and_path(location)\n    fs_args_config = OmegaConf.to_container(OmegaConf.load(fs_args)) if fs_args else {}\n\n    try:\n        return fsspec.filesystem(protocol, **fs_args_config)\n    except Exception as exc:\n        # Specified protocol is not supported by `fsspec`\n        # or requires extra dependencies\n        click.secho(str(exc), fg=\"red\")\n        click.secho(\"Trying to use 'pip download'...\", fg=\"red\")\n        return None\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._get_package_artifacts","title":"_get_package_artifacts","text":"<pre><code>_get_package_artifacts(source_path, package_name)\n</code></pre> <p>From existing package, returns in order: source_path, tests_path, config_path</p> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _get_package_artifacts(\n    source_path: Path, package_name: str\n) -&gt; tuple[Path, Path, Path]:\n    \"\"\"From existing package, returns in order:\n    source_path, tests_path, config_path\n    \"\"\"\n    artifacts = (\n        source_path / package_name,\n        source_path / \"tests\",\n        # package_data (non-python files) needs to live inside one of the packages\n        source_path / package_name / \"config\",\n    )\n    return artifacts\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._get_sdist_name","title":"_get_sdist_name","text":"<pre><code>_get_sdist_name(name, version)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _get_sdist_name(name: str, version: str) -&gt; str:\n    return f\"{name}-{version}.tar.gz\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._install_files","title":"_install_files","text":"<pre><code>_install_files(project_metadata, package_name, source_path, env=None, alias=None, destination=None)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _install_files(  # noqa: PLR0913, too-many-locals\n    project_metadata: ProjectMetadata,\n    package_name: str,\n    source_path: Path,\n    env: str | None = None,\n    alias: str | None = None,\n    destination: str | None = None,\n) -&gt; None:\n    env = env or \"base\"\n\n    package_source, test_source, conf_source = _get_package_artifacts(\n        source_path, package_name\n    )\n\n    if conf_source.is_dir() and alias:\n        _rename_files(conf_source, package_name, alias)\n\n    module_path = alias or package_name\n    if destination:\n        module_path = f\"{destination}.{module_path}\"\n\n    package_dest, test_dest, conf_dest = _get_artifacts_to_package(\n        project_metadata, module_path=module_path, env=env\n    )\n\n    if conf_source.is_dir():\n        _sync_dirs(conf_source, conf_dest)\n        # `config` dir was packaged under `package_name` directory with\n        # `kedro micropkg package`. Since `config` was already synced,\n        # we don't want to copy it again when syncing the package, so we remove it.\n        shutil.rmtree(str(conf_source))\n\n    project = Project(source_path)\n    refactored_package_source, refactored_test_source = _refactor_code_for_unpacking(\n        project, package_source, test_source, alias, destination, project_metadata\n    )\n    project.close()\n\n    if refactored_test_source.is_dir():\n        _sync_dirs(refactored_test_source, test_dest)\n\n    # Sync everything under package directory, except `config`\n    # since it has already been copied.\n    if refactored_package_source.is_dir():\n        _sync_dirs(refactored_package_source, package_dest)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._is_within_directory","title":"_is_within_directory","text":"<pre><code>_is_within_directory(directory, target)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _is_within_directory(directory: Path, target: Path) -&gt; bool:\n    abs_directory = directory.resolve()\n    abs_target = target.resolve()\n    return abs_directory in abs_target.parents\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._make_install_requires","title":"_make_install_requires","text":"<pre><code>_make_install_requires(requirements_txt)\n</code></pre> <p>Parses each line of requirements.txt into a version specifier valid to put in install_requires. Matches pkg_resources.parse_requirements</p> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _make_install_requires(requirements_txt: Path) -&gt; list[str]:\n    \"\"\"Parses each line of requirements.txt into a version specifier valid to put in\n    install_requires.\n    Matches pkg_resources.parse_requirements\"\"\"\n    if not requirements_txt.exists():\n        return []\n    return [\n        str(_EquivalentRequirement(_drop_comment(requirement_line)))\n        for requirement_line in requirements_txt.read_text().splitlines()\n        if requirement_line and not requirement_line.startswith(\"#\")\n    ]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._move_package","title":"_move_package","text":"<pre><code>_move_package(project, source, target)\n</code></pre> <p>Move a Python package, refactoring relevant imports along the way. A target of empty string means moving to the root of the <code>project</code>.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>Project</code>)           \u2013            <p>rope.base.Project holding the scope of the refactoring.</p> </li> <li> <code>source</code>               (<code>str</code>)           \u2013            <p>Name of the Python package to be moved. Can be a fully qualified module path relative to the <code>project</code> root, e.g. \"package.pipelines.pipeline\" or \"package/pipelines/pipeline\".</p> </li> <li> <code>target</code>               (<code>str</code>)           \u2013            <p>Destination of the Python package to be moved. Can be a fully qualified module path relative to the <code>project</code> root, e.g. \"package.pipelines.pipeline\" or \"package/pipelines/pipeline\".</p> </li> </ul> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _move_package(project: Project, source: str, target: str) -&gt; None:\n    \"\"\"\n    Move a Python package, refactoring relevant imports along the way.\n    A target of empty string means moving to the root of the `project`.\n\n    Args:\n        project: rope.base.Project holding the scope of the refactoring.\n        source: Name of the Python package to be moved. Can be a fully\n            qualified module path relative to the `project` root, e.g.\n            \"package.pipelines.pipeline\" or \"package/pipelines/pipeline\".\n        target: Destination of the Python package to be moved. Can be a fully\n            qualified module path relative to the `project` root, e.g.\n            \"package.pipelines.pipeline\" or \"package/pipelines/pipeline\".\n    \"\"\"\n    src_folder = project.get_module(source).get_resource()\n    target_folder = project.get_module(target).get_resource()\n    change = MoveModule(project, src_folder).get_changes(dest=target_folder)\n    project.do(change)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._package_micropkg","title":"_package_micropkg","text":"<pre><code>_package_micropkg(micropkg_module_path, metadata, alias=None, destination=None, env=None)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _package_micropkg(\n    micropkg_module_path: str,\n    metadata: ProjectMetadata,\n    alias: str | None = None,\n    destination: str | None = None,\n    env: str | None = None,\n) -&gt; Path:\n    micropkg_name = micropkg_module_path.split(\".\")[-1]\n    package_dir = metadata.source_dir / metadata.package_name\n    env = env or \"base\"\n\n    package_source, package_tests, package_conf = _get_artifacts_to_package(\n        metadata, module_path=micropkg_module_path, env=env\n    )\n    # as the source distribution will only contain parameters, we aren't listing other\n    # config files not to confuse users and avoid useless file copies\n    # collect configs to package not only from parameters folder, but from core conf folder also\n    # because parameters had been moved from foldername to yml filename\n    configs_to_package = _find_config_files(\n        package_conf,\n        [\n            f\"**/parameters_{micropkg_name}.yml\",\n            f\"**/{micropkg_name}/**/*\",\n            f\"parameters*/**/{micropkg_name}.yml\",\n            f\"parameters*/**/{micropkg_name}/**/*\",\n        ],\n    )\n\n    source_paths = (package_source, package_tests, configs_to_package)\n\n    # Check that micropkg directory exists and not empty\n    _validate_dir(package_source)\n\n    package_destination = (\n        Path(destination) if destination else metadata.project_path / \"dist\"\n    )\n    version = _get_default_version(metadata, micropkg_module_path)\n\n    _generate_sdist_file(\n        micropkg_name=micropkg_name,\n        destination=package_destination.resolve(),\n        source_paths=source_paths,\n        version=version,\n        metadata=metadata,\n        alias=alias,\n    )\n\n    _clean_pycache(package_dir)\n    _clean_pycache(metadata.project_path)\n\n    return package_destination\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._package_micropkgs_from_manifest","title":"_package_micropkgs_from_manifest","text":"<pre><code>_package_micropkgs_from_manifest(metadata)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _package_micropkgs_from_manifest(metadata: ProjectMetadata) -&gt; None:\n    config_dict = toml.load(metadata.config_file)\n    config_dict = config_dict[\"tool\"][\"kedro\"]\n    build_specs = config_dict.get(\"micropkg\", {}).get(\"package\")\n\n    if not build_specs:\n        click.secho(\n            \"Nothing to package. Please update the 'pyproject.toml' package manifest section.\",\n            fg=\"yellow\",\n        )\n        return\n\n    for package_name, specs in build_specs.items():\n        if \"alias\" in specs:\n            _assert_pkg_name_ok(specs[\"alias\"])\n        _package_micropkg(package_name, metadata, **specs)\n        click.secho(f\"Packaged '{package_name}' micro-package!\")\n\n    click.secho(\"Micro-packages packaged!\", fg=\"green\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._pull_package","title":"_pull_package","text":"<pre><code>_pull_package(package_path, metadata, env=None, alias=None, destination=None, fs_args=None)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _pull_package(  # noqa: PLR0913\n    package_path: str,\n    metadata: ProjectMetadata,\n    env: str | None = None,\n    alias: str | None = None,\n    destination: str | None = None,\n    fs_args: str | None = None,\n) -&gt; None:\n    with tempfile.TemporaryDirectory() as temp_dir:\n        temp_dir_path = Path(temp_dir).resolve()\n        _unpack_sdist(package_path, temp_dir_path, fs_args)\n\n        # temp_dir_path is the parent directory of the project root dir\n        contents = [member for member in temp_dir_path.iterdir() if member.is_dir()]\n        if len(contents) != 1:\n            raise KedroCliError(\n                \"Invalid sdist was extracted: exactly one directory was expected, \"\n                f\"got {contents}\"\n            )\n        project_root_dir = contents[0]\n\n        # This is much slower than parsing the requirements\n        # directly from the metadata files\n        # because it installs the package in an isolated environment,\n        # but it's the only reliable way of doing it\n        # without making assumptions on the project metadata.\n        library_meta = project_wheel_metadata(project_root_dir)\n\n        # Project name will be `my-pipeline` even if `pyproject.toml` says `my_pipeline`\n        # because standards mandate normalization of names for comparison,\n        # see https://packaging.python.org/en/latest/specifications/core-metadata/#name\n        # The proper way to get it would be\n        # project_name = library_meta.get(\"Name\")\n        # However, the rest of the code expects the non-normalized package name,\n        # so we have to find it.\n        packages = [\n            project_item.name\n            for project_item in project_root_dir.iterdir()\n            if project_item.is_dir()\n            and project_item.name != \"tests\"\n            and (project_item / \"__init__.py\").exists()\n        ]\n        if len(packages) != 1:\n            # Should not happen if user is calling `micropkg pull`\n            # with the result of a `micropkg package`,\n            # and in general if the distribution only contains one package (most likely),\n            # but helps give a sensible error message otherwise\n            raise KedroCliError(\n                \"Invalid package contents: exactly one package was expected, \"\n                f\"got {packages}\"\n            )\n        package_name = packages[0]\n\n        # Type ignored because of https://github.com/pypa/build/pull/693\n        package_reqs = _get_all_library_reqs(library_meta)  # type: ignore[arg-type]\n\n        if package_reqs:\n            requirements_txt = metadata.project_path / \"requirements.txt\"\n            _append_package_reqs(requirements_txt, package_reqs, package_name)\n\n        _clean_pycache(temp_dir_path)\n        _install_files(\n            metadata,\n            package_name,\n            project_root_dir,\n            env,\n            alias,\n            destination,\n        )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._pull_packages_from_manifest","title":"_pull_packages_from_manifest","text":"<pre><code>_pull_packages_from_manifest(metadata)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _pull_packages_from_manifest(metadata: ProjectMetadata) -&gt; None:\n    config_dict = toml.load(metadata.config_file)\n    config_dict = config_dict[\"tool\"][\"kedro\"]\n    build_specs = config_dict.get(\"micropkg\", {}).get(\"pull\")\n\n    if not build_specs:\n        click.secho(\n            \"Nothing to pull. Please update the 'pyproject.toml' package manifest section.\",\n            fg=\"yellow\",\n        )\n        return\n\n    for package_path, specs in build_specs.items():\n        if \"alias\" in specs:\n            _assert_pkg_name_ok(specs[\"alias\"].split(\".\")[-1])\n        _pull_package(package_path, metadata, **specs)\n        click.secho(f\"Pulled and unpacked '{package_path}'!\")\n\n    click.secho(\"Micro-packages pulled and unpacked!\", fg=\"green\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._refactor_code_for_package","title":"_refactor_code_for_package","text":"<pre><code>_refactor_code_for_package(project, package_path, tests_path, alias, project_metadata)\n</code></pre> <p>In order to refactor the imports properly, we need to recreate the same nested structure as in the project. Therefore, we create:   # also the root of the Rope project |      | init.py     |          | init.py         |              | init.py | tests     | init.py     | path_to_micro_package         | init.py         |              | init.py We then move  outside of package src to top level (\"\") in temp_dir, and rename folder &amp; imports if alias provided. <p>For tests, we need to extract all the contents of  at into top-level <code>tests</code> folder. This is not possible in one go with the Rope API, so we have to do it in a bit of a hacky way. We rename  to a <code>tmp_name</code> and move it at top-level (\"\") in temp_dir. We remove the old <code>tests</code> folder and rename <code>tmp_name</code> to <code>tests</code>. <p>The final structure should be:   # also the root of the Rope project |   # or      | init.py | tests  # only tests for      | init.py     |__ test.py Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _refactor_code_for_package(\n    project: Project,\n    package_path: Path,\n    tests_path: Path,\n    alias: str | None,\n    project_metadata: ProjectMetadata,\n) -&gt; None:\n    \"\"\"In order to refactor the imports properly, we need to recreate\n    the same nested structure as in the project. Therefore, we create:\n    &lt;temp_dir&gt;  # also the root of the Rope project\n    |__ &lt;package_name&gt;\n        |__ __init__.py\n        |__ &lt;path_to_micro_package&gt;\n            |__ __init__.py\n            |__ &lt;micro_package&gt;\n                |__ __init__.py\n    |__ tests\n        |__ __init__.py\n        |__ path_to_micro_package\n            |__ __init__.py\n            |__ &lt;micro_package&gt;\n                |__ __init__.py\n    We then move &lt;micro_package&gt; outside of package src to top level (\"\")\n    in temp_dir, and rename folder &amp; imports if alias provided.\n\n    For tests, we need to extract all the contents of &lt;micro_package&gt;\n    at into top-level `tests` folder. This is not possible in one go with\n    the Rope API, so we have to do it in a bit of a hacky way.\n    We rename &lt;micro_package&gt; to a `tmp_name` and move it at top-level (\"\")\n    in temp_dir. We remove the old `tests` folder and rename `tmp_name` to `tests`.\n\n    The final structure should be:\n    &lt;temp_dir&gt;  # also the root of the Rope project\n    |__ &lt;micro_package&gt;  # or &lt;alias&gt;\n        |__ __init__.py\n    |__ tests  # only tests for &lt;micro_package&gt;\n        |__ __init__.py\n        |__ test.py\n    \"\"\"\n\n    def _move_package_with_conflicting_name(\n        target: Path, conflicting_name: str\n    ) -&gt; None:\n        tmp_name = \"tmp_name\"\n        tmp_module = target.parent / tmp_name\n        _rename_package(project, target.as_posix(), tmp_name)\n        _move_package(project, tmp_module.as_posix(), \"\")\n        shutil.rmtree(Path(project.address) / conflicting_name)\n        _rename_package(project, tmp_name, conflicting_name)\n\n    # Copy source in appropriate folder structure\n    package_target = package_path.relative_to(project_metadata.source_dir)\n    full_path = _create_nested_package(project, package_target)\n    # overwrite=True to update the __init__.py files generated by create_package\n    _sync_dirs(package_path, full_path, overwrite=True)\n\n    # Copy tests in appropriate folder structure\n    if tests_path.exists():\n        tests_target = tests_path.relative_to(project_metadata.project_path)\n        full_path = _create_nested_package(project, tests_target)\n        # overwrite=True to update the __init__.py files generated by create_package\n        _sync_dirs(tests_path, full_path, overwrite=True)\n\n    # Refactor imports in src/package_name/.../micro_package\n    # and imports of `micro_package` in tests.\n    micro_package_name = package_target.stem\n    if micro_package_name == project_metadata.package_name:\n        _move_package_with_conflicting_name(package_target, micro_package_name)\n    else:\n        _move_package(project, package_target.as_posix(), \"\")\n        shutil.rmtree(Path(project.address) / project_metadata.package_name)\n\n    if alias:\n        _rename_package(project, micro_package_name, alias)\n\n    if tests_path.exists():\n        # we can't move the relevant tests folder as is because\n        # it will conflict with the top-level package &lt;micro_package&gt;;\n        # we can't rename it \"tests\" and move it, because it will conflict\n        # with the existing \"tests\" folder at top level;\n        # hence we give it a temp name, move it, delete tests/ and\n        # rename the temp name to tests.\n        _move_package_with_conflicting_name(tests_target, \"tests\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._refactor_code_for_unpacking","title":"_refactor_code_for_unpacking","text":"<pre><code>_refactor_code_for_unpacking(project, package_path, tests_path, alias, destination, project_metadata)\n</code></pre> <p>This is the reverse operation of <code>_refactor_code_for_package</code>, i.e we go from:   # also the root of the Rope project |   # or      | init.py | tests  # only tests for      | init.py     |__ tests.py <p>to:   # also the root of the Rope project |      | init.py     |          | init.py         |              | init.py | tests     | init.py     |          | init.py         |              | init.py Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _refactor_code_for_unpacking(  # noqa: PLR0913\n    project: Project,\n    package_path: Path,\n    tests_path: Path,\n    alias: str | None,\n    destination: str | None,\n    project_metadata: ProjectMetadata,\n) -&gt; tuple[Path, Path]:\n    \"\"\"This is the reverse operation of `_refactor_code_for_package`, i.e\n    we go from:\n    &lt;temp_dir&gt;  # also the root of the Rope project\n    |__ &lt;micro_package&gt;  # or &lt;alias&gt;\n        |__ __init__.py\n    |__ tests  # only tests for &lt;micro_package&gt;\n        |__ __init__.py\n        |__ tests.py\n\n    to:\n    &lt;temp_dir&gt;  # also the root of the Rope project\n    |__ &lt;package_name&gt;\n        |__ __init__.py\n        |__ &lt;path_to_micro_package&gt;\n            |__ __init__.py\n            |__ &lt;micro_package&gt;\n                |__ __init__.py\n    |__ tests\n        |__ __init__.py\n        |__ &lt;path_to_micro_package&gt;\n            |__ __init__.py\n            |__ &lt;micro_package&gt;\n                |__ __init__.py\n    \"\"\"\n\n    def _move_package_with_conflicting_name(\n        target: Path, original_name: str, desired_name: str | None = None\n    ) -&gt; Path:\n        _rename_package(project, original_name, \"tmp_name\")\n        full_path = _create_nested_package(project, target)\n        _move_package(project, \"tmp_name\", target.as_posix())\n        desired_name = desired_name or original_name\n        _rename_package(project, (target / \"tmp_name\").as_posix(), desired_name)\n        return full_path\n\n    package_name = package_path.stem\n    package_target = Path(project_metadata.package_name)\n    tests_target = Path(\"tests\")\n\n    if destination:\n        destination_path = Path(destination)\n        package_target = package_target / destination_path\n        tests_target = tests_target / destination_path\n\n    if alias and alias != package_name:\n        _rename_package(project, package_name, alias)\n        package_name = alias\n\n    if package_name == project_metadata.package_name:\n        full_path = _move_package_with_conflicting_name(package_target, package_name)\n    else:\n        full_path = _create_nested_package(project, package_target)\n        _move_package(project, package_name, package_target.as_posix())\n\n    refactored_package_path = full_path / package_name\n\n    if not tests_path.exists():\n        return refactored_package_path, tests_path\n\n    # we can't rename the tests package to &lt;package_name&gt;\n    # because it will conflict with existing top-level package;\n    # hence we give it a temp name, create the expected\n    # nested folder structure, move the contents there,\n    # then rename the temp name to &lt;package_name&gt;.\n    full_path = _move_package_with_conflicting_name(\n        tests_target, original_name=\"tests\", desired_name=package_name\n    )\n\n    refactored_tests_path = full_path / package_name\n\n    return refactored_package_path, refactored_tests_path\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._rename_files","title":"_rename_files","text":"<pre><code>_rename_files(conf_source, old_name, new_name)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _rename_files(conf_source: Path, old_name: str, new_name: str) -&gt; None:\n    config_files_to_rename = (\n        each\n        for each in conf_source.rglob(\"*\")\n        if each.is_file() and old_name in each.name\n    )\n    for config_file in config_files_to_rename:\n        new_config_name = config_file.name.replace(old_name, new_name)\n        config_file.rename(config_file.parent / new_config_name)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._rename_package","title":"_rename_package","text":"<pre><code>_rename_package(project, old_name, new_name)\n</code></pre> <p>Rename a Python package, refactoring relevant imports along the way, as well as references in comments.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>Project</code>)           \u2013            <p>rope.base.Project holding the scope of the refactoring.</p> </li> <li> <code>old_name</code>               (<code>str</code>)           \u2013            <p>Old module name. Can be a fully qualified module path, e.g. \"package.pipelines.pipeline\" or \"package/pipelines/pipeline\", relative to the <code>project</code> root.</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>New module name. Can't be a fully qualified module path.</p> </li> </ul> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _rename_package(project: Project, old_name: str, new_name: str) -&gt; None:\n    \"\"\"\n    Rename a Python package, refactoring relevant imports along the way,\n    as well as references in comments.\n\n    Args:\n        project: rope.base.Project holding the scope of the refactoring.\n        old_name: Old module name. Can be a fully qualified module path,\n            e.g. \"package.pipelines.pipeline\" or \"package/pipelines/pipeline\",\n            relative to the `project` root.\n        new_name: New module name. Can't be a fully qualified module path.\n    \"\"\"\n    folder = project.get_folder(old_name)\n    change = Rename(project, folder).get_changes(new_name, docs=True)\n    project.do(change)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._safe_parse_requirements","title":"_safe_parse_requirements","text":"<pre><code>_safe_parse_requirements(requirements)\n</code></pre> <p>Safely parse a requirement or set of requirements. This avoids blowing up when it encounters a requirement it cannot parse (e.g. <code>-r requirements.txt</code>). This way we can still extract all the parseable requirements out of a set containing some unparseable requirements.</p> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _safe_parse_requirements(\n    requirements: str | Iterable[str],\n) -&gt; set[_EquivalentRequirement]:\n    \"\"\"Safely parse a requirement or set of requirements. This avoids blowing up when it\n    encounters a requirement it cannot parse (e.g. `-r requirements.txt`). This way\n    we can still extract all the parseable requirements out of a set containing some\n    unparseable requirements.\n    \"\"\"\n    parseable_requirements = set()\n    if isinstance(requirements, str):\n        requirements = requirements.splitlines()\n    # TODO: Properly handle continuation lines,\n    # see https://github.com/pypa/setuptools/blob/v67.8.0/setuptools/_reqs.py\n    for requirement_line in requirements:\n        if (\n            requirement_line\n            and not requirement_line.startswith(\"#\")\n            and not requirement_line.startswith(\"-e\")\n        ):\n            try:\n                parseable_requirements.add(\n                    _EquivalentRequirement(_drop_comment(requirement_line))\n                )\n            except InvalidRequirement:\n                continue\n    return parseable_requirements\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._sync_dirs","title":"_sync_dirs","text":"<pre><code>_sync_dirs(source, target, prefix='', overwrite=False)\n</code></pre> <p>Recursively copies <code>source</code> directory (or file) into <code>target</code> directory without overwriting any existing files/directories in the target using the following rules:     1) Skip any files/directories which names match with files in target,     unless overwrite=True.     2) Copy all files from source to target.     3) Recursively copy all directories from source to target.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>Path</code>)           \u2013            <p>A local directory to copy from, must exist.</p> </li> <li> <code>target</code>               (<code>Path</code>)           \u2013            <p>A local directory to copy to, will be created if doesn't exist yet.</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Prefix for CLI message indentation.</p> </li> </ul> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _sync_dirs(\n    source: Path, target: Path, prefix: str = \"\", overwrite: bool = False\n) -&gt; None:\n    \"\"\"Recursively copies `source` directory (or file) into `target` directory without\n    overwriting any existing files/directories in the target using the following\n    rules:\n        1) Skip any files/directories which names match with files in target,\n        unless overwrite=True.\n        2) Copy all files from source to target.\n        3) Recursively copy all directories from source to target.\n\n    Args:\n        source: A local directory to copy from, must exist.\n        target: A local directory to copy to, will be created if doesn't exist yet.\n        prefix: Prefix for CLI message indentation.\n    \"\"\"\n\n    existing = list(target.iterdir()) if target.is_dir() else []\n    existing_files = {f.name for f in existing if f.is_file()}\n    existing_folders = {f.name for f in existing if f.is_dir()}\n\n    if source.is_dir():\n        content = list(source.iterdir())\n    elif source.is_file():\n        content = [source]\n    else:\n        # nothing to copy\n        content = []  # pragma: no cover\n\n    for source_path in content:\n        source_name = source_path.name\n        target_path = target / source_name\n        click.echo(indent(f\"Creating '{target_path}': \", prefix), nl=False)\n\n        if (  # rule #1\n            not overwrite\n            and source_name in existing_files\n            or source_path.is_file()\n            and source_name in existing_folders\n        ):\n            click.secho(\"SKIPPED (already exists)\", fg=\"yellow\")\n        elif source_path.is_file():  # rule #2\n            try:\n                target.mkdir(exist_ok=True, parents=True)\n                shutil.copyfile(str(source_path), str(target_path))\n            except Exception:\n                click.secho(\"FAILED\", fg=\"red\")\n                raise\n            click.secho(\"OK\", fg=\"green\")\n        else:  # source_path is a directory, rule #3\n            click.echo()\n            new_prefix = (prefix or \"\") + \" \" * 2\n            _sync_dirs(source_path, target_path, prefix=new_prefix)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._sync_path_list","title":"_sync_path_list","text":"<pre><code>_sync_path_list(source, target)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _sync_path_list(source: list[tuple[Path, str]], target: Path) -&gt; None:\n    for source_path, suffix in source:\n        target_with_suffix = (target / suffix).resolve()\n        _sync_dirs(source_path, target_with_suffix)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._unpack_sdist","title":"_unpack_sdist","text":"<pre><code>_unpack_sdist(location, destination, fs_args)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _unpack_sdist(location: str, destination: Path, fs_args: str | None) -&gt; None:\n    filesystem = _get_fsspec_filesystem(location, fs_args)\n\n    if location.endswith(\".tar.gz\") and filesystem and filesystem.exists(location):\n        with filesystem.open(location) as fs_file:\n            with tarfile.open(fileobj=fs_file, mode=\"r:gz\") as tar_file:\n                safe_extract(tar_file, destination)\n    else:\n        python_call(\n            \"pip\",\n            [\n                \"download\",\n                \"--no-deps\",\n                \"--no-binary\",  # the micropackaging expects an sdist,\n                \":all:\",  # wheels are not supported\n                \"--dest\",\n                str(destination),\n                location,\n            ],\n        )\n        sdist_file = list(destination.glob(\"*.tar.gz\"))\n        # `--no-deps --no-binary :all:` should fetch only one source distribution file,\n        # and CLI should fail if that's not the case.\n        if len(sdist_file) != 1:\n            file_names = [sf.name for sf in sdist_file]\n            raise KedroCliError(\n                f\"More than 1 or no sdist files found: {file_names}. \"\n                f\"There has to be exactly one source distribution file.\"\n            )\n        with tarfile.open(sdist_file[0], \"r:gz\") as fs_file:\n            safe_extract(fs_file, destination)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg._validate_dir","title":"_validate_dir","text":"<pre><code>_validate_dir(path)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def _validate_dir(path: Path) -&gt; None:\n    if not path.is_dir():\n        raise KedroCliError(f\"Directory '{path}' doesn't exist.\")\n    if not list(path.iterdir()):\n        raise KedroCliError(f\"'{path}' is an empty directory.\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg.call","title":"call","text":"<pre><code>call(cmd, **kwargs)\n</code></pre> <p>Run a subprocess command and raise if it fails.</p> <p>Parameters:</p> <ul> <li> <code>cmd</code>               (<code>list[str]</code>)           \u2013            <p>List of command parts.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Optional keyword arguments passed to <code>subprocess.run</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>Exit</code>             \u2013            <p>If <code>subprocess.run</code> returns non-zero code.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def call(cmd: list[str], **kwargs: Any) -&gt; None:  # pragma: no cover\n    \"\"\"Run a subprocess command and raise if it fails.\n\n    Args:\n        cmd: List of command parts.\n        **kwargs: Optional keyword arguments passed to `subprocess.run`.\n\n    Raises:\n        click.exceptions.Exit: If `subprocess.run` returns non-zero code.\n    \"\"\"\n    click.echo(shlex.join(cmd))\n    code = subprocess.run(cmd, **kwargs).returncode  # noqa: PLW1510, S603\n    if code:\n        raise click.exceptions.Exit(code=code)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg.command_with_verbosity","title":"command_with_verbosity","text":"<pre><code>command_with_verbosity(group, *args, **kwargs)\n</code></pre> <p>Custom command decorator with verbose flag added.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def command_with_verbosity(group: click.core.Group, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Custom command decorator with verbose flag added.\"\"\"\n\n    def decorator(func: Any) -&gt; Any:\n        func = _click_verbose(func)\n        func = group.command(*args, **kwargs)(func)\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg.env_option","title":"env_option","text":"<pre><code>env_option(func_=None, **kwargs)\n</code></pre> <p>Add <code>--env</code> CLI option to a function.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def env_option(func_: Any | None = None, **kwargs: Any) -&gt; Any:\n    \"\"\"Add `--env` CLI option to a function.\"\"\"\n    default_args = {\"type\": str, \"default\": None, \"help\": ENV_HELP}\n    kwargs = {**default_args, **kwargs}\n    opt = click.option(\"--env\", \"-e\", **kwargs)\n    return opt(func_) if func_ else opt\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg.micropkg","title":"micropkg","text":"<pre><code>micropkg()\n</code></pre> <p>(DEPRECATED) Commands for working with micro-packages. DeprecationWarning: micro-packaging is deprecated and will not be available from Kedro 0.20.0.</p> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>@micropkg_cli.group()\ndef micropkg() -&gt; None:\n    \"\"\"(DEPRECATED) Commands for working with micro-packages. DeprecationWarning: micro-packaging is deprecated\n    and will not be available from Kedro 0.20.0.\"\"\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg.micropkg_cli","title":"micropkg_cli","text":"<pre><code>micropkg_cli()\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>@click.group(name=\"Kedro\")\ndef micropkg_cli() -&gt; None:  # pragma: no cover\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg.package_micropkg","title":"package_micropkg","text":"<pre><code>package_micropkg(metadata, /, module_path, env, alias, destination, all_flag, **kwargs)\n</code></pre> <p>(DEPRECATED) Package up a modular pipeline or micro-package as a Python source distribution.</p> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>@command_with_verbosity(micropkg, \"package\")\n@env_option(\n    help=\"Environment where the micro-package configuration lives. Defaults to `base`.\"\n)\n@click.option(\n    \"--alias\",\n    type=str,\n    default=\"\",\n    callback=_check_pipeline_name,\n    help=\"Alternative name to package under.\",\n)\n@click.option(\n    \"-d\",\n    \"--destination\",\n    type=click.Path(resolve_path=True, file_okay=False),\n    help=\"Location where to create the source distribution file. Defaults to `dist/`.\",\n)\n@click.option(\n    \"--all\",\n    \"-a\",\n    \"all_flag\",\n    is_flag=True,\n    help=\"Package all micro-packages in the `pyproject.toml` package manifest section.\",\n)\n@click.argument(\"module_path\", nargs=1, required=False, callback=_check_module_path)\n@click.pass_obj  # this will pass the metadata as first argument\ndef package_micropkg(  # noqa: PLR0913\n    metadata: ProjectMetadata,\n    /,\n    module_path: str,\n    env: str,\n    alias: str,\n    destination: str,\n    all_flag: str,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"(DEPRECATED) Package up a modular pipeline or micro-package as a Python source distribution.\"\"\"\n    deprecation_message = (\n        \"DeprecationWarning: Command 'kedro micropkg package' is deprecated and \"\n        \"will not be available from Kedro 0.20.0.\"\n    )\n    click.secho(deprecation_message, fg=\"red\")\n\n    if not module_path and not all_flag:\n        click.secho(\n            \"Please specify a micro-package name or add '--all' to package all micro-packages in \"\n            \"the 'pyproject.toml' package manifest section.\"\n        )\n        sys.exit(1)\n\n    if all_flag:\n        _package_micropkgs_from_manifest(metadata)\n        return\n\n    result_path = _package_micropkg(\n        module_path, metadata, alias=alias, destination=destination, env=env\n    )\n\n    as_alias = f\" as '{alias}'\" if alias else \"\"\n    message = (\n        f\"'{metadata.package_name}.{module_path}' packaged{as_alias}! \"\n        f\"Location: {result_path}\"\n    )\n    click.secho(message, fg=\"green\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg.pull_package","title":"pull_package","text":"<pre><code>pull_package(metadata, /, package_path, env, alias, destination, fs_args, all_flag, **kwargs)\n</code></pre> <p>(DEPRECATED) Pull and unpack a modular pipeline and other micro-packages in your project.</p> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>@command_with_verbosity(micropkg, \"pull\")\n@click.argument(\"package_path\", nargs=1, required=False)\n@click.option(\n    \"--all\",\n    \"-a\",\n    \"all_flag\",\n    is_flag=True,\n    help=\"Pull and unpack all micro-packages in the `pyproject.toml` package manifest section.\",\n)\n@env_option(\n    help=\"Environment to install the micro-package configuration to. Defaults to `base`.\"\n)\n@click.option(\"--alias\", type=str, default=\"\", help=\"Rename the package.\")\n@click.option(\n    \"-d\",\n    \"--destination\",\n    type=click.Path(file_okay=False, dir_okay=False),\n    default=None,\n    help=\"Module location where to unpack under.\",\n)\n@click.option(\n    \"--fs-args\",\n    type=click.Path(\n        exists=True, file_okay=True, dir_okay=False, readable=True, resolve_path=True\n    ),\n    default=None,\n    help=\"Location of a configuration file for the fsspec filesystem used to pull the package.\",\n)\n@click.pass_obj  # this will pass the metadata as first argument\ndef pull_package(  # noqa: PLR0913\n    metadata: ProjectMetadata,\n    /,\n    package_path: str,\n    env: str,\n    alias: str,\n    destination: str,\n    fs_args: str,\n    all_flag: str,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"(DEPRECATED) Pull and unpack a modular pipeline and other micro-packages in your project.\"\"\"\n    deprecation_message = (\n        \"DeprecationWarning: Command 'kedro micropkg pull' is deprecated and \"\n        \"will not be available from Kedro 0.20.0.\"\n    )\n    click.secho(deprecation_message, fg=\"red\")\n\n    if not package_path and not all_flag:\n        click.secho(\n            \"Please specify a package path or add '--all' to pull all micro-packages in the \"\n            \"'pyproject.toml' package manifest section.\"\n        )\n        sys.exit(1)\n\n    if all_flag:\n        _pull_packages_from_manifest(metadata)\n        return\n\n    _pull_package(\n        package_path,\n        metadata,\n        env=env,\n        alias=alias,\n        destination=destination,\n        fs_args=fs_args,\n    )\n    as_alias = f\" as '{alias}'\" if alias else \"\"\n    message = f\"Micro-package {package_path} pulled and unpacked{as_alias}!\"\n    click.secho(message, fg=\"green\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg.python_call","title":"python_call","text":"<pre><code>python_call(module, arguments, **kwargs)\n</code></pre> <p>Run a subprocess command that invokes a Python module.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def python_call(\n    module: str, arguments: Iterable[str], **kwargs: Any\n) -&gt; None:  # pragma: no cover\n    \"\"\"Run a subprocess command that invokes a Python module.\"\"\"\n    call([sys.executable, \"-m\", module, *list(arguments)], **kwargs)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.micropkg.safe_extract","title":"safe_extract","text":"<pre><code>safe_extract(tar, path)\n</code></pre> Source code in <code>kedro/framework/cli/micropkg.py</code> <pre><code>def safe_extract(tar: tarfile.TarFile, path: Path) -&gt; None:\n    safe_members = []\n    for member in tar.getmembers():\n        member_path = path / member.name\n        if not _is_within_directory(path, member_path):\n            raise Exception(\"Failed to safely extract tar file.\")\n        safe_members.append(member)\n    tar.extractall(path, members=safe_members)  # noqa S202\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline","title":"kedro.framework.cli.pipeline","text":"<p>A collection of CLI commands for working with Kedro pipelines.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._SETUP_PY_TEMPLATE","title":"_SETUP_PY_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>_SETUP_PY_TEMPLATE = '# -*- coding: utf-8 -*-\\nfrom setuptools import setup, find_packages\\n\\nsetup(\\n    name=\"{name}\",\\n    version=\"{version}\",\\n    description=\"Modular pipeline `{name}`\",\\n    packages=find_packages(),\\n    include_package_data=True,\\n    install_requires={install_requires},\\n)\\n'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline.settings","title":"settings  <code>module-attribute</code>","text":"<pre><code>settings = _ProjectSettings()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline.KedroCliError","title":"KedroCliError","text":"<p>               Bases: <code>ClickException</code></p> <p>Exceptions generated from the Kedro CLI.</p> <p>Users should pass an appropriate message at the constructor.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline.PipelineArtifacts","title":"PipelineArtifacts","text":"<p>               Bases: <code>NamedTuple</code></p> <p>An ordered collection of source_path, tests_path, config_paths</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline.ProjectMetadata","title":"ProjectMetadata","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Structure holding project metadata derived from <code>pyproject.toml</code></p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._assert_pkg_name_ok","title":"_assert_pkg_name_ok","text":"<pre><code>_assert_pkg_name_ok(pkg_name)\n</code></pre> <p>Check that python package name is in line with PEP8 requirements.</p> <p>Parameters:</p> <ul> <li> <code>pkg_name</code>               (<code>str</code>)           \u2013            <p>Candidate Python package name.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KedroCliError</code>             \u2013            <p>If package name violates the requirements.</p> </li> </ul> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _assert_pkg_name_ok(pkg_name: str) -&gt; None:\n    \"\"\"Check that python package name is in line with PEP8 requirements.\n\n    Args:\n        pkg_name: Candidate Python package name.\n\n    Raises:\n        KedroCliError: If package name violates the requirements.\n    \"\"\"\n\n    base_message = f\"'{pkg_name}' is not a valid Python package name.\"\n    if not re.match(r\"^[a-zA-Z_]\", pkg_name):\n        message = base_message + \" It must start with a letter or underscore.\"\n        raise KedroCliError(message)\n    if len(pkg_name) &lt; 2:  # noqa: PLR2004\n        message = base_message + \" It must be at least 2 characters long.\"\n        raise KedroCliError(message)\n    if not re.match(r\"^\\w+$\", pkg_name[1:]):\n        message = (\n            base_message + \" It must contain only letters, digits, and/or underscores.\"\n        )\n        raise KedroCliError(message)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._check_pipeline_name","title":"_check_pipeline_name","text":"<pre><code>_check_pipeline_name(ctx, param, value)\n</code></pre> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _check_pipeline_name(ctx: click.Context, param: Any, value: str) -&gt; str:\n    if value:\n        _assert_pkg_name_ok(value)\n    return value\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._clean_pycache","title":"_clean_pycache","text":"<pre><code>_clean_pycache(path)\n</code></pre> <p>Recursively clean all pycache folders from <code>path</code>.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>Existing local directory to clean pycache folders from.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _clean_pycache(path: Path) -&gt; None:\n    \"\"\"Recursively clean all __pycache__ folders from `path`.\n\n    Args:\n        path: Existing local directory to clean __pycache__ folders from.\n    \"\"\"\n    to_delete = [each.resolve() for each in path.rglob(\"__pycache__\")]\n\n    for each in to_delete:\n        shutil.rmtree(each, ignore_errors=True)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._copy_pipeline_configs","title":"_copy_pipeline_configs","text":"<pre><code>_copy_pipeline_configs(result_path, conf_path, skip_config, env)\n</code></pre> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _copy_pipeline_configs(\n    result_path: Path, conf_path: Path, skip_config: bool, env: str\n) -&gt; None:\n    config_source = result_path / \"config\"\n    try:\n        if not skip_config:\n            config_target = conf_path / env\n            _sync_dirs(config_source, config_target)\n    finally:\n        shutil.rmtree(config_source)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._copy_pipeline_tests","title":"_copy_pipeline_tests","text":"<pre><code>_copy_pipeline_tests(pipeline_name, result_path, project_root)\n</code></pre> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _copy_pipeline_tests(\n    pipeline_name: str, result_path: Path, project_root: Path\n) -&gt; None:\n    tests_source = result_path / \"tests\"\n    tests_target = project_root.parent / \"tests\" / \"pipelines\" / pipeline_name\n    try:\n        _sync_dirs(tests_source, tests_target)\n    finally:\n        shutil.rmtree(tests_source)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._create_pipeline","title":"_create_pipeline","text":"<pre><code>_create_pipeline(name, template_path, output_dir)\n</code></pre> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _create_pipeline(name: str, template_path: Path, output_dir: Path) -&gt; Path:\n    from cookiecutter.main import cookiecutter\n\n    cookie_context = {\"pipeline_name\": name, \"kedro_version\": kedro.__version__}\n\n    click.echo(f\"Creating the pipeline '{name}': \", nl=False)\n\n    try:\n        cookiecutter_result = cookiecutter(\n            str(template_path),\n            output_dir=str(output_dir),\n            no_input=True,\n            extra_context=cookie_context,\n        )\n    except Exception as exc:\n        click.secho(\"FAILED\", fg=\"red\")\n        cls = exc.__class__\n        raise KedroCliError(f\"{cls.__module__}.{cls.__qualname__}: {exc}\") from exc\n\n    click.secho(\"OK\", fg=\"green\")\n    result_path = Path(cookiecutter_result)\n    message = indent(f\"Location: '{result_path.resolve()}'\", \" \" * 2)\n    click.secho(message, bold=True)\n\n    _clean_pycache(result_path)\n\n    return result_path\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._delete_artifacts","title":"_delete_artifacts","text":"<pre><code>_delete_artifacts(*artifacts)\n</code></pre> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _delete_artifacts(*artifacts: Path) -&gt; None:\n    for artifact in artifacts:\n        click.echo(f\"Deleting '{artifact}': \", nl=False)\n        try:\n            if artifact.is_dir():\n                shutil.rmtree(artifact)\n            else:\n                artifact.unlink()\n        except Exception as exc:\n            click.secho(\"FAILED\", fg=\"red\")\n            cls = exc.__class__\n            raise KedroCliError(f\"{cls.__module__}.{cls.__qualname__}: {exc}\") from exc\n        click.secho(\"OK\", fg=\"green\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._echo_deletion_warning","title":"_echo_deletion_warning","text":"<pre><code>_echo_deletion_warning(message, **paths)\n</code></pre> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _echo_deletion_warning(message: str, **paths: list[Path]) -&gt; None:\n    paths = {key: values for key, values in paths.items() if values}\n\n    if paths:\n        click.secho(message, bold=True)\n\n    for key, values in paths.items():\n        click.echo(f\"\\n{key.capitalize()}:\")\n        paths_str = \"\\n\".join(str(value) for value in values)\n        click.echo(indent(paths_str, \" \" * 2))\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._ensure_pipelines_init_file","title":"_ensure_pipelines_init_file","text":"<pre><code>_ensure_pipelines_init_file(pipelines_dir)\n</code></pre> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _ensure_pipelines_init_file(pipelines_dir: Path) -&gt; None:\n    # Ensure the pipelines directory exists\n    pipelines_dir.mkdir(exist_ok=True, parents=True)\n\n    # Create __init__.py if it doesn't exist\n    init_file = pipelines_dir / \"__init__.py\"\n    if not init_file.is_file():\n        click.echo(f\"Creating '{init_file}': \", nl=False)\n        init_file.touch()\n        click.secho(\"OK\", fg=\"green\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._get_artifacts_to_package","title":"_get_artifacts_to_package","text":"<pre><code>_get_artifacts_to_package(project_metadata, module_path, env)\n</code></pre> <p>From existing project, returns in order: source_path, tests_path, config_paths</p> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _get_artifacts_to_package(\n    project_metadata: ProjectMetadata, module_path: str, env: str\n) -&gt; tuple[Path, Path, Path]:\n    \"\"\"From existing project, returns in order: source_path, tests_path, config_paths\"\"\"\n    package_dir = project_metadata.source_dir / project_metadata.package_name\n    project_root = project_metadata.project_path\n    project_conf_path = project_metadata.project_path / settings.CONF_SOURCE\n    artifacts = (\n        Path(package_dir, *module_path.split(\".\")),\n        Path(project_root, \"tests\", *module_path.split(\".\")),\n        project_conf_path / env,\n    )\n    return artifacts\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._get_pipeline_artifacts","title":"_get_pipeline_artifacts","text":"<pre><code>_get_pipeline_artifacts(project_metadata, pipeline_name, env)\n</code></pre> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _get_pipeline_artifacts(\n    project_metadata: ProjectMetadata, pipeline_name: str, env: str\n) -&gt; PipelineArtifacts:\n    artifacts = _get_artifacts_to_package(\n        project_metadata, f\"pipelines.{pipeline_name}\", env\n    )\n    return PipelineArtifacts(*artifacts)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline._sync_dirs","title":"_sync_dirs","text":"<pre><code>_sync_dirs(source, target, prefix='', overwrite=False)\n</code></pre> <p>Recursively copies <code>source</code> directory (or file) into <code>target</code> directory without overwriting any existing files/directories in the target using the following rules:     1) Skip any files/directories which names match with files in target,     unless overwrite=True.     2) Copy all files from source to target.     3) Recursively copy all directories from source to target.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>Path</code>)           \u2013            <p>A local directory to copy from, must exist.</p> </li> <li> <code>target</code>               (<code>Path</code>)           \u2013            <p>A local directory to copy to, will be created if doesn't exist yet.</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Prefix for CLI message indentation.</p> </li> </ul> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>def _sync_dirs(\n    source: Path, target: Path, prefix: str = \"\", overwrite: bool = False\n) -&gt; None:\n    \"\"\"Recursively copies `source` directory (or file) into `target` directory without\n    overwriting any existing files/directories in the target using the following\n    rules:\n        1) Skip any files/directories which names match with files in target,\n        unless overwrite=True.\n        2) Copy all files from source to target.\n        3) Recursively copy all directories from source to target.\n\n    Args:\n        source: A local directory to copy from, must exist.\n        target: A local directory to copy to, will be created if doesn't exist yet.\n        prefix: Prefix for CLI message indentation.\n    \"\"\"\n\n    existing = list(target.iterdir()) if target.is_dir() else []\n    existing_files = {f.name for f in existing if f.is_file()}\n    existing_folders = {f.name for f in existing if f.is_dir()}\n\n    if source.is_dir():\n        content = list(source.iterdir())\n    elif source.is_file():\n        content = [source]\n    else:\n        # nothing to copy\n        content = []  # pragma: no cover\n\n    for source_path in content:\n        source_name = source_path.name\n        target_path = target / source_name\n        click.echo(indent(f\"Creating '{target_path}': \", prefix), nl=False)\n\n        if (  # rule #1\n            not overwrite\n            and source_name in existing_files\n            or source_path.is_file()\n            and source_name in existing_folders\n        ):\n            click.secho(\"SKIPPED (already exists)\", fg=\"yellow\")\n        elif source_path.is_file():  # rule #2\n            try:\n                target.mkdir(exist_ok=True, parents=True)\n                shutil.copyfile(str(source_path), str(target_path))\n            except Exception:\n                click.secho(\"FAILED\", fg=\"red\")\n                raise\n            click.secho(\"OK\", fg=\"green\")\n        else:  # source_path is a directory, rule #3\n            click.echo()\n            new_prefix = (prefix or \"\") + \" \" * 2\n            _sync_dirs(source_path, target_path, prefix=new_prefix)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline.command_with_verbosity","title":"command_with_verbosity","text":"<pre><code>command_with_verbosity(group, *args, **kwargs)\n</code></pre> <p>Custom command decorator with verbose flag added.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def command_with_verbosity(group: click.core.Group, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Custom command decorator with verbose flag added.\"\"\"\n\n    def decorator(func: Any) -&gt; Any:\n        func = _click_verbose(func)\n        func = group.command(*args, **kwargs)(func)\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline.create_pipeline","title":"create_pipeline","text":"<pre><code>create_pipeline(metadata, /, name, template_path, skip_config, env, **kwargs)\n</code></pre> <p>Create a new modular pipeline by providing a name.</p> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>@command_with_verbosity(pipeline, \"create\")\n@click.argument(\"name\", nargs=1, callback=_check_pipeline_name)\n@click.option(\n    \"--skip-config\",\n    is_flag=True,\n    help=\"Skip creation of config files for the new pipeline(s).\",\n)\n@click.option(\n    \"template_path\",\n    \"-t\",\n    \"--template\",\n    type=click.Path(file_okay=False, dir_okay=True, exists=True, path_type=Path),\n    help=\"Path to cookiecutter template to use for pipeline(s). Will override any local templates.\",\n)\n@env_option(help=\"Environment to create pipeline configuration in. Defaults to `base`.\")\n@click.pass_obj  # this will pass the metadata as first argument\ndef create_pipeline(\n    metadata: ProjectMetadata,\n    /,\n    name: str,\n    template_path: Path,\n    skip_config: bool,\n    env: str,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new modular pipeline by providing a name.\"\"\"\n    package_dir = metadata.source_dir / metadata.package_name\n    project_root = metadata.project_path / metadata.project_name\n    conf_source = settings.CONF_SOURCE\n    project_conf_path = metadata.project_path / conf_source\n    base_env = settings.CONFIG_LOADER_ARGS.get(\"base_env\", \"base\")\n    env = env or base_env\n    if not skip_config and not (project_conf_path / env).exists():\n        raise KedroCliError(\n            f\"Unable to locate environment '{env}'. \"\n            f\"Make sure it exists in the project configuration.\"\n        )\n\n    # Precedence for template_path is: command line &gt; project templates/pipeline dir &gt; global default\n    # If passed on the CLI, click will verify that the path exists so no need to check again\n    if template_path is None:\n        # No path provided on the CLI, try `PROJECT_PATH/templates/pipeline`\n        template_path = Path(metadata.project_path / \"templates\" / \"pipeline\")\n\n        if not template_path.exists():\n            # and if that folder doesn't exist fall back to the global default\n            template_path = Path(kedro.__file__).parent / \"templates\" / \"pipeline\"\n\n    click.secho(f\"Using pipeline template at: '{template_path}'\")\n\n    # Ensure pipelines directory has __init__.py\n    pipelines_dir = package_dir / \"pipelines\"\n    _ensure_pipelines_init_file(pipelines_dir)\n\n    result_path = _create_pipeline(name, template_path, pipelines_dir)\n    _copy_pipeline_tests(name, result_path, project_root)\n    _copy_pipeline_configs(result_path, project_conf_path, skip_config, env=env)\n    click.secho(f\"\\nPipeline '{name}' was successfully created.\\n\", fg=\"green\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline.delete_pipeline","title":"delete_pipeline","text":"<pre><code>delete_pipeline(metadata, /, name, env, yes, **kwargs)\n</code></pre> <p>Delete a modular pipeline by providing a name.</p> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>@command_with_verbosity(pipeline, \"delete\")\n@click.argument(\"name\", nargs=1, callback=_check_pipeline_name)\n@env_option(\n    help=\"Environment to delete pipeline configuration from. Defaults to 'base'.\"\n)\n@click.option(\n    \"-y\", \"--yes\", is_flag=True, help=\"Confirm deletion of pipeline non-interactively.\"\n)\n@click.pass_obj  # this will pass the metadata as first argument\ndef delete_pipeline(\n    metadata: ProjectMetadata, /, name: str, env: str, yes: bool, **kwargs: Any\n) -&gt; None:\n    \"\"\"Delete a modular pipeline by providing a name.\"\"\"\n    package_dir = metadata.source_dir / metadata.package_name\n    conf_source = settings.CONF_SOURCE\n    project_conf_path = metadata.project_path / conf_source\n    base_env = settings.CONFIG_LOADER_ARGS.get(\"base_env\", \"base\")\n    env = env or base_env\n    if not (project_conf_path / env).exists():\n        raise KedroCliError(\n            f\"Unable to locate environment '{env}'. \"\n            f\"Make sure it exists in the project configuration.\"\n        )\n\n    pipeline_artifacts = _get_pipeline_artifacts(metadata, pipeline_name=name, env=env)\n\n    files_to_delete = [\n        pipeline_artifacts.pipeline_conf / filepath\n        for confdir in (\"parameters\", \"catalog\")\n        # Since we remove nesting in 'parameters' and 'catalog' folders,\n        # we want to also del the old project's structure for backward compatibility\n        for filepath in (Path(f\"{confdir}_{name}.yml\"), Path(confdir) / f\"{name}.yml\")\n        if (pipeline_artifacts.pipeline_conf / filepath).is_file()\n    ]\n\n    dirs_to_delete = [\n        path\n        for path in (pipeline_artifacts.pipeline_dir, pipeline_artifacts.pipeline_tests)\n        if path.is_dir()\n    ]\n\n    if not files_to_delete and not dirs_to_delete:\n        raise KedroCliError(f\"Pipeline '{name}' not found.\")\n\n    if not yes:\n        _echo_deletion_warning(\n            \"The following paths will be removed:\",\n            directories=dirs_to_delete,\n            files=files_to_delete,\n        )\n        click.echo()\n        yes = click.confirm(f\"Are you sure you want to delete pipeline '{name}'?\")\n        click.echo()\n\n    if not yes:\n        raise KedroCliError(\"Deletion aborted!\")\n\n    _delete_artifacts(*files_to_delete, *dirs_to_delete)\n    click.secho(f\"\\nPipeline '{name}' was successfully deleted.\", fg=\"green\")\n    click.secho(\n        f\"\\nIf you added the pipeline '{name}' to 'register_pipelines()' in\"\n        f\"\"\" '{package_dir / \"pipeline_registry.py\"}', you will need to remove it.\"\"\",\n        fg=\"yellow\",\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline.env_option","title":"env_option","text":"<pre><code>env_option(func_=None, **kwargs)\n</code></pre> <p>Add <code>--env</code> CLI option to a function.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def env_option(func_: Any | None = None, **kwargs: Any) -&gt; Any:\n    \"\"\"Add `--env` CLI option to a function.\"\"\"\n    default_args = {\"type\": str, \"default\": None, \"help\": ENV_HELP}\n    kwargs = {**default_args, **kwargs}\n    opt = click.option(\"--env\", \"-e\", **kwargs)\n    return opt(func_) if func_ else opt\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline.pipeline","title":"pipeline","text":"<pre><code>pipeline()\n</code></pre> <p>Commands for working with pipelines.</p> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>@pipeline_cli.group()\ndef pipeline() -&gt; None:\n    \"\"\"Commands for working with pipelines.\"\"\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.pipeline.pipeline_cli","title":"pipeline_cli","text":"<pre><code>pipeline_cli()\n</code></pre> Source code in <code>kedro/framework/cli/pipeline.py</code> <pre><code>@click.group(name=\"Kedro\")\ndef pipeline_cli() -&gt; None:  # pragma: no cover\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project","title":"kedro.framework.cli.project","text":"<p>A collection of CLI commands for working with Kedro project.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.ASYNC_ARG_HELP","title":"ASYNC_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>ASYNC_ARG_HELP = 'Load and save node inputs and outputs asynchronously\\nwith threads. If not specified, load and save datasets synchronously.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.CONFIG_FILE_HELP","title":"CONFIG_FILE_HELP  <code>module-attribute</code>","text":"<pre><code>CONFIG_FILE_HELP = 'Specify a YAML configuration file to load the run\\ncommand arguments from. If command line arguments are provided, they will\\noverride the loaded ones.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.CONF_SOURCE_HELP","title":"CONF_SOURCE_HELP  <code>module-attribute</code>","text":"<pre><code>CONF_SOURCE_HELP = 'Path of a directory where project configuration is stored.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.FROM_INPUTS_HELP","title":"FROM_INPUTS_HELP  <code>module-attribute</code>","text":"<pre><code>FROM_INPUTS_HELP = 'A list of dataset names which should be used as a starting point.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.FROM_NODES_HELP","title":"FROM_NODES_HELP  <code>module-attribute</code>","text":"<pre><code>FROM_NODES_HELP = 'A list of node names which should be used as a starting point.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.INPUT_FILE_HELP","title":"INPUT_FILE_HELP  <code>module-attribute</code>","text":"<pre><code>INPUT_FILE_HELP = 'Name of the requirements file to compile.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.LINT_CHECK_ONLY_HELP","title":"LINT_CHECK_ONLY_HELP  <code>module-attribute</code>","text":"<pre><code>LINT_CHECK_ONLY_HELP = 'Check the files for style guide violations, unsorted /\\nunformatted imports, and unblackened Python code without modifying the files.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.LOAD_VERSION_HELP","title":"LOAD_VERSION_HELP  <code>module-attribute</code>","text":"<pre><code>LOAD_VERSION_HELP = 'Specify a particular dataset version (timestamp) for loading.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.NAMESPACE_ARG_HELP","title":"NAMESPACE_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>NAMESPACE_ARG_HELP = 'Name of the node namespace to run.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.NODE_ARG_HELP","title":"NODE_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>NODE_ARG_HELP = 'Run only nodes with specified names.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.NO_DEPENDENCY_MESSAGE","title":"NO_DEPENDENCY_MESSAGE  <code>module-attribute</code>","text":"<pre><code>NO_DEPENDENCY_MESSAGE = \"{module} is not installed. Please make sure {module} is in\\nrequirements.txt and run 'pip install -r requirements.txt'.\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.OPEN_ARG_HELP","title":"OPEN_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>OPEN_ARG_HELP = 'Open the documentation in your default browser after building.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.OUTPUT_FILE_HELP","title":"OUTPUT_FILE_HELP  <code>module-attribute</code>","text":"<pre><code>OUTPUT_FILE_HELP = 'Name of the file where compiled requirements should be stored.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.PARAMS_ARG_HELP","title":"PARAMS_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>PARAMS_ARG_HELP = \"Specify extra parameters that you want to pass\\nto the context initialiser. Items must be separated by comma, keys - by colon or equals sign,\\nexample: param1=value1,param2=value2. Each parameter is split by the first comma,\\nso parameter values are allowed to contain colons, parameter keys are not.\\nTo pass a nested dictionary as parameter, separate keys by '.', example:\\nparam_group.param1:value1.\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.PIPELINE_ARG_HELP","title":"PIPELINE_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>PIPELINE_ARG_HELP = \"Name of the registered pipeline to run.\\nIf not set, the '__default__' pipeline is run.\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.RUNNER_ARG_HELP","title":"RUNNER_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>RUNNER_ARG_HELP = \"Specify a runner that you want to run the pipeline with.\\nAvailable runners: 'SequentialRunner', 'ParallelRunner' and 'ThreadRunner'.\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.TAG_ARG_HELP","title":"TAG_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>TAG_ARG_HELP = 'Construct the pipeline using only nodes which have this tag\\nattached. Option can be used multiple times, what results in a\\npipeline constructed from nodes having any of those tags.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.TO_NODES_HELP","title":"TO_NODES_HELP  <code>module-attribute</code>","text":"<pre><code>TO_NODES_HELP = 'A list of node names which should be used as an end point.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.TO_OUTPUTS_HELP","title":"TO_OUTPUTS_HELP  <code>module-attribute</code>","text":"<pre><code>TO_OUTPUTS_HELP = 'A list of dataset names which should be used as an end point.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.settings","title":"settings  <code>module-attribute</code>","text":"<pre><code>settings = _ProjectSettings()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.KedroSession","title":"KedroSession","text":"<pre><code>KedroSession(session_id, package_name=None, project_path=None, save_on_close=False, conf_source=None)\n</code></pre> <p><code>KedroSession</code> is the object that is responsible for managing the lifecycle of a Kedro run. Use <code>KedroSession.create()</code> as a context manager to construct a new KedroSession with session data provided (see the example below).</p> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro.framework.session import KedroSession\n&gt;&gt;&gt; from kedro.framework.startup import bootstrap_project\n&gt;&gt;&gt; from pathlib import Path\n\n&gt;&gt;&gt; # If you are creating a session outside of a Kedro project (i.e. not using\n&gt;&gt;&gt; # `kedro run` or `kedro jupyter`), you need to run `bootstrap_project` to\n&gt;&gt;&gt; # let Kedro find your configuration.\n&gt;&gt;&gt; bootstrap_project(Path(\"&lt;project_root&gt;\"))\n&gt;&gt;&gt; with KedroSession.create() as session:\n&gt;&gt;&gt;     session.run()\n</code></pre> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    package_name: str | None = None,\n    project_path: Path | str | None = None,\n    save_on_close: bool = False,\n    conf_source: str | None = None,\n):\n    self._project_path = Path(\n        project_path or _find_kedro_project(Path.cwd()) or Path.cwd()\n    ).resolve()\n    self.session_id = session_id\n    self.save_on_close = save_on_close\n    self._package_name = package_name\n    self._store = self._init_store()\n    self._run_called = False\n\n    hook_manager = _create_hook_manager()\n    _register_hooks(hook_manager, settings.HOOKS)\n    _register_hooks_entry_points(hook_manager, settings.DISABLE_HOOKS_FOR_PLUGINS)\n    self._hook_manager = hook_manager\n\n    self._conf_source = conf_source or str(\n        self._project_path / settings.CONF_SOURCE\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.KedroSession.store","title":"store  <code>property</code>","text":"<pre><code>store\n</code></pre> <p>Return a copy of internal store.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.KedroSession.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the current session and save its store to disk if <code>save_on_close</code> attribute is True.</p> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the current session and save its store to disk\n    if `save_on_close` attribute is True.\n    \"\"\"\n    if self.save_on_close:\n        self._store.save()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.KedroSession.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(project_path=None, save_on_close=True, env=None, extra_params=None, conf_source=None)\n</code></pre> <p>Create a new instance of <code>KedroSession</code> with the session data.</p> <p>Parameters:</p> <ul> <li> <code>project_path</code>               (<code>Path | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the project root directory. Default is current working directory Path.cwd().</p> </li> <li> <code>save_on_close</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether or not to save the session when it's closed.</p> </li> <li> <code>conf_source</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to a directory containing configuration</p> </li> <li> <code>env</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Environment for the KedroContext.</p> </li> <li> <code>extra_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary containing extra project parameters for underlying KedroContext. If specified, will update (and therefore take precedence over) the parameters retrieved from the project configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>KedroSession</code>           \u2013            <p>A new <code>KedroSession</code> instance.</p> </li> </ul> Source code in <code>kedro/framework/session/session.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    project_path: Path | str | None = None,\n    save_on_close: bool = True,\n    env: str | None = None,\n    extra_params: dict[str, Any] | None = None,\n    conf_source: str | None = None,\n) -&gt; KedroSession:\n    \"\"\"Create a new instance of ``KedroSession`` with the session data.\n\n    Args:\n        project_path: Path to the project root directory. Default is\n            current working directory Path.cwd().\n        save_on_close: Whether or not to save the session when it's closed.\n        conf_source: Path to a directory containing configuration\n        env: Environment for the KedroContext.\n        extra_params: Optional dictionary containing extra project parameters\n            for underlying KedroContext. If specified, will update (and therefore\n            take precedence over) the parameters retrieved from the project\n            configuration.\n\n    Returns:\n        A new ``KedroSession`` instance.\n    \"\"\"\n    validate_settings()\n\n    session = cls(\n        project_path=project_path,\n        session_id=generate_timestamp(),\n        save_on_close=save_on_close,\n        conf_source=conf_source,\n    )\n\n    # have to explicitly type session_data otherwise mypy will complain\n    # possibly related to this: https://github.com/python/mypy/issues/1430\n    session_data: dict[str, Any] = {\n        \"project_path\": session._project_path,\n        \"session_id\": session.session_id,\n    }\n\n    ctx = click.get_current_context(silent=True)\n    if ctx:\n        session_data[\"cli\"] = _jsonify_cli_context(ctx)\n\n    env = env or os.getenv(\"KEDRO_ENV\")\n    if env:\n        session_data[\"env\"] = env\n\n    if extra_params:\n        session_data[\"extra_params\"] = extra_params\n\n    try:\n        session_data[\"username\"] = getpass.getuser()\n    except Exception as exc:\n        logging.getLogger(__name__).debug(\n            \"Unable to get username. Full exception: %s\", exc\n        )\n\n    session_data.update(**_describe_git(session._project_path))\n    session._store.update(session_data)\n\n    return session\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.KedroSession.load_context","title":"load_context","text":"<pre><code>load_context()\n</code></pre> <p>An instance of the project context.</p> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def load_context(self) -&gt; KedroContext:\n    \"\"\"An instance of the project context.\"\"\"\n    env = self.store.get(\"env\")\n    extra_params = self.store.get(\"extra_params\")\n    config_loader = self._get_config_loader()\n    context_class = settings.CONTEXT_CLASS\n    context = context_class(\n        package_name=self._package_name,\n        project_path=self._project_path,\n        config_loader=config_loader,\n        env=env,\n        extra_params=extra_params,\n        hook_manager=self._hook_manager,\n    )\n    self._hook_manager.hook.after_context_created(context=context)\n\n    return context  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.KedroSession.run","title":"run","text":"<pre><code>run(pipeline_name=None, tags=None, runner=None, node_names=None, from_nodes=None, to_nodes=None, from_inputs=None, to_outputs=None, load_versions=None, namespace=None)\n</code></pre> <p>Runs the pipeline with a specified runner.</p> <p>Parameters:</p> <ul> <li> <code>pipeline_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the pipeline that is being run.</p> </li> <li> <code>tags</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of node tags which should be used to filter the nodes of the <code>Pipeline</code>. If specified, only the nodes containing any of these tags will be run.</p> </li> <li> <code>runner</code>               (<code>AbstractRunner | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional parameter specifying the runner that you want to run the pipeline with.</p> </li> <li> <code>node_names</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of node names which should be used to filter the nodes of the <code>Pipeline</code>. If specified, only the nodes with these names will be run.</p> </li> <li> <code>from_nodes</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of node names which should be used as a starting point of the new <code>Pipeline</code>.</p> </li> <li> <code>to_nodes</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of node names which should be used as an end point of the new <code>Pipeline</code>.</p> </li> <li> <code>from_inputs</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of input datasets which should be used as a starting point of the new <code>Pipeline</code>.</p> </li> <li> <code>to_outputs</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of output datasets which should be used as an end point of the new <code>Pipeline</code>.</p> </li> <li> <code>load_versions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional flag to specify a particular dataset version timestamp to load.</p> </li> <li> <code>namespace</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The namespace of the nodes that is being run.</p> </li> </ul> <p>Raises:     ValueError: If the named or <code>__default__</code> pipeline is not         defined by <code>register_pipelines</code>.     Exception: Any uncaught exception during the run will be re-raised         after being passed to <code>on_pipeline_error</code> hook.     KedroSessionError: If more than one run is attempted to be executed during         a single session. Returns:     Any node outputs that cannot be processed by the <code>DataCatalog</code>.     These are returned in a dictionary, where the keys are defined     by the node outputs.</p> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def run(  # noqa: PLR0913\n    self,\n    pipeline_name: str | None = None,\n    tags: Iterable[str] | None = None,\n    runner: AbstractRunner | None = None,\n    node_names: Iterable[str] | None = None,\n    from_nodes: Iterable[str] | None = None,\n    to_nodes: Iterable[str] | None = None,\n    from_inputs: Iterable[str] | None = None,\n    to_outputs: Iterable[str] | None = None,\n    load_versions: dict[str, str] | None = None,\n    namespace: str | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Runs the pipeline with a specified runner.\n\n    Args:\n        pipeline_name: Name of the pipeline that is being run.\n        tags: An optional list of node tags which should be used to\n            filter the nodes of the ``Pipeline``. If specified, only the nodes\n            containing *any* of these tags will be run.\n        runner: An optional parameter specifying the runner that you want to run\n            the pipeline with.\n        node_names: An optional list of node names which should be used to\n            filter the nodes of the ``Pipeline``. If specified, only the nodes\n            with these names will be run.\n        from_nodes: An optional list of node names which should be used as a\n            starting point of the new ``Pipeline``.\n        to_nodes: An optional list of node names which should be used as an\n            end point of the new ``Pipeline``.\n        from_inputs: An optional list of input datasets which should be\n            used as a starting point of the new ``Pipeline``.\n        to_outputs: An optional list of output datasets which should be\n            used as an end point of the new ``Pipeline``.\n        load_versions: An optional flag to specify a particular dataset\n            version timestamp to load.\n        namespace: The namespace of the nodes that is being run.\n    Raises:\n        ValueError: If the named or `__default__` pipeline is not\n            defined by `register_pipelines`.\n        Exception: Any uncaught exception during the run will be re-raised\n            after being passed to ``on_pipeline_error`` hook.\n        KedroSessionError: If more than one run is attempted to be executed during\n            a single session.\n    Returns:\n        Any node outputs that cannot be processed by the ``DataCatalog``.\n        These are returned in a dictionary, where the keys are defined\n        by the node outputs.\n    \"\"\"\n    # Report project name\n    self._logger.info(\"Kedro project %s\", self._project_path.name)\n\n    if self._run_called:\n        raise KedroSessionError(\n            \"A run has already been completed as part of the\"\n            \" active KedroSession. KedroSession has a 1-1 mapping with\"\n            \" runs, and thus only one run should be executed per session.\"\n        )\n\n    session_id = self.store[\"session_id\"]\n    save_version = session_id\n    extra_params = self.store.get(\"extra_params\") or {}\n    context = self.load_context()\n\n    name = pipeline_name or \"__default__\"\n\n    try:\n        pipeline = pipelines[name]\n    except KeyError as exc:\n        raise ValueError(\n            f\"Failed to find the pipeline named '{name}'. \"\n            f\"It needs to be generated and returned \"\n            f\"by the 'register_pipelines' function.\"\n        ) from exc\n\n    filtered_pipeline = pipeline.filter(\n        tags=tags,\n        from_nodes=from_nodes,\n        to_nodes=to_nodes,\n        node_names=node_names,\n        from_inputs=from_inputs,\n        to_outputs=to_outputs,\n        node_namespace=namespace,\n    )\n\n    record_data = {\n        \"session_id\": session_id,\n        \"project_path\": self._project_path.as_posix(),\n        \"env\": context.env,\n        \"kedro_version\": kedro_version,\n        \"tags\": tags,\n        \"from_nodes\": from_nodes,\n        \"to_nodes\": to_nodes,\n        \"node_names\": node_names,\n        \"from_inputs\": from_inputs,\n        \"to_outputs\": to_outputs,\n        \"load_versions\": load_versions,\n        \"extra_params\": extra_params,\n        \"pipeline_name\": pipeline_name,\n        \"namespace\": namespace,\n        \"runner\": getattr(runner, \"__name__\", str(runner)),\n    }\n\n    catalog = context._get_catalog(\n        save_version=save_version,\n        load_versions=load_versions,\n    )\n\n    # Run the runner\n    hook_manager = self._hook_manager\n    runner = runner or SequentialRunner()\n    if not isinstance(runner, AbstractRunner):\n        raise KedroSessionError(\n            \"KedroSession expect an instance of Runner instead of a class.\"\n            \"Have you forgotten the `()` at the end of the statement?\"\n        )\n    hook_manager.hook.before_pipeline_run(\n        run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\n    )\n    try:\n        run_result = runner.run(\n            filtered_pipeline, catalog, hook_manager, session_id\n        )\n        self._run_called = True\n    except Exception as error:\n        hook_manager.hook.on_pipeline_error(\n            error=error,\n            run_params=record_data,\n            pipeline=filtered_pipeline,\n            catalog=catalog,\n        )\n        raise\n\n    hook_manager.hook.after_pipeline_run(\n        run_params=record_data,\n        run_result=run_result,\n        pipeline=filtered_pipeline,\n        catalog=catalog,\n    )\n    return run_result\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.ProjectMetadata","title":"ProjectMetadata","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Structure holding project metadata derived from <code>pyproject.toml</code></p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project._check_module_importable","title":"_check_module_importable","text":"<pre><code>_check_module_importable(module_name)\n</code></pre> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _check_module_importable(module_name: str) -&gt; None:\n    try:\n        import_module(module_name)\n    except ImportError as exc:\n        raise KedroCliError(\n            f\"Module '{module_name}' not found. Make sure to install required project \"\n            f\"dependencies by running the 'pip install -r requirements.txt' command first.\"\n        ) from exc\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project._config_file_callback","title":"_config_file_callback","text":"<pre><code>_config_file_callback(ctx, param, value)\n</code></pre> <p>CLI callback that replaces command line options with values specified in a config file. If command line options are passed, they override config file values.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>@typing.no_type_check\ndef _config_file_callback(ctx: click.Context, param: Any, value: Any) -&gt; Any:\n    \"\"\"CLI callback that replaces command line options\n    with values specified in a config file. If command line\n    options are passed, they override config file values.\n    \"\"\"\n\n    ctx.default_map = ctx.default_map or {}\n    section = ctx.info_name\n\n    if value:\n        config = OmegaConf.to_container(OmegaConf.load(value))[section]\n        for key, value in config.items():  # noqa: PLR1704\n            _validate_config_file(key)\n        ctx.default_map.update(config)\n\n    return value\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project._split_load_versions","title":"_split_load_versions","text":"<pre><code>_split_load_versions(ctx, param, value)\n</code></pre> <p>Split and format the string coming from the --load-versions flag in kedro run, e.g.: \"dataset1:time1,dataset2:time2\" -&gt; {\"dataset1\": \"time1\", \"dataset2\": \"time2\"}</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>str</code>)           \u2013            <p>the string with the contents of the --load-versions flag.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>A dictionary with the formatted load versions data.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _split_load_versions(ctx: click.Context, param: Any, value: str) -&gt; dict[str, str]:\n    \"\"\"Split and format the string coming from the --load-versions\n    flag in kedro run, e.g.:\n    \"dataset1:time1,dataset2:time2\" -&gt; {\"dataset1\": \"time1\", \"dataset2\": \"time2\"}\n\n    Args:\n        value: the string with the contents of the --load-versions flag.\n\n    Returns:\n        A dictionary with the formatted load versions data.\n    \"\"\"\n    if not value:\n        return {}\n\n    lv_tuple = tuple(chain.from_iterable(value.split(\",\") for value in [value]))\n\n    load_versions_dict = {}\n    for load_version in lv_tuple:\n        load_version = load_version.strip()  # noqa: PLW2901\n        load_version_list = load_version.split(\":\", 1)\n        if len(load_version_list) != 2:  # noqa: PLR2004\n            raise KedroCliError(\n                f\"Expected the form of 'load_versions' to be \"\n                f\"'dataset_name:YYYY-MM-DDThh.mm.ss.sssZ',\"\n                f\"found {load_version} instead\"\n            )\n        load_versions_dict[load_version_list[0]] = load_version_list[1]\n\n    return load_versions_dict\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project._split_params","title":"_split_params","text":"<pre><code>_split_params(ctx, param, value)\n</code></pre> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _split_params(ctx: click.Context, param: Any, value: Any) -&gt; Any:\n    if isinstance(value, dict):\n        return value\n    dot_list = []\n    for item in split_string(ctx, param, value):\n        equals_idx = item.find(\"=\")\n        if equals_idx == -1:\n            # If an equals sign is not found, fail with an error message.\n            ctx.fail(\n                f\"Invalid format of `{param.name}` option: \"\n                f\"Item `{item}` must contain a key and a value separated by `=`.\"\n            )\n        # Split the item into key and value\n        key, _, val = item.partition(\"=\")\n        key = key.strip()\n        if not key:\n            # If the key is empty after stripping whitespace, fail with an error message.\n            ctx.fail(\n                f\"Invalid format of `{param.name}` option: Parameter key \"\n                f\"cannot be an empty string.\"\n            )\n        # Add \"key=value\" pair to dot_list.\n        dot_list.append(f\"{key}={val}\")\n\n    conf = OmegaConf.from_dotlist(dot_list)\n    return OmegaConf.to_container(conf)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.call","title":"call","text":"<pre><code>call(cmd, **kwargs)\n</code></pre> <p>Run a subprocess command and raise if it fails.</p> <p>Parameters:</p> <ul> <li> <code>cmd</code>               (<code>list[str]</code>)           \u2013            <p>List of command parts.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Optional keyword arguments passed to <code>subprocess.run</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>Exit</code>             \u2013            <p>If <code>subprocess.run</code> returns non-zero code.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def call(cmd: list[str], **kwargs: Any) -&gt; None:  # pragma: no cover\n    \"\"\"Run a subprocess command and raise if it fails.\n\n    Args:\n        cmd: List of command parts.\n        **kwargs: Optional keyword arguments passed to `subprocess.run`.\n\n    Raises:\n        click.exceptions.Exit: If `subprocess.run` returns non-zero code.\n    \"\"\"\n    click.echo(shlex.join(cmd))\n    code = subprocess.run(cmd, **kwargs).returncode  # noqa: PLW1510, S603\n    if code:\n        raise click.exceptions.Exit(code=code)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.env_option","title":"env_option","text":"<pre><code>env_option(func_=None, **kwargs)\n</code></pre> <p>Add <code>--env</code> CLI option to a function.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def env_option(func_: Any | None = None, **kwargs: Any) -&gt; Any:\n    \"\"\"Add `--env` CLI option to a function.\"\"\"\n    default_args = {\"type\": str, \"default\": None, \"help\": ENV_HELP}\n    kwargs = {**default_args, **kwargs}\n    opt = click.option(\"--env\", \"-e\", **kwargs)\n    return opt(func_) if func_ else opt\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.forward_command","title":"forward_command","text":"<pre><code>forward_command(group, name=None, forward_help=False)\n</code></pre> <p>A command that receives the rest of the command line as 'args'.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def forward_command(\n    group: Any, name: str | None = None, forward_help: bool = False\n) -&gt; Any:\n    \"\"\"A command that receives the rest of the command line as 'args'.\"\"\"\n\n    def wrapit(func: Any) -&gt; Any:\n        func = click.argument(\"args\", nargs=-1, type=click.UNPROCESSED)(func)\n        func = command_with_verbosity(\n            group,\n            name=name,\n            context_settings={\n                \"ignore_unknown_options\": True,\n                \"help_option_names\": [] if forward_help else [\"-h\", \"--help\"],\n            },\n        )(func)\n        return func\n\n    return wrapit\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.ipython","title":"ipython","text":"<pre><code>ipython(metadata, /, env, args, **kwargs)\n</code></pre> <p>Open IPython with project specific variables loaded.</p> Source code in <code>kedro/framework/cli/project.py</code> <pre><code>@forward_command(project_group, forward_help=True)\n@env_option\n@click.pass_obj  # this will pass the metadata as first argument\ndef ipython(metadata: ProjectMetadata, /, env: str, args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Open IPython with project specific variables loaded.\"\"\"\n    _check_module_importable(\"IPython\")\n\n    if env:\n        os.environ[\"KEDRO_ENV\"] = env\n    call([\"ipython\", \"--ext\", \"kedro.ipython\", *list(args)])\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.load_obj","title":"load_obj","text":"<pre><code>load_obj(obj_path, default_obj_path='')\n</code></pre> <p>Extract an object from a given path.</p> <p>Parameters:</p> <ul> <li> <code>obj_path</code>               (<code>str</code>)           \u2013            <p>Path to an object to be extracted, including the object name.</p> </li> <li> <code>default_obj_path</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Default object path.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Extracted object.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AttributeError</code>             \u2013            <p>When the object does not have the given named attribute.</p> </li> </ul> Source code in <code>kedro/utils.py</code> <pre><code>def load_obj(obj_path: str, default_obj_path: str = \"\") -&gt; Any:\n    \"\"\"Extract an object from a given path.\n\n    Args:\n        obj_path: Path to an object to be extracted, including the object name.\n        default_obj_path: Default object path.\n\n    Returns:\n        Extracted object.\n\n    Raises:\n        AttributeError: When the object does not have the given named attribute.\n\n    \"\"\"\n    obj_path_list = obj_path.rsplit(\".\", 1)\n    obj_path = obj_path_list.pop(0) if len(obj_path_list) &gt; 1 else default_obj_path\n    obj_name = obj_path_list[0]\n    module_obj = importlib.import_module(obj_path)\n    return getattr(module_obj, obj_name)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.package","title":"package","text":"<pre><code>package(metadata)\n</code></pre> <p>Package the project as a Python wheel.</p> Source code in <code>kedro/framework/cli/project.py</code> <pre><code>@project_group.command()\n@click.pass_obj  # this will pass the metadata as first argument\ndef package(metadata: ProjectMetadata) -&gt; None:\n    \"\"\"Package the project as a Python wheel.\"\"\"\n    # Even if the user decides for the older setup.py on purpose,\n    # pyproject.toml is needed for Kedro metadata\n    if (metadata.project_path / \"pyproject.toml\").is_file():\n        metadata_dir = metadata.project_path\n        destination_dir = \"dist\"\n    else:\n        # Assume it's an old Kedro project, packaging metadata was under src\n        # (could be pyproject.toml or setup.py, it's not important)\n        metadata_dir = metadata.source_dir\n        destination_dir = \"../dist\"\n\n    call(\n        [\n            sys.executable,\n            \"-m\",\n            \"build\",\n            \"--wheel\",\n            \"--outdir\",\n            destination_dir,\n        ],\n        cwd=str(metadata_dir),\n    )\n\n    directory = (\n        str(Path(settings.CONF_SOURCE).parent)\n        if settings.CONF_SOURCE != \"conf\"\n        else metadata.project_path\n    )\n    call(\n        [\n            \"tar\",\n            \"--exclude=local/*.yml\",\n            \"-czf\",\n            f\"dist/conf-{metadata.package_name}.tar.gz\",\n            f\"--directory={directory}\",\n            str(Path(settings.CONF_SOURCE).stem),\n        ]\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.project_group","title":"project_group","text":"<pre><code>project_group()\n</code></pre> Source code in <code>kedro/framework/cli/project.py</code> <pre><code>@click.group(name=\"Kedro\")\ndef project_group() -&gt; None:  # pragma: no cover\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.run","title":"run","text":"<pre><code>run(tags, env, runner, is_async, node_names, to_nodes, from_nodes, from_inputs, to_outputs, load_versions, pipeline, config, conf_source, params, namespace)\n</code></pre> <p>Run the pipeline.</p> Source code in <code>kedro/framework/cli/project.py</code> <pre><code>@project_group.command()\n@click.option(\n    \"--from-inputs\",\n    type=str,\n    default=\"\",\n    help=FROM_INPUTS_HELP,\n    callback=split_string,\n)\n@click.option(\n    \"--to-outputs\",\n    type=str,\n    default=\"\",\n    help=TO_OUTPUTS_HELP,\n    callback=split_string,\n)\n@click.option(\n    \"--from-nodes\",\n    type=str,\n    default=\"\",\n    help=FROM_NODES_HELP,\n    callback=split_node_names,\n)\n@click.option(\n    \"--to-nodes\", type=str, default=\"\", help=TO_NODES_HELP, callback=split_node_names\n)\n@click.option(\n    \"--nodes\",\n    \"-n\",\n    \"node_names\",\n    type=str,\n    default=\"\",\n    help=NODE_ARG_HELP,\n    callback=split_node_names,\n)\n@click.option(\"--runner\", \"-r\", type=str, default=None, help=RUNNER_ARG_HELP)\n@click.option(\"--async\", \"is_async\", is_flag=True, help=ASYNC_ARG_HELP)\n@env_option\n@click.option(\n    \"--tags\",\n    \"-t\",\n    type=str,\n    default=\"\",\n    help=TAG_ARG_HELP,\n    callback=split_string,\n)\n@click.option(\n    \"--load-versions\",\n    \"-lv\",\n    type=str,\n    default=\"\",\n    help=LOAD_VERSION_HELP,\n    callback=_split_load_versions,\n)\n@click.option(\"--pipeline\", \"-p\", type=str, default=None, help=PIPELINE_ARG_HELP)\n@click.option(\"--namespace\", \"-ns\", type=str, default=None, help=NAMESPACE_ARG_HELP)\n@click.option(\n    \"--config\",\n    \"-c\",\n    type=click.Path(exists=True, dir_okay=False, resolve_path=True),\n    help=CONFIG_FILE_HELP,\n    callback=_config_file_callback,\n)\n@click.option(\n    \"--conf-source\",\n    callback=validate_conf_source,\n    help=CONF_SOURCE_HELP,\n)\n@click.option(\n    \"--params\",\n    type=click.UNPROCESSED,\n    default=\"\",\n    help=PARAMS_ARG_HELP,\n    callback=_split_params,\n)\ndef run(  # noqa: PLR0913\n    tags: str,\n    env: str,\n    runner: str,\n    is_async: bool,\n    node_names: str,\n    to_nodes: str,\n    from_nodes: str,\n    from_inputs: str,\n    to_outputs: str,\n    load_versions: dict[str, str] | None,\n    pipeline: str,\n    config: str,\n    conf_source: str,\n    params: dict[str, Any],\n    namespace: str,\n) -&gt; dict[str, Any]:\n    \"\"\"Run the pipeline.\"\"\"\n\n    runner_obj = load_obj(runner or \"SequentialRunner\", \"kedro.runner\")\n    tuple_tags = tuple(tags)\n    tuple_node_names = tuple(node_names)\n\n    with KedroSession.create(\n        env=env, conf_source=conf_source, extra_params=params\n    ) as session:\n        return session.run(\n            tags=tuple_tags,\n            runner=runner_obj(is_async=is_async),\n            node_names=tuple_node_names,\n            from_nodes=from_nodes,\n            to_nodes=to_nodes,\n            from_inputs=from_inputs,\n            to_outputs=to_outputs,\n            load_versions=load_versions,\n            pipeline_name=pipeline,\n            namespace=namespace,\n        )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.split_node_names","title":"split_node_names","text":"<pre><code>split_node_names(ctx, param, to_split)\n</code></pre> <p>Split string by comma, ignoring commas enclosed by square parentheses. This avoids splitting the string of nodes names on commas included in default node names, which have the pattern ([,...]) -&gt; [,...]) Note <ul> <li><code>to_split</code> will have such commas if and only if it includes a default node name. User-defined node names cannot include commas or square brackets.</li> <li>This function will no longer be necessary from Kedro 0.19.*, in which default node names will no longer contain commas</li> </ul> <p>Parameters:</p> <ul> <li> <code>to_split</code>               (<code>str</code>)           \u2013            <p>the string to split safely</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>A list containing the result of safe-splitting the string.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def split_node_names(ctx: click.Context, param: Any, to_split: str) -&gt; list[str]:\n    \"\"\"Split string by comma, ignoring commas enclosed by square parentheses.\n    This avoids splitting the string of nodes names on commas included in\n    default node names, which have the pattern\n    &lt;function_name&gt;([&lt;input_name&gt;,...]) -&gt; [&lt;output_name&gt;,...])\n\n    Note:\n        - `to_split` will have such commas if and only if it includes a\n        default node name. User-defined node names cannot include commas\n        or square brackets.\n        - This function will no longer be necessary from Kedro 0.19.*,\n        in which default node names will no longer contain commas\n\n    Args:\n        to_split: the string to split safely\n\n    Returns:\n        A list containing the result of safe-splitting the string.\n    \"\"\"\n    result = []\n    argument, match_state = \"\", 0\n    for char in to_split + \",\":\n        if char == \"[\":\n            match_state += 1\n        elif char == \"]\":\n            match_state -= 1\n        if char == \",\" and match_state == 0 and argument:\n            argument = argument.strip()\n            result.append(argument)\n            argument = \"\"\n        else:\n            argument += char\n    return result\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.split_string","title":"split_string","text":"<pre><code>split_string(ctx, param, value)\n</code></pre> <p>Split string by comma.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def split_string(ctx: click.Context, param: Any, value: str) -&gt; list[str]:\n    \"\"\"Split string by comma.\"\"\"\n    return [item.strip() for item in value.split(\",\") if item.strip()]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.project.validate_conf_source","title":"validate_conf_source","text":"<pre><code>validate_conf_source(ctx, param, value)\n</code></pre> <p>Validate the conf_source, only checking existence for local paths.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def validate_conf_source(ctx: click.Context, param: Any, value: str) -&gt; str | None:\n    \"\"\"Validate the conf_source, only checking existence for local paths.\"\"\"\n    if not value:\n        return None\n\n    # Check for remote URLs (except file://)\n    if \"://\" in value and not value.startswith(\"file://\"):\n        return value\n\n    # For local paths\n    try:\n        path = Path(value)\n        if not path.exists():\n            raise click.BadParameter(f\"Path '{value}' does not exist.\")\n        return str(path.resolve())\n    except click.BadParameter:\n        # Re-raise Click exceptions\n        raise\n    except Exception as exc:\n        # Wrap other exceptions\n        raise click.BadParameter(f\"Invalid path: {value}. Error: {exc!s}\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.registry","title":"kedro.framework.cli.registry","text":"<p>A collection of CLI commands for working with registered Kedro pipelines.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.registry.pipelines","title":"pipelines  <code>module-attribute</code>","text":"<pre><code>pipelines = _ProjectPipelines()\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.registry.KedroCliError","title":"KedroCliError","text":"<p>               Bases: <code>ClickException</code></p> <p>Exceptions generated from the Kedro CLI.</p> <p>Users should pass an appropriate message at the constructor.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.registry.ProjectMetadata","title":"ProjectMetadata","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Structure holding project metadata derived from <code>pyproject.toml</code></p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.registry.command_with_verbosity","title":"command_with_verbosity","text":"<pre><code>command_with_verbosity(group, *args, **kwargs)\n</code></pre> <p>Custom command decorator with verbose flag added.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def command_with_verbosity(group: click.core.Group, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Custom command decorator with verbose flag added.\"\"\"\n\n    def decorator(func: Any) -&gt; Any:\n        func = _click_verbose(func)\n        func = group.command(*args, **kwargs)(func)\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.registry.describe_registered_pipeline","title":"describe_registered_pipeline","text":"<pre><code>describe_registered_pipeline(metadata, /, name, **kwargs)\n</code></pre> <p>Describe a registered pipeline by providing a pipeline name. Defaults to the <code>__default__</code> pipeline.</p> Source code in <code>kedro/framework/cli/registry.py</code> <pre><code>@command_with_verbosity(registry, \"describe\")\n@click.argument(\"name\", nargs=1, default=\"__default__\")\n@click.pass_obj\ndef describe_registered_pipeline(\n    metadata: ProjectMetadata, /, name: str, **kwargs: Any\n) -&gt; None:\n    \"\"\"Describe a registered pipeline by providing a pipeline name.\n    Defaults to the `__default__` pipeline.\n    \"\"\"\n    pipeline_obj = pipelines.get(name)\n    if not pipeline_obj:\n        all_pipeline_names = pipelines.keys()\n        existing_pipelines = \", \".join(sorted(all_pipeline_names))\n        raise KedroCliError(\n            f\"'{name}' pipeline not found. Existing pipelines: [{existing_pipelines}]\"\n        )\n\n    nodes = []\n    for node in pipeline_obj.nodes:\n        namespace = f\"{node.namespace}.\" if node.namespace else \"\"\n        nodes.append(f\"{namespace}{node._name or node._func_name} ({node._func_name})\")\n    result = {\"Nodes\": nodes}\n\n    click.echo(yaml.dump(result))\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.registry.list_registered_pipelines","title":"list_registered_pipelines","text":"<pre><code>list_registered_pipelines()\n</code></pre> <p>List all pipelines defined in your pipeline_registry.py file.</p> Source code in <code>kedro/framework/cli/registry.py</code> <pre><code>@registry.command(\"list\")\ndef list_registered_pipelines() -&gt; None:\n    \"\"\"List all pipelines defined in your pipeline_registry.py file.\"\"\"\n    click.echo(yaml.dump(sorted(pipelines)))\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.registry.registry","title":"registry","text":"<pre><code>registry()\n</code></pre> <p>Commands for working with registered pipelines.</p> Source code in <code>kedro/framework/cli/registry.py</code> <pre><code>@registry_cli.group()\ndef registry() -&gt; None:\n    \"\"\"Commands for working with registered pipelines.\"\"\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.registry.registry_cli","title":"registry_cli","text":"<pre><code>registry_cli()\n</code></pre> Source code in <code>kedro/framework/cli/registry.py</code> <pre><code>@click.group(name=\"Kedro\")\ndef registry_cli() -&gt; None:  # pragma: no cover\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters","title":"kedro.framework.cli.starters","text":"<p>kedro is a CLI for managing Kedro projects.</p> <p>This module implements commands available from the kedro CLI for creating projects.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.CHECKOUT_ARG_HELP","title":"CHECKOUT_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>CHECKOUT_ARG_HELP = 'An optional tag, branch or commit to checkout in the starter repository.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.CONFIG_ARG_HELP","title":"CONFIG_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>CONFIG_ARG_HELP = \"Non-interactive mode, using a configuration yaml file. This file\\nmust supply  the keys required by the template's prompts.yml. When not using a starter,\\nthese are `project_name`, `repo_name` and `python_package`.\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.CONTEXT_SETTINGS","title":"CONTEXT_SETTINGS  <code>module-attribute</code>","text":"<pre><code>CONTEXT_SETTINGS = {'help_option_names': ['-h', '--help']}\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.DIRECTORY_ARG_HELP","title":"DIRECTORY_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>DIRECTORY_ARG_HELP = 'An optional directory inside the repository where the starter resides.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.EXAMPLE_ARG_HELP","title":"EXAMPLE_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>EXAMPLE_ARG_HELP = 'Enter y to enable, n to disable the example pipeline.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.KEDRO_PATH","title":"KEDRO_PATH  <code>module-attribute</code>","text":"<pre><code>KEDRO_PATH = parent\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.NAME_ARG_HELP","title":"NAME_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>NAME_ARG_HELP = 'The name of your new Kedro project.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.NUMBER_TO_TOOLS_NAME","title":"NUMBER_TO_TOOLS_NAME  <code>module-attribute</code>","text":"<pre><code>NUMBER_TO_TOOLS_NAME = {'1': 'Linting', '2': 'Testing', '3': 'Custom Logging', '4': 'Documentation', '5': 'Data Structure', '6': 'PySpark'}\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.STARTER_ARG_HELP","title":"STARTER_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>STARTER_ARG_HELP = 'Specify the starter template to use when creating the project.\\nThis can be the path to a local directory, a URL to a remote VCS repository supported\\nby `cookiecutter` or one of the aliases listed in ``kedro starter list``.\\n'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.TELEMETRY_ARG_HELP","title":"TELEMETRY_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>TELEMETRY_ARG_HELP = 'Allow or not allow Kedro to collect usage analytics.\\nWe cannot see nor store information contained into a Kedro project. Opt in with \"yes\"\\nand out with \"no\".\\n'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.TEMPLATE_PATH","title":"TEMPLATE_PATH  <code>module-attribute</code>","text":"<pre><code>TEMPLATE_PATH = KEDRO_PATH / 'templates' / 'project'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.TOOLS_ARG_HELP","title":"TOOLS_ARG_HELP  <code>module-attribute</code>","text":"<pre><code>TOOLS_ARG_HELP = \"\\nSelect which tools you'd like to include. By default, none are included.\\n\\n\\nTools\\n\\n1) Linting: Provides a basic linting setup with Ruff\\n\\n2) Testing: Provides basic testing setup with pytest\\n\\n3) Custom Logging: Provides more logging options\\n\\n4) Documentation: Basic documentation setup with Sphinx\\n\\n5) Data Structure: Provides a directory structure for storing data\\n\\n6) PySpark: Provides set up configuration for working with PySpark\\n\\n\\nExample usage:\\n\\nkedro new --tools=lint,test,log,docs,data,pyspark (or any subset of these options)\\n\\nkedro new --tools=all\\n\\nkedro new --tools=none\\n\\nFor more information on using tools, see https://docs.kedro.org/en/stable/starters/new_project_tools.html\\n\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.TOOLS_SHORTNAME_TO_NUMBER","title":"TOOLS_SHORTNAME_TO_NUMBER  <code>module-attribute</code>","text":"<pre><code>TOOLS_SHORTNAME_TO_NUMBER = {'lint': '1', 'test': '2', 'tests': '2', 'log': '3', 'logs': '3', 'docs': '4', 'doc': '4', 'data': '5', 'pyspark': '6'}\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._OFFICIAL_STARTER_SPECS","title":"_OFFICIAL_STARTER_SPECS  <code>module-attribute</code>","text":"<pre><code>_OFFICIAL_STARTER_SPECS = [KedroStarterSpec('astro-airflow-iris', _STARTERS_REPO, 'astro-airflow-iris'), KedroStarterSpec('spaceflights-pandas', _STARTERS_REPO, 'spaceflights-pandas'), KedroStarterSpec('spaceflights-pyspark', _STARTERS_REPO, 'spaceflights-pyspark'), KedroStarterSpec('databricks-iris', _STARTERS_REPO, 'databricks-iris')]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._OFFICIAL_STARTER_SPECS_DICT","title":"_OFFICIAL_STARTER_SPECS_DICT  <code>module-attribute</code>","text":"<pre><code>_OFFICIAL_STARTER_SPECS_DICT = {alias: specfor spec in _OFFICIAL_STARTER_SPECS}\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._STARTERS_REPO","title":"_STARTERS_REPO  <code>module-attribute</code>","text":"<pre><code>_STARTERS_REPO = 'git+https://github.com/kedro-org/kedro-starters.git'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.version","title":"version  <code>module-attribute</code>","text":"<pre><code>version = '0.19.12'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.KedroCliError","title":"KedroCliError","text":"<p>               Bases: <code>ClickException</code></p> <p>Exceptions generated from the Kedro CLI.</p> <p>Users should pass an appropriate message at the constructor.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.KedroStarterSpec","title":"KedroStarterSpec","text":"<p>Specification of custom kedro starter template Args:     alias: alias of the starter which shows up on <code>kedro starter list</code> and is used     by the starter argument of <code>kedro new</code>     template_path: path to a directory or a URL to a remote VCS repository supported     by <code>cookiecutter</code>     directory: optional directory inside the repository where the starter resides.     origin: reserved field used by kedro internally to determine where the starter     comes from, users do not need to provide this field.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._Prompt","title":"_Prompt","text":"<pre><code>_Prompt(*args, **kwargs)\n</code></pre> <p>Represent a single CLI prompt for <code>kedro new</code></p> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    try:\n        self.title = kwargs[\"title\"]\n    except KeyError as exc:\n        raise KedroCliError(\n            \"Each prompt must have a title field to be valid.\"\n        ) from exc\n\n    self.text = kwargs.get(\"text\", \"\")\n    self.regexp = kwargs.get(\"regex_validator\", None)\n    self.error_message = kwargs.get(\"error_message\", \"\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._Prompt.validate","title":"validate","text":"<pre><code>validate(user_input)\n</code></pre> <p>Validate a given prompt value against the regex validator</p> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def validate(self, user_input: str) -&gt; None:\n    \"\"\"Validate a given prompt value against the regex validator\"\"\"\n    if self.regexp and not re.match(self.regexp, user_input):\n        message = f\"'{user_input}' is an invalid value for {(self.title).lower()}.\"\n        click.secho(message, fg=\"red\", err=True)\n        click.secho(self.error_message, fg=\"red\", err=True)\n        sys.exit(1)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._clean_pycache","title":"_clean_pycache","text":"<pre><code>_clean_pycache(path)\n</code></pre> <p>Recursively clean all pycache folders from <code>path</code>.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>Existing local directory to clean pycache folders from.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _clean_pycache(path: Path) -&gt; None:\n    \"\"\"Recursively clean all __pycache__ folders from `path`.\n\n    Args:\n        path: Existing local directory to clean __pycache__ folders from.\n    \"\"\"\n    to_delete = [each.resolve() for each in path.rglob(\"__pycache__\")]\n\n    for each in to_delete:\n        shutil.rmtree(each, ignore_errors=True)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._convert_tool_numbers_to_readable_names","title":"_convert_tool_numbers_to_readable_names","text":"<pre><code>_convert_tool_numbers_to_readable_names(tools_numbers)\n</code></pre> <p>Transform the list of tool numbers into a list of readable names, using 'None' for empty lists. Then, convert the result into a string format to prevent issues with Cookiecutter.</p> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _convert_tool_numbers_to_readable_names(tools_numbers: list) -&gt; str:\n    \"\"\"Transform the list of tool numbers into a list of readable names, using 'None' for empty lists.\n    Then, convert the result into a string format to prevent issues with Cookiecutter.\n    \"\"\"\n    tools_names = [NUMBER_TO_TOOLS_NAME[tool] for tool in tools_numbers]\n    if tools_names == []:\n        tools_names = [\"None\"]\n    return str(tools_names)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._convert_tool_short_names_to_numbers","title":"_convert_tool_short_names_to_numbers","text":"<pre><code>_convert_tool_short_names_to_numbers(selected_tools)\n</code></pre> <p>Prepares tools selection from the CLI or config input to the correct format to be put in the project configuration, if it exists. Replaces tool strings with the corresponding prompt number.</p> <p>Parameters:</p> <ul> <li> <code>selected_tools</code>               (<code>str</code>)           \u2013            <p>a string containing the value for the --tools flag or config file, or None in case none were provided, i.e. lint,docs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code>           \u2013            <p>String with the numbers corresponding to the desired tools, or</p> </li> <li> <code>list</code>           \u2013            <p>None in case the --tools flag was not used.</p> </li> </ul> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _convert_tool_short_names_to_numbers(selected_tools: str) -&gt; list:\n    \"\"\"Prepares tools selection from the CLI or config input to the correct format\n    to be put in the project configuration, if it exists.\n    Replaces tool strings with the corresponding prompt number.\n\n    Args:\n        selected_tools: a string containing the value for the --tools flag or config file,\n            or None in case none were provided, i.e. lint,docs.\n\n    Returns:\n        String with the numbers corresponding to the desired tools, or\n        None in case the --tools flag was not used.\n    \"\"\"\n    if selected_tools.lower() == \"none\":\n        return []\n    if selected_tools.lower() == \"all\":\n        return list(NUMBER_TO_TOOLS_NAME.keys())\n\n    tools = []\n    for tool in selected_tools.lower().split(\",\"):\n        tool_short_name = tool.strip()\n        if tool_short_name in TOOLS_SHORTNAME_TO_NUMBER:\n            tools.append(TOOLS_SHORTNAME_TO_NUMBER[tool_short_name])\n\n    # Remove duplicates if any\n    tools = sorted(list(set(tools)))\n\n    return tools\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._create_project","title":"_create_project","text":"<pre><code>_create_project(template_path, cookiecutter_args, telemetry_consent)\n</code></pre> <p>Creates a new kedro project using cookiecutter.</p> <p>Parameters:</p> <ul> <li> <code>template_path</code>               (<code>str</code>)           \u2013            <p>The path to the cookiecutter template to create the project. It could either be a local directory or a remote VCS repository supported by cookiecutter. For more details, please see: https://cookiecutter.readthedocs.io/en/stable/usage.html#generate-your-project</p> </li> <li> <code>cookiecutter_args</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Arguments to pass to cookiecutter.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KedroCliError</code>             \u2013            <p>If it fails to generate a project.</p> </li> </ul> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _create_project(\n    template_path: str, cookiecutter_args: dict[str, Any], telemetry_consent: str | None\n) -&gt; None:\n    \"\"\"Creates a new kedro project using cookiecutter.\n\n    Args:\n        template_path: The path to the cookiecutter template to create the project.\n            It could either be a local directory or a remote VCS repository\n            supported by cookiecutter. For more details, please see:\n            https://cookiecutter.readthedocs.io/en/stable/usage.html#generate-your-project\n        cookiecutter_args: Arguments to pass to cookiecutter.\n\n    Raises:\n        KedroCliError: If it fails to generate a project.\n    \"\"\"\n    from cookiecutter.main import cookiecutter  # for performance reasons\n\n    try:\n        result_path = cookiecutter(template=template_path, **cookiecutter_args)\n\n        if telemetry_consent is not None:\n            with open(result_path + \"/.telemetry\", \"w\") as telemetry_file:\n                telemetry_file.write(\"consent: \" + telemetry_consent)\n    except Exception as exc:\n        raise KedroCliError(\n            \"Failed to generate project when running cookiecutter.\"\n        ) from exc\n\n    _clean_pycache(Path(result_path))\n    extra_context = cookiecutter_args[\"extra_context\"]\n    project_name = extra_context.get(\"project_name\", \"New Kedro Project\")\n\n    # Print success message\n    click.secho(\n        \"\\nCongratulations!\"\n        f\"\\nYour project '{project_name}' has been created in the directory \\n{result_path}\\n\"\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._fetch_validate_parse_config_from_file","title":"_fetch_validate_parse_config_from_file","text":"<pre><code>_fetch_validate_parse_config_from_file(config_path, prompts_required, starter_alias)\n</code></pre> <p>Obtains configuration for a new kedro project non-interactively from a file. Validates that: 1. All keys specified in prompts_required are retrieved from the configuration. 2. The options 'tools' and 'example_pipeline' are not used in the configuration when any starter option is selected. 3. Variables sourced from the configuration file adhere to the expected format.</p> <p>Parse tools from short names to list of numbers</p> <p>Parameters:</p> <ul> <li> <code>config_path</code>               (<code>str</code>)           \u2013            <p>The path of the config.yml which should contain the data required by <code>prompts.yml</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>Configuration for starting a new project. This is passed as <code>extra_context</code> to cookiecutter and will overwrite the cookiecutter.json defaults.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KedroCliError</code>             \u2013            <p>If the file cannot be parsed.</p> </li> </ul> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _fetch_validate_parse_config_from_file(\n    config_path: str, prompts_required: dict, starter_alias: str | None\n) -&gt; dict[str, str]:\n    \"\"\"Obtains configuration for a new kedro project non-interactively from a file.\n    Validates that:\n    1. All keys specified in prompts_required are retrieved from the configuration.\n    2. The options 'tools' and 'example_pipeline' are not used in the configuration when any starter option is selected.\n    3. Variables sourced from the configuration file adhere to the expected format.\n\n    Parse tools from short names to list of numbers\n\n    Args:\n        config_path: The path of the config.yml which should contain the data required\n            by ``prompts.yml``.\n\n    Returns:\n        Configuration for starting a new project. This is passed as ``extra_context``\n            to cookiecutter and will overwrite the cookiecutter.json defaults.\n\n    Raises:\n        KedroCliError: If the file cannot be parsed.\n\n    \"\"\"\n    try:\n        with open(config_path, encoding=\"utf-8\") as config_file:\n            config: dict[str, str] = yaml.safe_load(config_file)\n\n        if KedroCliError.VERBOSE_ERROR:\n            click.echo(config_path + \":\")\n            click.echo(yaml.dump(config, default_flow_style=False))\n    except Exception as exc:\n        raise KedroCliError(\n            f\"Failed to generate project: could not load config at {config_path}.\"\n        ) from exc\n\n    if starter_alias and (\"tools\" in config or \"example_pipeline\" in config):\n        raise KedroCliError(\n            \"The --starter flag can not be used with `example_pipeline` and/or `tools` keys in the config file.\"\n        )\n\n    _validate_config_file_against_prompts(config, prompts_required)\n\n    _validate_input_with_regex_pattern(\n        \"project_name\", config.get(\"project_name\", \"New Kedro Project\")\n    )\n\n    example_pipeline = config.get(\"example_pipeline\", \"no\")\n    _validate_input_with_regex_pattern(\"yes_no\", example_pipeline)\n    config[\"example_pipeline\"] = str(_parse_yes_no_to_bool(example_pipeline))\n\n    tools_short_names = config.get(\"tools\", \"none\").lower()\n    _validate_selected_tools(tools_short_names)\n    tools_numbers = _convert_tool_short_names_to_numbers(tools_short_names)\n    config[\"tools\"] = _convert_tool_numbers_to_readable_names(tools_numbers)\n\n    return config\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._fetch_validate_parse_config_from_user_prompts","title":"_fetch_validate_parse_config_from_user_prompts","text":"<pre><code>_fetch_validate_parse_config_from_user_prompts(prompts, cookiecutter_context)\n</code></pre> <p>Interactively obtains information from user prompts.</p> <p>Parameters:</p> <ul> <li> <code>prompts</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Prompts from prompts.yml.</p> </li> <li> <code>cookiecutter_context</code>               (<code>OrderedDict | None</code>)           \u2013            <p>Cookiecutter context generated from cookiecutter.json.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>Configuration for starting a new project. This is passed as <code>extra_context</code> to cookiecutter and will overwrite the cookiecutter.json defaults.</p> </li> </ul> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _fetch_validate_parse_config_from_user_prompts(\n    prompts: dict[str, Any],\n    cookiecutter_context: OrderedDict | None,\n) -&gt; dict[str, str]:\n    \"\"\"Interactively obtains information from user prompts.\n\n    Args:\n        prompts: Prompts from prompts.yml.\n        cookiecutter_context: Cookiecutter context generated from cookiecutter.json.\n\n    Returns:\n        Configuration for starting a new project. This is passed as ``extra_context``\n            to cookiecutter and will overwrite the cookiecutter.json defaults.\n    \"\"\"\n    if not cookiecutter_context:\n        raise Exception(\"No cookiecutter context available.\")\n\n    config: dict[str, str] = {}\n\n    for variable_name, prompt_dict in prompts.items():\n        prompt = _Prompt(**prompt_dict)\n\n        # render the variable on the command line\n        default_value = cookiecutter_context.get(variable_name) or \"\"\n\n        # read the user's input for the variable\n        user_input = click.prompt(\n            str(prompt),\n            default=default_value,\n            show_default=True,\n            type=str,\n        ).strip()\n\n        if user_input:\n            prompt.validate(user_input)\n            config[variable_name] = user_input\n\n    if \"tools\" in config:\n        # convert tools input to list of numbers and validate\n        tools_numbers = _parse_tools_input(config[\"tools\"])\n        _validate_tool_selection(tools_numbers)\n        config[\"tools\"] = _convert_tool_numbers_to_readable_names(tools_numbers)\n    if \"example_pipeline\" in config:\n        example_pipeline_bool = _parse_yes_no_to_bool(config[\"example_pipeline\"])\n        config[\"example_pipeline\"] = str(example_pipeline_bool)\n\n    return config\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._get_available_tags","title":"_get_available_tags","text":"<pre><code>_get_available_tags(template_path)\n</code></pre> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _get_available_tags(template_path: str) -&gt; list:\n    # Not at top level so that kedro CLI works without a working git executable.\n    import git\n\n    try:\n        tags = git.cmd.Git().ls_remote(\"--tags\", template_path.replace(\"git+\", \"\"))\n\n        unique_tags = {\n            tag.split(\"/\")[-1].replace(\"^{}\", \"\") for tag in tags.split(\"\\n\")\n        }\n        # Remove git ref \"^{}\" and duplicates. For example,\n        # tags: ['/tags/version', '/tags/version^{}']\n        # unique_tags: {'version'}\n\n    except git.GitCommandError:  # pragma: no cover\n        return []\n    return sorted(unique_tags)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._get_cookiecutter_dir","title":"_get_cookiecutter_dir","text":"<pre><code>_get_cookiecutter_dir(template_path, checkout, directory, tmpdir)\n</code></pre> <p>Gives a path to the cookiecutter directory. If template_path is a repo then clones it to <code>tmpdir</code>; if template_path is a file path then directly uses that path without copying anything.</p> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _get_cookiecutter_dir(\n    template_path: str, checkout: str, directory: str, tmpdir: str\n) -&gt; Path:\n    \"\"\"Gives a path to the cookiecutter directory. If template_path is a repo then\n    clones it to ``tmpdir``; if template_path is a file path then directly uses that\n    path without copying anything.\n    \"\"\"\n    from cookiecutter.exceptions import RepositoryCloneFailed, RepositoryNotFound\n    from cookiecutter.repository import determine_repo_dir  # for performance reasons\n\n    try:\n        cookiecutter_dir, _ = determine_repo_dir(\n            template=template_path,\n            abbreviations={},\n            clone_to_dir=Path(tmpdir).resolve(),\n            checkout=checkout,\n            no_input=True,\n            directory=directory,\n        )\n    except (RepositoryNotFound, RepositoryCloneFailed) as exc:\n        error_message = f\"Kedro project template not found at {template_path}.\"\n\n        if checkout:\n            error_message += (\n                f\" Specified tag {checkout}. The following tags are available: \"\n                + \", \".join(_get_available_tags(template_path))\n            )\n        official_starters = sorted(_OFFICIAL_STARTER_SPECS_DICT)\n        raise KedroCliError(\n            f\"{error_message}. The aliases for the official Kedro starters are: \\n\"\n            f\"{yaml.safe_dump(official_starters, sort_keys=False)}\"\n        ) from exc\n\n    return Path(cookiecutter_dir)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._get_entry_points","title":"_get_entry_points","text":"<pre><code>_get_entry_points(name)\n</code></pre> <p>Get all kedro related entry points</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _get_entry_points(name: str) -&gt; Any:\n    \"\"\"Get all kedro related entry points\"\"\"\n    return importlib_metadata.entry_points().select(  # type: ignore[no-untyped-call]\n        group=ENTRY_POINT_GROUPS[name]\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._get_extra_context","title":"_get_extra_context","text":"<pre><code>_get_extra_context(prompts_required, config_path, cookiecutter_context, selected_tools, project_name, example_pipeline, starter_alias)\n</code></pre> <p>Generates a config dictionary that will be passed to cookiecutter as <code>extra_context</code>, based on CLI flags, user prompts, configuration file or Default values. It is crucial to return a dictionary with string values, otherwise, there will be issues with Cookiecutter.</p> <p>Parameters:</p> <ul> <li> <code>prompts_required</code>               (<code>dict</code>)           \u2013            <p>a dictionary of all the prompts that will be shown to the user on project creation.</p> </li> <li> <code>config_path</code>               (<code>str</code>)           \u2013            <p>a string containing the value for the --config flag, or None in case the flag wasn't used.</p> </li> <li> <code>cookiecutter_context</code>               (<code>OrderedDict | None</code>)           \u2013            <p>the context for Cookiecutter templates.</p> </li> <li> <code>selected_tools</code>               (<code>str | None</code>)           \u2013            <p>a string containing the value for the --tools flag, or None in case the flag wasn't used.</p> </li> <li> <code>project_name</code>               (<code>str | None</code>)           \u2013            <p>a string containing the value for the --name flag, or None in case the flag wasn't used.</p> </li> <li> <code>example_pipeline</code>               (<code>str | None</code>)           \u2013            <p>a string containing the value for the --example flag, or None in case the flag wasn't used</p> </li> <li> <code>starter_alias</code>               (<code>str | None</code>)           \u2013            <p>a string containing the value for the --starter flag, or None in case the flag wasn't used</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>Config dictionary, passed the necessary processing, with default values if needed.</p> </li> </ul> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _get_extra_context(  # noqa: PLR0913\n    prompts_required: dict,\n    config_path: str,\n    cookiecutter_context: OrderedDict | None,\n    selected_tools: str | None,\n    project_name: str | None,\n    example_pipeline: str | None,\n    starter_alias: str | None,\n) -&gt; dict[str, str]:\n    \"\"\"Generates a config dictionary that will be passed to cookiecutter as `extra_context`, based\n    on CLI flags, user prompts, configuration file or Default values.\n    It is crucial to return a dictionary with string values, otherwise, there will be issues with Cookiecutter.\n\n    Args:\n        prompts_required: a dictionary of all the prompts that will be shown to\n            the user on project creation.\n        config_path: a string containing the value for the --config flag, or\n            None in case the flag wasn't used.\n        cookiecutter_context: the context for Cookiecutter templates.\n        selected_tools: a string containing the value for the --tools flag,\n            or None in case the flag wasn't used.\n        project_name: a string containing the value for the --name flag, or\n            None in case the flag wasn't used.\n        example_pipeline: a string containing the value for the --example flag,\n            or None in case the flag wasn't used\n        starter_alias: a string containing the value for the --starter flag, or\n            None in case the flag wasn't used\n\n    Returns:\n        Config dictionary, passed the necessary processing, with default values if needed.\n    \"\"\"\n    if config_path:\n        extra_context = _fetch_validate_parse_config_from_file(\n            config_path, prompts_required, starter_alias\n        )\n    else:\n        extra_context = _fetch_validate_parse_config_from_user_prompts(\n            prompts_required, cookiecutter_context\n        )\n\n    # Update extra_context, if CLI inputs are available\n    if selected_tools is not None:\n        tools_numbers = _convert_tool_short_names_to_numbers(selected_tools)\n        extra_context[\"tools\"] = _convert_tool_numbers_to_readable_names(tools_numbers)\n    if project_name is not None:\n        extra_context[\"project_name\"] = project_name\n    if example_pipeline is not None:\n        extra_context[\"example_pipeline\"] = str(_parse_yes_no_to_bool(example_pipeline))\n\n    # set defaults for required fields, will be used mostly for starters\n    extra_context.setdefault(\"kedro_version\", version)\n    extra_context.setdefault(\"tools\", str([\"None\"]))\n    extra_context.setdefault(\"example_pipeline\", \"False\")\n\n    return extra_context\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._get_latest_starters_version","title":"_get_latest_starters_version","text":"<pre><code>_get_latest_starters_version()\n</code></pre> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _get_latest_starters_version() -&gt; str:\n    if \"KEDRO_STARTERS_VERSION\" not in os.environ:\n        GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n        headers = {}\n        if GITHUB_TOKEN:\n            headers[\"Authorization\"] = f\"token {GITHUB_TOKEN}\"\n\n        try:\n            response = requests.get(\n                \"https://api.github.com/repos/kedro-org/kedro-starters/releases/latest\",\n                headers=headers,\n                timeout=10,\n            )\n            response.raise_for_status()  # Raise an HTTPError for bad status codes\n            latest_release = response.json()\n        except requests.exceptions.RequestException as e:\n            logging.error(f\"Error fetching kedro-starters latest release version: {e}\")\n            return \"\"\n\n        os.environ[\"KEDRO_STARTERS_VERSION\"] = latest_release[\"tag_name\"]\n        return str(latest_release[\"tag_name\"])\n    else:\n        return str(os.getenv(\"KEDRO_STARTERS_VERSION\"))\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._get_prompts_required_and_clear_from_CLI_provided","title":"_get_prompts_required_and_clear_from_CLI_provided","text":"<pre><code>_get_prompts_required_and_clear_from_CLI_provided(cookiecutter_dir, selected_tools, project_name, example_pipeline)\n</code></pre> <p>Finds the information a user must supply according to prompts.yml, and clear it from what has already been provided via the CLI(validate it before)</p> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _get_prompts_required_and_clear_from_CLI_provided(\n    cookiecutter_dir: Path,\n    selected_tools: str,\n    project_name: str,\n    example_pipeline: str,\n) -&gt; Any:\n    \"\"\"Finds the information a user must supply according to prompts.yml,\n    and clear it from what has already been provided via the CLI(validate it before)\"\"\"\n    prompts_yml = cookiecutter_dir / \"prompts.yml\"\n    if not prompts_yml.is_file():\n        return {}\n\n    try:\n        with prompts_yml.open(\"r\") as prompts_file:\n            prompts_required = yaml.safe_load(prompts_file)\n    except Exception as exc:\n        raise KedroCliError(\n            \"Failed to generate project: could not load prompts.yml.\"\n        ) from exc\n\n    if selected_tools is not None:\n        _validate_selected_tools(selected_tools)\n        del prompts_required[\"tools\"]\n\n    if project_name is not None:\n        _validate_input_with_regex_pattern(\"project_name\", project_name)\n        del prompts_required[\"project_name\"]\n\n    if example_pipeline is not None:\n        _validate_input_with_regex_pattern(\"yes_no\", example_pipeline)\n        del prompts_required[\"example_pipeline\"]\n\n    return prompts_required\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._get_starters_dict","title":"_get_starters_dict","text":"<pre><code>_get_starters_dict()\n</code></pre> <p>This function lists all the starter aliases declared in the core repo and in plugins entry points.</p> <p>For example, the output for official kedro starters looks like: {\"astro-airflow-iris\":     KedroStarterSpec(         name=\"astro-airflow-iris\",         template_path=\"git+https://github.com/kedro-org/kedro-starters.git\",         directory=\"astro-airflow-iris\",         origin=\"kedro\"     ), }</p> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _get_starters_dict() -&gt; dict[str, KedroStarterSpec]:\n    \"\"\"This function lists all the starter aliases declared in\n    the core repo and in plugins entry points.\n\n    For example, the output for official kedro starters looks like:\n    {\"astro-airflow-iris\":\n        KedroStarterSpec(\n            name=\"astro-airflow-iris\",\n            template_path=\"git+https://github.com/kedro-org/kedro-starters.git\",\n            directory=\"astro-airflow-iris\",\n            origin=\"kedro\"\n        ),\n    }\n    \"\"\"\n    starter_specs = _OFFICIAL_STARTER_SPECS_DICT\n\n    for starter_entry_point in _get_entry_points(name=\"starters\"):\n        origin = starter_entry_point.module.split(\".\")[0]\n        specs: EntryPoints | list = _safe_load_entry_point(starter_entry_point) or []\n        for spec in specs:\n            if not isinstance(spec, KedroStarterSpec):\n                click.secho(\n                    f\"The starter configuration loaded from module {origin}\"\n                    f\"should be a 'KedroStarterSpec', got '{type(spec)}' instead\",\n                    fg=\"red\",\n                )\n            elif spec.alias in starter_specs:\n                click.secho(\n                    f\"Starter alias `{spec.alias}` from `{origin}` \"\n                    f\"has been ignored as it is already defined by\"\n                    f\"`{starter_specs[spec.alias].origin}`\",\n                    fg=\"red\",\n                )\n            else:\n                spec.origin = origin\n                starter_specs[spec.alias] = spec\n    return starter_specs\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._kedro_version_equal_or_lower_to_starters","title":"_kedro_version_equal_or_lower_to_starters","text":"<pre><code>_kedro_version_equal_or_lower_to_starters(version)\n</code></pre> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _kedro_version_equal_or_lower_to_starters(version: str) -&gt; bool:\n    starters_version = _get_latest_starters_version()\n    return parse(version) &lt;= parse(starters_version)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._make_cookiecutter_args_and_fetch_template","title":"_make_cookiecutter_args_and_fetch_template","text":"<pre><code>_make_cookiecutter_args_and_fetch_template(config, checkout, directory, template_path)\n</code></pre> <p>Creates a dictionary of arguments to pass to cookiecutter and returns project template path.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, str]</code>)           \u2013            <p>Configuration for starting a new project. This is passed as <code>extra_context</code> to cookiecutter and will overwrite the cookiecutter.json defaults.</p> </li> <li> <code>checkout</code>               (<code>str</code>)           \u2013            <p>The tag, branch or commit in the starter repository to checkout. Maps directly to cookiecutter's <code>checkout</code> argument.</p> </li> <li> <code>directory</code>               (<code>str</code>)           \u2013            <p>The directory of a specific starter inside a repository containing multiple starters. Maps directly to cookiecutter's <code>directory</code> argument. Relevant only when using a starter. https://cookiecutter.readthedocs.io/en/1.7.2/advanced/directories.html</p> </li> <li> <code>template_path</code>               (<code>str</code>)           \u2013            <p>Starter path or kedro template path</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[dict[str, object], str]</code>           \u2013            <p>Arguments to pass to cookiecutter, project template path</p> </li> </ul> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _make_cookiecutter_args_and_fetch_template(\n    config: dict[str, str],\n    checkout: str,\n    directory: str,\n    template_path: str,\n) -&gt; tuple[dict[str, object], str]:\n    \"\"\"Creates a dictionary of arguments to pass to cookiecutter and returns project template path.\n\n    Args:\n        config: Configuration for starting a new project. This is passed as\n            ``extra_context`` to cookiecutter and will overwrite the cookiecutter.json\n            defaults.\n        checkout: The tag, branch or commit in the starter repository to checkout.\n            Maps directly to cookiecutter's ``checkout`` argument.\n        directory: The directory of a specific starter inside a repository containing\n            multiple starters. Maps directly to cookiecutter's ``directory`` argument.\n            Relevant only when using a starter.\n            https://cookiecutter.readthedocs.io/en/1.7.2/advanced/directories.html\n        template_path: Starter path or kedro template path\n\n    Returns:\n        Arguments to pass to cookiecutter, project template path\n    \"\"\"\n\n    cookiecutter_args = {\n        \"output_dir\": config.get(\"output_dir\", str(Path.cwd().resolve())),\n        \"no_input\": True,\n        \"extra_context\": config,\n    }\n\n    if directory:\n        cookiecutter_args[\"directory\"] = directory\n    cookiecutter_args[\"checkout\"] = checkout\n\n    tools = config[\"tools\"]\n    example_pipeline = config[\"example_pipeline\"]\n    starter_path = \"git+https://github.com/kedro-org/kedro-starters.git\"\n\n    if \"PySpark\" in tools:\n        # Use the spaceflights-pyspark starter if only PySpark is chosen.\n        cookiecutter_args[\"directory\"] = \"spaceflights-pyspark\"\n        cookiecutter_args[\"checkout\"] = _select_checkout_branch_for_cookiecutter(\n            checkout\n        )\n    elif example_pipeline == \"True\":\n        # Use spaceflights-pandas starter if example was selected, but PySpark wasn't\n        cookiecutter_args[\"directory\"] = \"spaceflights-pandas\"\n        cookiecutter_args[\"checkout\"] = _select_checkout_branch_for_cookiecutter(\n            checkout\n        )\n    else:\n        # Use the default template path for non PySpark or example options:\n        starter_path = template_path\n\n    return cookiecutter_args, starter_path\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._make_cookiecutter_context_for_prompts","title":"_make_cookiecutter_context_for_prompts","text":"<pre><code>_make_cookiecutter_context_for_prompts(cookiecutter_dir)\n</code></pre> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _make_cookiecutter_context_for_prompts(cookiecutter_dir: Path) -&gt; OrderedDict:\n    from cookiecutter.generate import generate_context\n\n    cookiecutter_context = generate_context(cookiecutter_dir / \"cookiecutter.json\")\n    return cookiecutter_context.get(\"cookiecutter\", {})  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._parse_tools_input","title":"_parse_tools_input","text":"<pre><code>_parse_tools_input(tools_str)\n</code></pre> <p>Parse the tools input string.</p> <p>Parameters:</p> <ul> <li> <code>tools_str</code>               (<code>str | None</code>)           \u2013            <p>Input string from prompts.yml.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code>           \u2013            <p>List of selected tools as strings.</p> </li> </ul> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _parse_tools_input(tools_str: str | None) -&gt; list[str]:\n    \"\"\"Parse the tools input string.\n\n    Args:\n        tools_str: Input string from prompts.yml.\n\n    Returns:\n        list: List of selected tools as strings.\n    \"\"\"\n\n    def _validate_range(start: Any, end: Any) -&gt; None:\n        if int(start) &gt; int(end):\n            message = f\"'{start}-{end}' is an invalid range for project tools.\\nPlease ensure range values go from smaller to larger.\"\n            click.secho(message, fg=\"red\", err=True)\n            sys.exit(1)\n        # safeguard to prevent passing of excessively large intervals that could cause freezing:\n        if int(end) &gt; len(NUMBER_TO_TOOLS_NAME):\n            message = f\"'{end}' is not a valid selection.\\nPlease select from the available tools: 1, 2, 3, 4, 5, 6.\"  # nosec\n            if end == \"7\":\n                message += \"\\nKedro Viz is automatically included in the project. Please remove 7 from your tool selection.\"\n            click.secho(message, fg=\"red\", err=True)\n            sys.exit(1)\n\n    if not tools_str:\n        return []  # pragma: no cover\n\n    tools_str = tools_str.lower()\n    if tools_str == \"all\":\n        return list(NUMBER_TO_TOOLS_NAME)\n    if tools_str == \"none\":\n        return []\n\n    # Split by comma\n    tools_choices = tools_str.replace(\" \", \"\").split(\",\")\n    selected: list[str] = []\n\n    for choice in tools_choices:\n        if \"-\" in choice:\n            start, end = choice.split(\"-\")\n            _validate_range(start, end)\n            selected.extend(str(i) for i in range(int(start), int(end) + 1))\n        else:\n            selected.append(choice.strip())\n\n    return selected\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._parse_yes_no_to_bool","title":"_parse_yes_no_to_bool","text":"<pre><code>_parse_yes_no_to_bool(value)\n</code></pre> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _parse_yes_no_to_bool(value: str) -&gt; Any:\n    return value.strip().lower() in [\"y\", \"yes\"] if value is not None else None\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._print_selection_and_prompt_info","title":"_print_selection_and_prompt_info","text":"<pre><code>_print_selection_and_prompt_info(selected_tools, example_pipeline, interactive)\n</code></pre> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _print_selection_and_prompt_info(\n    selected_tools: str, example_pipeline: str, interactive: bool\n) -&gt; None:\n    # Confirm tools selection\n    if selected_tools == \"['None']\":\n        click.secho(\n            \"You have selected no project tools\",\n            fg=\"green\",\n        )\n    else:\n        click.secho(\n            f\"You have selected the following project tools: {selected_tools}\",\n            fg=\"green\",\n        )\n\n    # Confirm example selection\n    if example_pipeline == \"True\":\n        click.secho(\n            \"It has been created with an example pipeline.\",\n            fg=\"green\",\n        )\n    else:\n        warnings.warn(\n            \"Your project does not contain any pipelines with nodes. \"\n            \"Please ensure that at least one pipeline has been defined before \"\n            \"executing 'kedro run'.\",\n            UserWarning,\n        )\n\n    # Give hint for skipping interactive flow\n    if interactive:\n        click.secho(\n            \"\\nTo skip the interactive flow you can run `kedro new` with\"\n            \"\\nkedro new --name=&lt;your-project-name&gt; --tools=&lt;your-project-tools&gt; --example=&lt;yes/no&gt;\",\n            fg=\"green\",\n        )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._remove_readonly","title":"_remove_readonly","text":"<pre><code>_remove_readonly(func, path, excinfo)\n</code></pre> <p>Remove readonly files on Windows See: https://docs.python.org/3/library/shutil.html?highlight=shutil#rmtree-example</p> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _remove_readonly(\n    func: Callable, path: Path, excinfo: tuple\n) -&gt; None:  # pragma: no cover\n    \"\"\"Remove readonly files on Windows\n    See: https://docs.python.org/3/library/shutil.html?highlight=shutil#rmtree-example\n    \"\"\"\n    os.chmod(path, stat.S_IWRITE)\n    func(path)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._safe_load_entry_point","title":"_safe_load_entry_point","text":"<pre><code>_safe_load_entry_point(entry_point)\n</code></pre> <p>Load entrypoint safely, if fails it will just skip the entrypoint.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _safe_load_entry_point(\n    entry_point: Any,\n) -&gt; Any:\n    \"\"\"Load entrypoint safely, if fails it will just skip the entrypoint.\"\"\"\n    try:\n        return entry_point.load()\n    except Exception as exc:\n        logger.warning(\n            \"Failed to load %s commands from %s. Full exception: %s\",\n            entry_point.module,\n            entry_point,\n            exc,\n        )\n        return\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._select_checkout_branch_for_cookiecutter","title":"_select_checkout_branch_for_cookiecutter","text":"<pre><code>_select_checkout_branch_for_cookiecutter(checkout)\n</code></pre> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _select_checkout_branch_for_cookiecutter(checkout: str | None) -&gt; str:\n    if checkout:\n        return checkout\n    elif _kedro_version_equal_or_lower_to_starters(version):\n        return version\n    else:\n        return \"main\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._starter_spec_to_dict","title":"_starter_spec_to_dict","text":"<pre><code>_starter_spec_to_dict(starter_specs)\n</code></pre> <p>Convert a dictionary of starters spec to a nicely formatted dictionary</p> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _starter_spec_to_dict(\n    starter_specs: dict[str, KedroStarterSpec],\n) -&gt; dict[str, dict[str, str]]:\n    \"\"\"Convert a dictionary of starters spec to a nicely formatted dictionary\"\"\"\n    format_dict: dict[str, dict[str, str]] = {}\n    for alias, spec in starter_specs.items():\n        format_dict[alias] = {}  # Each dictionary represent 1 starter\n        format_dict[alias][\"template_path\"] = spec.template_path\n        if spec.directory:\n            format_dict[alias][\"directory\"] = spec.directory\n    return format_dict\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._validate_config_file_against_prompts","title":"_validate_config_file_against_prompts","text":"<pre><code>_validate_config_file_against_prompts(config, prompts)\n</code></pre> <p>Checks that the configuration file contains all needed variables.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, str]</code>)           \u2013            <p>The config as a dictionary.</p> </li> <li> <code>prompts</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Prompts from prompts.yml.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KedroCliError</code>             \u2013            <p>If the config file is empty or does not contain all the keys required in prompts, or if the output_dir specified does not exist.</p> </li> </ul> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _validate_config_file_against_prompts(\n    config: dict[str, str], prompts: dict[str, Any]\n) -&gt; None:\n    \"\"\"Checks that the configuration file contains all needed variables.\n\n    Args:\n        config: The config as a dictionary.\n        prompts: Prompts from prompts.yml.\n\n    Raises:\n        KedroCliError: If the config file is empty or does not contain all the keys\n            required in prompts, or if the output_dir specified does not exist.\n    \"\"\"\n    if not config:\n        raise KedroCliError(\"Config file is empty.\")\n    additional_keys = {\"tools\": \"none\", \"example_pipeline\": \"no\"}\n    missing_keys = set(prompts) - set(config)\n    missing_mandatory_keys = missing_keys - set(additional_keys)\n    if missing_mandatory_keys:\n        click.echo(yaml.dump(config, default_flow_style=False))\n        raise KedroCliError(\n            f\"{', '.join(missing_mandatory_keys)} not found in config file.\"\n        )\n    for key, default_value in additional_keys.items():\n        if key in missing_keys:\n            click.secho(\n                f\"The `{key}` key not found in the config file, default value '{default_value}' is being used.\",\n                fg=\"yellow\",\n            )\n\n    if \"output_dir\" in config and not Path(config[\"output_dir\"]).exists():\n        raise KedroCliError(\n            f\"'{config['output_dir']}' is not a valid output directory. \"\n            \"It must be a relative or absolute path to an existing directory.\"\n        )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._validate_flag_inputs","title":"_validate_flag_inputs","text":"<pre><code>_validate_flag_inputs(flag_inputs)\n</code></pre> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _validate_flag_inputs(flag_inputs: dict[str, Any]) -&gt; None:\n    if flag_inputs.get(\"checkout\") and not flag_inputs.get(\"starter\"):\n        raise KedroCliError(\"Cannot use the --checkout flag without a --starter value.\")\n\n    if flag_inputs.get(\"directory\") and not flag_inputs.get(\"starter\"):\n        raise KedroCliError(\n            \"Cannot use the --directory flag without a --starter value.\"\n        )\n\n    if (flag_inputs.get(\"tools\") or flag_inputs.get(\"example\")) and flag_inputs.get(\n        \"starter\"\n    ):\n        raise KedroCliError(\n            \"Cannot use the --starter flag with the --example and/or --tools flag.\"\n        )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._validate_input_with_regex_pattern","title":"_validate_input_with_regex_pattern","text":"<pre><code>_validate_input_with_regex_pattern(pattern_name, input)\n</code></pre> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _validate_input_with_regex_pattern(pattern_name: str, input: str) -&gt; None:\n    VALIDATION_PATTERNS = {\n        \"yes_no\": {\n            \"regex\": r\"(?i)^\\s*(y|yes|n|no)\\s*$\",\n            \"error_message\": f\"'{input}' is an invalid value for example pipeline. It must contain only y, n, YES, or NO (case insensitive).\",\n        },\n        \"project_name\": {\n            \"regex\": r\"^[\\w -]{2,}$\",\n            \"error_message\": f\"'{input}' is an invalid value for project name. It must contain only alphanumeric symbols, spaces, underscores and hyphens and be at least 2 characters long\",\n        },\n        \"tools\": {\n            \"regex\": r\"\"\"^(\n                all|none|                        # A: \"all\" or \"none\" or\n                (\\ *\\d+                          # B: any number of spaces followed by one or more digits\n                (\\ *-\\ *\\d+)?                    # C: zero or one instances of: a hyphen followed by one or more digits, spaces allowed\n                (\\ *,\\ *\\d+(\\ *-\\ *\\d+)?)*       # D: any number of instances of: a comma followed by B and C, spaces allowed\n                \\ *)?)                           # E: zero or one instances of (B,C,D) as empty strings are also permissible\n                $\"\"\",\n            \"error_message\": f\"'{input}' is an invalid value for project tools. Please select valid options for tools using comma-separated values, ranges, or 'all/none'.\",\n        },\n    }\n\n    if not re.match(VALIDATION_PATTERNS[pattern_name][\"regex\"], input, flags=re.X):\n        click.secho(\n            VALIDATION_PATTERNS[pattern_name][\"error_message\"],\n            fg=\"red\",\n            err=True,\n        )\n        sys.exit(1)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._validate_selected_tools","title":"_validate_selected_tools","text":"<pre><code>_validate_selected_tools(selected_tools)\n</code></pre> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _validate_selected_tools(selected_tools: str | None) -&gt; None:\n    valid_tools = [*list(TOOLS_SHORTNAME_TO_NUMBER), \"all\", \"none\"]\n\n    if selected_tools is not None:\n        tools = re.sub(r\"\\s\", \"\", selected_tools).split(\",\")\n        for tool in tools:\n            if tool not in valid_tools:\n                message = \"Please select from the available tools: lint, test, log, docs, data, pyspark, all, none.\"\n                if tool == \"viz\":\n                    message += \" Kedro Viz is automatically included in the project. Please remove 'viz' from your tool selection.\"\n                click.secho(\n                    message,\n                    fg=\"red\",\n                    err=True,\n                )\n                sys.exit(1)\n        if (\"none\" in tools or \"all\" in tools) and len(tools) &gt; 1:\n            click.secho(\n                \"Tools options 'all' and 'none' cannot be used with other options\",\n                fg=\"red\",\n                err=True,\n            )\n            sys.exit(1)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters._validate_tool_selection","title":"_validate_tool_selection","text":"<pre><code>_validate_tool_selection(tools)\n</code></pre> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>def _validate_tool_selection(tools: list[str]) -&gt; None:\n    # start validating from the end, when user select 1-20, it will generate a message\n    # '20' is not a valid selection instead of '8'\n    for tool in tools[::-1]:\n        if tool not in NUMBER_TO_TOOLS_NAME:\n            message = f\"'{tool}' is not a valid selection.\\nPlease select from the available tools: 1, 2, 3, 4, 5, 6.\"  # nosec\n            if tool == \"7\":\n                message += \"\\nKedro Viz is automatically included in the project. Please remove 7 from your tool selection.\"\n            click.secho(message, fg=\"red\", err=True)\n            sys.exit(1)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.command_with_verbosity","title":"command_with_verbosity","text":"<pre><code>command_with_verbosity(group, *args, **kwargs)\n</code></pre> <p>Custom command decorator with verbose flag added.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def command_with_verbosity(group: click.core.Group, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Custom command decorator with verbose flag added.\"\"\"\n\n    def decorator(func: Any) -&gt; Any:\n        func = _click_verbose(func)\n        func = group.command(*args, **kwargs)(func)\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.create_cli","title":"create_cli","text":"<pre><code>create_cli()\n</code></pre> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>@click.group(context_settings=CONTEXT_SETTINGS, name=\"Kedro\")\ndef create_cli() -&gt; None:  # pragma: no cover\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.list_starters","title":"list_starters","text":"<pre><code>list_starters()\n</code></pre> <p>List all official project starters available.</p> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>@starter.command(\"list\")\ndef list_starters() -&gt; None:\n    \"\"\"List all official project starters available.\"\"\"\n    starters_dict = _get_starters_dict()\n\n    # Group all specs by origin as nested dict and sort it.\n    sorted_starters_dict: dict[str, dict[str, KedroStarterSpec]] = {\n        origin: dict(sorted(starters_dict_by_origin))\n        for origin, starters_dict_by_origin in groupby(\n            starters_dict.items(), lambda item: item[1].origin\n        )\n    }\n\n    # ensure kedro starters are listed first\n    sorted_starters_dict = dict(\n        sorted(sorted_starters_dict.items(), key=lambda x: x == \"kedro\")  # type: ignore[comparison-overlap]\n    )\n\n    for origin, starters_spec in sorted_starters_dict.items():\n        click.secho(f\"\\nStarters from {origin}\\n\", fg=\"yellow\")\n        click.echo(\n            yaml.safe_dump(_starter_spec_to_dict(starters_spec), sort_keys=False)\n        )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.new","title":"new","text":"<pre><code>new(config_path, starter_alias, selected_tools, project_name, checkout, directory, example_pipeline, telemetry_consent, **kwargs)\n</code></pre> <p>Create a new kedro project.</p> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>@command_with_verbosity(create_cli, short_help=\"Create a new kedro project.\")\n@click.option(\n    \"--config\",\n    \"-c\",\n    \"config_path\",\n    type=click.Path(exists=True),\n    help=CONFIG_ARG_HELP,\n)\n@click.option(\"--starter\", \"-s\", \"starter_alias\", help=STARTER_ARG_HELP)\n@click.option(\"--checkout\", help=CHECKOUT_ARG_HELP)\n@click.option(\"--directory\", help=DIRECTORY_ARG_HELP)\n@click.option(\n    \"--name\",\n    \"-n\",\n    \"project_name\",\n    help=NAME_ARG_HELP,\n)\n@click.option(\n    \"--tools\",\n    \"-t\",\n    \"selected_tools\",\n    help=TOOLS_ARG_HELP,\n)\n@click.option(\n    \"--example\",\n    \"-e\",\n    \"example_pipeline\",\n    help=EXAMPLE_ARG_HELP,\n)\n@click.option(\n    \"--telemetry\",\n    \"-tc\",\n    \"telemetry_consent\",\n    help=TELEMETRY_ARG_HELP,\n    type=click.Choice([\"yes\", \"no\", \"y\", \"n\"], case_sensitive=False),\n)\ndef new(  # noqa: PLR0913\n    config_path: str,\n    starter_alias: str,\n    selected_tools: str,\n    project_name: str,\n    checkout: str,\n    directory: str,\n    example_pipeline: str,\n    telemetry_consent: str,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new kedro project.\"\"\"\n    flag_inputs = {\n        \"config\": config_path,\n        \"starter\": starter_alias,\n        \"tools\": selected_tools,\n        \"name\": project_name,\n        \"checkout\": checkout,\n        \"directory\": directory,\n        \"example\": example_pipeline,\n        \"telemetry_consent\": telemetry_consent,\n    }\n\n    _validate_flag_inputs(flag_inputs)\n    starters_dict = _get_starters_dict()\n\n    if starter_alias in starters_dict:\n        if directory:\n            raise KedroCliError(\n                \"Cannot use the --directory flag with a --starter alias.\"\n            )\n        spec = starters_dict[starter_alias]\n        template_path = spec.template_path\n        # \"directory\" is an optional key for starters from plugins, so if the key is\n        # not present we will use \"None\".\n        directory = spec.directory  # type: ignore[assignment]\n        checkout = _select_checkout_branch_for_cookiecutter(checkout)\n    elif starter_alias is not None:\n        template_path = starter_alias\n    else:\n        template_path = str(TEMPLATE_PATH)\n\n    # Format user input where necessary\n    if selected_tools is not None:\n        selected_tools = selected_tools.lower()\n\n    # Get prompts.yml to find what information the user needs to supply as config.\n    tmpdir = tempfile.mkdtemp()\n    cookiecutter_dir = _get_cookiecutter_dir(template_path, checkout, directory, tmpdir)\n    prompts_required = _get_prompts_required_and_clear_from_CLI_provided(\n        cookiecutter_dir, selected_tools, project_name, example_pipeline\n    )\n\n    # We only need to make cookiecutter_context if interactive prompts are needed.\n    cookiecutter_context = None\n\n    if not config_path:\n        cookiecutter_context = _make_cookiecutter_context_for_prompts(cookiecutter_dir)\n\n    # Cleanup the tmpdir after it's no longer required.\n    # Ideally we would want to be able to use tempfile.TemporaryDirectory() context manager\n    # but it causes an issue with readonly files on windows\n    # see: https://bugs.python.org/issue26660.\n    # So on error, we will attempt to clear the readonly bits and re-attempt the cleanup\n    shutil.rmtree(tmpdir, onerror=_remove_readonly)  # type: ignore[arg-type]\n\n    # Obtain config, either from a file or from interactive user prompts.\n\n    extra_context = _get_extra_context(\n        prompts_required=prompts_required,\n        config_path=config_path,\n        cookiecutter_context=cookiecutter_context,\n        selected_tools=selected_tools,\n        project_name=project_name,\n        example_pipeline=example_pipeline,\n        starter_alias=starter_alias,\n    )\n\n    cookiecutter_args, project_template = _make_cookiecutter_args_and_fetch_template(\n        config=extra_context,\n        checkout=checkout,\n        directory=directory,\n        template_path=template_path,\n    )\n\n    if telemetry_consent is not None:\n        telemetry_consent = (\n            \"true\" if _parse_yes_no_to_bool(telemetry_consent) else \"false\"\n        )\n\n    _create_project(project_template, cookiecutter_args, telemetry_consent)\n\n    # If not a starter, print tools and example selection\n    if not starter_alias:\n        # If interactive flow used, print hint\n        interactive_flow = prompts_required and not config_path\n        _print_selection_and_prompt_info(\n            extra_context[\"tools\"],\n            extra_context[\"example_pipeline\"],\n            interactive_flow,\n        )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.starters.starter","title":"starter","text":"<pre><code>starter()\n</code></pre> <p>Commands for working with project starters.</p> Source code in <code>kedro/framework/cli/starters.py</code> <pre><code>@create_cli.group()\ndef starter() -&gt; None:\n    \"\"\"Commands for working with project starters.\"\"\"\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils","title":"kedro.framework.cli.utils","text":"<p>Utilities for use with click.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.CONTEXT_SETTINGS","title":"CONTEXT_SETTINGS  <code>module-attribute</code>","text":"<pre><code>CONTEXT_SETTINGS = {'help_option_names': ['-h', '--help']}\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.CUTOFF","title":"CUTOFF  <code>module-attribute</code>","text":"<pre><code>CUTOFF = 0.5\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.ENTRY_POINT_GROUPS","title":"ENTRY_POINT_GROUPS  <code>module-attribute</code>","text":"<pre><code>ENTRY_POINT_GROUPS = {'global': 'kedro.global_commands', 'project': 'kedro.project_commands', 'init': 'kedro.init', 'line_magic': 'kedro.line_magic', 'hooks': 'kedro.hooks', 'cli_hooks': 'kedro.cli_hooks', 'starters': 'kedro.starters'}\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.ENV_HELP","title":"ENV_HELP  <code>module-attribute</code>","text":"<pre><code>ENV_HELP = 'Kedro configuration environment name. Defaults to `local`.'\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.MAX_SUGGESTIONS","title":"MAX_SUGGESTIONS  <code>module-attribute</code>","text":"<pre><code>MAX_SUGGESTIONS = 3\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.CommandCollection","title":"CommandCollection","text":"<pre><code>CommandCollection(*groups)\n</code></pre> <p>               Bases: <code>CommandCollection</code></p> <p>Modified from the Click one to still run the source groups function.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def __init__(self, *groups: tuple[str, Sequence[click.MultiCommand]]):\n    self.groups = [\n        (title, self._merge_same_name_collections(cli_list))\n        for title, cli_list in groups\n    ]\n    sources = list(chain.from_iterable(cli_list for _, cli_list in self.groups))\n    help_texts = [\n        cli.help\n        for cli_collection in sources\n        for cli in cli_collection.sources\n        if cli.help\n    ]\n    super().__init__(\n        sources=sources,  # type: ignore[arg-type]\n        help=\"\\n\\n\".join(help_texts),\n        context_settings=CONTEXT_SETTINGS,\n    )\n    self.params = sources[0].params\n    self.callback = sources[0].callback\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.KedroCliError","title":"KedroCliError","text":"<p>               Bases: <code>ClickException</code></p> <p>Exceptions generated from the Kedro CLI.</p> <p>Users should pass an appropriate message at the constructor.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.KedroDeprecationWarning","title":"KedroDeprecationWarning","text":"<p>               Bases: <code>DeprecationWarning</code></p> <p>Custom class for warnings about deprecated Kedro features.</p>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.LazyGroup","title":"LazyGroup","text":"<pre><code>LazyGroup(*args, lazy_subcommands=None, **kwargs)\n</code></pre> <p>               Bases: <code>Group</code></p> <p>A click Group that supports lazy loading of subcommands.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def __init__(\n    self,\n    *args: Any,\n    lazy_subcommands: dict[str, str] | None = None,\n    **kwargs: Any,\n):\n    super().__init__(*args, **kwargs)\n    # lazy_subcommands is a map of the form:\n    #\n    #   {command-name} -&gt; {module-name}.{command-object-name}\n    #\n    self.lazy_subcommands = lazy_subcommands or {}\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils._check_module_importable","title":"_check_module_importable","text":"<pre><code>_check_module_importable(module_name)\n</code></pre> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _check_module_importable(module_name: str) -&gt; None:\n    try:\n        import_module(module_name)\n    except ImportError as exc:\n        raise KedroCliError(\n            f\"Module '{module_name}' not found. Make sure to install required project \"\n            f\"dependencies by running the 'pip install -r requirements.txt' command first.\"\n        ) from exc\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils._clean_pycache","title":"_clean_pycache","text":"<pre><code>_clean_pycache(path)\n</code></pre> <p>Recursively clean all pycache folders from <code>path</code>.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>Existing local directory to clean pycache folders from.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _clean_pycache(path: Path) -&gt; None:\n    \"\"\"Recursively clean all __pycache__ folders from `path`.\n\n    Args:\n        path: Existing local directory to clean __pycache__ folders from.\n    \"\"\"\n    to_delete = [each.resolve() for each in path.rglob(\"__pycache__\")]\n\n    for each in to_delete:\n        shutil.rmtree(each, ignore_errors=True)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils._click_verbose","title":"_click_verbose","text":"<pre><code>_click_verbose(func)\n</code></pre> <p>Click option for enabling verbose mode.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _click_verbose(func: Any) -&gt; Any:\n    \"\"\"Click option for enabling verbose mode.\"\"\"\n    return click.option(\n        \"--verbose\",\n        \"-v\",\n        is_flag=True,\n        callback=_update_verbose_flag,\n        help=\"See extensive logging and error stack traces.\",\n    )(func)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils._config_file_callback","title":"_config_file_callback","text":"<pre><code>_config_file_callback(ctx, param, value)\n</code></pre> <p>CLI callback that replaces command line options with values specified in a config file. If command line options are passed, they override config file values.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>@typing.no_type_check\ndef _config_file_callback(ctx: click.Context, param: Any, value: Any) -&gt; Any:\n    \"\"\"CLI callback that replaces command line options\n    with values specified in a config file. If command line\n    options are passed, they override config file values.\n    \"\"\"\n\n    ctx.default_map = ctx.default_map or {}\n    section = ctx.info_name\n\n    if value:\n        config = OmegaConf.to_container(OmegaConf.load(value))[section]\n        for key, value in config.items():  # noqa: PLR1704\n            _validate_config_file(key)\n        ctx.default_map.update(config)\n\n    return value\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils._find_run_command_in_plugins","title":"_find_run_command_in_plugins","text":"<pre><code>_find_run_command_in_plugins(plugins)\n</code></pre> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _find_run_command_in_plugins(plugins: Any) -&gt; Any:\n    for group in plugins:\n        if \"run\" in group.commands:\n            return group.commands[\"run\"]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils._get_entry_points","title":"_get_entry_points","text":"<pre><code>_get_entry_points(name)\n</code></pre> <p>Get all kedro related entry points</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _get_entry_points(name: str) -&gt; Any:\n    \"\"\"Get all kedro related entry points\"\"\"\n    return importlib_metadata.entry_points().select(  # type: ignore[no-untyped-call]\n        group=ENTRY_POINT_GROUPS[name]\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils._safe_load_entry_point","title":"_safe_load_entry_point","text":"<pre><code>_safe_load_entry_point(entry_point)\n</code></pre> <p>Load entrypoint safely, if fails it will just skip the entrypoint.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _safe_load_entry_point(\n    entry_point: Any,\n) -&gt; Any:\n    \"\"\"Load entrypoint safely, if fails it will just skip the entrypoint.\"\"\"\n    try:\n        return entry_point.load()\n    except Exception as exc:\n        logger.warning(\n            \"Failed to load %s commands from %s. Full exception: %s\",\n            entry_point.module,\n            entry_point,\n            exc,\n        )\n        return\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils._split_load_versions","title":"_split_load_versions","text":"<pre><code>_split_load_versions(ctx, param, value)\n</code></pre> <p>Split and format the string coming from the --load-versions flag in kedro run, e.g.: \"dataset1:time1,dataset2:time2\" -&gt; {\"dataset1\": \"time1\", \"dataset2\": \"time2\"}</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>str</code>)           \u2013            <p>the string with the contents of the --load-versions flag.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>A dictionary with the formatted load versions data.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _split_load_versions(ctx: click.Context, param: Any, value: str) -&gt; dict[str, str]:\n    \"\"\"Split and format the string coming from the --load-versions\n    flag in kedro run, e.g.:\n    \"dataset1:time1,dataset2:time2\" -&gt; {\"dataset1\": \"time1\", \"dataset2\": \"time2\"}\n\n    Args:\n        value: the string with the contents of the --load-versions flag.\n\n    Returns:\n        A dictionary with the formatted load versions data.\n    \"\"\"\n    if not value:\n        return {}\n\n    lv_tuple = tuple(chain.from_iterable(value.split(\",\") for value in [value]))\n\n    load_versions_dict = {}\n    for load_version in lv_tuple:\n        load_version = load_version.strip()  # noqa: PLW2901\n        load_version_list = load_version.split(\":\", 1)\n        if len(load_version_list) != 2:  # noqa: PLR2004\n            raise KedroCliError(\n                f\"Expected the form of 'load_versions' to be \"\n                f\"'dataset_name:YYYY-MM-DDThh.mm.ss.sssZ',\"\n                f\"found {load_version} instead\"\n            )\n        load_versions_dict[load_version_list[0]] = load_version_list[1]\n\n    return load_versions_dict\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils._split_params","title":"_split_params","text":"<pre><code>_split_params(ctx, param, value)\n</code></pre> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _split_params(ctx: click.Context, param: Any, value: Any) -&gt; Any:\n    if isinstance(value, dict):\n        return value\n    dot_list = []\n    for item in split_string(ctx, param, value):\n        equals_idx = item.find(\"=\")\n        if equals_idx == -1:\n            # If an equals sign is not found, fail with an error message.\n            ctx.fail(\n                f\"Invalid format of `{param.name}` option: \"\n                f\"Item `{item}` must contain a key and a value separated by `=`.\"\n            )\n        # Split the item into key and value\n        key, _, val = item.partition(\"=\")\n        key = key.strip()\n        if not key:\n            # If the key is empty after stripping whitespace, fail with an error message.\n            ctx.fail(\n                f\"Invalid format of `{param.name}` option: Parameter key \"\n                f\"cannot be an empty string.\"\n            )\n        # Add \"key=value\" pair to dot_list.\n        dot_list.append(f\"{key}={val}\")\n\n    conf = OmegaConf.from_dotlist(dot_list)\n    return OmegaConf.to_container(conf)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils._suggest_cli_command","title":"_suggest_cli_command","text":"<pre><code>_suggest_cli_command(original_command_name, existing_command_names)\n</code></pre> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _suggest_cli_command(\n    original_command_name: str, existing_command_names: Iterable[str]\n) -&gt; str:\n    matches = difflib.get_close_matches(\n        original_command_name, existing_command_names, MAX_SUGGESTIONS, CUTOFF\n    )\n\n    if not matches:\n        return \"\"\n\n    if len(matches) == 1:\n        suggestion = \"\\n\\nDid you mean this?\"\n    else:\n        suggestion = \"\\n\\nDid you mean one of these?\\n\"\n    suggestion += textwrap.indent(\"\\n\".join(matches), \" \" * 4)\n    return suggestion\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils._update_verbose_flag","title":"_update_verbose_flag","text":"<pre><code>_update_verbose_flag(ctx, param, value)\n</code></pre> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _update_verbose_flag(ctx: click.Context, param: Any, value: bool) -&gt; None:\n    KedroCliError.VERBOSE_ERROR = value\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils._validate_config_file","title":"_validate_config_file","text":"<pre><code>_validate_config_file(key)\n</code></pre> <p>Validate the keys provided in the config file against the accepted keys.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def _validate_config_file(key: str) -&gt; None:\n    \"\"\"Validate the keys provided in the config file against the accepted keys.\"\"\"\n    from kedro.framework.cli.project import run\n\n    run_args = [click_arg.name for click_arg in run.params]\n    run_args.remove(\"config\")\n    if key not in run_args:\n        KedroCliError.VERBOSE_EXISTS = False\n        message = _suggest_cli_command(key, run_args)  # type: ignore[arg-type]\n        raise KedroCliError(\n            f\"Key `{key}` in provided configuration is not valid. {message}\"\n        )\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.call","title":"call","text":"<pre><code>call(cmd, **kwargs)\n</code></pre> <p>Run a subprocess command and raise if it fails.</p> <p>Parameters:</p> <ul> <li> <code>cmd</code>               (<code>list[str]</code>)           \u2013            <p>List of command parts.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Optional keyword arguments passed to <code>subprocess.run</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>Exit</code>             \u2013            <p>If <code>subprocess.run</code> returns non-zero code.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def call(cmd: list[str], **kwargs: Any) -&gt; None:  # pragma: no cover\n    \"\"\"Run a subprocess command and raise if it fails.\n\n    Args:\n        cmd: List of command parts.\n        **kwargs: Optional keyword arguments passed to `subprocess.run`.\n\n    Raises:\n        click.exceptions.Exit: If `subprocess.run` returns non-zero code.\n    \"\"\"\n    click.echo(shlex.join(cmd))\n    code = subprocess.run(cmd, **kwargs).returncode  # noqa: PLW1510, S603\n    if code:\n        raise click.exceptions.Exit(code=code)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.command_with_verbosity","title":"command_with_verbosity","text":"<pre><code>command_with_verbosity(group, *args, **kwargs)\n</code></pre> <p>Custom command decorator with verbose flag added.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def command_with_verbosity(group: click.core.Group, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Custom command decorator with verbose flag added.\"\"\"\n\n    def decorator(func: Any) -&gt; Any:\n        func = _click_verbose(func)\n        func = group.command(*args, **kwargs)(func)\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.env_option","title":"env_option","text":"<pre><code>env_option(func_=None, **kwargs)\n</code></pre> <p>Add <code>--env</code> CLI option to a function.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def env_option(func_: Any | None = None, **kwargs: Any) -&gt; Any:\n    \"\"\"Add `--env` CLI option to a function.\"\"\"\n    default_args = {\"type\": str, \"default\": None, \"help\": ENV_HELP}\n    kwargs = {**default_args, **kwargs}\n    opt = click.option(\"--env\", \"-e\", **kwargs)\n    return opt(func_) if func_ else opt\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.find_run_command","title":"find_run_command","text":"<pre><code>find_run_command(package_name)\n</code></pre> <p>Find the run command to be executed.    This is either the default run command defined in the Kedro framework or a run command defined by    an installed plugin.</p> <p>Parameters:</p> <ul> <li> <code>package_name</code>               (<code>str</code>)           \u2013            <p>The name of the package being run.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KedroCliError</code>             \u2013            <p>If the run command is not found.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Callable</code>           \u2013            <p>Run command to be executed.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def find_run_command(package_name: str) -&gt; Callable:\n    \"\"\"Find the run command to be executed.\n       This is either the default run command defined in the Kedro framework or a run command defined by\n       an installed plugin.\n\n    Args:\n        package_name: The name of the package being run.\n\n    Raises:\n        KedroCliError: If the run command is not found.\n\n    Returns:\n        Run command to be executed.\n    \"\"\"\n    try:\n        project_cli = importlib.import_module(f\"{package_name}.cli\")\n        # fail gracefully if cli.py does not exist\n    except ModuleNotFoundError as exc:\n        if f\"{package_name}.cli\" not in str(exc):\n            raise\n        plugins = load_entry_points(\"project\")\n        run = _find_run_command_in_plugins(plugins) if plugins else None\n        if run:\n            # use run command from installed plugin if it exists\n            return run  # type: ignore[no-any-return]\n        # use run command from `kedro.framework.cli.project`\n        from kedro.framework.cli.project import run\n\n        return run  # type: ignore[return-value]\n    # fail badly if cli.py exists, but has no `cli` in it\n    if not hasattr(project_cli, \"cli\"):\n        raise KedroCliError(f\"Cannot load commands from {package_name}.cli\")\n    return project_cli.run  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.find_stylesheets","title":"find_stylesheets","text":"<pre><code>find_stylesheets()\n</code></pre> <p>Fetch all stylesheets used in the official Kedro documentation</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def find_stylesheets() -&gt; Iterable[str]:  # pragma: no cover\n    # TODO: Deprecate this function in favour of kedro-sphinx-theme\n    \"\"\"Fetch all stylesheets used in the official Kedro documentation\"\"\"\n    css_path = Path(__file__).resolve().parents[1] / \"html\" / \"_static\" / \"css\"\n    return (str(css_path / \"copybutton.css\"),)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.forward_command","title":"forward_command","text":"<pre><code>forward_command(group, name=None, forward_help=False)\n</code></pre> <p>A command that receives the rest of the command line as 'args'.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def forward_command(\n    group: Any, name: str | None = None, forward_help: bool = False\n) -&gt; Any:\n    \"\"\"A command that receives the rest of the command line as 'args'.\"\"\"\n\n    def wrapit(func: Any) -&gt; Any:\n        func = click.argument(\"args\", nargs=-1, type=click.UNPROCESSED)(func)\n        func = command_with_verbosity(\n            group,\n            name=name,\n            context_settings={\n                \"ignore_unknown_options\": True,\n                \"help_option_names\": [] if forward_help else [\"-h\", \"--help\"],\n            },\n        )(func)\n        return func\n\n    return wrapit\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.get_pkg_version","title":"get_pkg_version","text":"<pre><code>get_pkg_version(reqs_path, package_name)\n</code></pre> <p>Get package version from requirements.txt.</p> <p>Parameters:</p> <ul> <li> <code>reqs_path</code>               (<code>str | Path</code>)           \u2013            <p>Path to requirements.txt file.</p> </li> <li> <code>package_name</code>               (<code>str</code>)           \u2013            <p>Package to search for.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Package and its version as specified in requirements.txt.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KedroCliError</code>             \u2013            <p>If the file specified in <code>reqs_path</code> does not exist or <code>package_name</code> was not found in that file.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def get_pkg_version(reqs_path: (str | Path), package_name: str) -&gt; str:\n    \"\"\"Get package version from requirements.txt.\n\n    Args:\n        reqs_path: Path to requirements.txt file.\n        package_name: Package to search for.\n\n    Returns:\n        Package and its version as specified in requirements.txt.\n\n    Raises:\n        KedroCliError: If the file specified in ``reqs_path`` does not exist\n            or ``package_name`` was not found in that file.\n    \"\"\"\n    warnings.warn(\n        \"`get_pkg_version()` has been deprecated and will be removed in Kedro 0.20.0\",\n        KedroDeprecationWarning,\n    )\n    reqs_path = Path(reqs_path).absolute()\n    if not reqs_path.is_file():\n        raise KedroCliError(f\"Given path '{reqs_path}' is not a regular file.\")\n\n    pattern = re.compile(package_name + r\"([^\\w]|$)\")\n    with reqs_path.open(\"r\", encoding=\"utf-8\") as reqs_file:\n        for req_line in reqs_file:\n            req_line = req_line.strip()  # noqa: PLW2901\n            if pattern.search(req_line):\n                return req_line\n\n    raise KedroCliError(f\"Cannot find '{package_name}' package in '{reqs_path}'.\")\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.load_entry_points","title":"load_entry_points","text":"<pre><code>load_entry_points(name)\n</code></pre> <p>Load package entry point commands.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The key value specified in ENTRY_POINT_GROUPS.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KedroCliError</code>             \u2013            <p>If loading an entry point failed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[MultiCommand]</code>           \u2013            <p>List of entry point commands.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def load_entry_points(name: str) -&gt; Sequence[click.MultiCommand]:\n    \"\"\"Load package entry point commands.\n\n    Args:\n        name: The key value specified in ENTRY_POINT_GROUPS.\n\n    Raises:\n        KedroCliError: If loading an entry point failed.\n\n    Returns:\n        List of entry point commands.\n\n    \"\"\"\n\n    entry_point_commands = []\n    for entry_point in _get_entry_points(name):\n        loaded_entry_point = _safe_load_entry_point(entry_point)\n        if loaded_entry_point:\n            entry_point_commands.append(loaded_entry_point)\n    return entry_point_commands\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.python_call","title":"python_call","text":"<pre><code>python_call(module, arguments, **kwargs)\n</code></pre> <p>Run a subprocess command that invokes a Python module.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def python_call(\n    module: str, arguments: Iterable[str], **kwargs: Any\n) -&gt; None:  # pragma: no cover\n    \"\"\"Run a subprocess command that invokes a Python module.\"\"\"\n    call([sys.executable, \"-m\", module, *list(arguments)], **kwargs)\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.split_node_names","title":"split_node_names","text":"<pre><code>split_node_names(ctx, param, to_split)\n</code></pre> <p>Split string by comma, ignoring commas enclosed by square parentheses. This avoids splitting the string of nodes names on commas included in default node names, which have the pattern ([,...]) -&gt; [,...]) Note <ul> <li><code>to_split</code> will have such commas if and only if it includes a default node name. User-defined node names cannot include commas or square brackets.</li> <li>This function will no longer be necessary from Kedro 0.19.*, in which default node names will no longer contain commas</li> </ul> <p>Parameters:</p> <ul> <li> <code>to_split</code>               (<code>str</code>)           \u2013            <p>the string to split safely</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>A list containing the result of safe-splitting the string.</p> </li> </ul> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def split_node_names(ctx: click.Context, param: Any, to_split: str) -&gt; list[str]:\n    \"\"\"Split string by comma, ignoring commas enclosed by square parentheses.\n    This avoids splitting the string of nodes names on commas included in\n    default node names, which have the pattern\n    &lt;function_name&gt;([&lt;input_name&gt;,...]) -&gt; [&lt;output_name&gt;,...])\n\n    Note:\n        - `to_split` will have such commas if and only if it includes a\n        default node name. User-defined node names cannot include commas\n        or square brackets.\n        - This function will no longer be necessary from Kedro 0.19.*,\n        in which default node names will no longer contain commas\n\n    Args:\n        to_split: the string to split safely\n\n    Returns:\n        A list containing the result of safe-splitting the string.\n    \"\"\"\n    result = []\n    argument, match_state = \"\", 0\n    for char in to_split + \",\":\n        if char == \"[\":\n            match_state += 1\n        elif char == \"]\":\n            match_state -= 1\n        if char == \",\" and match_state == 0 and argument:\n            argument = argument.strip()\n            result.append(argument)\n            argument = \"\"\n        else:\n            argument += char\n    return result\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.split_string","title":"split_string","text":"<pre><code>split_string(ctx, param, value)\n</code></pre> <p>Split string by comma.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def split_string(ctx: click.Context, param: Any, value: str) -&gt; list[str]:\n    \"\"\"Split string by comma.\"\"\"\n    return [item.strip() for item in value.split(\",\") if item.strip()]\n</code></pre>"},{"location":"api/framework/kedro.framework.cli/#kedro.framework.cli.utils.validate_conf_source","title":"validate_conf_source","text":"<pre><code>validate_conf_source(ctx, param, value)\n</code></pre> <p>Validate the conf_source, only checking existence for local paths.</p> Source code in <code>kedro/framework/cli/utils.py</code> <pre><code>def validate_conf_source(ctx: click.Context, param: Any, value: str) -&gt; str | None:\n    \"\"\"Validate the conf_source, only checking existence for local paths.\"\"\"\n    if not value:\n        return None\n\n    # Check for remote URLs (except file://)\n    if \"://\" in value and not value.startswith(\"file://\"):\n        return value\n\n    # For local paths\n    try:\n        path = Path(value)\n        if not path.exists():\n            raise click.BadParameter(f\"Path '{value}' does not exist.\")\n        return str(path.resolve())\n    except click.BadParameter:\n        # Re-raise Click exceptions\n        raise\n    except Exception as exc:\n        # Wrap other exceptions\n        raise click.BadParameter(f\"Invalid path: {value}. Error: {exc!s}\")\n</code></pre>"},{"location":"api/framework/kedro.framework.context/","title":"Context","text":"Name Type Description <code>KedroContext</code> Class The base class for Kedro project contexts. <code>KedroContextError</code> Exception Error occurred when loading project and running context pipeline."},{"location":"api/framework/kedro.framework.context/#kedro.framework.context","title":"kedro.framework.context","text":"<p><code>kedro.framework.context</code> provides functionality for loading Kedro project context.</p>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContext","title":"kedro.framework.context.KedroContext","text":"<p><code>KedroContext</code> is the base class which holds the configuration and Kedro's main functionality.</p> <p>Create a context object by providing the root of a Kedro project and the environment configuration subfolders (see <code>kedro.config.OmegaConfigLoader</code>) Raises:     KedroContextError: If there is a mismatch         between Kedro project version and package version. Args:     project_path: Project path to define the context for.     config_loader: Kedro's <code>OmegaConfigLoader</code> for loading the configuration files.     env: Optional argument for configuration default environment to be used         for running the pipeline. If not specified, it defaults to \"local\".     package_name: Package name for the Kedro project the context is         created for.     hook_manager: The <code>PluginManager</code> to activate hooks, supplied by the session.     extra_params: Optional dictionary containing extra project parameters.         If specified, will update (and therefore take precedence over)         the parameters retrieved from the project configuration.</p>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContext._extra_params","title":"_extra_params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_extra_params = field(init=True, default=None, converter=deepcopy)\n</code></pre>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContext._hook_manager","title":"_hook_manager  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_hook_manager = field(init=True)\n</code></pre>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContext._package_name","title":"_package_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_package_name = field(init=True)\n</code></pre>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContext.catalog","title":"catalog  <code>property</code>","text":"<pre><code>catalog\n</code></pre> <p>Read-only property referring to Kedro's catalog` for this context.</p> <p>Returns:</p> <ul> <li> <code>CatalogProtocol</code>           \u2013            <p>catalog defined in <code>catalog.yml</code>.</p> </li> </ul> <p>Raises:     KedroContextError: Incorrect catalog registered for the project.</p>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContext.config_loader","title":"config_loader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_loader = field(init=True)\n</code></pre>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContext.env","title":"env  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env = field(init=True)\n</code></pre>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContext.params","title":"params  <code>property</code>","text":"<pre><code>params\n</code></pre> <p>Read-only property referring to Kedro's parameters for this context.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Parameters defined in <code>parameters.yml</code> with the addition of any extra parameters passed at initialization.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContext.project_path","title":"project_path  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>project_path = field(init=True, converter=_expand_full_path)\n</code></pre>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContext._get_catalog","title":"_get_catalog","text":"<pre><code>_get_catalog(save_version=None, load_versions=None)\n</code></pre> <p>A hook for changing the creation of a catalog instance.</p> <p>Returns:</p> <ul> <li> <code>CatalogProtocol</code>           \u2013            <p>catalog defined in <code>catalog.yml</code>.</p> </li> </ul> <p>Raises:     KedroContextError: Incorrect catalog registered for the project.</p> Source code in <code>kedro/framework/context/context.py</code> <pre><code>def _get_catalog(\n    self,\n    save_version: str | None = None,\n    load_versions: dict[str, str] | None = None,\n) -&gt; CatalogProtocol:\n    \"\"\"A hook for changing the creation of a catalog instance.\n\n    Returns:\n        catalog defined in `catalog.yml`.\n    Raises:\n        KedroContextError: Incorrect catalog registered for the project.\n\n    \"\"\"\n    # '**/catalog*' reads modular pipeline configs\n    conf_catalog = self.config_loader[\"catalog\"]\n    # turn relative paths in conf_catalog into absolute paths\n    # before initializing the catalog\n    conf_catalog = _convert_paths_to_absolute_posix(\n        project_path=self.project_path, conf_dictionary=conf_catalog\n    )\n    conf_creds = self._get_config_credentials()\n\n    catalog: DataCatalog = settings.DATA_CATALOG_CLASS.from_config(\n        catalog=conf_catalog,\n        credentials=conf_creds,\n        load_versions=load_versions,\n        save_version=save_version,\n    )\n\n    feed_dict = self._get_feed_dict()\n    catalog.add_feed_dict(feed_dict)\n    _validate_transcoded_datasets(catalog)\n    self._hook_manager.hook.after_catalog_created(\n        catalog=catalog,\n        conf_catalog=conf_catalog,\n        conf_creds=conf_creds,\n        feed_dict=feed_dict,\n        save_version=save_version,\n        load_versions=load_versions,\n    )\n    return catalog\n</code></pre>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContext._get_config_credentials","title":"_get_config_credentials","text":"<pre><code>_get_config_credentials()\n</code></pre> <p>Getter for credentials specified in credentials directory.</p> Source code in <code>kedro/framework/context/context.py</code> <pre><code>def _get_config_credentials(self) -&gt; dict[str, Any]:\n    \"\"\"Getter for credentials specified in credentials directory.\"\"\"\n    try:\n        conf_creds: dict[str, Any] = self.config_loader[\"credentials\"]\n    except MissingConfigException as exc:\n        logging.getLogger(__name__).debug(\n            \"Credentials not found in your Kedro project config.\\n %s\", str(exc)\n        )\n        conf_creds = {}\n    return conf_creds\n</code></pre>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContext._get_feed_dict","title":"_get_feed_dict","text":"<pre><code>_get_feed_dict()\n</code></pre> <p>Get parameters and return the feed dictionary.</p> Source code in <code>kedro/framework/context/context.py</code> <pre><code>def _get_feed_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Get parameters and return the feed dictionary.\"\"\"\n    params = self.params\n    feed_dict = {\"parameters\": params}\n\n    def _add_param_to_feed_dict(param_name: str, param_value: Any) -&gt; None:\n        \"\"\"This recursively adds parameter paths to the `feed_dict`,\n        whenever `param_value` is a dictionary itself, so that users can\n        specify specific nested parameters in their node inputs.\n\n        Example:\n\n            &gt;&gt;&gt; param_name = \"a\"\n            &gt;&gt;&gt; param_value = {\"b\": 1}\n            &gt;&gt;&gt; _add_param_to_feed_dict(param_name, param_value)\n            &gt;&gt;&gt; assert feed_dict[\"params:a\"] == {\"b\": 1}\n            &gt;&gt;&gt; assert feed_dict[\"params:a.b\"] == 1\n        \"\"\"\n        key = f\"params:{param_name}\"\n        feed_dict[key] = param_value\n        if isinstance(param_value, dict):\n            for key, val in param_value.items():\n                _add_param_to_feed_dict(f\"{param_name}.{key}\", val)\n\n    for param_name, param_value in params.items():\n        _add_param_to_feed_dict(param_name, param_value)\n\n    return feed_dict\n</code></pre>"},{"location":"api/framework/kedro.framework.context/#kedro.framework.context.KedroContextError","title":"kedro.framework.context.KedroContextError","text":"<p>               Bases: <code>Exception</code></p> <p>Error occurred when loading project and running context pipeline.</p>"},{"location":"api/framework/kedro.framework.hooks/","title":"Hooks","text":"Module Description <code>kedro.framework.hooks.manager</code> Provides a utility function to retrieve the global <code>hook_manager</code> singleton in Kedro's execution process. <code>kedro.framework.hooks.markers</code> Provides markers to declare Kedro's hook specs and implementations. <code>kedro.framework.hooks.specs</code> Contains specifications for all callable hooks in Kedro's execution timeline."},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks","title":"kedro.framework.hooks","text":"<p><code>kedro.framework.hooks</code> provides primitives to use hooks to extend KedroContext's behaviour</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager","title":"kedro.framework.hooks.manager","text":"<p>This module provides an utility function to retrieve the global hook_manager singleton in a Kedro's execution process.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.HOOK_NAMESPACE","title":"HOOK_NAMESPACE  <code>module-attribute</code>","text":"<pre><code>HOOK_NAMESPACE = 'kedro'\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager._PLUGIN_HOOKS","title":"_PLUGIN_HOOKS  <code>module-attribute</code>","text":"<pre><code>_PLUGIN_HOOKS = 'kedro.hooks'\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.DataCatalogSpecs","title":"DataCatalogSpecs","text":"<p>Namespace that defines all specifications for a data catalog's lifecycle hooks.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.DataCatalogSpecs.after_catalog_created","title":"after_catalog_created","text":"<pre><code>after_catalog_created(catalog, conf_catalog, conf_creds, feed_dict, save_version, load_versions)\n</code></pre> <p>Hooks to be invoked after a data catalog is created. It receives the <code>catalog</code> as well as all the arguments for <code>KedroContext._create_catalog</code>.</p> <p>Parameters:</p> <ul> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>The catalog that was created.</p> </li> <li> <code>conf_catalog</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The config from which the catalog was created.</p> </li> <li> <code>conf_creds</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The credentials conf from which the catalog was created.</p> </li> <li> <code>feed_dict</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The feed_dict that was added to the catalog after creation.</p> </li> <li> <code>save_version</code>               (<code>str</code>)           \u2013            <p>The save_version used in <code>save</code> operations for all datasets in the catalog.</p> </li> <li> <code>load_versions</code>               (<code>dict[str, str]</code>)           \u2013            <p>The load_versions used in <code>load</code> operations for each dataset in the catalog.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef after_catalog_created(  # noqa: PLR0913\n    self,\n    catalog: CatalogProtocol,\n    conf_catalog: dict[str, Any],\n    conf_creds: dict[str, Any],\n    feed_dict: dict[str, Any],\n    save_version: str,\n    load_versions: dict[str, str],\n) -&gt; None:\n    \"\"\"Hooks to be invoked after a data catalog is created.\n    It receives the ``catalog`` as well as\n    all the arguments for ``KedroContext._create_catalog``.\n\n    Args:\n        catalog: The catalog that was created.\n        conf_catalog: The config from which the catalog was created.\n        conf_creds: The credentials conf from which the catalog was created.\n        feed_dict: The feed_dict that was added to the catalog after creation.\n        save_version: The save_version used in ``save`` operations\n            for all datasets in the catalog.\n        load_versions: The load_versions used in ``load`` operations\n            for each dataset in the catalog.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.DatasetSpecs","title":"DatasetSpecs","text":"<p>Namespace that defines all specifications for a dataset's lifecycle hooks.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.DatasetSpecs.after_dataset_loaded","title":"after_dataset_loaded","text":"<pre><code>after_dataset_loaded(dataset_name, data, node)\n</code></pre> <p>Hook to be invoked after a dataset is loaded from the catalog.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>name of the dataset that was loaded from the catalog.</p> </li> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>the actual data that was loaded from the catalog.</p> </li> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> to run.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef after_dataset_loaded(self, dataset_name: str, data: Any, node: Node) -&gt; None:\n    \"\"\"Hook to be invoked after a dataset is loaded from the catalog.\n\n    Args:\n        dataset_name: name of the dataset that was loaded from the catalog.\n        data: the actual data that was loaded from the catalog.\n        node: The ``Node`` to run.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.DatasetSpecs.after_dataset_saved","title":"after_dataset_saved","text":"<pre><code>after_dataset_saved(dataset_name, data, node)\n</code></pre> <p>Hook to be invoked after a dataset is saved in the catalog.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>name of the dataset that was saved to the catalog.</p> </li> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>the actual data that was saved to the catalog.</p> </li> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> that ran.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef after_dataset_saved(self, dataset_name: str, data: Any, node: Node) -&gt; None:\n    \"\"\"Hook to be invoked after a dataset is saved in the catalog.\n\n    Args:\n        dataset_name: name of the dataset that was saved to the catalog.\n        data: the actual data that was saved to the catalog.\n        node: The ``Node`` that ran.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.DatasetSpecs.before_dataset_loaded","title":"before_dataset_loaded","text":"<pre><code>before_dataset_loaded(dataset_name, node)\n</code></pre> <p>Hook to be invoked before a dataset is loaded from the catalog.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>name of the dataset to be loaded from the catalog.</p> </li> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> to run.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef before_dataset_loaded(self, dataset_name: str, node: Node) -&gt; None:\n    \"\"\"Hook to be invoked before a dataset is loaded from the catalog.\n\n    Args:\n        dataset_name: name of the dataset to be loaded from the catalog.\n        node: The ``Node`` to run.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.DatasetSpecs.before_dataset_saved","title":"before_dataset_saved","text":"<pre><code>before_dataset_saved(dataset_name, data, node)\n</code></pre> <p>Hook to be invoked before a dataset is saved to the catalog.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>name of the dataset to be saved to the catalog.</p> </li> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>the actual data to be saved to the catalog.</p> </li> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> that ran.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef before_dataset_saved(self, dataset_name: str, data: Any, node: Node) -&gt; None:\n    \"\"\"Hook to be invoked before a dataset is saved to the catalog.\n\n    Args:\n        dataset_name: name of the dataset to be saved to the catalog.\n        data: the actual data to be saved to the catalog.\n        node: The ``Node`` that ran.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.KedroContextSpecs","title":"KedroContextSpecs","text":"<p>Namespace that defines all specifications for a Kedro context's lifecycle hooks.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.KedroContextSpecs.after_context_created","title":"after_context_created","text":"<pre><code>after_context_created(context)\n</code></pre> <p>Hooks to be invoked after a <code>KedroContext</code> is created. This is the earliest hook triggered within a Kedro run. The <code>KedroContext</code> stores useful information such as <code>credentials</code>, <code>config_loader</code> and <code>env</code>.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>KedroContext</code>)           \u2013            <p>The context that was created.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef after_context_created(\n    self,\n    context: KedroContext,\n) -&gt; None:\n    \"\"\"Hooks to be invoked after a `KedroContext` is created. This is the earliest\n    hook triggered within a Kedro run. The `KedroContext` stores useful information\n    such as `credentials`, `config_loader` and `env`.\n\n    Args:\n        context: The context that was created.\n    \"\"\"\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.NodeSpecs","title":"NodeSpecs","text":"<p>Namespace that defines all specifications for a node's lifecycle hooks.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.NodeSpecs.after_node_run","title":"after_node_run","text":"<pre><code>after_node_run(node, catalog, inputs, outputs, is_async, session_id)\n</code></pre> <p>Hook to be invoked after a node runs. The arguments received are the same as those used by <code>kedro.runner.run_node</code> as well as the <code>outputs</code> of the node run.</p> <p>Parameters:</p> <ul> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> that ran.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> containing the node's inputs and outputs.</p> </li> <li> <code>inputs</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The dictionary of inputs dataset. The keys are dataset names and the values are the actual loaded input data, not the dataset instance.</p> </li> <li> <code>outputs</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The dictionary of outputs dataset. The keys are dataset names and the values are the actual computed output data, not the dataset instance.</p> </li> <li> <code>is_async</code>               (<code>bool</code>)           \u2013            <p>Whether the node was run in <code>async</code> mode.</p> </li> <li> <code>session_id</code>               (<code>str</code>)           \u2013            <p>The id of the session.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef after_node_run(  # noqa: PLR0913\n    self,\n    node: Node,\n    catalog: CatalogProtocol,\n    inputs: dict[str, Any],\n    outputs: dict[str, Any],\n    is_async: bool,\n    session_id: str,\n) -&gt; None:\n    \"\"\"Hook to be invoked after a node runs.\n    The arguments received are the same as those used by ``kedro.runner.run_node``\n    as well as the ``outputs`` of the node run.\n\n    Args:\n        node: The ``Node`` that ran.\n        catalog: An implemented instance of ``CatalogProtocol`` containing the node's inputs and outputs.\n        inputs: The dictionary of inputs dataset.\n            The keys are dataset names and the values are the actual loaded input data,\n            not the dataset instance.\n        outputs: The dictionary of outputs dataset.\n            The keys are dataset names and the values are the actual computed output data,\n            not the dataset instance.\n        is_async: Whether the node was run in ``async`` mode.\n        session_id: The id of the session.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.NodeSpecs.before_node_run","title":"before_node_run","text":"<pre><code>before_node_run(node, catalog, inputs, is_async, session_id)\n</code></pre> <p>Hook to be invoked before a node runs. The arguments received are the same as those used by <code>kedro.runner.run_node</code></p> <p>Parameters:</p> <ul> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> containing the node's inputs and outputs.</p> </li> <li> <code>inputs</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The dictionary of inputs dataset. The keys are dataset names and the values are the actual loaded input data, not the dataset instance.</p> </li> <li> <code>is_async</code>               (<code>bool</code>)           \u2013            <p>Whether the node was run in <code>async</code> mode.</p> </li> <li> <code>session_id</code>               (<code>str</code>)           \u2013            <p>The id of the session.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any] | None</code>           \u2013            <p>Either None or a dictionary mapping dataset name(s) to new value(s). If returned, this dictionary will be used to update the node inputs, which allows to overwrite the node inputs.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef before_node_run(\n    self,\n    node: Node,\n    catalog: CatalogProtocol,\n    inputs: dict[str, Any],\n    is_async: bool,\n    session_id: str,\n) -&gt; dict[str, Any] | None:\n    \"\"\"Hook to be invoked before a node runs.\n    The arguments received are the same as those used by ``kedro.runner.run_node``\n\n    Args:\n        node: The ``Node`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` containing the node's inputs and outputs.\n        inputs: The dictionary of inputs dataset.\n            The keys are dataset names and the values are the actual loaded input data,\n            not the dataset instance.\n        is_async: Whether the node was run in ``async`` mode.\n        session_id: The id of the session.\n\n    Returns:\n        Either None or a dictionary mapping dataset name(s) to new value(s).\n            If returned, this dictionary will be used to update the node inputs,\n            which allows to overwrite the node inputs.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.NodeSpecs.on_node_error","title":"on_node_error","text":"<pre><code>on_node_error(error, node, catalog, inputs, is_async, session_id)\n</code></pre> <p>Hook to be invoked if a node run throws an uncaught error. The signature of this error hook should match the signature of <code>before_node_run</code> along with the error that was raised.</p> <p>Parameters:</p> <ul> <li> <code>error</code>               (<code>Exception</code>)           \u2013            <p>The uncaught exception thrown during the node run.</p> </li> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> containing the node's inputs and outputs.</p> </li> <li> <code>inputs</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The dictionary of inputs dataset. The keys are dataset names and the values are the actual loaded input data, not the dataset instance.</p> </li> <li> <code>is_async</code>               (<code>bool</code>)           \u2013            <p>Whether the node was run in <code>async</code> mode.</p> </li> <li> <code>session_id</code>               (<code>str</code>)           \u2013            <p>The id of the session.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef on_node_error(  # noqa: PLR0913\n    self,\n    error: Exception,\n    node: Node,\n    catalog: CatalogProtocol,\n    inputs: dict[str, Any],\n    is_async: bool,\n    session_id: str,\n) -&gt; None:\n    \"\"\"Hook to be invoked if a node run throws an uncaught error.\n    The signature of this error hook should match the signature of ``before_node_run``\n    along with the error that was raised.\n\n    Args:\n        error: The uncaught exception thrown during the node run.\n        node: The ``Node`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` containing the node's inputs and outputs.\n        inputs: The dictionary of inputs dataset.\n            The keys are dataset names and the values are the actual loaded input data,\n            not the dataset instance.\n        is_async: Whether the node was run in ``async`` mode.\n        session_id: The id of the session.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.PipelineSpecs","title":"PipelineSpecs","text":"<p>Namespace that defines all specifications for a pipeline's lifecycle hooks.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.PipelineSpecs.after_pipeline_run","title":"after_pipeline_run","text":"<pre><code>after_pipeline_run(run_params, run_result, pipeline, catalog)\n</code></pre> <p>Hook to be invoked after a pipeline runs.</p> <p>Parameters:</p> <ul> <li> <code>run_params</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The params used to run the pipeline. Should have the following schema::</p> <p>{      \"session_id\": str      \"project_path\": str,      \"env\": str,      \"kedro_version\": str,      \"tags\": Optional[List[str]],      \"from_nodes\": Optional[List[str]],      \"to_nodes\": Optional[List[str]],      \"node_names\": Optional[List[str]],      \"from_inputs\": Optional[List[str]],      \"to_outputs\": Optional[List[str]],      \"load_versions\": Optional[List[str]],      \"extra_params\": Optional[Dict[str, Any]]      \"pipeline_name\": str,      \"namespace\": Optional[str],      \"runner\": str,    }</p> </li> <li> <code>run_result</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The output of <code>Pipeline</code> run.</p> </li> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> that was run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> used during the run.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef after_pipeline_run(\n    self,\n    run_params: dict[str, Any],\n    run_result: dict[str, Any],\n    pipeline: Pipeline,\n    catalog: CatalogProtocol,\n) -&gt; None:\n    \"\"\"Hook to be invoked after a pipeline runs.\n\n    Args:\n        run_params: The params used to run the pipeline.\n            Should have the following schema::\n\n               {\n                 \"session_id\": str\n                 \"project_path\": str,\n                 \"env\": str,\n                 \"kedro_version\": str,\n                 \"tags\": Optional[List[str]],\n                 \"from_nodes\": Optional[List[str]],\n                 \"to_nodes\": Optional[List[str]],\n                 \"node_names\": Optional[List[str]],\n                 \"from_inputs\": Optional[List[str]],\n                 \"to_outputs\": Optional[List[str]],\n                 \"load_versions\": Optional[List[str]],\n                 \"extra_params\": Optional[Dict[str, Any]]\n                 \"pipeline_name\": str,\n                 \"namespace\": Optional[str],\n                 \"runner\": str,\n               }\n\n        run_result: The output of ``Pipeline`` run.\n        pipeline: The ``Pipeline`` that was run.\n        catalog: An implemented instance of ``CatalogProtocol`` used during the run.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.PipelineSpecs.before_pipeline_run","title":"before_pipeline_run","text":"<pre><code>before_pipeline_run(run_params, pipeline, catalog)\n</code></pre> <p>Hook to be invoked before a pipeline runs.</p> <p>Parameters:</p> <ul> <li> <code>run_params</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The params used to run the pipeline. Should have the following schema::</p> <p>{      \"session_id\": str      \"project_path\": str,      \"env\": str,      \"kedro_version\": str,      \"tags\": Optional[List[str]],      \"from_nodes\": Optional[List[str]],      \"to_nodes\": Optional[List[str]],      \"node_names\": Optional[List[str]],      \"from_inputs\": Optional[List[str]],      \"to_outputs\": Optional[List[str]],      \"load_versions\": Optional[List[str]],      \"extra_params\": Optional[Dict[str, Any]]      \"pipeline_name\": str,      \"namespace\": Optional[str],      \"runner\": str,    }</p> </li> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> that will be run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> to be used during the run.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef before_pipeline_run(\n    self, run_params: dict[str, Any], pipeline: Pipeline, catalog: CatalogProtocol\n) -&gt; None:\n    \"\"\"Hook to be invoked before a pipeline runs.\n\n    Args:\n        run_params: The params used to run the pipeline.\n            Should have the following schema::\n\n               {\n                 \"session_id\": str\n                 \"project_path\": str,\n                 \"env\": str,\n                 \"kedro_version\": str,\n                 \"tags\": Optional[List[str]],\n                 \"from_nodes\": Optional[List[str]],\n                 \"to_nodes\": Optional[List[str]],\n                 \"node_names\": Optional[List[str]],\n                 \"from_inputs\": Optional[List[str]],\n                 \"to_outputs\": Optional[List[str]],\n                 \"load_versions\": Optional[List[str]],\n                 \"extra_params\": Optional[Dict[str, Any]]\n                 \"pipeline_name\": str,\n                 \"namespace\": Optional[str],\n                 \"runner\": str,\n               }\n\n        pipeline: The ``Pipeline`` that will be run.\n        catalog: An implemented instance of ``CatalogProtocol`` to be used during the run.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager.PipelineSpecs.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(error, run_params, pipeline, catalog)\n</code></pre> <p>Hook to be invoked if a pipeline run throws an uncaught Exception. The signature of this error hook should match the signature of <code>before_pipeline_run</code> along with the error that was raised.</p> <p>Parameters:</p> <ul> <li> <code>error</code>               (<code>Exception</code>)           \u2013            <p>The uncaught exception thrown during the pipeline run.</p> </li> <li> <code>run_params</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The params used to run the pipeline. Should have the following schema::</p> <p>{      \"session_id\": str      \"project_path\": str,      \"env\": str,      \"kedro_version\": str,      \"tags\": Optional[List[str]],      \"from_nodes\": Optional[List[str]],      \"to_nodes\": Optional[List[str]],      \"node_names\": Optional[List[str]],      \"from_inputs\": Optional[List[str]],      \"to_outputs\": Optional[List[str]],      \"load_versions\": Optional[List[str]],      \"extra_params\": Optional[Dict[str, Any]]      \"pipeline_name\": str,      \"namespace\": Optional[str],      \"runner\": str,    }</p> </li> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> that will was run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> used during the run.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef on_pipeline_error(\n    self,\n    error: Exception,\n    run_params: dict[str, Any],\n    pipeline: Pipeline,\n    catalog: CatalogProtocol,\n) -&gt; None:\n    \"\"\"Hook to be invoked if a pipeline run throws an uncaught Exception.\n    The signature of this error hook should match the signature of ``before_pipeline_run``\n    along with the error that was raised.\n\n    Args:\n        error: The uncaught exception thrown during the pipeline run.\n        run_params: The params used to run the pipeline.\n            Should have the following schema::\n\n               {\n                 \"session_id\": str\n                 \"project_path\": str,\n                 \"env\": str,\n                 \"kedro_version\": str,\n                 \"tags\": Optional[List[str]],\n                 \"from_nodes\": Optional[List[str]],\n                 \"to_nodes\": Optional[List[str]],\n                 \"node_names\": Optional[List[str]],\n                 \"from_inputs\": Optional[List[str]],\n                 \"to_outputs\": Optional[List[str]],\n                 \"load_versions\": Optional[List[str]],\n                 \"extra_params\": Optional[Dict[str, Any]]\n                 \"pipeline_name\": str,\n                 \"namespace\": Optional[str],\n                 \"runner\": str,\n               }\n\n        pipeline: The ``Pipeline`` that will was run.\n        catalog: An implemented instance of ``CatalogProtocol`` used during the run.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager._NullPluginManager","title":"_NullPluginManager","text":"<pre><code>_NullPluginManager(*args, **kwargs)\n</code></pre> <p>This class creates an empty <code>hook_manager</code> that will ignore all calls to hooks, allowing the runner to function if no <code>hook_manager</code> has been instantiated.</p> Source code in <code>kedro/framework/hooks/manager.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager._create_hook_manager","title":"_create_hook_manager","text":"<pre><code>_create_hook_manager()\n</code></pre> <p>Create a new PluginManager instance and register Kedro's hook specs.</p> Source code in <code>kedro/framework/hooks/manager.py</code> <pre><code>def _create_hook_manager() -&gt; PluginManager:\n    \"\"\"Create a new PluginManager instance and register Kedro's hook specs.\"\"\"\n    manager = PluginManager(HOOK_NAMESPACE)\n    manager.trace.root.setwriter(logger.debug)\n    manager.enable_tracing()\n    manager.add_hookspecs(NodeSpecs)\n    manager.add_hookspecs(PipelineSpecs)\n    manager.add_hookspecs(DataCatalogSpecs)\n    manager.add_hookspecs(DatasetSpecs)\n    manager.add_hookspecs(KedroContextSpecs)\n    return manager\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager._register_hooks","title":"_register_hooks","text":"<pre><code>_register_hooks(hook_manager, hooks)\n</code></pre> <p>Register all hooks as specified in <code>hooks</code> with the global <code>hook_manager</code>.</p> <p>Parameters:</p> <ul> <li> <code>hook_manager</code>               (<code>PluginManager</code>)           \u2013            <p>Hook manager instance to register the hooks with.</p> </li> <li> <code>hooks</code>               (<code>Iterable[Any]</code>)           \u2013            <p>Hooks that need to be registered.</p> </li> </ul> Source code in <code>kedro/framework/hooks/manager.py</code> <pre><code>def _register_hooks(hook_manager: PluginManager, hooks: Iterable[Any]) -&gt; None:\n    \"\"\"Register all hooks as specified in ``hooks`` with the global ``hook_manager``.\n\n    Args:\n        hook_manager: Hook manager instance to register the hooks with.\n        hooks: Hooks that need to be registered.\n\n    \"\"\"\n    for hooks_collection in hooks:\n        # Sometimes users might call hook registration more than once, in which\n        # case hooks have already been registered, so we perform a simple check\n        # here to avoid an error being raised and break user's workflow.\n        if not hook_manager.is_registered(hooks_collection):\n            if isclass(hooks_collection):\n                raise TypeError(\n                    \"KedroSession expects hooks to be registered as instances. \"\n                    \"Have you forgotten the `()` when registering a hook class ?\"\n                )\n            hook_manager.register(hooks_collection)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.manager._register_hooks_entry_points","title":"_register_hooks_entry_points","text":"<pre><code>_register_hooks_entry_points(hook_manager, disabled_plugins)\n</code></pre> <p>Register pluggy hooks from python package entrypoints.</p> <p>Parameters:</p> <ul> <li> <code>hook_manager</code>               (<code>PluginManager</code>)           \u2013            <p>Hook manager instance to register the hooks with.</p> </li> <li> <code>disabled_plugins</code>               (<code>Iterable[str]</code>)           \u2013            <p>An iterable returning the names of plugins which hooks must not be registered; any already registered hooks will be unregistered.</p> </li> </ul> Source code in <code>kedro/framework/hooks/manager.py</code> <pre><code>def _register_hooks_entry_points(\n    hook_manager: PluginManager, disabled_plugins: Iterable[str]\n) -&gt; None:\n    \"\"\"Register pluggy hooks from python package entrypoints.\n\n    Args:\n        hook_manager: Hook manager instance to register the hooks with.\n        disabled_plugins: An iterable returning the names of plugins\n            which hooks must not be registered; any already registered\n            hooks will be unregistered.\n\n    \"\"\"\n    already_registered = hook_manager.get_plugins()\n    # Method name is misleading:\n    # entry points are standard and don't require setuptools,\n    # see https://packaging.python.org/en/latest/specifications/entry-points/\n    hook_manager.load_setuptools_entrypoints(_PLUGIN_HOOKS)\n    disabled_plugins = set(disabled_plugins)\n\n    # Get list of plugin/distinfo tuples for all registered plugins.\n    plugininfo = hook_manager.list_plugin_distinfo()\n    plugin_names = set()\n    disabled_plugin_names = set()\n    for plugin, dist in plugininfo:\n        if dist.project_name in disabled_plugins:\n            # `unregister()` is used instead of `set_blocked()` because\n            # we want to disable hooks for specific plugin based on project\n            # name and not `entry_point` name. Also, we log project names with\n            # version for which hooks were registered.\n            hook_manager.unregister(plugin=plugin)\n            disabled_plugin_names.add(f\"{dist.project_name}-{dist.version}\")\n        elif plugin not in already_registered:\n            plugin_names.add(f\"{dist.project_name}-{dist.version}\")\n\n    if disabled_plugin_names:\n        logger.debug(\n            \"Hooks are disabled for plugin(s): %s\",\n            \", \".join(sorted(disabled_plugin_names)),\n        )\n\n    if plugin_names:\n        logger.debug(\n            \"Registered hooks from %d installed plugin(s): %s\",\n            len(plugin_names),\n            \", \".join(sorted(plugin_names)),\n        )\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.markers","title":"kedro.framework.hooks.markers","text":"<p>This module provides markers to declare Kedro's hook specs and implementations. For more information, please see Pluggy's documentation.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.markers.HOOK_NAMESPACE","title":"HOOK_NAMESPACE  <code>module-attribute</code>","text":"<pre><code>HOOK_NAMESPACE = 'kedro'\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.markers.hook_impl","title":"hook_impl  <code>module-attribute</code>","text":"<pre><code>hook_impl = HookimplMarker(HOOK_NAMESPACE)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.markers.hook_spec","title":"hook_spec  <code>module-attribute</code>","text":"<pre><code>hook_spec = HookspecMarker(HOOK_NAMESPACE)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs","title":"kedro.framework.hooks.specs","text":"<p>A module containing specifications for all callable hooks in the Kedro's execution timeline. For more information about these specifications, please visit Pluggy's documentation</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.hook_spec","title":"hook_spec  <code>module-attribute</code>","text":"<pre><code>hook_spec = HookspecMarker(HOOK_NAMESPACE)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol","title":"CatalogProtocol","text":"<p>               Bases: <code>Protocol[_C]</code></p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol.config_resolver","title":"config_resolver  <code>property</code>","text":"<pre><code>config_resolver\n</code></pre> <p>Return a copy of the datasets dictionary.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol.__contains__","title":"__contains__","text":"<pre><code>__contains__(ds_name)\n</code></pre> <p>Check if a dataset is in the catalog.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def __contains__(self, ds_name: str) -&gt; bool:\n    \"\"\"Check if a dataset is in the catalog.\"\"\"\n    ...\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol.add","title":"add","text":"<pre><code>add(ds_name, dataset, replace=False)\n</code></pre> <p>Add a new dataset to the catalog.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def add(self, ds_name: str, dataset: Any, replace: bool = False) -&gt; None:\n    \"\"\"Add a new dataset to the catalog.\"\"\"\n    ...\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol.add_feed_dict","title":"add_feed_dict","text":"<pre><code>add_feed_dict(datasets, replace=False)\n</code></pre> <p>Add datasets to the catalog using the data provided through the <code>feed_dict</code>.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def add_feed_dict(self, datasets: dict[str, Any], replace: bool = False) -&gt; None:\n    \"\"\"Add datasets to the catalog using the data provided through the `feed_dict`.\"\"\"\n    ...\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol.confirm","title":"confirm","text":"<pre><code>confirm(name)\n</code></pre> <p>Confirm a dataset by its name.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def confirm(self, name: str) -&gt; None:\n    \"\"\"Confirm a dataset by its name.\"\"\"\n    ...\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol.exists","title":"exists","text":"<pre><code>exists(name)\n</code></pre> <p>Checks whether registered dataset exists by calling its <code>exists()</code> method.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def exists(self, name: str) -&gt; bool:\n    \"\"\"Checks whether registered dataset exists by calling its `exists()` method.\"\"\"\n    ...\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(catalog)\n</code></pre> <p>Create a catalog instance from configuration.</p> Source code in <code>kedro/io/core.py</code> <pre><code>@classmethod\ndef from_config(cls, catalog: dict[str, dict[str, Any]] | None) -&gt; _C:\n    \"\"\"Create a catalog instance from configuration.\"\"\"\n    ...\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol.list","title":"list","text":"<pre><code>list(regex_search=None)\n</code></pre> <p>List all dataset names registered in the catalog.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def list(self, regex_search: str | None = None) -&gt; list[str]:\n    \"\"\"List all dataset names registered in the catalog.\"\"\"\n    ...\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol.load","title":"load","text":"<pre><code>load(name, version=None)\n</code></pre> <p>Load data from a registered dataset.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def load(self, name: str, version: str | None = None) -&gt; Any:\n    \"\"\"Load data from a registered dataset.\"\"\"\n    ...\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol.release","title":"release","text":"<pre><code>release(name)\n</code></pre> <p>Release any cached data associated with a dataset.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def release(self, name: str) -&gt; None:\n    \"\"\"Release any cached data associated with a dataset.\"\"\"\n    ...\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol.save","title":"save","text":"<pre><code>save(name, data)\n</code></pre> <p>Save data to a registered dataset.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def save(self, name: str, data: Any) -&gt; None:\n    \"\"\"Save data to a registered dataset.\"\"\"\n    ...\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.CatalogProtocol.shallow_copy","title":"shallow_copy","text":"<pre><code>shallow_copy(extra_dataset_patterns=None)\n</code></pre> <p>Returns a shallow copy of the current object.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def shallow_copy(self, extra_dataset_patterns: Patterns | None = None) -&gt; _C:\n    \"\"\"Returns a shallow copy of the current object.\"\"\"\n    ...\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.DataCatalogSpecs","title":"DataCatalogSpecs","text":"<p>Namespace that defines all specifications for a data catalog's lifecycle hooks.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.DataCatalogSpecs.after_catalog_created","title":"after_catalog_created","text":"<pre><code>after_catalog_created(catalog, conf_catalog, conf_creds, feed_dict, save_version, load_versions)\n</code></pre> <p>Hooks to be invoked after a data catalog is created. It receives the <code>catalog</code> as well as all the arguments for <code>KedroContext._create_catalog</code>.</p> <p>Parameters:</p> <ul> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>The catalog that was created.</p> </li> <li> <code>conf_catalog</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The config from which the catalog was created.</p> </li> <li> <code>conf_creds</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The credentials conf from which the catalog was created.</p> </li> <li> <code>feed_dict</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The feed_dict that was added to the catalog after creation.</p> </li> <li> <code>save_version</code>               (<code>str</code>)           \u2013            <p>The save_version used in <code>save</code> operations for all datasets in the catalog.</p> </li> <li> <code>load_versions</code>               (<code>dict[str, str]</code>)           \u2013            <p>The load_versions used in <code>load</code> operations for each dataset in the catalog.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef after_catalog_created(  # noqa: PLR0913\n    self,\n    catalog: CatalogProtocol,\n    conf_catalog: dict[str, Any],\n    conf_creds: dict[str, Any],\n    feed_dict: dict[str, Any],\n    save_version: str,\n    load_versions: dict[str, str],\n) -&gt; None:\n    \"\"\"Hooks to be invoked after a data catalog is created.\n    It receives the ``catalog`` as well as\n    all the arguments for ``KedroContext._create_catalog``.\n\n    Args:\n        catalog: The catalog that was created.\n        conf_catalog: The config from which the catalog was created.\n        conf_creds: The credentials conf from which the catalog was created.\n        feed_dict: The feed_dict that was added to the catalog after creation.\n        save_version: The save_version used in ``save`` operations\n            for all datasets in the catalog.\n        load_versions: The load_versions used in ``load`` operations\n            for each dataset in the catalog.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.DatasetSpecs","title":"DatasetSpecs","text":"<p>Namespace that defines all specifications for a dataset's lifecycle hooks.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.DatasetSpecs.after_dataset_loaded","title":"after_dataset_loaded","text":"<pre><code>after_dataset_loaded(dataset_name, data, node)\n</code></pre> <p>Hook to be invoked after a dataset is loaded from the catalog.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>name of the dataset that was loaded from the catalog.</p> </li> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>the actual data that was loaded from the catalog.</p> </li> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> to run.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef after_dataset_loaded(self, dataset_name: str, data: Any, node: Node) -&gt; None:\n    \"\"\"Hook to be invoked after a dataset is loaded from the catalog.\n\n    Args:\n        dataset_name: name of the dataset that was loaded from the catalog.\n        data: the actual data that was loaded from the catalog.\n        node: The ``Node`` to run.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.DatasetSpecs.after_dataset_saved","title":"after_dataset_saved","text":"<pre><code>after_dataset_saved(dataset_name, data, node)\n</code></pre> <p>Hook to be invoked after a dataset is saved in the catalog.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>name of the dataset that was saved to the catalog.</p> </li> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>the actual data that was saved to the catalog.</p> </li> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> that ran.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef after_dataset_saved(self, dataset_name: str, data: Any, node: Node) -&gt; None:\n    \"\"\"Hook to be invoked after a dataset is saved in the catalog.\n\n    Args:\n        dataset_name: name of the dataset that was saved to the catalog.\n        data: the actual data that was saved to the catalog.\n        node: The ``Node`` that ran.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.DatasetSpecs.before_dataset_loaded","title":"before_dataset_loaded","text":"<pre><code>before_dataset_loaded(dataset_name, node)\n</code></pre> <p>Hook to be invoked before a dataset is loaded from the catalog.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>name of the dataset to be loaded from the catalog.</p> </li> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> to run.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef before_dataset_loaded(self, dataset_name: str, node: Node) -&gt; None:\n    \"\"\"Hook to be invoked before a dataset is loaded from the catalog.\n\n    Args:\n        dataset_name: name of the dataset to be loaded from the catalog.\n        node: The ``Node`` to run.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.DatasetSpecs.before_dataset_saved","title":"before_dataset_saved","text":"<pre><code>before_dataset_saved(dataset_name, data, node)\n</code></pre> <p>Hook to be invoked before a dataset is saved to the catalog.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>name of the dataset to be saved to the catalog.</p> </li> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>the actual data to be saved to the catalog.</p> </li> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> that ran.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef before_dataset_saved(self, dataset_name: str, data: Any, node: Node) -&gt; None:\n    \"\"\"Hook to be invoked before a dataset is saved to the catalog.\n\n    Args:\n        dataset_name: name of the dataset to be saved to the catalog.\n        data: the actual data to be saved to the catalog.\n        node: The ``Node`` that ran.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.KedroContext","title":"KedroContext","text":"<p><code>KedroContext</code> is the base class which holds the configuration and Kedro's main functionality.</p> <p>Create a context object by providing the root of a Kedro project and the environment configuration subfolders (see <code>kedro.config.OmegaConfigLoader</code>) Raises:     KedroContextError: If there is a mismatch         between Kedro project version and package version. Args:     project_path: Project path to define the context for.     config_loader: Kedro's <code>OmegaConfigLoader</code> for loading the configuration files.     env: Optional argument for configuration default environment to be used         for running the pipeline. If not specified, it defaults to \"local\".     package_name: Package name for the Kedro project the context is         created for.     hook_manager: The <code>PluginManager</code> to activate hooks, supplied by the session.     extra_params: Optional dictionary containing extra project parameters.         If specified, will update (and therefore take precedence over)         the parameters retrieved from the project configuration.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.KedroContext.catalog","title":"catalog  <code>property</code>","text":"<pre><code>catalog\n</code></pre> <p>Read-only property referring to Kedro's catalog` for this context.</p> <p>Returns:</p> <ul> <li> <code>CatalogProtocol</code>           \u2013            <p>catalog defined in <code>catalog.yml</code>.</p> </li> </ul> <p>Raises:     KedroContextError: Incorrect catalog registered for the project.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.KedroContext.params","title":"params  <code>property</code>","text":"<pre><code>params\n</code></pre> <p>Read-only property referring to Kedro's parameters for this context.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Parameters defined in <code>parameters.yml</code> with the addition of any extra parameters passed at initialization.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.KedroContextSpecs","title":"KedroContextSpecs","text":"<p>Namespace that defines all specifications for a Kedro context's lifecycle hooks.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.KedroContextSpecs.after_context_created","title":"after_context_created","text":"<pre><code>after_context_created(context)\n</code></pre> <p>Hooks to be invoked after a <code>KedroContext</code> is created. This is the earliest hook triggered within a Kedro run. The <code>KedroContext</code> stores useful information such as <code>credentials</code>, <code>config_loader</code> and <code>env</code>.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>KedroContext</code>)           \u2013            <p>The context that was created.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef after_context_created(\n    self,\n    context: KedroContext,\n) -&gt; None:\n    \"\"\"Hooks to be invoked after a `KedroContext` is created. This is the earliest\n    hook triggered within a Kedro run. The `KedroContext` stores useful information\n    such as `credentials`, `config_loader` and `env`.\n\n    Args:\n        context: The context that was created.\n    \"\"\"\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Node","title":"Node","text":"<pre><code>Node(func, inputs, outputs, *, name=None, tags=None, confirms=None, namespace=None)\n</code></pre> <p><code>Node</code> is an auxiliary class facilitating the operations required to run user-provided functions as part of Kedro pipelines.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable</code>)           \u2013            <p>A function that corresponds to the node logic. The function should have at least one input or output.</p> </li> <li> <code>inputs</code>               (<code>str | list[str] | dict[str, str] | None</code>)           \u2013            <p>The name or the list of the names of variables used as inputs to the function. The number of names should match the number of arguments in the definition of the provided function. When dict[str, str] is provided, variable names will be mapped to function argument names.</p> </li> <li> <code>outputs</code>               (<code>str | list[str] | dict[str, str] | None</code>)           \u2013            <p>The name or the list of the names of variables used as outputs of the function. The number of names should match the number of outputs returned by the provided function. When dict[str, str] is provided, variable names will be mapped to the named outputs the function returns.</p> </li> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional node name to be used when displaying the node in logs or any other visualisations. Valid node name must contain only letters, digits, hyphens, underscores and/or fullstops.</p> </li> <li> <code>tags</code>               (<code>str | Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional set of tags to be applied to the node. Valid node tag must contain only letters, digits, hyphens, underscores and/or fullstops.</p> </li> <li> <code>confirms</code>               (<code>str | list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional name or the list of the names of the datasets that should be confirmed. This will result in calling <code>confirm()</code> method of the corresponding dataset instance. Specified dataset names do not necessarily need to be present in the node <code>inputs</code> or <code>outputs</code>.</p> </li> <li> <code>namespace</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional node namespace.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised in the following cases: a) When the provided arguments do not conform to the format suggested by the type hint of the argument. b) When the node produces multiple outputs with the same name. c) When an input has the same name as an output. d) When the given node name violates the requirements: it must contain only letters, digits, hyphens, underscores and/or fullstops.</p> </li> </ul> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    func: Callable,\n    inputs: str | list[str] | dict[str, str] | None,\n    outputs: str | list[str] | dict[str, str] | None,\n    *,\n    name: str | None = None,\n    tags: str | Iterable[str] | None = None,\n    confirms: str | list[str] | None = None,\n    namespace: str | None = None,\n):\n    \"\"\"Create a node in the pipeline by providing a function to be called\n    along with variable names for inputs and/or outputs.\n\n    Args:\n        func: A function that corresponds to the node logic.\n            The function should have at least one input or output.\n        inputs: The name or the list of the names of variables used as\n            inputs to the function. The number of names should match\n            the number of arguments in the definition of the provided\n            function. When dict[str, str] is provided, variable names\n            will be mapped to function argument names.\n        outputs: The name or the list of the names of variables used\n            as outputs of the function. The number of names should match\n            the number of outputs returned by the provided function.\n            When dict[str, str] is provided, variable names will be mapped\n            to the named outputs the function returns.\n        name: Optional node name to be used when displaying the node in\n            logs or any other visualisations. Valid node name must contain\n            only letters, digits, hyphens, underscores and/or fullstops.\n        tags: Optional set of tags to be applied to the node. Valid node tag must\n            contain only letters, digits, hyphens, underscores and/or fullstops.\n        confirms: Optional name or the list of the names of the datasets\n            that should be confirmed. This will result in calling\n            ``confirm()`` method of the corresponding dataset instance.\n            Specified dataset names do not necessarily need to be present\n            in the node ``inputs`` or ``outputs``.\n        namespace: Optional node namespace.\n\n    Raises:\n        ValueError: Raised in the following cases:\n            a) When the provided arguments do not conform to\n            the format suggested by the type hint of the argument.\n            b) When the node produces multiple outputs with the same name.\n            c) When an input has the same name as an output.\n            d) When the given node name violates the requirements:\n            it must contain only letters, digits, hyphens, underscores\n            and/or fullstops.\n\n    \"\"\"\n    if not callable(func):\n        raise ValueError(\n            _node_error_message(\n                f\"first argument must be a function, not '{type(func).__name__}'.\"\n            )\n        )\n\n    if inputs and not isinstance(inputs, (list, dict, str)):\n        raise ValueError(\n            _node_error_message(\n                f\"'inputs' type must be one of [String, List, Dict, None], \"\n                f\"not '{type(inputs).__name__}'.\"\n            )\n        )\n\n    for _input in _to_list(inputs):\n        if not isinstance(_input, str):\n            raise ValueError(\n                _node_error_message(\n                    f\"names of variables used as inputs to the function \"\n                    f\"must be of 'String' type, but {_input} from {inputs} \"\n                    f\"is '{type(_input)}'.\"\n                )\n            )\n\n    if outputs and not isinstance(outputs, (list, dict, str)):\n        raise ValueError(\n            _node_error_message(\n                f\"'outputs' type must be one of [String, List, Dict, None], \"\n                f\"not '{type(outputs).__name__}'.\"\n            )\n        )\n\n    for _output in _to_list(outputs):\n        if not isinstance(_output, str):\n            raise ValueError(\n                _node_error_message(\n                    f\"names of variables used as outputs of the function \"\n                    f\"must be of 'String' type, but {_output} from {outputs} \"\n                    f\"is '{type(_output)}'.\"\n                )\n            )\n\n    if not inputs and not outputs:\n        raise ValueError(\n            _node_error_message(\"it must have some 'inputs' or 'outputs'.\")\n        )\n\n    self._validate_inputs(func, inputs)\n\n    self._func = func\n    self._inputs = inputs\n    # The type of _outputs is picked up as possibly being None, however the checks above prevent that\n    # ever being the case. Mypy doesn't get that though, so it complains about the assignment of outputs to\n    # _outputs with different types.\n    self._outputs: str | list[str] | dict[str, str] = outputs  # type: ignore[assignment]\n    if name and not re.match(r\"[\\w\\.-]+$\", name):\n        raise ValueError(\n            f\"'{name}' is not a valid node name. It must contain only \"\n            f\"letters, digits, hyphens, underscores and/or fullstops.\"\n        )\n    self._name = name\n    self._namespace = namespace\n    self._tags = set(_to_list(tags))\n    for tag in self._tags:\n        if not re.match(r\"[\\w\\.-]+$\", tag):\n            raise ValueError(\n                f\"'{tag}' is not a valid node tag. It must contain only \"\n                f\"letters, digits, hyphens, underscores and/or fullstops.\"\n            )\n\n    self._validate_unique_outputs()\n    self._validate_inputs_dif_than_outputs()\n    self._confirms = confirms\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Node.confirms","title":"confirms  <code>property</code>","text":"<pre><code>confirms\n</code></pre> <p>Return dataset names to confirm as a list.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>Dataset names to confirm as a list.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Node.func","title":"func  <code>property</code> <code>writable</code>","text":"<pre><code>func\n</code></pre> <p>Exposes the underlying function of the node.</p> <p>Returns:</p> <ul> <li> <code>Callable</code>           \u2013            <p>Return the underlying function of the node.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Node.inputs","title":"inputs  <code>property</code>","text":"<pre><code>inputs\n</code></pre> <p>Return node inputs as a list, in the order required to bind them properly to the node's function.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>Node input names as a list.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Node.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Node's name.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Node's name if provided or the name of its function.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Node.namespace","title":"namespace  <code>property</code>","text":"<pre><code>namespace\n</code></pre> <p>Node's namespace.</p> <p>Returns:</p> <ul> <li> <code>str | None</code>           \u2013            <p>String representing node's namespace, typically from outer to inner scopes.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Node.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs\n</code></pre> <p>Return node outputs as a list preserving the original order     if possible.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>Node output names as a list.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Node.short_name","title":"short_name  <code>property</code>","text":"<pre><code>short_name\n</code></pre> <p>Node's name.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Returns a short, user-friendly name that is not guaranteed to be unique.</p> </li> <li> <code>str</code>           \u2013            <p>The namespace is stripped out of the node name.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Node.tags","title":"tags  <code>property</code>","text":"<pre><code>tags\n</code></pre> <p>Return the tags assigned to the node.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>Return the set of all assigned tags to the node.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Node.run","title":"run","text":"<pre><code>run(inputs=None)\n</code></pre> <p>Run this node using the provided inputs and return its results in a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>inputs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary of inputs as specified at the creation of the node.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>In the following cases: a) The node function inputs are incompatible with the node input definition. Example 1: node definition input is a list of 2 DataFrames, whereas only 1 was provided or 2 different ones were provided. b) The node function outputs are incompatible with the node output definition. Example 1: node function definition is a dictionary, whereas function returns a list. Example 2: node definition output is a list of 5 strings, whereas the function returns a list of 4 objects.</p> </li> <li> <code>Exception</code>             \u2013            <p>Any exception thrown during execution of the node.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>All produced node outputs are returned in a dictionary, where the</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>keys are defined by the node outputs.</p> </li> </ul> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def run(self, inputs: dict[str, Any] | None = None) -&gt; dict[str, Any]:\n    \"\"\"Run this node using the provided inputs and return its results\n    in a dictionary.\n\n    Args:\n        inputs: Dictionary of inputs as specified at the creation of\n            the node.\n\n    Raises:\n        ValueError: In the following cases:\n            a) The node function inputs are incompatible with the node\n            input definition.\n            Example 1: node definition input is a list of 2\n            DataFrames, whereas only 1 was provided or 2 different ones\n            were provided.\n            b) The node function outputs are incompatible with the node\n            output definition.\n            Example 1: node function definition is a dictionary,\n            whereas function returns a list.\n            Example 2: node definition output is a list of 5\n            strings, whereas the function returns a list of 4 objects.\n        Exception: Any exception thrown during execution of the node.\n\n    Returns:\n        All produced node outputs are returned in a dictionary, where the\n        keys are defined by the node outputs.\n\n    \"\"\"\n    self._logger.info(\"Running node: %s\", str(self))\n\n    outputs = None\n\n    if not (inputs is None or isinstance(inputs, dict)):\n        raise ValueError(\n            f\"Node.run() expects a dictionary or None, \"\n            f\"but got {type(inputs)} instead\"\n        )\n\n    try:\n        inputs = {} if inputs is None else inputs\n        if not self._inputs:\n            outputs = self._run_with_no_inputs(inputs)\n        elif isinstance(self._inputs, str):\n            outputs = self._run_with_one_input(inputs, self._inputs)\n        elif isinstance(self._inputs, list):\n            outputs = self._run_with_list(inputs, self._inputs)\n        elif isinstance(self._inputs, dict):\n            outputs = self._run_with_dict(inputs, self._inputs)\n\n        return self._outputs_to_dictionary(outputs)\n\n    # purposely catch all exceptions\n    except Exception as exc:\n        self._logger.error(\n            \"Node %s failed with error: \\n%s\",\n            str(self),\n            str(exc),\n            extra={\"markup\": True},\n        )\n        raise exc\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Node.tag","title":"tag","text":"<pre><code>tag(tags)\n</code></pre> <p>Create a new <code>Node</code> which is an exact copy of the current one,     but with more tags added to it.</p> <p>Parameters:</p> <ul> <li> <code>tags</code>               (<code>str | Iterable[str]</code>)           \u2013            <p>The tags to be added to the new node.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Node</code>           \u2013            <p>A copy of the current <code>Node</code> object with the tags added.</p> </li> </ul> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def tag(self, tags: str | Iterable[str]) -&gt; Node:\n    \"\"\"Create a new ``Node`` which is an exact copy of the current one,\n        but with more tags added to it.\n\n    Args:\n        tags: The tags to be added to the new node.\n\n    Returns:\n        A copy of the current ``Node`` object with the tags added.\n\n    \"\"\"\n    return self._copy(tags=self.tags | set(_to_list(tags)))\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.NodeSpecs","title":"NodeSpecs","text":"<p>Namespace that defines all specifications for a node's lifecycle hooks.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.NodeSpecs.after_node_run","title":"after_node_run","text":"<pre><code>after_node_run(node, catalog, inputs, outputs, is_async, session_id)\n</code></pre> <p>Hook to be invoked after a node runs. The arguments received are the same as those used by <code>kedro.runner.run_node</code> as well as the <code>outputs</code> of the node run.</p> <p>Parameters:</p> <ul> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> that ran.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> containing the node's inputs and outputs.</p> </li> <li> <code>inputs</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The dictionary of inputs dataset. The keys are dataset names and the values are the actual loaded input data, not the dataset instance.</p> </li> <li> <code>outputs</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The dictionary of outputs dataset. The keys are dataset names and the values are the actual computed output data, not the dataset instance.</p> </li> <li> <code>is_async</code>               (<code>bool</code>)           \u2013            <p>Whether the node was run in <code>async</code> mode.</p> </li> <li> <code>session_id</code>               (<code>str</code>)           \u2013            <p>The id of the session.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef after_node_run(  # noqa: PLR0913\n    self,\n    node: Node,\n    catalog: CatalogProtocol,\n    inputs: dict[str, Any],\n    outputs: dict[str, Any],\n    is_async: bool,\n    session_id: str,\n) -&gt; None:\n    \"\"\"Hook to be invoked after a node runs.\n    The arguments received are the same as those used by ``kedro.runner.run_node``\n    as well as the ``outputs`` of the node run.\n\n    Args:\n        node: The ``Node`` that ran.\n        catalog: An implemented instance of ``CatalogProtocol`` containing the node's inputs and outputs.\n        inputs: The dictionary of inputs dataset.\n            The keys are dataset names and the values are the actual loaded input data,\n            not the dataset instance.\n        outputs: The dictionary of outputs dataset.\n            The keys are dataset names and the values are the actual computed output data,\n            not the dataset instance.\n        is_async: Whether the node was run in ``async`` mode.\n        session_id: The id of the session.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.NodeSpecs.before_node_run","title":"before_node_run","text":"<pre><code>before_node_run(node, catalog, inputs, is_async, session_id)\n</code></pre> <p>Hook to be invoked before a node runs. The arguments received are the same as those used by <code>kedro.runner.run_node</code></p> <p>Parameters:</p> <ul> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> containing the node's inputs and outputs.</p> </li> <li> <code>inputs</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The dictionary of inputs dataset. The keys are dataset names and the values are the actual loaded input data, not the dataset instance.</p> </li> <li> <code>is_async</code>               (<code>bool</code>)           \u2013            <p>Whether the node was run in <code>async</code> mode.</p> </li> <li> <code>session_id</code>               (<code>str</code>)           \u2013            <p>The id of the session.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any] | None</code>           \u2013            <p>Either None or a dictionary mapping dataset name(s) to new value(s). If returned, this dictionary will be used to update the node inputs, which allows to overwrite the node inputs.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef before_node_run(\n    self,\n    node: Node,\n    catalog: CatalogProtocol,\n    inputs: dict[str, Any],\n    is_async: bool,\n    session_id: str,\n) -&gt; dict[str, Any] | None:\n    \"\"\"Hook to be invoked before a node runs.\n    The arguments received are the same as those used by ``kedro.runner.run_node``\n\n    Args:\n        node: The ``Node`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` containing the node's inputs and outputs.\n        inputs: The dictionary of inputs dataset.\n            The keys are dataset names and the values are the actual loaded input data,\n            not the dataset instance.\n        is_async: Whether the node was run in ``async`` mode.\n        session_id: The id of the session.\n\n    Returns:\n        Either None or a dictionary mapping dataset name(s) to new value(s).\n            If returned, this dictionary will be used to update the node inputs,\n            which allows to overwrite the node inputs.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.NodeSpecs.on_node_error","title":"on_node_error","text":"<pre><code>on_node_error(error, node, catalog, inputs, is_async, session_id)\n</code></pre> <p>Hook to be invoked if a node run throws an uncaught error. The signature of this error hook should match the signature of <code>before_node_run</code> along with the error that was raised.</p> <p>Parameters:</p> <ul> <li> <code>error</code>               (<code>Exception</code>)           \u2013            <p>The uncaught exception thrown during the node run.</p> </li> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> containing the node's inputs and outputs.</p> </li> <li> <code>inputs</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The dictionary of inputs dataset. The keys are dataset names and the values are the actual loaded input data, not the dataset instance.</p> </li> <li> <code>is_async</code>               (<code>bool</code>)           \u2013            <p>Whether the node was run in <code>async</code> mode.</p> </li> <li> <code>session_id</code>               (<code>str</code>)           \u2013            <p>The id of the session.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef on_node_error(  # noqa: PLR0913\n    self,\n    error: Exception,\n    node: Node,\n    catalog: CatalogProtocol,\n    inputs: dict[str, Any],\n    is_async: bool,\n    session_id: str,\n) -&gt; None:\n    \"\"\"Hook to be invoked if a node run throws an uncaught error.\n    The signature of this error hook should match the signature of ``before_node_run``\n    along with the error that was raised.\n\n    Args:\n        error: The uncaught exception thrown during the node run.\n        node: The ``Node`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` containing the node's inputs and outputs.\n        inputs: The dictionary of inputs dataset.\n            The keys are dataset names and the values are the actual loaded input data,\n            not the dataset instance.\n        is_async: Whether the node was run in ``async`` mode.\n        session_id: The id of the session.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline","title":"Pipeline","text":"<pre><code>Pipeline(nodes, *, tags=None)\n</code></pre> <p>A <code>Pipeline</code> defined as a collection of <code>Node</code> objects. This class treats nodes as part of a graph representation and provides inputs, outputs and execution order.</p> <p>Parameters:</p> <ul> <li> <code>nodes</code>               (<code>Iterable[Node | Pipeline]</code>)           \u2013            <p>The iterable of nodes the <code>Pipeline</code> will be made of. If you provide pipelines among the list of nodes, those pipelines will be expanded and all their nodes will become part of this new pipeline.</p> </li> <li> <code>tags</code>               (<code>str | Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional set of tags to be applied to all the pipeline nodes.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When an empty list of nodes is provided, or when not all nodes have unique names.</p> </li> <li> <code>CircularDependencyError</code>             \u2013            <p>When visiting all the nodes is not possible due to the existence of a circular dependency.</p> </li> <li> <code>OutputNotUniqueError</code>             \u2013            <p>When multiple <code>Node</code> instances produce the same output.</p> </li> <li> <code>ConfirmNotUniqueError</code>             \u2013            <p>When multiple <code>Node</code> instances attempt to confirm the same dataset.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro.pipeline import Pipeline\n&gt;&gt;&gt; from kedro.pipeline import node\n&gt;&gt;&gt;\n&gt;&gt;&gt; # In the following scenario first_ds and second_ds\n&gt;&gt;&gt; # are datasets provided by io. Pipeline will pass these\n&gt;&gt;&gt; # datasets to first_node function and provides the result\n&gt;&gt;&gt; # to the second_node as input.\n&gt;&gt;&gt;\n&gt;&gt;&gt; def first_node(first_ds, second_ds):\n&gt;&gt;&gt;     return dict(third_ds=first_ds+second_ds)\n&gt;&gt;&gt;\n&gt;&gt;&gt; def second_node(third_ds):\n&gt;&gt;&gt;     return third_ds\n&gt;&gt;&gt;\n&gt;&gt;&gt; pipeline = Pipeline([\n&gt;&gt;&gt;     node(first_node, ['first_ds', 'second_ds'], ['third_ds']),\n&gt;&gt;&gt;     node(second_node, dict(third_ds='third_ds'), 'fourth_ds')])\n&gt;&gt;&gt;\n&gt;&gt;&gt; pipeline.describe()\n&gt;&gt;&gt;\n</code></pre> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def __init__(\n    self,\n    nodes: Iterable[Node | Pipeline],\n    *,\n    tags: str | Iterable[str] | None = None,\n):\n    \"\"\"Initialise ``Pipeline`` with a list of ``Node`` instances.\n\n    Args:\n        nodes: The iterable of nodes the ``Pipeline`` will be made of. If you\n            provide pipelines among the list of nodes, those pipelines will\n            be expanded and all their nodes will become part of this\n            new pipeline.\n        tags: Optional set of tags to be applied to all the pipeline nodes.\n\n    Raises:\n        ValueError:\n            When an empty list of nodes is provided, or when not all\n            nodes have unique names.\n        CircularDependencyError:\n            When visiting all the nodes is not\n            possible due to the existence of a circular dependency.\n        OutputNotUniqueError:\n            When multiple ``Node`` instances produce the same output.\n        ConfirmNotUniqueError:\n            When multiple ``Node`` instances attempt to confirm the same\n            dataset.\n    Example:\n    ::\n\n        &gt;&gt;&gt; from kedro.pipeline import Pipeline\n        &gt;&gt;&gt; from kedro.pipeline import node\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # In the following scenario first_ds and second_ds\n        &gt;&gt;&gt; # are datasets provided by io. Pipeline will pass these\n        &gt;&gt;&gt; # datasets to first_node function and provides the result\n        &gt;&gt;&gt; # to the second_node as input.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; def first_node(first_ds, second_ds):\n        &gt;&gt;&gt;     return dict(third_ds=first_ds+second_ds)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; def second_node(third_ds):\n        &gt;&gt;&gt;     return third_ds\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; pipeline = Pipeline([\n        &gt;&gt;&gt;     node(first_node, ['first_ds', 'second_ds'], ['third_ds']),\n        &gt;&gt;&gt;     node(second_node, dict(third_ds='third_ds'), 'fourth_ds')])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; pipeline.describe()\n        &gt;&gt;&gt;\n\n    \"\"\"\n    if nodes is None:\n        raise ValueError(\n            \"'nodes' argument of 'Pipeline' is None. It must be an \"\n            \"iterable of nodes and/or pipelines instead.\"\n        )\n    nodes_list = list(nodes)  # in case it's a generator\n    _validate_duplicate_nodes(nodes_list)\n\n    nodes_chain = list(\n        chain.from_iterable(\n            [[n] if isinstance(n, Node) else n.nodes for n in nodes_list]\n        )\n    )\n    _validate_transcoded_inputs_outputs(nodes_chain)\n    _tags = set(_to_list(tags))\n\n    if _tags:\n        tagged_nodes = [n.tag(_tags) for n in nodes_chain]\n    else:\n        tagged_nodes = nodes_chain\n\n    self._nodes_by_name = {node.name: node for node in tagged_nodes}\n    _validate_unique_outputs(tagged_nodes)\n    _validate_unique_confirms(tagged_nodes)\n\n    # input -&gt; nodes with input\n    self._nodes_by_input: dict[str, set[Node]] = defaultdict(set)\n    for node in tagged_nodes:\n        for input_ in node.inputs:\n            self._nodes_by_input[_strip_transcoding(input_)].add(node)\n\n    # output -&gt; node with output\n    self._nodes_by_output: dict[str, Node] = {}\n    for node in tagged_nodes:\n        for output in node.outputs:\n            self._nodes_by_output[_strip_transcoding(output)] = node\n\n    self._nodes = tagged_nodes\n    self._toposorter = TopologicalSorter(self.node_dependencies)\n\n    # test for circular dependencies without executing the toposort for efficiency\n    try:\n        self._toposorter.prepare()\n    except CycleError as exc:\n        loop = list(set(exc.args[1]))\n        message = f\"Circular dependencies exist among the following {len(loop)} item(s): {loop}\"\n        raise CircularDependencyError(message) from exc\n\n    self._toposorted_nodes: list[Node] = []\n    self._toposorted_groups: list[list[Node]] = []\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.grouped_nodes","title":"grouped_nodes  <code>property</code>","text":"<pre><code>grouped_nodes\n</code></pre> <p>Return a list of the pipeline nodes in topologically ordered groups, i.e. if node A needs to be run before node B, it will appear in an earlier group.</p> <p>Returns:</p> <ul> <li> <code>list[list[Node]]</code>           \u2013            <p>The pipeline nodes in topologically ordered groups.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.grouped_nodes_by_namespace","title":"grouped_nodes_by_namespace  <code>property</code>","text":"<pre><code>grouped_nodes_by_namespace\n</code></pre> <p>Return a dictionary of the pipeline nodes grouped by top-level namespace with information about the nodes, their type, and dependencies. The structure of the dictionary is: {'node_name/namespace_name' : {'name': 'node_name/namespace_name','type': 'namespace' or 'node','nodes': [list of nodes],'dependencies': [list of dependencies]}} This property is intended to be used by deployment plugins to group nodes by namespace.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.node_dependencies","title":"node_dependencies  <code>property</code>","text":"<pre><code>node_dependencies\n</code></pre> <p>All dependencies of nodes where the first Node has a direct dependency on the second Node.</p> <p>Returns:</p> <ul> <li> <code>dict[Node, set[Node]]</code>           \u2013            <p>Dictionary where keys are nodes and values are sets made up of</p> </li> <li> <code>dict[Node, set[Node]]</code>           \u2013            <p>their parent nodes. Independent nodes have this as empty sets.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.nodes","title":"nodes  <code>property</code>","text":"<pre><code>nodes\n</code></pre> <p>Return a list of the pipeline nodes in topological order, i.e. if node A needs to be run before node B, it will appear earlier in the list.</p> <p>Returns:</p> <ul> <li> <code>list[Node]</code>           \u2013            <p>The list of all pipeline nodes in topological order.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Pipeline ([node1, ..., node10 ...], name='pipeline_name')</p> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def __repr__(self) -&gt; str:  # pragma: no cover\n    \"\"\"Pipeline ([node1, ..., node10 ...], name='pipeline_name')\"\"\"\n    max_nodes_to_display = 10\n\n    nodes_reprs = [repr(node) for node in self.nodes[:max_nodes_to_display]]\n    if len(self.nodes) &gt; max_nodes_to_display:\n        nodes_reprs.append(\"...\")\n    sep = \",\\n\"\n    nodes_reprs_str = f\"[\\n{sep.join(nodes_reprs)}\\n]\" if nodes_reprs else \"[]\"\n    constructor_repr = f\"({nodes_reprs_str})\"\n    return f\"{self.__class__.__name__}{constructor_repr}\"\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.all_inputs","title":"all_inputs","text":"<pre><code>all_inputs()\n</code></pre> <p>All inputs for all nodes in the pipeline.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>All node input names as a Set.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def all_inputs(self) -&gt; set[str]:\n    \"\"\"All inputs for all nodes in the pipeline.\n\n    Returns:\n        All node input names as a Set.\n\n    \"\"\"\n    return set.union(set(), *(node.inputs for node in self._nodes))\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.all_outputs","title":"all_outputs","text":"<pre><code>all_outputs()\n</code></pre> <p>All outputs of all nodes in the pipeline.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>All node outputs.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def all_outputs(self) -&gt; set[str]:\n    \"\"\"All outputs of all nodes in the pipeline.\n\n    Returns:\n        All node outputs.\n\n    \"\"\"\n    return set.union(set(), *(node.outputs for node in self._nodes))\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.datasets","title":"datasets","text":"<pre><code>datasets()\n</code></pre> <p>The names of all datasets used by the <code>Pipeline</code>, including inputs and outputs.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>The set of all pipeline datasets.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def datasets(self) -&gt; set[str]:\n    \"\"\"The names of all datasets used by the ``Pipeline``,\n    including inputs and outputs.\n\n    Returns:\n        The set of all pipeline datasets.\n\n    \"\"\"\n    return self.all_outputs() | self.all_inputs()\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.describe","title":"describe","text":"<pre><code>describe(names_only=True)\n</code></pre> <p>Obtain the order of execution and expected free input variables in a loggable pre-formatted string. The order of nodes matches the order of execution given by the topological sort.</p> <p>Parameters:</p> <ul> <li> <code>names_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>The flag to describe names_only pipeline with just node names.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; pipeline = Pipeline([ ... ])\n&gt;&gt;&gt;\n&gt;&gt;&gt; logger = logging.getLogger(__name__)\n&gt;&gt;&gt;\n&gt;&gt;&gt; logger.info(pipeline.describe())\n</code></pre> <p>After invocation the following will be printed as an info level log statement: ::</p> <pre><code>#### Pipeline execution order ####\nInputs: C, D\n\nfunc1([C]) -&gt; [A]\nfunc2([D]) -&gt; [B]\nfunc3([A, D]) -&gt; [E]\n\nOutputs: B, E\n##################################\n</code></pre> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The pipeline description as a formatted string.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def describe(self, names_only: bool = True) -&gt; str:\n    \"\"\"Obtain the order of execution and expected free input variables in\n    a loggable pre-formatted string. The order of nodes matches the order\n    of execution given by the topological sort.\n\n    Args:\n        names_only: The flag to describe names_only pipeline with just\n            node names.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; pipeline = Pipeline([ ... ])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; logger = logging.getLogger(__name__)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; logger.info(pipeline.describe())\n\n    After invocation the following will be printed as an info level log\n    statement:\n    ::\n\n        #### Pipeline execution order ####\n        Inputs: C, D\n\n        func1([C]) -&gt; [A]\n        func2([D]) -&gt; [B]\n        func3([A, D]) -&gt; [E]\n\n        Outputs: B, E\n        ##################################\n\n    Returns:\n        The pipeline description as a formatted string.\n\n    \"\"\"\n\n    def set_to_string(set_of_strings: set[str]) -&gt; str:\n        \"\"\"Convert set to a string but return 'None' in case of an empty\n        set.\n        \"\"\"\n        return \", \".join(sorted(set_of_strings)) if set_of_strings else \"None\"\n\n    nodes_as_string = \"\\n\".join(\n        node.name if names_only else str(node) for node in self.nodes\n    )\n\n    str_representation = (\n        \"#### Pipeline execution order ####\\n\"\n        \"Inputs: {0}\\n\\n\"\n        \"{1}\\n\\n\"\n        \"Outputs: {2}\\n\"\n        \"##################################\"\n    )\n\n    return str_representation.format(\n        set_to_string(self.inputs()), nodes_as_string, set_to_string(self.outputs())\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.filter","title":"filter","text":"<pre><code>filter(tags=None, from_nodes=None, to_nodes=None, node_names=None, from_inputs=None, to_outputs=None, node_namespace=None)\n</code></pre> <p>Creates a new <code>Pipeline</code> object with the nodes that meet all of the specified filtering conditions.</p> <p>The new pipeline object is the intersection of pipelines that meet each filtering condition. This is distinct from chaining multiple filters together.</p> <p>Parameters:</p> <ul> <li> <code>tags</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of node tags which should be used to lookup the nodes of the new <code>Pipeline</code>.</p> </li> <li> <code>from_nodes</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of node names which should be used as a starting point of the new <code>Pipeline</code>.</p> </li> <li> <code>to_nodes</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of node names which should be used as an end point of the new <code>Pipeline</code>.</p> </li> <li> <code>node_names</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of node names which should be selected for the new <code>Pipeline</code>.</p> </li> <li> <code>from_inputs</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of inputs which should be used as a starting point of the new <code>Pipeline</code></p> </li> <li> <code>to_outputs</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of outputs which should be the final outputs of the new <code>Pipeline</code>.</p> </li> <li> <code>node_namespace</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>One node namespace which should be used to select nodes in the new <code>Pipeline</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> object with nodes that meet all of the specified filtering conditions.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>The filtered <code>Pipeline</code> has no nodes.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; pipeline = Pipeline(\n&gt;&gt;&gt;     [\n&gt;&gt;&gt;         node(func, \"A\", \"B\", name=\"node1\"),\n&gt;&gt;&gt;         node(func, \"B\", \"C\", name=\"node2\"),\n&gt;&gt;&gt;         node(func, \"C\", \"D\", name=\"node3\"),\n&gt;&gt;&gt;     ]\n&gt;&gt;&gt; )\n&gt;&gt;&gt; pipeline.filter(node_names=[\"node1\", \"node3\"], from_inputs=[\"A\"])\n&gt;&gt;&gt; # Gives a new pipeline object containing node1 and node3.\n</code></pre> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def filter(  # noqa: PLR0913\n    self,\n    tags: Iterable[str] | None = None,\n    from_nodes: Iterable[str] | None = None,\n    to_nodes: Iterable[str] | None = None,\n    node_names: Iterable[str] | None = None,\n    from_inputs: Iterable[str] | None = None,\n    to_outputs: Iterable[str] | None = None,\n    node_namespace: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Creates a new ``Pipeline`` object with the nodes that meet all of the\n    specified filtering conditions.\n\n    The new pipeline object is the intersection of pipelines that meet each\n    filtering condition. This is distinct from chaining multiple filters together.\n\n    Args:\n        tags: A list of node tags which should be used to lookup\n            the nodes of the new ``Pipeline``.\n        from_nodes: A list of node names which should be used as a\n            starting point of the new ``Pipeline``.\n        to_nodes:  A list of node names which should be used as an\n            end point of the new ``Pipeline``.\n        node_names: A list of node names which should be selected for the\n            new ``Pipeline``.\n        from_inputs: A list of inputs which should be used as a starting point\n            of the new ``Pipeline``\n        to_outputs: A list of outputs which should be the final outputs of\n            the new ``Pipeline``.\n        node_namespace: One node namespace which should be used to select\n            nodes in the new ``Pipeline``.\n\n    Returns:\n        A new ``Pipeline`` object with nodes that meet all of the specified\n            filtering conditions.\n\n    Raises:\n        ValueError: The filtered ``Pipeline`` has no nodes.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; pipeline = Pipeline(\n        &gt;&gt;&gt;     [\n        &gt;&gt;&gt;         node(func, \"A\", \"B\", name=\"node1\"),\n        &gt;&gt;&gt;         node(func, \"B\", \"C\", name=\"node2\"),\n        &gt;&gt;&gt;         node(func, \"C\", \"D\", name=\"node3\"),\n        &gt;&gt;&gt;     ]\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; pipeline.filter(node_names=[\"node1\", \"node3\"], from_inputs=[\"A\"])\n        &gt;&gt;&gt; # Gives a new pipeline object containing node1 and node3.\n    \"\"\"\n    # Use [node_namespace] so only_nodes_with_namespace can follow the same\n    # *filter_args pattern as the other filtering methods, which all take iterables.\n    node_namespace_iterable = [node_namespace] if node_namespace else None\n\n    filter_methods = {\n        self.only_nodes_with_tags: tags,\n        self.from_nodes: from_nodes,\n        self.to_nodes: to_nodes,\n        self.only_nodes: node_names,\n        self.from_inputs: from_inputs,\n        self.to_outputs: to_outputs,\n        self.only_nodes_with_namespace: node_namespace_iterable,\n    }\n\n    subset_pipelines = {\n        filter_method(*filter_args)  # type: ignore\n        for filter_method, filter_args in filter_methods.items()\n        if filter_args\n    }\n\n    # Intersect all the pipelines subsets. We apply each filter to the original\n    # pipeline object (self) rather than incrementally chaining filter methods\n    # together. Hence the order of filtering does not affect the outcome, and the\n    # resultant pipeline is unambiguously defined.\n    # If this were not the case then, for example,\n    # pipeline.filter(node_names=[\"node1\", \"node3\"], from_inputs=[\"A\"])\n    # would give different outcomes depending on the order of filter methods:\n    # only_nodes and then from_inputs would give node1, while only_nodes and then\n    # from_inputs would give node1 and node3.\n    filtered_pipeline = Pipeline(self._nodes)\n    for subset_pipeline in subset_pipelines:\n        filtered_pipeline &amp;= subset_pipeline\n\n    if not filtered_pipeline.nodes:\n        raise ValueError(\n            \"Pipeline contains no nodes after applying all provided filters. \"\n            \"Please ensure that at least one pipeline with nodes has been defined.\"\n        )\n    return filtered_pipeline\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.from_inputs","title":"from_inputs","text":"<pre><code>from_inputs(*inputs)\n</code></pre> <p>Create a new <code>Pipeline</code> object with the nodes which depend directly or transitively on the provided inputs. If provided a name, but no format, for a transcoded input, it includes all the nodes that use inputs with that name, otherwise it matches to the fully-qualified name only (i.e. name@format).</p> <p>Parameters:</p> <ul> <li> <code>*inputs</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of inputs which should be used as a starting point of the new <code>Pipeline</code></p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised when any of the given inputs do not exist in the <code>Pipeline</code> object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> object, containing a subset of the nodes of the current one such that only nodes depending directly or transitively on the provided inputs are being copied.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def from_inputs(self, *inputs: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` object with the nodes which depend\n    directly or transitively on the provided inputs.\n    If provided a name, but no format, for a transcoded input, it\n    includes all the nodes that use inputs with that name, otherwise it\n    matches to the fully-qualified name only (i.e. name@format).\n\n    Args:\n        *inputs: A list of inputs which should be used as a starting point\n            of the new ``Pipeline``\n\n    Raises:\n        ValueError: Raised when any of the given inputs do not exist in the\n            ``Pipeline`` object.\n\n    Returns:\n        A new ``Pipeline`` object, containing a subset of the\n            nodes of the current one such that only nodes depending\n            directly or transitively on the provided inputs are being\n            copied.\n\n    \"\"\"\n    starting = set(inputs)\n    result: set[Node] = set()\n    next_nodes = self._get_nodes_with_inputs_transcode_compatible(starting)\n\n    while next_nodes:\n        result |= next_nodes\n        outputs = set(chain.from_iterable(node.outputs for node in next_nodes))\n        starting = outputs\n\n        next_nodes = set(\n            chain.from_iterable(\n                self._nodes_by_input[_strip_transcoding(input_)]\n                for input_ in starting\n            )\n        )\n\n    return Pipeline(result)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.from_nodes","title":"from_nodes","text":"<pre><code>from_nodes(*node_names)\n</code></pre> <p>Create a new <code>Pipeline</code> object with the nodes which depend directly or transitively on the provided nodes.</p> <p>Parameters:</p> <ul> <li> <code>*node_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of node_names which should be used as a starting point of the new <code>Pipeline</code>.</p> </li> </ul> <p>Raises:     ValueError: Raised when any of the given names do not exist in the         <code>Pipeline</code> object. Returns:     A new <code>Pipeline</code> object, containing a subset of the nodes of         the current one such that only nodes depending directly or         transitively on the provided nodes are being copied.</p> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def from_nodes(self, *node_names: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` object with the nodes which depend\n    directly or transitively on the provided nodes.\n\n    Args:\n        *node_names: A list of node_names which should be used as a\n            starting point of the new ``Pipeline``.\n    Raises:\n        ValueError: Raised when any of the given names do not exist in the\n            ``Pipeline`` object.\n    Returns:\n        A new ``Pipeline`` object, containing a subset of the nodes of\n            the current one such that only nodes depending directly or\n            transitively on the provided nodes are being copied.\n\n    \"\"\"\n\n    res = self.only_nodes(*node_names)\n    res += self.from_inputs(*map(_strip_transcoding, res.all_outputs()))\n    return res\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.inputs","title":"inputs","text":"<pre><code>inputs()\n</code></pre> <p>The names of free inputs that must be provided at runtime so that the pipeline is runnable. Does not include intermediate inputs which are produced and consumed by the inner pipeline nodes. Resolves transcoded names where necessary.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>The set of free input names needed by the pipeline.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def inputs(self) -&gt; set[str]:\n    \"\"\"The names of free inputs that must be provided at runtime so that\n    the pipeline is runnable. Does not include intermediate inputs which\n    are produced and consumed by the inner pipeline nodes. Resolves\n    transcoded names where necessary.\n\n    Returns:\n        The set of free input names needed by the pipeline.\n\n    \"\"\"\n    return self._remove_intermediates(self.all_inputs())\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.only_nodes","title":"only_nodes","text":"<pre><code>only_nodes(*node_names)\n</code></pre> <p>Create a new <code>Pipeline</code> which will contain only the specified nodes by name.</p> <p>Parameters:</p> <ul> <li> <code>*node_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>One or more node names. The returned <code>Pipeline</code> will only contain these nodes.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When some invalid node name is given.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code>, containing only <code>nodes</code>.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def only_nodes(self, *node_names: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` which will contain only the specified\n    nodes by name.\n\n    Args:\n        *node_names: One or more node names. The returned ``Pipeline``\n            will only contain these nodes.\n\n    Raises:\n        ValueError: When some invalid node name is given.\n\n    Returns:\n        A new ``Pipeline``, containing only ``nodes``.\n\n    \"\"\"\n    unregistered_nodes = set(node_names) - set(self._nodes_by_name.keys())\n    if unregistered_nodes:\n        # check if unregistered nodes are available under namespace\n        namespaces = []\n        for unregistered_node in unregistered_nodes:\n            namespaces.extend(\n                [\n                    node_name\n                    for node_name in self._nodes_by_name.keys()\n                    if node_name.endswith(f\".{unregistered_node}\")\n                ]\n            )\n        if namespaces:\n            raise ValueError(\n                f\"Pipeline does not contain nodes named {list(unregistered_nodes)}. \"\n                f\"Did you mean: {namespaces}?\"\n            )\n        raise ValueError(\n            f\"Pipeline does not contain nodes named {list(unregistered_nodes)}.\"\n        )\n\n    nodes = [self._nodes_by_name[name] for name in node_names]\n    return Pipeline(nodes)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.only_nodes_with_inputs","title":"only_nodes_with_inputs","text":"<pre><code>only_nodes_with_inputs(*inputs)\n</code></pre> <p>Create a new <code>Pipeline</code> object with the nodes which depend directly on the provided inputs. If provided a name, but no format, for a transcoded input, it includes all the nodes that use inputs with that name, otherwise it matches to the fully-qualified name only (i.e. name@format).</p> <p>Parameters:</p> <ul> <li> <code>*inputs</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of inputs which should be used as a starting point of the new <code>Pipeline</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised when any of the given inputs do not exist in the <code>Pipeline</code> object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> object, containing a subset of the nodes of the current one such that only nodes depending directly on the provided inputs are being copied.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def only_nodes_with_inputs(self, *inputs: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` object with the nodes which depend\n    directly on the provided inputs.\n    If provided a name, but no format, for a transcoded input, it\n    includes all the nodes that use inputs with that name, otherwise it\n    matches to the fully-qualified name only (i.e. name@format).\n\n    Args:\n        *inputs: A list of inputs which should be used as a starting\n            point of the new ``Pipeline``.\n\n    Raises:\n        ValueError: Raised when any of the given inputs do not exist in the\n            ``Pipeline`` object.\n\n    Returns:\n        A new ``Pipeline`` object, containing a subset of the\n            nodes of the current one such that only nodes depending\n            directly on the provided inputs are being copied.\n\n    \"\"\"\n    starting = set(inputs)\n    nodes = self._get_nodes_with_inputs_transcode_compatible(starting)\n\n    return Pipeline(nodes)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.only_nodes_with_namespace","title":"only_nodes_with_namespace","text":"<pre><code>only_nodes_with_namespace(node_namespace)\n</code></pre> <p>Creates a new <code>Pipeline</code> containing only nodes with the specified namespace.</p> <p>Parameters:</p> <ul> <li> <code>node_namespace</code>               (<code>str</code>)           \u2013            <p>One node namespace.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When pipeline contains no nodes with the specified namespace.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> containing nodes with the specified namespace.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def only_nodes_with_namespace(self, node_namespace: str) -&gt; Pipeline:\n    \"\"\"Creates a new ``Pipeline`` containing only nodes with the specified\n    namespace.\n\n    Args:\n        node_namespace: One node namespace.\n\n    Raises:\n        ValueError: When pipeline contains no nodes with the specified namespace.\n\n    Returns:\n        A new ``Pipeline`` containing nodes with the specified namespace.\n    \"\"\"\n    nodes = [\n        n\n        for n in self._nodes\n        if n.namespace and n.namespace.startswith(node_namespace)\n    ]\n    if not nodes:\n        raise ValueError(\n            f\"Pipeline does not contain nodes with namespace '{node_namespace}'\"\n        )\n    return Pipeline(nodes)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.only_nodes_with_outputs","title":"only_nodes_with_outputs","text":"<pre><code>only_nodes_with_outputs(*outputs)\n</code></pre> <p>Create a new <code>Pipeline</code> object with the nodes which are directly required to produce the provided outputs. If provided a name, but no format, for a transcoded dataset, it includes all the nodes that output to that name, otherwise it matches to the fully-qualified name only (i.e. name@format).</p> <p>Parameters:</p> <ul> <li> <code>*outputs</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of outputs which should be the final outputs of the new <code>Pipeline</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised when any of the given outputs do not exist in the <code>Pipeline</code> object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> object, containing a subset of the nodes of the</p> </li> <li> <code>Pipeline</code>           \u2013            <p>current one such that only nodes which are directly required to</p> </li> <li> <code>Pipeline</code>           \u2013            <p>produce the provided outputs are being copied.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def only_nodes_with_outputs(self, *outputs: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` object with the nodes which are directly\n    required to produce the provided outputs.\n    If provided a name, but no format, for a transcoded dataset, it\n    includes all the nodes that output to that name, otherwise it matches\n    to the fully-qualified name only (i.e. name@format).\n\n    Args:\n        *outputs: A list of outputs which should be the final outputs\n            of the new ``Pipeline``.\n\n    Raises:\n        ValueError: Raised when any of the given outputs do not exist in the\n            ``Pipeline`` object.\n\n    Returns:\n        A new ``Pipeline`` object, containing a subset of the nodes of the\n        current one such that only nodes which are directly required to\n        produce the provided outputs are being copied.\n    \"\"\"\n    starting = set(outputs)\n    nodes = self._get_nodes_with_outputs_transcode_compatible(starting)\n\n    return Pipeline(nodes)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.only_nodes_with_tags","title":"only_nodes_with_tags","text":"<pre><code>only_nodes_with_tags(*tags)\n</code></pre> <p>Creates a new <code>Pipeline</code> object with the nodes which contain any of the provided tags. The resulting <code>Pipeline</code> is empty if no tags are provided.</p> <p>Parameters:</p> <ul> <li> <code>*tags</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of node tags which should be used to lookup the nodes of the new <code>Pipeline</code>.</p> </li> </ul> <p>Returns:     Pipeline: A new <code>Pipeline</code> object, containing a subset of the         nodes of the current one such that only nodes containing any         of the tags provided are being copied.</p> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def only_nodes_with_tags(self, *tags: str) -&gt; Pipeline:\n    \"\"\"Creates a new ``Pipeline`` object with the nodes which contain *any*\n    of the provided tags. The resulting ``Pipeline`` is empty if no tags\n    are provided.\n\n    Args:\n        *tags: A list of node tags which should be used to lookup\n            the nodes of the new ``Pipeline``.\n    Returns:\n        Pipeline: A new ``Pipeline`` object, containing a subset of the\n            nodes of the current one such that only nodes containing *any*\n            of the tags provided are being copied.\n    \"\"\"\n    unique_tags = set(tags)\n    nodes = [node for node in self._nodes if unique_tags &amp; node.tags]\n    return Pipeline(nodes)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.outputs","title":"outputs","text":"<pre><code>outputs()\n</code></pre> <p>The names of outputs produced when the whole pipeline is run. Does not include intermediate outputs that are consumed by other pipeline nodes. Resolves transcoded names where necessary.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>The set of final pipeline outputs.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def outputs(self) -&gt; set[str]:\n    \"\"\"The names of outputs produced when the whole pipeline is run.\n    Does not include intermediate outputs that are consumed by\n    other pipeline nodes. Resolves transcoded names where necessary.\n\n    Returns:\n        The set of final pipeline outputs.\n\n    \"\"\"\n    return self._remove_intermediates(self.all_outputs())\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.tag","title":"tag","text":"<pre><code>tag(tags)\n</code></pre> <p>Tags all the nodes in the pipeline.</p> <p>Parameters:</p> <ul> <li> <code>tags</code>               (<code>str | Iterable[str]</code>)           \u2013            <p>The tags to be added to the nodes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>New <code>Pipeline</code> object with nodes tagged.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def tag(self, tags: str | Iterable[str]) -&gt; Pipeline:\n    \"\"\"Tags all the nodes in the pipeline.\n\n    Args:\n        tags: The tags to be added to the nodes.\n\n    Returns:\n        New ``Pipeline`` object with nodes tagged.\n    \"\"\"\n    nodes = [n.tag(tags) for n in self._nodes]\n    return Pipeline(nodes)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.to_json","title":"to_json","text":"<pre><code>to_json()\n</code></pre> <p>Return a json representation of the pipeline.</p> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Return a json representation of the pipeline.\"\"\"\n    transformed = [\n        {\n            \"name\": n.name,\n            \"inputs\": list(n.inputs),\n            \"outputs\": list(n.outputs),\n            \"tags\": list(n.tags),\n        }\n        for n in self._nodes\n    ]\n    pipeline_versioned = {\n        \"kedro_version\": kedro.__version__,\n        \"pipeline\": transformed,\n    }\n\n    return json.dumps(pipeline_versioned)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.to_nodes","title":"to_nodes","text":"<pre><code>to_nodes(*node_names)\n</code></pre> <p>Create a new <code>Pipeline</code> object with the nodes required directly or transitively by the provided nodes.</p> <p>Parameters:</p> <ul> <li> <code>*node_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of node_names which should be used as an end point of the new <code>Pipeline</code>.</p> </li> </ul> <p>Raises:     ValueError: Raised when any of the given names do not exist in the         <code>Pipeline</code> object. Returns:     A new <code>Pipeline</code> object, containing a subset of the nodes of the         current one such that only nodes required directly or         transitively by the provided nodes are being copied.</p> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def to_nodes(self, *node_names: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` object with the nodes required directly\n    or transitively by the provided nodes.\n\n    Args:\n        *node_names: A list of node_names which should be used as an\n            end point of the new ``Pipeline``.\n    Raises:\n        ValueError: Raised when any of the given names do not exist in the\n            ``Pipeline`` object.\n    Returns:\n        A new ``Pipeline`` object, containing a subset of the nodes of the\n            current one such that only nodes required directly or\n            transitively by the provided nodes are being copied.\n\n    \"\"\"\n\n    res = self.only_nodes(*node_names)\n    res += self.to_outputs(*map(_strip_transcoding, res.all_inputs()))\n    return res\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.Pipeline.to_outputs","title":"to_outputs","text":"<pre><code>to_outputs(*outputs)\n</code></pre> <p>Create a new <code>Pipeline</code> object with the nodes which are directly or transitively required to produce the provided outputs. If provided a name, but no format, for a transcoded dataset, it includes all the nodes that output to that name, otherwise it matches to the fully-qualified name only (i.e. name@format).</p> <p>Parameters:</p> <ul> <li> <code>*outputs</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of outputs which should be the final outputs of the new <code>Pipeline</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised when any of the given outputs do not exist in the <code>Pipeline</code> object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> object, containing a subset of the nodes of the</p> </li> <li> <code>Pipeline</code>           \u2013            <p>current one such that only nodes which are directly or transitively</p> </li> <li> <code>Pipeline</code>           \u2013            <p>required to produce the provided outputs are being copied.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def to_outputs(self, *outputs: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` object with the nodes which are directly\n    or transitively required to produce the provided outputs.\n    If provided a name, but no format, for a transcoded dataset, it\n    includes all the nodes that output to that name, otherwise it matches\n    to the fully-qualified name only (i.e. name@format).\n\n    Args:\n        *outputs: A list of outputs which should be the final outputs of\n            the new ``Pipeline``.\n\n    Raises:\n        ValueError: Raised when any of the given outputs do not exist in the\n            ``Pipeline`` object.\n\n\n    Returns:\n        A new ``Pipeline`` object, containing a subset of the nodes of the\n        current one such that only nodes which are directly or transitively\n        required to produce the provided outputs are being copied.\n\n    \"\"\"\n    starting = set(outputs)\n    result: set[Node] = set()\n    next_nodes = self._get_nodes_with_outputs_transcode_compatible(starting)\n\n    while next_nodes:\n        result |= next_nodes\n        inputs = set(chain.from_iterable(node.inputs for node in next_nodes))\n        starting = inputs\n\n        next_nodes = {\n            self._nodes_by_output[_strip_transcoding(output)]\n            for output in starting\n            if _strip_transcoding(output) in self._nodes_by_output\n        }\n\n    return Pipeline(result)\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.PipelineSpecs","title":"PipelineSpecs","text":"<p>Namespace that defines all specifications for a pipeline's lifecycle hooks.</p>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.PipelineSpecs.after_pipeline_run","title":"after_pipeline_run","text":"<pre><code>after_pipeline_run(run_params, run_result, pipeline, catalog)\n</code></pre> <p>Hook to be invoked after a pipeline runs.</p> <p>Parameters:</p> <ul> <li> <code>run_params</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The params used to run the pipeline. Should have the following schema::</p> <p>{      \"session_id\": str      \"project_path\": str,      \"env\": str,      \"kedro_version\": str,      \"tags\": Optional[List[str]],      \"from_nodes\": Optional[List[str]],      \"to_nodes\": Optional[List[str]],      \"node_names\": Optional[List[str]],      \"from_inputs\": Optional[List[str]],      \"to_outputs\": Optional[List[str]],      \"load_versions\": Optional[List[str]],      \"extra_params\": Optional[Dict[str, Any]]      \"pipeline_name\": str,      \"namespace\": Optional[str],      \"runner\": str,    }</p> </li> <li> <code>run_result</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The output of <code>Pipeline</code> run.</p> </li> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> that was run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> used during the run.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef after_pipeline_run(\n    self,\n    run_params: dict[str, Any],\n    run_result: dict[str, Any],\n    pipeline: Pipeline,\n    catalog: CatalogProtocol,\n) -&gt; None:\n    \"\"\"Hook to be invoked after a pipeline runs.\n\n    Args:\n        run_params: The params used to run the pipeline.\n            Should have the following schema::\n\n               {\n                 \"session_id\": str\n                 \"project_path\": str,\n                 \"env\": str,\n                 \"kedro_version\": str,\n                 \"tags\": Optional[List[str]],\n                 \"from_nodes\": Optional[List[str]],\n                 \"to_nodes\": Optional[List[str]],\n                 \"node_names\": Optional[List[str]],\n                 \"from_inputs\": Optional[List[str]],\n                 \"to_outputs\": Optional[List[str]],\n                 \"load_versions\": Optional[List[str]],\n                 \"extra_params\": Optional[Dict[str, Any]]\n                 \"pipeline_name\": str,\n                 \"namespace\": Optional[str],\n                 \"runner\": str,\n               }\n\n        run_result: The output of ``Pipeline`` run.\n        pipeline: The ``Pipeline`` that was run.\n        catalog: An implemented instance of ``CatalogProtocol`` used during the run.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.PipelineSpecs.before_pipeline_run","title":"before_pipeline_run","text":"<pre><code>before_pipeline_run(run_params, pipeline, catalog)\n</code></pre> <p>Hook to be invoked before a pipeline runs.</p> <p>Parameters:</p> <ul> <li> <code>run_params</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The params used to run the pipeline. Should have the following schema::</p> <p>{      \"session_id\": str      \"project_path\": str,      \"env\": str,      \"kedro_version\": str,      \"tags\": Optional[List[str]],      \"from_nodes\": Optional[List[str]],      \"to_nodes\": Optional[List[str]],      \"node_names\": Optional[List[str]],      \"from_inputs\": Optional[List[str]],      \"to_outputs\": Optional[List[str]],      \"load_versions\": Optional[List[str]],      \"extra_params\": Optional[Dict[str, Any]]      \"pipeline_name\": str,      \"namespace\": Optional[str],      \"runner\": str,    }</p> </li> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> that will be run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> to be used during the run.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef before_pipeline_run(\n    self, run_params: dict[str, Any], pipeline: Pipeline, catalog: CatalogProtocol\n) -&gt; None:\n    \"\"\"Hook to be invoked before a pipeline runs.\n\n    Args:\n        run_params: The params used to run the pipeline.\n            Should have the following schema::\n\n               {\n                 \"session_id\": str\n                 \"project_path\": str,\n                 \"env\": str,\n                 \"kedro_version\": str,\n                 \"tags\": Optional[List[str]],\n                 \"from_nodes\": Optional[List[str]],\n                 \"to_nodes\": Optional[List[str]],\n                 \"node_names\": Optional[List[str]],\n                 \"from_inputs\": Optional[List[str]],\n                 \"to_outputs\": Optional[List[str]],\n                 \"load_versions\": Optional[List[str]],\n                 \"extra_params\": Optional[Dict[str, Any]]\n                 \"pipeline_name\": str,\n                 \"namespace\": Optional[str],\n                 \"runner\": str,\n               }\n\n        pipeline: The ``Pipeline`` that will be run.\n        catalog: An implemented instance of ``CatalogProtocol`` to be used during the run.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework.hooks/#kedro.framework.hooks.specs.PipelineSpecs.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(error, run_params, pipeline, catalog)\n</code></pre> <p>Hook to be invoked if a pipeline run throws an uncaught Exception. The signature of this error hook should match the signature of <code>before_pipeline_run</code> along with the error that was raised.</p> <p>Parameters:</p> <ul> <li> <code>error</code>               (<code>Exception</code>)           \u2013            <p>The uncaught exception thrown during the pipeline run.</p> </li> <li> <code>run_params</code>               (<code>dict[str, Any]</code>)           \u2013            <p>The params used to run the pipeline. Should have the following schema::</p> <p>{      \"session_id\": str      \"project_path\": str,      \"env\": str,      \"kedro_version\": str,      \"tags\": Optional[List[str]],      \"from_nodes\": Optional[List[str]],      \"to_nodes\": Optional[List[str]],      \"node_names\": Optional[List[str]],      \"from_inputs\": Optional[List[str]],      \"to_outputs\": Optional[List[str]],      \"load_versions\": Optional[List[str]],      \"extra_params\": Optional[Dict[str, Any]]      \"pipeline_name\": str,      \"namespace\": Optional[str],      \"runner\": str,    }</p> </li> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> that will was run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> used during the run.</p> </li> </ul> Source code in <code>kedro/framework/hooks/specs.py</code> <pre><code>@hook_spec\ndef on_pipeline_error(\n    self,\n    error: Exception,\n    run_params: dict[str, Any],\n    pipeline: Pipeline,\n    catalog: CatalogProtocol,\n) -&gt; None:\n    \"\"\"Hook to be invoked if a pipeline run throws an uncaught Exception.\n    The signature of this error hook should match the signature of ``before_pipeline_run``\n    along with the error that was raised.\n\n    Args:\n        error: The uncaught exception thrown during the pipeline run.\n        run_params: The params used to run the pipeline.\n            Should have the following schema::\n\n               {\n                 \"session_id\": str\n                 \"project_path\": str,\n                 \"env\": str,\n                 \"kedro_version\": str,\n                 \"tags\": Optional[List[str]],\n                 \"from_nodes\": Optional[List[str]],\n                 \"to_nodes\": Optional[List[str]],\n                 \"node_names\": Optional[List[str]],\n                 \"from_inputs\": Optional[List[str]],\n                 \"to_outputs\": Optional[List[str]],\n                 \"load_versions\": Optional[List[str]],\n                 \"extra_params\": Optional[Dict[str, Any]]\n                 \"pipeline_name\": str,\n                 \"namespace\": Optional[str],\n                 \"runner\": str,\n               }\n\n        pipeline: The ``Pipeline`` that will was run.\n        catalog: An implemented instance of ``CatalogProtocol`` used during the run.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/framework/kedro.framework/","title":"Overview","text":"Module Description <code>kedro.framework.cli</code> Implements commands available from Kedro's CLI. <code>kedro.framework.context</code> Provides functionality for loading Kedro project context. <code>kedro.framework.hooks</code> Provides primitives to use hooks to extend KedroContext's behavior. <code>kedro.framework.project</code> Provides utilities to configure a Kedro project and access its settings. <code>kedro.framework.session</code> Provides access to <code>KedroSession</code> responsible for project lifecycle. <code>kedro.framework.startup</code> Provides metadata for a Kedro project."},{"location":"api/framework/kedro.framework/#kedro.framework","title":"kedro.framework","text":"<p><code>kedro.framework</code> provides Kedro's framework components</p>"},{"location":"api/framework/kedro.framework.project/","title":"Project","text":"Function Description <code>configure_logging</code> Configure logging according to the <code>logging_config</code> dictionary. <code>configure_project</code> Configure a Kedro project by populating its settings with values defined in <code>settings.py</code> and <code>pipeline_registry.py</code>. <code>find_pipelines</code> Automatically find modular pipelines having a <code>create_pipeline</code> function. <code>validate_settings</code> Eagerly validate that the settings module is importable if it exists."},{"location":"api/framework/kedro.framework.project/#kedro.framework.project","title":"kedro.framework.project","text":"<p><code>kedro.framework.project</code> module provides utility to configure a Kedro project and access its settings.</p>"},{"location":"api/framework/kedro.framework.project/#kedro.framework.project.configure_logging","title":"kedro.framework.project.configure_logging","text":"<pre><code>configure_logging(logging_config)\n</code></pre> <p>Configure logging according to <code>logging_config</code> dictionary.</p> Source code in <code>kedro/framework/project/__init__.py</code> <pre><code>def configure_logging(logging_config: dict[str, Any]) -&gt; None:\n    \"\"\"Configure logging according to ``logging_config`` dictionary.\"\"\"\n    LOGGING.configure(logging_config)\n</code></pre>"},{"location":"api/framework/kedro.framework.project/#kedro.framework.project.configure_project","title":"kedro.framework.project.configure_project","text":"<pre><code>configure_project(package_name)\n</code></pre> <p>Configure a Kedro project by populating its settings with values defined in user's settings.py and pipeline_registry.py.</p> Source code in <code>kedro/framework/project/__init__.py</code> <pre><code>def configure_project(package_name: str) -&gt; None:\n    \"\"\"Configure a Kedro project by populating its settings with values\n    defined in user's settings.py and pipeline_registry.py.\n    \"\"\"\n    settings_module = f\"{package_name}.settings\"\n    settings.configure(settings_module)\n\n    pipelines_module = f\"{package_name}.pipeline_registry\"\n    pipelines.configure(pipelines_module)\n\n    # Once the project is successfully configured once, store PACKAGE_NAME as a\n    # global variable to make it easily accessible. This is used by validate_settings()\n    # below, and also by ParallelRunner on Windows, as package_name is required every\n    # time a new subprocess is spawned.\n    global PACKAGE_NAME  # noqa: PLW0603\n    PACKAGE_NAME = package_name\n\n    if PACKAGE_NAME:\n        LOGGING.set_project_logging(PACKAGE_NAME)\n</code></pre>"},{"location":"api/framework/kedro.framework.project/#kedro.framework.project.find_pipelines","title":"kedro.framework.project.find_pipelines","text":"<pre><code>find_pipelines(raise_errors=False)\n</code></pre> <p>Automatically find modular pipelines having a <code>create_pipeline</code> function. By default, projects created using Kedro 0.18.3 and higher call this function to autoregister pipelines upon creation/addition.</p> <p>Projects that require more fine-grained control can still define the pipeline registry without calling this function. Alternatively, they can modify the mapping generated by the <code>find_pipelines</code> function.</p> <p>For more information on the pipeline registry and autodiscovery, see https://docs.kedro.org/en/stable/nodes_and_pipelines/pipeline_registry.html</p> <p>Parameters:</p> <ul> <li> <code>raise_errors</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, raise an error upon failed discovery.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Pipeline]</code>           \u2013            <p>A generated mapping from pipeline names to <code>Pipeline</code> objects.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>When a module does not expose a <code>create_pipeline</code> function, the <code>create_pipeline</code> function does not return a <code>Pipeline</code> object, or if the module import fails up front. If <code>raise_errors</code> is <code>False</code>, see Warns section instead.</p> </li> </ul> <p>Warns:</p> <ul> <li> <code>UserWarning</code>             \u2013            <p>When a module does not expose a <code>create_pipeline</code> function, the <code>create_pipeline</code> function does not return a <code>Pipeline</code> object, or if the module import fails up front. If <code>raise_errors</code> is <code>True</code>, see Raises section instead.</p> </li> </ul> Source code in <code>kedro/framework/project/__init__.py</code> <pre><code>def find_pipelines(raise_errors: bool = False) -&gt; dict[str, Pipeline]:  # noqa: PLR0912\n    \"\"\"Automatically find modular pipelines having a ``create_pipeline``\n    function. By default, projects created using Kedro 0.18.3 and higher\n    call this function to autoregister pipelines upon creation/addition.\n\n    Projects that require more fine-grained control can still define the\n    pipeline registry without calling this function. Alternatively, they\n    can modify the mapping generated by the ``find_pipelines`` function.\n\n    For more information on the pipeline registry and autodiscovery, see\n    https://docs.kedro.org/en/stable/nodes_and_pipelines/pipeline_registry.html\n\n    Args:\n        raise_errors: If ``True``, raise an error upon failed discovery.\n\n    Returns:\n        A generated mapping from pipeline names to ``Pipeline`` objects.\n\n    Raises:\n        ImportError: When a module does not expose a ``create_pipeline``\n            function, the ``create_pipeline`` function does not return a\n            ``Pipeline`` object, or if the module import fails up front.\n            If ``raise_errors`` is ``False``, see Warns section instead.\n\n    Warns:\n        UserWarning: When a module does not expose a ``create_pipeline``\n            function, the ``create_pipeline`` function does not return a\n            ``Pipeline`` object, or if the module import fails up front.\n            If ``raise_errors`` is ``True``, see Raises section instead.\n    \"\"\"\n    pipeline_obj = None\n\n    # Handle the simplified project structure found in several starters.\n    pipeline_module_name = f\"{PACKAGE_NAME}.pipeline\"\n    try:\n        pipeline_module = importlib.import_module(pipeline_module_name)\n    except Exception as exc:\n        if str(exc) != f\"No module named '{pipeline_module_name}'\":\n            if raise_errors:\n                raise ImportError(\n                    f\"An error occurred while importing the \"\n                    f\"'{pipeline_module_name}' module.\"\n                ) from exc\n\n            warnings.warn(\n                IMPORT_ERROR_MESSAGE.format(\n                    module=pipeline_module_name, tb_exc=traceback.format_exc()\n                )\n            )\n    else:\n        pipeline_obj = _create_pipeline(pipeline_module)\n\n    pipelines_dict = {\"__default__\": pipeline_obj or pipeline([])}\n\n    # Handle the case that a project doesn't have a pipelines directory.\n    try:\n        pipelines_package = importlib_resources.files(f\"{PACKAGE_NAME}.pipelines\")\n    except ModuleNotFoundError as exc:\n        if str(exc) == f\"No module named '{PACKAGE_NAME}.pipelines'\":\n            return pipelines_dict\n\n    for pipeline_dir in pipelines_package.iterdir():\n        if not pipeline_dir.is_dir():\n            continue\n\n        pipeline_name = pipeline_dir.name\n        if pipeline_name == \"__pycache__\":\n            continue\n        # Prevent imports of hidden directories/files\n        if pipeline_name.startswith(\".\"):\n            continue\n\n        pipeline_module_name = f\"{PACKAGE_NAME}.pipelines.{pipeline_name}\"\n        try:\n            pipeline_module = importlib.import_module(pipeline_module_name)\n        except Exception as exc:\n            if raise_errors:\n                raise ImportError(\n                    f\"An error occurred while importing the \"\n                    f\"'{pipeline_module_name}' module.\"\n                ) from exc\n\n            warnings.warn(\n                IMPORT_ERROR_MESSAGE.format(\n                    module=pipeline_module_name, tb_exc=traceback.format_exc()\n                )\n            )\n            continue\n\n        pipeline_obj = _create_pipeline(pipeline_module)\n        if pipeline_obj is not None:\n            pipelines_dict[pipeline_name] = pipeline_obj\n    return pipelines_dict\n</code></pre>"},{"location":"api/framework/kedro.framework.project/#kedro.framework.project.validate_settings","title":"kedro.framework.project.validate_settings","text":"<pre><code>validate_settings()\n</code></pre> <p>Eagerly validate that the settings module is importable if it exists. This is desirable to surface any syntax or import errors early. In particular, without eagerly importing the settings module, dynaconf would silence any import error (e.g. missing dependency, missing/mislabelled pipeline), and users would instead get a cryptic error message <code>Expected an instance of `ConfigLoader`, got `NoneType` instead</code>. More info on the dynaconf issue: https://github.com/dynaconf/dynaconf/issues/460</p> Source code in <code>kedro/framework/project/__init__.py</code> <pre><code>def validate_settings() -&gt; None:\n    \"\"\"Eagerly validate that the settings module is importable if it exists. This is desirable to\n    surface any syntax or import errors early. In particular, without eagerly importing\n    the settings module, dynaconf would silence any import error (e.g. missing\n    dependency, missing/mislabelled pipeline), and users would instead get a cryptic\n    error message ``Expected an instance of `ConfigLoader`, got `NoneType` instead``.\n    More info on the dynaconf issue: https://github.com/dynaconf/dynaconf/issues/460\n    \"\"\"\n    if PACKAGE_NAME is None:\n        raise ValueError(\n            \"Package name not found. Make sure you have configured the project using \"\n            \"'bootstrap_project'. This should happen automatically if you are using \"\n            \"Kedro command line interface.\"\n        )\n    # Check if file exists, if it does, validate it.\n    if importlib.util.find_spec(f\"{PACKAGE_NAME}.settings\") is not None:\n        importlib.import_module(f\"{PACKAGE_NAME}.settings\")\n    else:\n        logger = logging.getLogger(__name__)\n        logger.warning(\"No 'settings.py' found, defaults will be used.\")\n</code></pre>"},{"location":"api/framework/kedro.framework.session/","title":"Session","text":"Module Description <code>kedro.framework.session.session</code> Implements Kedro session responsible for project lifecycle. <code>kedro.framework.session.store</code> Implements a dict-like store object used to persist Kedro sessions."},{"location":"api/framework/kedro.framework.session/#kedro.framework.session","title":"kedro.framework.session","text":"<p><code>kedro.framework.session</code> provides access to KedroSession responsible for project lifecycle.</p>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session","title":"kedro.framework.session.session","text":"<p>This module implements Kedro session responsible for project lifecycle.</p>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.kedro_version","title":"kedro_version  <code>module-attribute</code>","text":"<pre><code>kedro_version = '0.19.12'\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.pipelines","title":"pipelines  <code>module-attribute</code>","text":"<pre><code>pipelines = _ProjectPipelines()\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.settings","title":"settings  <code>module-attribute</code>","text":"<pre><code>settings = _ProjectSettings()\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.AbstractConfigLoader","title":"AbstractConfigLoader","text":"<pre><code>AbstractConfigLoader(conf_source, env=None, runtime_params=None, **kwargs)\n</code></pre> <p>               Bases: <code>UserDict</code></p> <p><code>AbstractConfigLoader</code> is the abstract base class     for all <code>ConfigLoader</code> implementations. All user-defined <code>ConfigLoader</code> implementations should inherit     from <code>AbstractConfigLoader</code> and implement all relevant abstract methods.</p> Source code in <code>kedro/config/abstract_config.py</code> <pre><code>def __init__(\n    self,\n    conf_source: str,\n    env: str | None = None,\n    runtime_params: dict[str, Any] | None = None,\n    **kwargs: Any,\n):\n    super().__init__()\n    self.conf_source = conf_source\n    self.env = env\n    self.runtime_params = runtime_params or {}\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.AbstractConfigLoader.get","title":"get","text":"<pre><code>get(key, default=None)\n</code></pre> <p>D.get(k[,d]) -&gt; D[k] if k in D, else d.  d defaults to None.</p> Source code in <code>kedro/config/abstract_config.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n    \"D.get(k[,d]) -&gt; D[k] if k in D, else d.  d defaults to None.\"\n    try:\n        return self[key]\n    except KeyError:\n        return default\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.AbstractRunner","title":"AbstractRunner","text":"<pre><code>AbstractRunner(is_async=False, extra_dataset_patterns=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p><code>AbstractRunner</code> is the base class for all <code>Pipeline</code> runner implementations.</p> <p>Parameters:</p> <ul> <li> <code>is_async</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the node inputs and outputs are loaded and saved asynchronously with threads. Defaults to False.</p> </li> <li> <code>extra_dataset_patterns</code>               (<code>dict[str, dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra dataset factory patterns to be added to the catalog during the run. This is used to set the default datasets on the Runner instances.</p> </li> </ul> Source code in <code>kedro/runner/runner.py</code> <pre><code>def __init__(\n    self,\n    is_async: bool = False,\n    extra_dataset_patterns: dict[str, dict[str, Any]] | None = None,\n):\n    \"\"\"Instantiates the runner class.\n\n    Args:\n        is_async: If True, the node inputs and outputs are loaded and saved\n            asynchronously with threads. Defaults to False.\n        extra_dataset_patterns: Extra dataset factory patterns to be added to the catalog\n            during the run. This is used to set the default datasets on the Runner instances.\n\n    \"\"\"\n    self._is_async = is_async\n    self._extra_dataset_patterns = extra_dataset_patterns\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.AbstractRunner.run","title":"run","text":"<pre><code>run(pipeline, catalog, hook_manager=None, session_id=None)\n</code></pre> <p>Run the <code>Pipeline</code> using the datasets provided by <code>catalog</code> and save results back to the same objects.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> from which to fetch data.</p> </li> <li> <code>hook_manager</code>               (<code>PluginManager | None</code>, default:                   <code>None</code> )           \u2013            <p>The <code>PluginManager</code> to activate hooks.</p> </li> <li> <code>session_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The id of the session.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised when <code>Pipeline</code> inputs cannot be satisfied.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Any node outputs that cannot be processed by the catalog.</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>These are returned in a dictionary, where the keys are defined</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>by the node outputs.</p> </li> </ul> Source code in <code>kedro/runner/runner.py</code> <pre><code>def run(\n    self,\n    pipeline: Pipeline,\n    catalog: CatalogProtocol,\n    hook_manager: PluginManager | None = None,\n    session_id: str | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Run the ``Pipeline`` using the datasets provided by ``catalog``\n    and save results back to the same objects.\n\n    Args:\n        pipeline: The ``Pipeline`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` from which to fetch data.\n        hook_manager: The ``PluginManager`` to activate hooks.\n        session_id: The id of the session.\n\n    Raises:\n        ValueError: Raised when ``Pipeline`` inputs cannot be satisfied.\n\n    Returns:\n        Any node outputs that cannot be processed by the catalog.\n        These are returned in a dictionary, where the keys are defined\n        by the node outputs.\n\n    \"\"\"\n    # Check which datasets used in the pipeline are in the catalog or match\n    # a pattern in the catalog, not including extra dataset patterns\n    # Run a warm-up to materialize all datasets in the catalog before run\n    warmed_up_ds = []\n    for ds in pipeline.datasets():\n        if ds in catalog:\n            warmed_up_ds.append(ds)\n            _ = catalog._get_dataset(ds)\n\n    # Check if there are any input datasets that aren't in the catalog and\n    # don't match a pattern in the catalog.\n    unsatisfied = pipeline.inputs() - set(warmed_up_ds)\n\n    if unsatisfied:\n        raise ValueError(\n            f\"Pipeline input(s) {unsatisfied} not found in the {catalog.__class__.__name__}\"\n        )\n\n    # Register the default dataset pattern with the catalog\n    # TODO: replace with catalog.config_resolver.add_runtime_patterns() when removing old catalog\n    catalog = catalog.shallow_copy(\n        extra_dataset_patterns=self._extra_dataset_patterns\n    )\n\n    hook_or_null_manager = hook_manager or _NullPluginManager()\n\n    # Check which datasets used in the pipeline are in the catalog or match\n    # a pattern in the catalog, including added extra_dataset_patterns\n    registered_ds = [ds for ds in pipeline.datasets() if ds in catalog]\n\n    if self._is_async:\n        self._logger.info(\n            \"Asynchronous mode is enabled for loading and saving data\"\n        )\n\n    self._run(pipeline, catalog, hook_or_null_manager, session_id)  # type: ignore[arg-type]\n\n    self._logger.info(\"Pipeline execution completed successfully.\")\n\n    # Identify MemoryDataset in the catalog\n    memory_datasets = {\n        ds_name\n        for ds_name, ds in catalog._datasets.items()\n        if isinstance(ds, MemoryDataset) or isinstance(ds, SharedMemoryDataset)\n    }\n\n    # Check if there's any output datasets that aren't in the catalog and don't match a pattern\n    # in the catalog and include MemoryDataset.\n    free_outputs = pipeline.outputs() - (set(registered_ds) - memory_datasets)\n\n    run_output = {ds_name: catalog.load(ds_name) for ds_name in free_outputs}\n\n    # Remove runtime patterns after run, so they do not affect further runs\n    if self._extra_dataset_patterns:\n        catalog.config_resolver.remove_runtime_patterns(\n            self._extra_dataset_patterns\n        )\n\n    return run_output\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.AbstractRunner.run_only_missing","title":"run_only_missing","text":"<pre><code>run_only_missing(pipeline, catalog, hook_manager)\n</code></pre> <p>Run only the missing outputs from the <code>Pipeline</code> using the datasets provided by <code>catalog</code>, and save results back to the same objects.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> from which to fetch data.</p> </li> <li> <code>hook_manager</code>               (<code>PluginManager</code>)           \u2013            <p>The <code>PluginManager</code> to activate hooks.</p> </li> </ul> <p>Raises:     ValueError: Raised when <code>Pipeline</code> inputs cannot be         satisfied.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Any node outputs that cannot be processed by the</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>catalog. These are returned in a dictionary, where</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>the keys are defined by the node outputs.</p> </li> </ul> Source code in <code>kedro/runner/runner.py</code> <pre><code>def run_only_missing(\n    self, pipeline: Pipeline, catalog: CatalogProtocol, hook_manager: PluginManager\n) -&gt; dict[str, Any]:\n    \"\"\"Run only the missing outputs from the ``Pipeline`` using the\n    datasets provided by ``catalog``, and save results back to the\n    same objects.\n\n    Args:\n        pipeline: The ``Pipeline`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` from which to fetch data.\n        hook_manager: The ``PluginManager`` to activate hooks.\n    Raises:\n        ValueError: Raised when ``Pipeline`` inputs cannot be\n            satisfied.\n\n    Returns:\n        Any node outputs that cannot be processed by the\n        catalog. These are returned in a dictionary, where\n        the keys are defined by the node outputs.\n\n    \"\"\"\n    free_outputs = pipeline.outputs() - set(catalog.list())\n    missing = {ds for ds in catalog.list() if not catalog.exists(ds)}\n    to_build = free_outputs | missing\n    to_rerun = pipeline.only_nodes_with_outputs(*to_build) + pipeline.from_inputs(\n        *to_build\n    )\n\n    # We also need any missing datasets that are required to run the\n    # `to_rerun` pipeline, including any chains of missing datasets.\n    unregistered_ds = pipeline.datasets() - set(catalog.list())\n    output_to_unregistered = pipeline.only_nodes_with_outputs(*unregistered_ds)\n    input_from_unregistered = to_rerun.inputs() &amp; unregistered_ds\n    to_rerun += output_to_unregistered.to_outputs(*input_from_unregistered)\n\n    return self.run(to_rerun, catalog, hook_manager)\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.BaseSessionStore","title":"BaseSessionStore","text":"<pre><code>BaseSessionStore(path, session_id)\n</code></pre> <p>               Bases: <code>UserDict</code></p> <p><code>BaseSessionStore</code> is the base class for all session stores. <code>BaseSessionStore</code> is an ephemeral store implementation that doesn't persist the session data.</p> Source code in <code>kedro/framework/session/store.py</code> <pre><code>def __init__(self, path: str, session_id: str):\n    self._path = path\n    self._session_id = session_id\n    super().__init__(self.read())\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.BaseSessionStore.read","title":"read","text":"<pre><code>read()\n</code></pre> <p>Read the data from the session store.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>A mapping containing the session store data.</p> </li> </ul> Source code in <code>kedro/framework/session/store.py</code> <pre><code>def read(self) -&gt; dict[str, Any]:\n    \"\"\"Read the data from the session store.\n\n    Returns:\n        A mapping containing the session store data.\n    \"\"\"\n    self._logger.debug(\n        \"'read()' not implemented for '%s'. Assuming empty store.\",\n        self.__class__.__name__,\n    )\n    return {}\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.BaseSessionStore.save","title":"save","text":"<pre><code>save()\n</code></pre> <p>Persist the session store</p> Source code in <code>kedro/framework/session/store.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Persist the session store\"\"\"\n    self._logger.debug(\n        \"'save()' not implemented for '%s'. Skipping the step.\",\n        self.__class__.__name__,\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.KedroContext","title":"KedroContext","text":"<p><code>KedroContext</code> is the base class which holds the configuration and Kedro's main functionality.</p> <p>Create a context object by providing the root of a Kedro project and the environment configuration subfolders (see <code>kedro.config.OmegaConfigLoader</code>) Raises:     KedroContextError: If there is a mismatch         between Kedro project version and package version. Args:     project_path: Project path to define the context for.     config_loader: Kedro's <code>OmegaConfigLoader</code> for loading the configuration files.     env: Optional argument for configuration default environment to be used         for running the pipeline. If not specified, it defaults to \"local\".     package_name: Package name for the Kedro project the context is         created for.     hook_manager: The <code>PluginManager</code> to activate hooks, supplied by the session.     extra_params: Optional dictionary containing extra project parameters.         If specified, will update (and therefore take precedence over)         the parameters retrieved from the project configuration.</p>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.KedroContext.catalog","title":"catalog  <code>property</code>","text":"<pre><code>catalog\n</code></pre> <p>Read-only property referring to Kedro's catalog` for this context.</p> <p>Returns:</p> <ul> <li> <code>CatalogProtocol</code>           \u2013            <p>catalog defined in <code>catalog.yml</code>.</p> </li> </ul> <p>Raises:     KedroContextError: Incorrect catalog registered for the project.</p>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.KedroContext.params","title":"params  <code>property</code>","text":"<pre><code>params\n</code></pre> <p>Read-only property referring to Kedro's parameters for this context.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Parameters defined in <code>parameters.yml</code> with the addition of any extra parameters passed at initialization.</p> </li> </ul>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.KedroSession","title":"KedroSession","text":"<pre><code>KedroSession(session_id, package_name=None, project_path=None, save_on_close=False, conf_source=None)\n</code></pre> <p><code>KedroSession</code> is the object that is responsible for managing the lifecycle of a Kedro run. Use <code>KedroSession.create()</code> as a context manager to construct a new KedroSession with session data provided (see the example below).</p> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro.framework.session import KedroSession\n&gt;&gt;&gt; from kedro.framework.startup import bootstrap_project\n&gt;&gt;&gt; from pathlib import Path\n\n&gt;&gt;&gt; # If you are creating a session outside of a Kedro project (i.e. not using\n&gt;&gt;&gt; # `kedro run` or `kedro jupyter`), you need to run `bootstrap_project` to\n&gt;&gt;&gt; # let Kedro find your configuration.\n&gt;&gt;&gt; bootstrap_project(Path(\"&lt;project_root&gt;\"))\n&gt;&gt;&gt; with KedroSession.create() as session:\n&gt;&gt;&gt;     session.run()\n</code></pre> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    package_name: str | None = None,\n    project_path: Path | str | None = None,\n    save_on_close: bool = False,\n    conf_source: str | None = None,\n):\n    self._project_path = Path(\n        project_path or _find_kedro_project(Path.cwd()) or Path.cwd()\n    ).resolve()\n    self.session_id = session_id\n    self.save_on_close = save_on_close\n    self._package_name = package_name\n    self._store = self._init_store()\n    self._run_called = False\n\n    hook_manager = _create_hook_manager()\n    _register_hooks(hook_manager, settings.HOOKS)\n    _register_hooks_entry_points(hook_manager, settings.DISABLE_HOOKS_FOR_PLUGINS)\n    self._hook_manager = hook_manager\n\n    self._conf_source = conf_source or str(\n        self._project_path / settings.CONF_SOURCE\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.KedroSession.store","title":"store  <code>property</code>","text":"<pre><code>store\n</code></pre> <p>Return a copy of internal store.</p>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.KedroSession.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the current session and save its store to disk if <code>save_on_close</code> attribute is True.</p> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the current session and save its store to disk\n    if `save_on_close` attribute is True.\n    \"\"\"\n    if self.save_on_close:\n        self._store.save()\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.KedroSession.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(project_path=None, save_on_close=True, env=None, extra_params=None, conf_source=None)\n</code></pre> <p>Create a new instance of <code>KedroSession</code> with the session data.</p> <p>Parameters:</p> <ul> <li> <code>project_path</code>               (<code>Path | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the project root directory. Default is current working directory Path.cwd().</p> </li> <li> <code>save_on_close</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether or not to save the session when it's closed.</p> </li> <li> <code>conf_source</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to a directory containing configuration</p> </li> <li> <code>env</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Environment for the KedroContext.</p> </li> <li> <code>extra_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary containing extra project parameters for underlying KedroContext. If specified, will update (and therefore take precedence over) the parameters retrieved from the project configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>KedroSession</code>           \u2013            <p>A new <code>KedroSession</code> instance.</p> </li> </ul> Source code in <code>kedro/framework/session/session.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    project_path: Path | str | None = None,\n    save_on_close: bool = True,\n    env: str | None = None,\n    extra_params: dict[str, Any] | None = None,\n    conf_source: str | None = None,\n) -&gt; KedroSession:\n    \"\"\"Create a new instance of ``KedroSession`` with the session data.\n\n    Args:\n        project_path: Path to the project root directory. Default is\n            current working directory Path.cwd().\n        save_on_close: Whether or not to save the session when it's closed.\n        conf_source: Path to a directory containing configuration\n        env: Environment for the KedroContext.\n        extra_params: Optional dictionary containing extra project parameters\n            for underlying KedroContext. If specified, will update (and therefore\n            take precedence over) the parameters retrieved from the project\n            configuration.\n\n    Returns:\n        A new ``KedroSession`` instance.\n    \"\"\"\n    validate_settings()\n\n    session = cls(\n        project_path=project_path,\n        session_id=generate_timestamp(),\n        save_on_close=save_on_close,\n        conf_source=conf_source,\n    )\n\n    # have to explicitly type session_data otherwise mypy will complain\n    # possibly related to this: https://github.com/python/mypy/issues/1430\n    session_data: dict[str, Any] = {\n        \"project_path\": session._project_path,\n        \"session_id\": session.session_id,\n    }\n\n    ctx = click.get_current_context(silent=True)\n    if ctx:\n        session_data[\"cli\"] = _jsonify_cli_context(ctx)\n\n    env = env or os.getenv(\"KEDRO_ENV\")\n    if env:\n        session_data[\"env\"] = env\n\n    if extra_params:\n        session_data[\"extra_params\"] = extra_params\n\n    try:\n        session_data[\"username\"] = getpass.getuser()\n    except Exception as exc:\n        logging.getLogger(__name__).debug(\n            \"Unable to get username. Full exception: %s\", exc\n        )\n\n    session_data.update(**_describe_git(session._project_path))\n    session._store.update(session_data)\n\n    return session\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.KedroSession.load_context","title":"load_context","text":"<pre><code>load_context()\n</code></pre> <p>An instance of the project context.</p> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def load_context(self) -&gt; KedroContext:\n    \"\"\"An instance of the project context.\"\"\"\n    env = self.store.get(\"env\")\n    extra_params = self.store.get(\"extra_params\")\n    config_loader = self._get_config_loader()\n    context_class = settings.CONTEXT_CLASS\n    context = context_class(\n        package_name=self._package_name,\n        project_path=self._project_path,\n        config_loader=config_loader,\n        env=env,\n        extra_params=extra_params,\n        hook_manager=self._hook_manager,\n    )\n    self._hook_manager.hook.after_context_created(context=context)\n\n    return context  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.KedroSession.run","title":"run","text":"<pre><code>run(pipeline_name=None, tags=None, runner=None, node_names=None, from_nodes=None, to_nodes=None, from_inputs=None, to_outputs=None, load_versions=None, namespace=None)\n</code></pre> <p>Runs the pipeline with a specified runner.</p> <p>Parameters:</p> <ul> <li> <code>pipeline_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the pipeline that is being run.</p> </li> <li> <code>tags</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of node tags which should be used to filter the nodes of the <code>Pipeline</code>. If specified, only the nodes containing any of these tags will be run.</p> </li> <li> <code>runner</code>               (<code>AbstractRunner | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional parameter specifying the runner that you want to run the pipeline with.</p> </li> <li> <code>node_names</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of node names which should be used to filter the nodes of the <code>Pipeline</code>. If specified, only the nodes with these names will be run.</p> </li> <li> <code>from_nodes</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of node names which should be used as a starting point of the new <code>Pipeline</code>.</p> </li> <li> <code>to_nodes</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of node names which should be used as an end point of the new <code>Pipeline</code>.</p> </li> <li> <code>from_inputs</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of input datasets which should be used as a starting point of the new <code>Pipeline</code>.</p> </li> <li> <code>to_outputs</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of output datasets which should be used as an end point of the new <code>Pipeline</code>.</p> </li> <li> <code>load_versions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional flag to specify a particular dataset version timestamp to load.</p> </li> <li> <code>namespace</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The namespace of the nodes that is being run.</p> </li> </ul> <p>Raises:     ValueError: If the named or <code>__default__</code> pipeline is not         defined by <code>register_pipelines</code>.     Exception: Any uncaught exception during the run will be re-raised         after being passed to <code>on_pipeline_error</code> hook.     KedroSessionError: If more than one run is attempted to be executed during         a single session. Returns:     Any node outputs that cannot be processed by the <code>DataCatalog</code>.     These are returned in a dictionary, where the keys are defined     by the node outputs.</p> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def run(  # noqa: PLR0913\n    self,\n    pipeline_name: str | None = None,\n    tags: Iterable[str] | None = None,\n    runner: AbstractRunner | None = None,\n    node_names: Iterable[str] | None = None,\n    from_nodes: Iterable[str] | None = None,\n    to_nodes: Iterable[str] | None = None,\n    from_inputs: Iterable[str] | None = None,\n    to_outputs: Iterable[str] | None = None,\n    load_versions: dict[str, str] | None = None,\n    namespace: str | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Runs the pipeline with a specified runner.\n\n    Args:\n        pipeline_name: Name of the pipeline that is being run.\n        tags: An optional list of node tags which should be used to\n            filter the nodes of the ``Pipeline``. If specified, only the nodes\n            containing *any* of these tags will be run.\n        runner: An optional parameter specifying the runner that you want to run\n            the pipeline with.\n        node_names: An optional list of node names which should be used to\n            filter the nodes of the ``Pipeline``. If specified, only the nodes\n            with these names will be run.\n        from_nodes: An optional list of node names which should be used as a\n            starting point of the new ``Pipeline``.\n        to_nodes: An optional list of node names which should be used as an\n            end point of the new ``Pipeline``.\n        from_inputs: An optional list of input datasets which should be\n            used as a starting point of the new ``Pipeline``.\n        to_outputs: An optional list of output datasets which should be\n            used as an end point of the new ``Pipeline``.\n        load_versions: An optional flag to specify a particular dataset\n            version timestamp to load.\n        namespace: The namespace of the nodes that is being run.\n    Raises:\n        ValueError: If the named or `__default__` pipeline is not\n            defined by `register_pipelines`.\n        Exception: Any uncaught exception during the run will be re-raised\n            after being passed to ``on_pipeline_error`` hook.\n        KedroSessionError: If more than one run is attempted to be executed during\n            a single session.\n    Returns:\n        Any node outputs that cannot be processed by the ``DataCatalog``.\n        These are returned in a dictionary, where the keys are defined\n        by the node outputs.\n    \"\"\"\n    # Report project name\n    self._logger.info(\"Kedro project %s\", self._project_path.name)\n\n    if self._run_called:\n        raise KedroSessionError(\n            \"A run has already been completed as part of the\"\n            \" active KedroSession. KedroSession has a 1-1 mapping with\"\n            \" runs, and thus only one run should be executed per session.\"\n        )\n\n    session_id = self.store[\"session_id\"]\n    save_version = session_id\n    extra_params = self.store.get(\"extra_params\") or {}\n    context = self.load_context()\n\n    name = pipeline_name or \"__default__\"\n\n    try:\n        pipeline = pipelines[name]\n    except KeyError as exc:\n        raise ValueError(\n            f\"Failed to find the pipeline named '{name}'. \"\n            f\"It needs to be generated and returned \"\n            f\"by the 'register_pipelines' function.\"\n        ) from exc\n\n    filtered_pipeline = pipeline.filter(\n        tags=tags,\n        from_nodes=from_nodes,\n        to_nodes=to_nodes,\n        node_names=node_names,\n        from_inputs=from_inputs,\n        to_outputs=to_outputs,\n        node_namespace=namespace,\n    )\n\n    record_data = {\n        \"session_id\": session_id,\n        \"project_path\": self._project_path.as_posix(),\n        \"env\": context.env,\n        \"kedro_version\": kedro_version,\n        \"tags\": tags,\n        \"from_nodes\": from_nodes,\n        \"to_nodes\": to_nodes,\n        \"node_names\": node_names,\n        \"from_inputs\": from_inputs,\n        \"to_outputs\": to_outputs,\n        \"load_versions\": load_versions,\n        \"extra_params\": extra_params,\n        \"pipeline_name\": pipeline_name,\n        \"namespace\": namespace,\n        \"runner\": getattr(runner, \"__name__\", str(runner)),\n    }\n\n    catalog = context._get_catalog(\n        save_version=save_version,\n        load_versions=load_versions,\n    )\n\n    # Run the runner\n    hook_manager = self._hook_manager\n    runner = runner or SequentialRunner()\n    if not isinstance(runner, AbstractRunner):\n        raise KedroSessionError(\n            \"KedroSession expect an instance of Runner instead of a class.\"\n            \"Have you forgotten the `()` at the end of the statement?\"\n        )\n    hook_manager.hook.before_pipeline_run(\n        run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\n    )\n    try:\n        run_result = runner.run(\n            filtered_pipeline, catalog, hook_manager, session_id\n        )\n        self._run_called = True\n    except Exception as error:\n        hook_manager.hook.on_pipeline_error(\n            error=error,\n            run_params=record_data,\n            pipeline=filtered_pipeline,\n            catalog=catalog,\n        )\n        raise\n\n    hook_manager.hook.after_pipeline_run(\n        run_params=record_data,\n        run_result=run_result,\n        pipeline=filtered_pipeline,\n        catalog=catalog,\n    )\n    return run_result\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.KedroSessionError","title":"KedroSessionError","text":"<p>               Bases: <code>Exception</code></p> <p><code>KedroSessionError</code> raised by <code>KedroSession</code> in the case that multiple runs are attempted in one session.</p>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.SequentialRunner","title":"SequentialRunner","text":"<pre><code>SequentialRunner(is_async=False, extra_dataset_patterns=None)\n</code></pre> <p>               Bases: <code>AbstractRunner</code></p> <p><code>SequentialRunner</code> is an <code>AbstractRunner</code> implementation. It can be used to run the <code>Pipeline</code> in a sequential manner using a topological sort of provided nodes.</p> <p>Parameters:</p> <ul> <li> <code>is_async</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the node inputs and outputs are loaded and saved asynchronously with threads. Defaults to False.</p> </li> <li> <code>extra_dataset_patterns</code>               (<code>dict[str, dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra dataset factory patterns to be added to the catalog during the run. This is used to set the default datasets to MemoryDataset for <code>SequentialRunner</code>.</p> </li> </ul> Source code in <code>kedro/runner/sequential_runner.py</code> <pre><code>def __init__(\n    self,\n    is_async: bool = False,\n    extra_dataset_patterns: dict[str, dict[str, Any]] | None = None,\n):\n    \"\"\"Instantiates the runner class.\n\n    Args:\n        is_async: If True, the node inputs and outputs are loaded and saved\n            asynchronously with threads. Defaults to False.\n        extra_dataset_patterns: Extra dataset factory patterns to be added to the catalog\n            during the run. This is used to set the default datasets to MemoryDataset\n            for `SequentialRunner`.\n\n    \"\"\"\n    default_dataset_pattern = {\"{default}\": {\"type\": \"MemoryDataset\"}}\n    self._extra_dataset_patterns = extra_dataset_patterns or default_dataset_pattern\n    super().__init__(\n        is_async=is_async, extra_dataset_patterns=self._extra_dataset_patterns\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session._create_hook_manager","title":"_create_hook_manager","text":"<pre><code>_create_hook_manager()\n</code></pre> <p>Create a new PluginManager instance and register Kedro's hook specs.</p> Source code in <code>kedro/framework/hooks/manager.py</code> <pre><code>def _create_hook_manager() -&gt; PluginManager:\n    \"\"\"Create a new PluginManager instance and register Kedro's hook specs.\"\"\"\n    manager = PluginManager(HOOK_NAMESPACE)\n    manager.trace.root.setwriter(logger.debug)\n    manager.enable_tracing()\n    manager.add_hookspecs(NodeSpecs)\n    manager.add_hookspecs(PipelineSpecs)\n    manager.add_hookspecs(DataCatalogSpecs)\n    manager.add_hookspecs(DatasetSpecs)\n    manager.add_hookspecs(KedroContextSpecs)\n    return manager\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session._describe_git","title":"_describe_git","text":"<pre><code>_describe_git(project_path)\n</code></pre> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def _describe_git(project_path: Path) -&gt; dict[str, dict[str, Any]]:\n    path = str(project_path)\n    try:\n        res = subprocess.check_output(  # noqa: S603\n            [\"git\", \"rev-parse\", \"--short\", \"HEAD\"],  # noqa: S607\n            cwd=path,\n            stderr=subprocess.STDOUT,\n        )\n        git_data: dict[str, Any] = {\"commit_sha\": res.decode().strip()}\n        git_status_res = subprocess.check_output(  # noqa: S603\n            [\"git\", \"status\", \"--short\"],  # noqa: S607\n            cwd=path,\n            stderr=subprocess.STDOUT,\n        )\n        git_data[\"dirty\"] = bool(git_status_res.decode().strip())\n\n    # `subprocess.check_output()` raises `NotADirectoryError` on Windows\n    except Exception:\n        logger = logging.getLogger(__name__)\n        logger.debug(\"Unable to git describe %s\", path)\n        logger.debug(traceback.format_exc())\n        return {}\n\n    return {\"git\": git_data}\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session._find_kedro_project","title":"_find_kedro_project","text":"<pre><code>_find_kedro_project(current_dir)\n</code></pre> <p>Given a path, find a Kedro project associated with it.</p> Can be <ul> <li>Itself, if a path is a root directory of a Kedro project.</li> <li>One of its parents, if self is not a Kedro project but one of the parent path is.</li> <li>None, if neither self nor any parent path is a Kedro project.</li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Kedro project associated with a given path,</p> </li> <li> <code>Any</code>           \u2013            <p>or None if no relevant Kedro project is found.</p> </li> </ul> Source code in <code>kedro/utils.py</code> <pre><code>def _find_kedro_project(current_dir: Path) -&gt; Any:  # pragma: no cover\n    \"\"\"Given a path, find a Kedro project associated with it.\n\n    Can be:\n        - Itself, if a path is a root directory of a Kedro project.\n        - One of its parents, if self is not a Kedro project but one of the parent path is.\n        - None, if neither self nor any parent path is a Kedro project.\n\n    Returns:\n        Kedro project associated with a given path,\n        or None if no relevant Kedro project is found.\n    \"\"\"\n    paths_to_check = [current_dir, *list(current_dir.parents)]\n    for parent_dir in paths_to_check:\n        if _is_project(parent_dir):\n            return parent_dir\n    return None\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session._jsonify_cli_context","title":"_jsonify_cli_context","text":"<pre><code>_jsonify_cli_context(ctx)\n</code></pre> Source code in <code>kedro/framework/session/session.py</code> <pre><code>def _jsonify_cli_context(ctx: click.core.Context) -&gt; dict[str, Any]:\n    return {\n        \"args\": ctx.args,\n        \"params\": ctx.params,\n        \"command_name\": ctx.command.name,\n        \"command_path\": \" \".join([\"kedro\"] + sys.argv[1:]),\n    }\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session._register_hooks","title":"_register_hooks","text":"<pre><code>_register_hooks(hook_manager, hooks)\n</code></pre> <p>Register all hooks as specified in <code>hooks</code> with the global <code>hook_manager</code>.</p> <p>Parameters:</p> <ul> <li> <code>hook_manager</code>               (<code>PluginManager</code>)           \u2013            <p>Hook manager instance to register the hooks with.</p> </li> <li> <code>hooks</code>               (<code>Iterable[Any]</code>)           \u2013            <p>Hooks that need to be registered.</p> </li> </ul> Source code in <code>kedro/framework/hooks/manager.py</code> <pre><code>def _register_hooks(hook_manager: PluginManager, hooks: Iterable[Any]) -&gt; None:\n    \"\"\"Register all hooks as specified in ``hooks`` with the global ``hook_manager``.\n\n    Args:\n        hook_manager: Hook manager instance to register the hooks with.\n        hooks: Hooks that need to be registered.\n\n    \"\"\"\n    for hooks_collection in hooks:\n        # Sometimes users might call hook registration more than once, in which\n        # case hooks have already been registered, so we perform a simple check\n        # here to avoid an error being raised and break user's workflow.\n        if not hook_manager.is_registered(hooks_collection):\n            if isclass(hooks_collection):\n                raise TypeError(\n                    \"KedroSession expects hooks to be registered as instances. \"\n                    \"Have you forgotten the `()` when registering a hook class ?\"\n                )\n            hook_manager.register(hooks_collection)\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session._register_hooks_entry_points","title":"_register_hooks_entry_points","text":"<pre><code>_register_hooks_entry_points(hook_manager, disabled_plugins)\n</code></pre> <p>Register pluggy hooks from python package entrypoints.</p> <p>Parameters:</p> <ul> <li> <code>hook_manager</code>               (<code>PluginManager</code>)           \u2013            <p>Hook manager instance to register the hooks with.</p> </li> <li> <code>disabled_plugins</code>               (<code>Iterable[str]</code>)           \u2013            <p>An iterable returning the names of plugins which hooks must not be registered; any already registered hooks will be unregistered.</p> </li> </ul> Source code in <code>kedro/framework/hooks/manager.py</code> <pre><code>def _register_hooks_entry_points(\n    hook_manager: PluginManager, disabled_plugins: Iterable[str]\n) -&gt; None:\n    \"\"\"Register pluggy hooks from python package entrypoints.\n\n    Args:\n        hook_manager: Hook manager instance to register the hooks with.\n        disabled_plugins: An iterable returning the names of plugins\n            which hooks must not be registered; any already registered\n            hooks will be unregistered.\n\n    \"\"\"\n    already_registered = hook_manager.get_plugins()\n    # Method name is misleading:\n    # entry points are standard and don't require setuptools,\n    # see https://packaging.python.org/en/latest/specifications/entry-points/\n    hook_manager.load_setuptools_entrypoints(_PLUGIN_HOOKS)\n    disabled_plugins = set(disabled_plugins)\n\n    # Get list of plugin/distinfo tuples for all registered plugins.\n    plugininfo = hook_manager.list_plugin_distinfo()\n    plugin_names = set()\n    disabled_plugin_names = set()\n    for plugin, dist in plugininfo:\n        if dist.project_name in disabled_plugins:\n            # `unregister()` is used instead of `set_blocked()` because\n            # we want to disable hooks for specific plugin based on project\n            # name and not `entry_point` name. Also, we log project names with\n            # version for which hooks were registered.\n            hook_manager.unregister(plugin=plugin)\n            disabled_plugin_names.add(f\"{dist.project_name}-{dist.version}\")\n        elif plugin not in already_registered:\n            plugin_names.add(f\"{dist.project_name}-{dist.version}\")\n\n    if disabled_plugin_names:\n        logger.debug(\n            \"Hooks are disabled for plugin(s): %s\",\n            \", \".join(sorted(disabled_plugin_names)),\n        )\n\n    if plugin_names:\n        logger.debug(\n            \"Registered hooks from %d installed plugin(s): %s\",\n            len(plugin_names),\n            \", \".join(sorted(plugin_names)),\n        )\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.generate_timestamp","title":"generate_timestamp","text":"<pre><code>generate_timestamp()\n</code></pre> <p>Generate the timestamp to be used by versioning.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>String representation of the current timestamp.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>def generate_timestamp() -&gt; str:\n    \"\"\"Generate the timestamp to be used by versioning.\n\n    Returns:\n        String representation of the current timestamp.\n\n    \"\"\"\n    current_ts = datetime.now(tz=timezone.utc).strftime(VERSION_FORMAT)\n    return current_ts[:-4] + current_ts[-1:]  # Don't keep microseconds\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.session.validate_settings","title":"validate_settings","text":"<pre><code>validate_settings()\n</code></pre> <p>Eagerly validate that the settings module is importable if it exists. This is desirable to surface any syntax or import errors early. In particular, without eagerly importing the settings module, dynaconf would silence any import error (e.g. missing dependency, missing/mislabelled pipeline), and users would instead get a cryptic error message <code>Expected an instance of `ConfigLoader`, got `NoneType` instead</code>. More info on the dynaconf issue: https://github.com/dynaconf/dynaconf/issues/460</p> Source code in <code>kedro/framework/project/__init__.py</code> <pre><code>def validate_settings() -&gt; None:\n    \"\"\"Eagerly validate that the settings module is importable if it exists. This is desirable to\n    surface any syntax or import errors early. In particular, without eagerly importing\n    the settings module, dynaconf would silence any import error (e.g. missing\n    dependency, missing/mislabelled pipeline), and users would instead get a cryptic\n    error message ``Expected an instance of `ConfigLoader`, got `NoneType` instead``.\n    More info on the dynaconf issue: https://github.com/dynaconf/dynaconf/issues/460\n    \"\"\"\n    if PACKAGE_NAME is None:\n        raise ValueError(\n            \"Package name not found. Make sure you have configured the project using \"\n            \"'bootstrap_project'. This should happen automatically if you are using \"\n            \"Kedro command line interface.\"\n        )\n    # Check if file exists, if it does, validate it.\n    if importlib.util.find_spec(f\"{PACKAGE_NAME}.settings\") is not None:\n        importlib.import_module(f\"{PACKAGE_NAME}.settings\")\n    else:\n        logger = logging.getLogger(__name__)\n        logger.warning(\"No 'settings.py' found, defaults will be used.\")\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.store","title":"kedro.framework.session.store","text":"<p>This module implements a dict-like store object used to persist Kedro sessions.</p>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.store.BaseSessionStore","title":"BaseSessionStore","text":"<pre><code>BaseSessionStore(path, session_id)\n</code></pre> <p>               Bases: <code>UserDict</code></p> <p><code>BaseSessionStore</code> is the base class for all session stores. <code>BaseSessionStore</code> is an ephemeral store implementation that doesn't persist the session data.</p> Source code in <code>kedro/framework/session/store.py</code> <pre><code>def __init__(self, path: str, session_id: str):\n    self._path = path\n    self._session_id = session_id\n    super().__init__(self.read())\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.store.BaseSessionStore.read","title":"read","text":"<pre><code>read()\n</code></pre> <p>Read the data from the session store.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>A mapping containing the session store data.</p> </li> </ul> Source code in <code>kedro/framework/session/store.py</code> <pre><code>def read(self) -&gt; dict[str, Any]:\n    \"\"\"Read the data from the session store.\n\n    Returns:\n        A mapping containing the session store data.\n    \"\"\"\n    self._logger.debug(\n        \"'read()' not implemented for '%s'. Assuming empty store.\",\n        self.__class__.__name__,\n    )\n    return {}\n</code></pre>"},{"location":"api/framework/kedro.framework.session/#kedro.framework.session.store.BaseSessionStore.save","title":"save","text":"<pre><code>save()\n</code></pre> <p>Persist the session store</p> Source code in <code>kedro/framework/session/store.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Persist the session store\"\"\"\n    self._logger.debug(\n        \"'save()' not implemented for '%s'. Skipping the step.\",\n        self.__class__.__name__,\n    )\n</code></pre>"},{"location":"api/framework/kedro.framework.startup/","title":"Startup","text":"Name Type Description <code>bootstrap_project</code> Function Run setup required at the beginning of the workflow when running in project mode, and return project metadata. <code>ProjectMetadata</code> Class Structure holding project metadata derived from <code>pyproject.toml</code>."},{"location":"api/framework/kedro.framework.startup/#kedro.framework.startup","title":"kedro.framework.startup","text":"<p>This module provides metadata for a Kedro project.</p>"},{"location":"api/framework/kedro.framework.startup/#kedro.framework.startup.bootstrap_project","title":"kedro.framework.startup.bootstrap_project","text":"<pre><code>bootstrap_project(project_path)\n</code></pre> <p>Run setup required at the beginning of the workflow when running in project mode, and return project metadata.</p> Source code in <code>kedro/framework/startup.py</code> <pre><code>def bootstrap_project(project_path: str | Path) -&gt; ProjectMetadata:\n    \"\"\"Run setup required at the beginning of the workflow\n    when running in project mode, and return project metadata.\n    \"\"\"\n\n    project_path = Path(project_path).expanduser().resolve()\n    metadata = _get_project_metadata(project_path)\n    _add_src_to_path(metadata.source_dir, project_path)\n    configure_project(metadata.package_name)\n    return metadata\n</code></pre>"},{"location":"api/framework/kedro.framework.startup/#kedro.framework.startup.ProjectMetadata","title":"kedro.framework.startup.ProjectMetadata","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Structure holding project metadata derived from <code>pyproject.toml</code></p>"},{"location":"api/framework/kedro.framework.startup/#kedro.framework.startup.ProjectMetadata.config_file","title":"config_file  <code>instance-attribute</code>","text":"<pre><code>config_file\n</code></pre>"},{"location":"api/framework/kedro.framework.startup/#kedro.framework.startup.ProjectMetadata.example_pipeline","title":"example_pipeline  <code>instance-attribute</code>","text":"<pre><code>example_pipeline\n</code></pre>"},{"location":"api/framework/kedro.framework.startup/#kedro.framework.startup.ProjectMetadata.kedro_init_version","title":"kedro_init_version  <code>instance-attribute</code>","text":"<pre><code>kedro_init_version\n</code></pre>"},{"location":"api/framework/kedro.framework.startup/#kedro.framework.startup.ProjectMetadata.package_name","title":"package_name  <code>instance-attribute</code>","text":"<pre><code>package_name\n</code></pre>"},{"location":"api/framework/kedro.framework.startup/#kedro.framework.startup.ProjectMetadata.project_name","title":"project_name  <code>instance-attribute</code>","text":"<pre><code>project_name\n</code></pre>"},{"location":"api/framework/kedro.framework.startup/#kedro.framework.startup.ProjectMetadata.project_path","title":"project_path  <code>instance-attribute</code>","text":"<pre><code>project_path\n</code></pre>"},{"location":"api/framework/kedro.framework.startup/#kedro.framework.startup.ProjectMetadata.source_dir","title":"source_dir  <code>instance-attribute</code>","text":"<pre><code>source_dir\n</code></pre>"},{"location":"api/framework/kedro.framework.startup/#kedro.framework.startup.ProjectMetadata.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/","title":"AbstractDataset","text":""},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset","title":"kedro.io.AbstractDataset","text":"<p>               Bases: <code>ABC</code>, <code>Generic[_DI, _DO]</code></p> <p><code>AbstractDataset</code> is the base class for all dataset implementations.</p> <p>All dataset implementations should extend this abstract class and implement the methods marked as abstract. If a specific dataset implementation cannot be used in conjunction with the <code>ParallelRunner</code>, such user-defined dataset should have the attribute <code>_SINGLE_PROCESS = True</code>. Example: ::</p> <pre><code>&gt;&gt;&gt; from pathlib import Path, PurePosixPath\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from kedro.io import AbstractDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; class MyOwnDataset(AbstractDataset[pd.DataFrame, pd.DataFrame]):\n&gt;&gt;&gt;     def __init__(self, filepath, param1, param2=True):\n&gt;&gt;&gt;         self._filepath = PurePosixPath(filepath)\n&gt;&gt;&gt;         self._param1 = param1\n&gt;&gt;&gt;         self._param2 = param2\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def load(self) -&gt; pd.DataFrame:\n&gt;&gt;&gt;         return pd.read_csv(self._filepath)\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def save(self, df: pd.DataFrame) -&gt; None:\n&gt;&gt;&gt;         df.to_csv(str(self._filepath))\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _exists(self) -&gt; bool:\n&gt;&gt;&gt;         return Path(self._filepath.as_posix()).exists()\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _describe(self):\n&gt;&gt;&gt;         return dict(param1=self._param1, param2=self._param2)\n</code></pre> <p>Example catalog.yml specification: ::</p> <pre><code>my_dataset:\n    type: &lt;path-to-my-own-dataset&gt;.MyOwnDataset\n    filepath: data/01_raw/my_data.csv\n    param1: &lt;param1-value&gt; # param1 is a required argument\n    # param2 will be True by default\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset._EPHEMERAL","title":"_EPHEMERAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_EPHEMERAL = False\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset._logger","title":"_logger  <code>property</code>","text":"<pre><code>_logger\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset.__init_subclass__","title":"__init_subclass__","text":"<pre><code>__init_subclass__(**kwargs)\n</code></pre> <p>Customizes the behavior of subclasses of AbstractDataset during their creation. This method is automatically invoked when a subclass of AbstractDataset is defined.</p> <p>Decorates the <code>load</code> and <code>save</code> methods provided by the class. If <code>_load</code> or <code>_save</code> are defined, alias them as a prerequisite.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def __init_subclass__(cls, **kwargs: Any) -&gt; None:\n    \"\"\"Customizes the behavior of subclasses of AbstractDataset during\n    their creation. This method is automatically invoked when a subclass\n    of AbstractDataset is defined.\n\n    Decorates the `load` and `save` methods provided by the class.\n    If `_load` or `_save` are defined, alias them as a prerequisite.\n    \"\"\"\n\n    # Save the original __init__ method of the subclass\n    init_func: Callable = cls.__init__\n\n    @wraps(init_func)\n    def new_init(self, *args, **kwargs) -&gt; None:  # type: ignore[no-untyped-def]\n        \"\"\"Executes the original __init__, then save the arguments used\n        to initialize the instance.\n        \"\"\"\n        # Call the original __init__ method\n        init_func(self, *args, **kwargs)\n        # Capture and save the arguments passed to the original __init__\n        self._init_args = getcallargs(init_func, self, *args, **kwargs)\n\n    # Replace the subclass's __init__ with the new_init\n    # A hook for subclasses to capture initialization arguments and save them\n    # in the AbstractDataset._init_args field\n    cls.__init__ = new_init  # type: ignore[method-assign]\n\n    super().__init_subclass__(**kwargs)\n\n    if hasattr(cls, \"_load\") and not cls._load.__qualname__.startswith(\"Abstract\"):\n        cls.load = cls._load  # type: ignore[method-assign]\n\n    if hasattr(cls, \"_save\") and not cls._save.__qualname__.startswith(\"Abstract\"):\n        cls.save = cls._save  # type: ignore[method-assign]\n\n    if hasattr(cls, \"load\") and not cls.load.__qualname__.startswith(\"Abstract\"):\n        cls.load = cls._load_wrapper(  # type: ignore[assignment]\n            cls.load\n            if not getattr(cls.load, \"__loadwrapped__\", False)\n            else cls.load.__wrapped__  # type: ignore[attr-defined]\n        )\n\n    if hasattr(cls, \"save\") and not cls.save.__qualname__.startswith(\"Abstract\"):\n        cls.save = cls._save_wrapper(  # type: ignore[assignment]\n            cls.save\n            if not getattr(cls.save, \"__savewrapped__\", False)\n            else cls.save.__wrapped__  # type: ignore[attr-defined]\n        )\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>kedro/io/core.py</code> <pre><code>def __repr__(self) -&gt; str:\n    object_description = self._describe()\n    if isinstance(object_description, dict) and all(\n        isinstance(key, str) for key in object_description\n    ):\n        return self._pretty_repr(object_description)\n\n    self._logger.warning(\n        f\"'{type(self).__module__}.{type(self).__name__}' is a subclass of AbstractDataset and it must \"\n        f\"implement the '_describe' method following the signature of AbstractDataset's '_describe'.\"\n    )\n    return f\"{type(self).__module__}.{type(self).__name__}()\"\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>kedro/io/core.py</code> <pre><code>def __str__(self) -&gt; str:\n    # TODO: Replace with __repr__ implementation in 0.20.0 release.\n    def _to_str(obj: Any, is_root: bool = False) -&gt; str:\n        \"\"\"Returns a string representation where\n        1. The root level (i.e. the Dataset.__init__ arguments) are\n        formatted like Dataset(key=value).\n        2. Dictionaries have the keys alphabetically sorted recursively.\n        3. None values are not shown.\n        \"\"\"\n\n        fmt = \"{}={}\" if is_root else \"'{}': {}\"  # 1\n\n        if isinstance(obj, dict):\n            sorted_dict = sorted(obj.items(), key=lambda pair: str(pair[0]))  # 2\n\n            text = \", \".join(\n                fmt.format(key, _to_str(value))  # 2\n                for key, value in sorted_dict\n                if value is not None  # 3\n            )\n\n            return text if is_root else \"{\" + text + \"}\"  # 1\n\n        # not a dictionary\n        return str(obj)\n\n    return f\"{type(self).__name__}({_to_str(self._describe(), True)})\"\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset._copy","title":"_copy","text":"<pre><code>_copy(**overwrite_params)\n</code></pre> Source code in <code>kedro/io/core.py</code> <pre><code>def _copy(self, **overwrite_params: Any) -&gt; AbstractDataset:\n    dataset_copy = copy.deepcopy(self)\n    for name, value in overwrite_params.items():\n        setattr(dataset_copy, name, value)\n    return dataset_copy\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset._describe","title":"_describe  <code>abstractmethod</code>","text":"<pre><code>_describe()\n</code></pre> Source code in <code>kedro/io/core.py</code> <pre><code>@abc.abstractmethod\ndef _describe(self) -&gt; dict[str, Any]:\n    raise NotImplementedError(\n        f\"'{self.__class__.__name__}' is a subclass of AbstractDataset and \"\n        f\"it must implement the '_describe' method\"\n    )\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset._exists","title":"_exists","text":"<pre><code>_exists()\n</code></pre> Source code in <code>kedro/io/core.py</code> <pre><code>def _exists(self) -&gt; bool:\n    self._logger.warning(\n        \"'exists()' not implemented for '%s'. Assuming output does not exist.\",\n        self.__class__.__name__,\n    )\n    return False\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset._load_wrapper","title":"_load_wrapper  <code>classmethod</code>","text":"<pre><code>_load_wrapper(load_func)\n</code></pre> <p>Decorate <code>load_func</code> with logging and error handling code.</p> Source code in <code>kedro/io/core.py</code> <pre><code>@classmethod\ndef _load_wrapper(cls, load_func: Callable[[Self], _DO]) -&gt; Callable[[Self], _DO]:\n    \"\"\"Decorate `load_func` with logging and error handling code.\"\"\"\n\n    @wraps(load_func)\n    def load(self: Self) -&gt; _DO:\n        self._logger.debug(\"Loading %s\", str(self))\n\n        try:\n            return load_func(self)\n        except DatasetError:\n            raise\n        except Exception as exc:\n            # This exception handling is by design as the composed datasets\n            # can throw any type of exception.\n            message = f\"Failed while loading data from dataset {self!s}.\\n{exc!s}\"\n            raise DatasetError(message) from exc\n\n    load.__annotations__[\"return\"] = load_func.__annotations__.get(\"return\")\n    load.__loadwrapped__ = True  # type: ignore[attr-defined]\n    return load\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset._pretty_repr","title":"_pretty_repr","text":"<pre><code>_pretty_repr(object_description)\n</code></pre> Source code in <code>kedro/io/core.py</code> <pre><code>def _pretty_repr(self, object_description: dict[str, Any]) -&gt; str:\n    str_keys = []\n    for arg_name, arg_descr in object_description.items():\n        if arg_descr is not None:\n            descr = pprint.pformat(\n                arg_descr,\n                sort_dicts=False,\n                compact=True,\n                depth=2,\n                width=sys.maxsize,\n            )\n            str_keys.append(f\"{arg_name}={descr}\")\n\n    return f\"{type(self).__module__}.{type(self).__name__}({', '.join(str_keys)})\"\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset._release","title":"_release","text":"<pre><code>_release()\n</code></pre> Source code in <code>kedro/io/core.py</code> <pre><code>def _release(self) -&gt; None:\n    pass\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset._save_wrapper","title":"_save_wrapper  <code>classmethod</code>","text":"<pre><code>_save_wrapper(save_func)\n</code></pre> <p>Decorate <code>save_func</code> with logging and error handling code.</p> Source code in <code>kedro/io/core.py</code> <pre><code>@classmethod\ndef _save_wrapper(\n    cls, save_func: Callable[[Self, _DI], None]\n) -&gt; Callable[[Self, _DI], None]:\n    \"\"\"Decorate `save_func` with logging and error handling code.\"\"\"\n\n    @wraps(save_func)\n    def save(self: Self, data: _DI) -&gt; None:\n        if data is None:\n            raise DatasetError(\"Saving 'None' to a 'Dataset' is not allowed\")\n\n        try:\n            self._logger.debug(\"Saving %s\", str(self))\n            save_func(self, data)\n        except (DatasetError, FileNotFoundError, NotADirectoryError):\n            raise\n        except Exception as exc:\n            message = f\"Failed while saving data to dataset {self!s}.\\n{exc!s}\"\n            raise DatasetError(message) from exc\n\n    save.__annotations__[\"data\"] = save_func.__annotations__.get(\"data\", Any)\n    save.__annotations__[\"return\"] = save_func.__annotations__.get(\"return\")\n    save.__savewrapped__ = True  # type: ignore[attr-defined]\n    return save\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset.exists","title":"exists","text":"<pre><code>exists()\n</code></pre> <p>Checks whether a dataset's output already exists by calling the provided _exists() method.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>Flag indicating whether the output already exists.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>when underlying exists method raises error.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Checks whether a dataset's output already exists by calling\n    the provided _exists() method.\n\n    Returns:\n        Flag indicating whether the output already exists.\n\n    Raises:\n        DatasetError: when underlying exists method raises error.\n\n    \"\"\"\n    try:\n        self._logger.debug(\"Checking whether target of %s exists\", str(self))\n        return self._exists()\n    except Exception as exc:\n        message = f\"Failed during exists check for dataset {self!s}.\\n{exc!s}\"\n        raise DatasetError(message) from exc\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(name, config, load_version=None, save_version=None)\n</code></pre> <p>Create a dataset instance using the configuration provided.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Data set name.</p> </li> <li> <code>config</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Data set config dictionary.</p> </li> <li> <code>load_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Version string to be used for <code>load</code> operation if the dataset is versioned. Has no effect on the dataset if versioning was not enabled.</p> </li> <li> <code>save_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Version string to be used for <code>save</code> operation if the dataset is versioned. Has no effect on the dataset if versioning was not enabled.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractDataset</code>           \u2013            <p>An instance of an <code>AbstractDataset</code> subclass.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>When the function fails to create the dataset from its config.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>@classmethod\ndef from_config(\n    cls: type,\n    name: str,\n    config: dict[str, Any],\n    load_version: str | None = None,\n    save_version: str | None = None,\n) -&gt; AbstractDataset:\n    \"\"\"Create a dataset instance using the configuration provided.\n\n    Args:\n        name: Data set name.\n        config: Data set config dictionary.\n        load_version: Version string to be used for ``load`` operation if\n            the dataset is versioned. Has no effect on the dataset\n            if versioning was not enabled.\n        save_version: Version string to be used for ``save`` operation if\n            the dataset is versioned. Has no effect on the dataset\n            if versioning was not enabled.\n\n    Returns:\n        An instance of an ``AbstractDataset`` subclass.\n\n    Raises:\n        DatasetError: When the function fails to create the dataset\n            from its config.\n\n    \"\"\"\n    try:\n        class_obj, config = parse_dataset_definition(\n            config, load_version, save_version\n        )\n    except Exception as exc:\n        raise DatasetError(\n            f\"An exception occurred when parsing config \"\n            f\"for dataset '{name}':\\n{exc!s}\"\n        ) from exc\n\n    try:\n        dataset = class_obj(**config)\n    except TypeError as err:\n        raise DatasetError(\n            f\"\\n{err}.\\nDataset '{name}' must only contain arguments valid for the \"\n            f\"constructor of '{class_obj.__module__}.{class_obj.__qualname__}'.\"\n        ) from err\n    except Exception as err:\n        raise DatasetError(\n            f\"\\n{err}.\\nFailed to instantiate dataset '{name}' \"\n            f\"of type '{class_obj.__module__}.{class_obj.__qualname__}'.\"\n        ) from err\n    return dataset\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load()\n</code></pre> <p>Loads data by delegation to the provided load method.</p> <p>Returns:</p> <ul> <li> <code>_DO</code>           \u2013            <p>Data returned by the provided load method.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>When underlying load method raises error.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>@abc.abstractmethod\ndef load(self) -&gt; _DO:\n    \"\"\"Loads data by delegation to the provided load method.\n\n    Returns:\n        Data returned by the provided load method.\n\n    Raises:\n        DatasetError: When underlying load method raises error.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"'{self.__class__.__name__}' is a subclass of AbstractDataset and \"\n        f\"it must implement the 'load' method\"\n    )\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset.release","title":"release","text":"<pre><code>release()\n</code></pre> <p>Release any cached data.</p> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>when underlying release method raises error.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>def release(self) -&gt; None:\n    \"\"\"Release any cached data.\n\n    Raises:\n        DatasetError: when underlying release method raises error.\n\n    \"\"\"\n    try:\n        self._logger.debug(\"Releasing %s\", str(self))\n        self._release()\n    except Exception as exc:\n        message = f\"Failed during release for dataset {self!s}.\\n{exc!s}\"\n        raise DatasetError(message) from exc\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(data)\n</code></pre> <p>Saves data by delegation to the provided save method.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>_DI</code>)           \u2013            <p>the value to be saved by provided save method.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>when underlying save method raises error.</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>when save method got file instead of dir, on Windows.</p> </li> <li> <code>NotADirectoryError</code>             \u2013            <p>when save method got file instead of dir, on Unix.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>@abc.abstractmethod\ndef save(self, data: _DI) -&gt; None:\n    \"\"\"Saves data by delegation to the provided save method.\n\n    Args:\n        data: the value to be saved by provided save method.\n\n    Raises:\n        DatasetError: when underlying save method raises error.\n        FileNotFoundError: when save method got file instead of dir, on Windows.\n        NotADirectoryError: when save method got file instead of dir, on Unix.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"'{self.__class__.__name__}' is a subclass of AbstractDataset and \"\n        f\"it must implement the 'save' method\"\n    )\n</code></pre>"},{"location":"api/io/kedro.io.AbstractDataset/#kedro.io.AbstractDataset.to_config","title":"to_config","text":"<pre><code>to_config()\n</code></pre> <p>Converts the dataset instance into a dictionary-based configuration for serialization. Ensures that any subclass-specific details are handled, with additional logic for versioning and caching implemented for <code>CachedDataset</code>.</p> <p>Adds a key for the dataset's type using its module and class name and includes the initialization arguments.</p> <p>For <code>CachedDataset</code> it extracts the underlying dataset's configuration, handles the <code>versioned</code> flag and removes unnecessary metadata. It also ensures the embedded dataset's configuration is appropriately flattened or transformed.</p> <p>If the dataset has a version key, it sets the <code>versioned</code> flag in the configuration.</p> <p>Removes the <code>metadata</code> key from the configuration if present.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>A dictionary containing the dataset's type and initialization arguments.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>def to_config(self) -&gt; dict[str, Any]:\n    \"\"\"Converts the dataset instance into a dictionary-based configuration for\n    serialization. Ensures that any subclass-specific details are handled, with\n    additional logic for versioning and caching implemented for `CachedDataset`.\n\n    Adds a key for the dataset's type using its module and class name and\n    includes the initialization arguments.\n\n    For `CachedDataset` it extracts the underlying dataset's configuration,\n    handles the `versioned` flag and removes unnecessary metadata. It also\n    ensures the embedded dataset's configuration is appropriately flattened\n    or transformed.\n\n    If the dataset has a version key, it sets the `versioned` flag in the\n    configuration.\n\n    Removes the `metadata` key from the configuration if present.\n\n    Returns:\n        A dictionary containing the dataset's type and initialization arguments.\n    \"\"\"\n    return_config: dict[str, Any] = {\n        f\"{TYPE_KEY}\": f\"{type(self).__module__}.{type(self).__name__}\"\n    }\n\n    if self._init_args:  # type: ignore[attr-defined]\n        self._init_args.pop(\"self\", None)  # type: ignore[attr-defined]\n        return_config.update(self._init_args)  # type: ignore[attr-defined]\n\n    if type(self).__name__ == \"CachedDataset\":\n        cached_ds = return_config.pop(\"dataset\")\n        cached_ds_return_config: dict[str, Any] = {}\n        if isinstance(cached_ds, dict):\n            cached_ds_return_config = cached_ds\n        elif isinstance(cached_ds, AbstractDataset):\n            cached_ds_return_config = cached_ds.to_config()\n        if VERSIONED_FLAG_KEY in cached_ds_return_config:\n            return_config[VERSIONED_FLAG_KEY] = cached_ds_return_config.pop(\n                VERSIONED_FLAG_KEY\n            )\n        # Pop metadata from configuration\n        cached_ds_return_config.pop(\"metadata\", None)\n        return_config[\"dataset\"] = cached_ds_return_config\n\n    # Set `versioned` key if version present in the dataset\n    if return_config.pop(VERSION_KEY, None):\n        return_config[VERSIONED_FLAG_KEY] = True\n\n    # Pop metadata from configuration\n    return_config.pop(\"metadata\", None)\n\n    return return_config\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/","title":"AbstractVersionedDataset","text":""},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset","title":"kedro.io.AbstractVersionedDataset","text":"<pre><code>AbstractVersionedDataset(filepath, version, exists_function=None, glob_function=None)\n</code></pre> <p>               Bases: <code>AbstractDataset[_DI, _DO]</code>, <code>ABC</code></p> <p><code>AbstractVersionedDataset</code> is the base class for all versioned dataset implementations.</p> <p>All datasets that implement versioning should extend this abstract class and implement the methods marked as abstract.</p> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from pathlib import Path, PurePosixPath\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from kedro.io import AbstractVersionedDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; class MyOwnDataset(AbstractVersionedDataset):\n&gt;&gt;&gt;     def __init__(self, filepath, version, param1, param2=True):\n&gt;&gt;&gt;         super().__init__(PurePosixPath(filepath), version)\n&gt;&gt;&gt;         self._param1 = param1\n&gt;&gt;&gt;         self._param2 = param2\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def load(self) -&gt; pd.DataFrame:\n&gt;&gt;&gt;         load_path = self._get_load_path()\n&gt;&gt;&gt;         return pd.read_csv(load_path)\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def save(self, df: pd.DataFrame) -&gt; None:\n&gt;&gt;&gt;         save_path = self._get_save_path()\n&gt;&gt;&gt;         df.to_csv(str(save_path))\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _exists(self) -&gt; bool:\n&gt;&gt;&gt;         path = self._get_load_path()\n&gt;&gt;&gt;         return Path(path.as_posix()).exists()\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _describe(self):\n&gt;&gt;&gt;         return dict(version=self._version, param1=self._param1, param2=self._param2)\n</code></pre> <p>Example catalog.yml specification: ::</p> <pre><code>my_dataset:\n    type: &lt;path-to-my-own-dataset&gt;.MyOwnDataset\n    filepath: data/01_raw/my_data.csv\n    versioned: true\n    param1: &lt;param1-value&gt; # param1 is a required argument\n    # param2 will be True by default\n</code></pre> <p>Parameters:</p> <ul> <li> <code>filepath</code>               (<code>PurePosixPath</code>)           \u2013            <p>Filepath in POSIX format to a file.</p> </li> <li> <code>version</code>               (<code>Version | None</code>)           \u2013            <p>If specified, should be an instance of <code>kedro.io.core.Version</code>. If its <code>load</code> attribute is None, the latest version will be loaded. If its <code>save</code> attribute is None, save version will be autogenerated.</p> </li> <li> <code>exists_function</code>               (<code>Callable[[str], bool] | None</code>, default:                   <code>None</code> )           \u2013            <p>Function that is used for determining whether a path exists in a filesystem.</p> </li> <li> <code>glob_function</code>               (<code>Callable[[str], list[str]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Function that is used for finding all paths in a filesystem, which match a given pattern.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>def __init__(\n    self,\n    filepath: PurePosixPath,\n    version: Version | None,\n    exists_function: Callable[[str], bool] | None = None,\n    glob_function: Callable[[str], list[str]] | None = None,\n):\n    \"\"\"Creates a new instance of ``AbstractVersionedDataset``.\n\n    Args:\n        filepath: Filepath in POSIX format to a file.\n        version: If specified, should be an instance of\n            ``kedro.io.core.Version``. If its ``load`` attribute is\n            None, the latest version will be loaded. If its ``save``\n            attribute is None, save version will be autogenerated.\n        exists_function: Function that is used for determining whether\n            a path exists in a filesystem.\n        glob_function: Function that is used for finding all paths\n            in a filesystem, which match a given pattern.\n    \"\"\"\n    self._filepath = filepath\n    self._version = version\n    self._exists_function = exists_function or _local_exists\n    self._glob_function = glob_function or iglob\n    # 1 entry for load version, 1 for save version\n    self._version_cache = Cache(maxsize=2)  # type: Cache\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset._exists_function","title":"_exists_function  <code>instance-attribute</code>","text":"<pre><code>_exists_function = exists_function or _local_exists\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset._filepath","title":"_filepath  <code>instance-attribute</code>","text":"<pre><code>_filepath = filepath\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset._glob_function","title":"_glob_function  <code>instance-attribute</code>","text":"<pre><code>_glob_function = glob_function or iglob\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset._version","title":"_version  <code>instance-attribute</code>","text":"<pre><code>_version = version\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset._version_cache","title":"_version_cache  <code>instance-attribute</code>","text":"<pre><code>_version_cache = Cache(maxsize=2)\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset._fetch_latest_load_version","title":"_fetch_latest_load_version","text":"<pre><code>_fetch_latest_load_version()\n</code></pre> Source code in <code>kedro/io/core.py</code> <pre><code>@cachedmethod(cache=attrgetter(\"_version_cache\"), key=partial(hashkey, \"load\"))\ndef _fetch_latest_load_version(self) -&gt; str:\n    # When load version is unpinned, fetch the most recent existing\n    # version from the given path.\n    pattern = str(self._get_versioned_path(\"*\"))\n    try:\n        version_paths = sorted(self._glob_function(pattern), reverse=True)\n    except Exception as exc:\n        message = (\n            f\"Did not find any versions for {self}. This could be \"\n            f\"due to insufficient permission. Exception: {exc}\"\n        )\n        raise VersionNotFoundError(message) from exc\n    most_recent = next(\n        (path for path in version_paths if self._exists_function(path)), None\n    )\n    if not most_recent:\n        message = f\"Did not find any versions for {self}\"\n        raise VersionNotFoundError(message)\n    return PurePath(most_recent).parent.name\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset._fetch_latest_save_version","title":"_fetch_latest_save_version","text":"<pre><code>_fetch_latest_save_version()\n</code></pre> <p>Generate and cache the current save version</p> Source code in <code>kedro/io/core.py</code> <pre><code>@cachedmethod(cache=attrgetter(\"_version_cache\"), key=partial(hashkey, \"save\"))\ndef _fetch_latest_save_version(self) -&gt; str:\n    \"\"\"Generate and cache the current save version\"\"\"\n    return generate_timestamp()\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset._get_load_path","title":"_get_load_path","text":"<pre><code>_get_load_path()\n</code></pre> Source code in <code>kedro/io/core.py</code> <pre><code>def _get_load_path(self) -&gt; PurePosixPath:\n    if not self._version:\n        # When versioning is disabled, load from original filepath\n        return self._filepath\n\n    load_version = self.resolve_load_version()\n    return self._get_versioned_path(load_version)  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset._get_save_path","title":"_get_save_path","text":"<pre><code>_get_save_path()\n</code></pre> Source code in <code>kedro/io/core.py</code> <pre><code>def _get_save_path(self) -&gt; PurePosixPath:\n    if not self._version:\n        # When versioning is disabled, return original filepath\n        return self._filepath\n\n    save_version = self.resolve_save_version()\n    versioned_path = self._get_versioned_path(save_version)  # type: ignore[arg-type]\n\n    if self._exists_function(str(versioned_path)):\n        raise DatasetError(\n            f\"Save path '{versioned_path}' for {self!s} must not exist if \"\n            f\"versioning is enabled.\"\n        )\n\n    return versioned_path\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset._get_versioned_path","title":"_get_versioned_path","text":"<pre><code>_get_versioned_path(version)\n</code></pre> Source code in <code>kedro/io/core.py</code> <pre><code>def _get_versioned_path(self, version: str) -&gt; PurePosixPath:\n    return self._filepath / version / self._filepath.name\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset._release","title":"_release","text":"<pre><code>_release()\n</code></pre> Source code in <code>kedro/io/core.py</code> <pre><code>def _release(self) -&gt; None:\n    super()._release()\n    self._version_cache.clear()\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset._save_wrapper","title":"_save_wrapper  <code>classmethod</code>","text":"<pre><code>_save_wrapper(save_func)\n</code></pre> <p>Decorate <code>save_func</code> with logging and error handling code.</p> Source code in <code>kedro/io/core.py</code> <pre><code>@classmethod\ndef _save_wrapper(\n    cls, save_func: Callable[[Self, _DI], None]\n) -&gt; Callable[[Self, _DI], None]:\n    \"\"\"Decorate `save_func` with logging and error handling code.\"\"\"\n\n    @wraps(save_func)\n    def save(self: Self, data: _DI) -&gt; None:\n        self._version_cache.clear()\n        save_version = (\n            self.resolve_save_version()\n        )  # Make sure last save version is set\n        try:\n            super()._save_wrapper(save_func)(self, data)\n        except (FileNotFoundError, NotADirectoryError) as err:\n            # FileNotFoundError raised in Win, NotADirectoryError raised in Unix\n            _default_version = \"YYYY-MM-DDThh.mm.ss.sssZ\"\n            raise DatasetError(\n                f\"Cannot save versioned dataset '{self._filepath.name}' to \"\n                f\"'{self._filepath.parent.as_posix()}' because a file with the same \"\n                f\"name already exists in the directory. This is likely because \"\n                f\"versioning was enabled on a dataset already saved previously. Either \"\n                f\"remove '{self._filepath.name}' from the directory or manually \"\n                f\"convert it into a versioned dataset by placing it in a versioned \"\n                f\"directory (e.g. with default versioning format \"\n                f\"'{self._filepath.as_posix()}/{_default_version}/{self._filepath.name}\"\n                f\"').\"\n            ) from err\n\n        load_version = self.resolve_load_version()\n        if load_version != save_version:\n            warnings.warn(\n                _CONSISTENCY_WARNING.format(save_version, load_version, str(self))\n            )\n            self._version_cache.clear()\n\n    return save\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset.exists","title":"exists","text":"<pre><code>exists()\n</code></pre> <p>Checks whether a dataset's output already exists by calling the provided _exists() method.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>Flag indicating whether the output already exists.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>when underlying exists method raises error.</p> </li> </ul> Source code in <code>kedro/io/core.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Checks whether a dataset's output already exists by calling\n    the provided _exists() method.\n\n    Returns:\n        Flag indicating whether the output already exists.\n\n    Raises:\n        DatasetError: when underlying exists method raises error.\n\n    \"\"\"\n    self._logger.debug(\"Checking whether target of %s exists\", str(self))\n    try:\n        return self._exists()\n    except VersionNotFoundError:\n        return False\n    except Exception as exc:  # SKIP_IF_NO_SPARK\n        message = f\"Failed during exists check for dataset {self!s}.\\n{exc!s}\"\n        raise DatasetError(message) from exc\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset.resolve_load_version","title":"resolve_load_version","text":"<pre><code>resolve_load_version()\n</code></pre> <p>Compute the version the dataset should be loaded with.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def resolve_load_version(self) -&gt; str | None:\n    \"\"\"Compute the version the dataset should be loaded with.\"\"\"\n    if not self._version:\n        return None\n    if self._version.load:\n        return self._version.load  # type: ignore[no-any-return]\n    return self._fetch_latest_load_version()\n</code></pre>"},{"location":"api/io/kedro.io.AbstractVersionedDataset/#kedro.io.AbstractVersionedDataset.resolve_save_version","title":"resolve_save_version","text":"<pre><code>resolve_save_version()\n</code></pre> <p>Compute the version the dataset should be saved with.</p> Source code in <code>kedro/io/core.py</code> <pre><code>def resolve_save_version(self) -&gt; str | None:\n    \"\"\"Compute the version the dataset should be saved with.\"\"\"\n    if not self._version:\n        return None\n    if self._version.save:\n        return self._version.save  # type: ignore[no-any-return]\n    return self._fetch_latest_save_version()\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/","title":"CachedDataset","text":""},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset","title":"kedro.io.CachedDataset","text":"<pre><code>CachedDataset(dataset, version=None, copy_mode=None, metadata=None)\n</code></pre> <p>               Bases: <code>AbstractDataset</code></p> <p><code>CachedDataset</code> is a dataset wrapper which caches in memory the data saved, so that the user avoids io operations with slow storage media.</p> <p>You can also specify a <code>CachedDataset</code> in catalog.yml: ::</p> <pre><code>&gt;&gt;&gt; test_ds:\n&gt;&gt;&gt;    type: CachedDataset\n&gt;&gt;&gt;    versioned: true\n&gt;&gt;&gt;    dataset:\n&gt;&gt;&gt;       type: pandas.CSVDataset\n&gt;&gt;&gt;       filepath: example.csv\n</code></pre> <p>Please note that if your dataset is versioned, this should be indicated in the wrapper class as shown above.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>AbstractDataset | dict</code>)           \u2013            <p>A Kedro Dataset object or a dictionary to cache.</p> </li> <li> <code>version</code>               (<code>Version | None</code>, default:                   <code>None</code> )           \u2013            <p>If specified, should be an instance of <code>kedro.io.core.Version</code>. If its <code>load</code> attribute is None, the latest version will be loaded. If its <code>save</code> attribute is None, save version will be autogenerated.</p> </li> <li> <code>copy_mode</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The copy mode used to copy the data. Possible values are: \"deepcopy\", \"copy\" and \"assign\". If not provided, it is inferred based on the data type.</p> </li> <li> <code>metadata</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Any arbitrary metadata. This is ignored by Kedro, but may be consumed by users or external plugins.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the provided dataset is not a valid dict/YAML representation of a dataset or an actual dataset.</p> </li> </ul> Source code in <code>kedro/io/cached_dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset: AbstractDataset | dict,\n    version: Version | None = None,\n    copy_mode: str | None = None,\n    metadata: dict[str, Any] | None = None,\n):\n    \"\"\"Creates a new instance of ``CachedDataset`` pointing to the\n    provided Python object.\n\n    Args:\n        dataset: A Kedro Dataset object or a dictionary to cache.\n        version: If specified, should be an instance of\n            ``kedro.io.core.Version``. If its ``load`` attribute is\n            None, the latest version will be loaded. If its ``save``\n            attribute is None, save version will be autogenerated.\n        copy_mode: The copy mode used to copy the data. Possible\n            values are: \"deepcopy\", \"copy\" and \"assign\". If not\n            provided, it is inferred based on the data type.\n        metadata: Any arbitrary metadata.\n            This is ignored by Kedro, but may be consumed by users or external plugins.\n\n    Raises:\n        ValueError: If the provided dataset is not a valid dict/YAML\n            representation of a dataset or an actual dataset.\n    \"\"\"\n    self._EPHEMERAL = True\n\n    if isinstance(dataset, dict):\n        self._dataset = self._from_config(dataset, version)\n    elif isinstance(dataset, AbstractDataset):\n        self._dataset = dataset\n    else:\n        raise ValueError(\n            \"The argument type of 'dataset' should be either a dict/YAML \"\n            \"representation of the dataset, or the actual dataset object.\"\n        )\n    self._cache = MemoryDataset(copy_mode=copy_mode)  # type: ignore[abstract]\n    self.metadata = metadata\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset._EPHEMERAL","title":"_EPHEMERAL  <code>instance-attribute</code>","text":"<pre><code>_EPHEMERAL = True\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset._SINGLE_PROCESS","title":"_SINGLE_PROCESS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_SINGLE_PROCESS = True\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset._cache","title":"_cache  <code>instance-attribute</code>","text":"<pre><code>_cache = MemoryDataset(copy_mode=copy_mode)\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset._dataset","title":"_dataset  <code>instance-attribute</code>","text":"<pre><code>_dataset = _from_config(dataset, version)\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata = metadata\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset.__getstate__","title":"__getstate__","text":"<pre><code>__getstate__()\n</code></pre> Source code in <code>kedro/io/cached_dataset.py</code> <pre><code>def __getstate__(self) -&gt; dict[str, Any]:\n    # clearing the cache can be prevented by modifying\n    # how parallel runner handles datasets (not trivial!)\n    logging.getLogger(__name__).warning(\"%s: clearing cache to pickle.\", str(self))\n    self._cache.release()\n    return self.__dict__\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>kedro/io/cached_dataset.py</code> <pre><code>def __repr__(self) -&gt; str:\n    object_description = {\n        \"dataset\": self._dataset._pretty_repr(self._dataset._describe()),\n        \"cache\": self._dataset._pretty_repr(self._cache._describe()),\n    }\n    return self._pretty_repr(object_description)\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset._describe","title":"_describe","text":"<pre><code>_describe()\n</code></pre> Source code in <code>kedro/io/cached_dataset.py</code> <pre><code>def _describe(self) -&gt; dict[str, Any]:\n    return {\"dataset\": self._dataset._describe(), \"cache\": self._cache._describe()}\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset._exists","title":"_exists","text":"<pre><code>_exists()\n</code></pre> Source code in <code>kedro/io/cached_dataset.py</code> <pre><code>def _exists(self) -&gt; bool:\n    return self._cache.exists() or self._dataset.exists()\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset._from_config","title":"_from_config  <code>staticmethod</code>","text":"<pre><code>_from_config(config, version)\n</code></pre> Source code in <code>kedro/io/cached_dataset.py</code> <pre><code>@staticmethod\ndef _from_config(config: dict, version: Version | None) -&gt; AbstractDataset:\n    if VERSIONED_FLAG_KEY in config:\n        raise ValueError(\n            \"Cached datasets should specify that they are versioned in the \"\n            \"'CachedDataset', not in the wrapped dataset.\"\n        )\n    if version:\n        config[VERSIONED_FLAG_KEY] = True\n        return AbstractDataset.from_config(\n            \"_cached\", config, version.load, version.save\n        )\n    return AbstractDataset.from_config(\"_cached\", config)\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset._release","title":"_release","text":"<pre><code>_release()\n</code></pre> Source code in <code>kedro/io/cached_dataset.py</code> <pre><code>def _release(self) -&gt; None:\n    self._cache.release()\n    self._dataset.release()\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset.load","title":"load","text":"<pre><code>load()\n</code></pre> Source code in <code>kedro/io/cached_dataset.py</code> <pre><code>def load(self) -&gt; Any:\n    data = self._cache.load() if self._cache.exists() else self._dataset.load()\n\n    if not self._cache.exists():\n        self._cache.save(data)\n\n    return data\n</code></pre>"},{"location":"api/io/kedro.io.CachedDataset/#kedro.io.CachedDataset.save","title":"save","text":"<pre><code>save(data)\n</code></pre> Source code in <code>kedro/io/cached_dataset.py</code> <pre><code>def save(self, data: Any) -&gt; None:\n    self._dataset.save(data)\n    self._cache.save(data)\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/","title":"DataCatalog","text":""},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog","title":"kedro.io.DataCatalog","text":"<pre><code>DataCatalog(datasets=None, feed_dict=None, dataset_patterns=None, load_versions=None, save_version=None, default_pattern=None, config_resolver=None)\n</code></pre> <p><code>DataCatalog</code> stores instances of <code>AbstractDataset</code> implementations to provide <code>load</code> and <code>save</code> capabilities from anywhere in the program. To use a <code>DataCatalog</code>, you need to instantiate it with a dictionary of datasets. Then it will act as a single point of reference for your calls, relaying load and save functions to the underlying datasets.</p> <p>anywhere in the program. To use a <code>DataCatalog</code>, you need to instantiate it with a dictionary of datasets. Then it will act as a single point of reference for your calls, relaying load and save functions to the underlying datasets.</p> <p>Parameters:</p> <ul> <li> <code>datasets</code>               (<code>dict[str, AbstractDataset] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of dataset names and dataset instances.</p> </li> <li> <code>feed_dict</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>A feed dict with data to be added in memory.</p> </li> <li> <code>dataset_patterns</code>               (<code>Patterns | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of dataset factory patterns and corresponding dataset configuration. When fetched from catalog configuration these patterns will be sorted by: 1. Decreasing specificity (number of characters outside the curly brackets) 2. Decreasing number of placeholders (number of curly bracket pairs) 3. Alphabetically A pattern of specificity 0 is a catch-all pattern and will overwrite the default pattern provided through the runners if it comes before \"default\" in the alphabet. Such an overwriting pattern will emit a warning. The <code>\"{default}\"</code> name will not emit a warning.</p> </li> <li> <code>load_versions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A mapping between dataset names and versions to load. Has no effect on datasets without enabled versioning.</p> </li> <li> <code>save_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Version string to be used for <code>save</code> operations by all datasets with enabled versioning. It must: a) be a case-insensitive string that conforms with operating system filename limitations, b) always return the latest version when sorted in lexicographical order.</p> </li> <li> <code>default_pattern</code>               (<code>Patterns | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of the default catch-all pattern that overrides the default pattern provided through the runners.</p> </li> <li> <code>config_resolver</code>               (<code>CatalogConfigResolver | None</code>, default:                   <code>None</code> )           \u2013            <p>An instance of CatalogConfigResolver to resolve dataset patterns and configurations.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt; cars = CSVDataset(filepath=\"cars.csv\",\n&gt;&gt;&gt;                   load_args=None,\n&gt;&gt;&gt;                   save_args={\"index\": False})\n&gt;&gt;&gt; catalog = DataCatalog(datasets={'cars': cars})\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    datasets: dict[str, AbstractDataset] | None = None,\n    feed_dict: dict[str, Any] | None = None,\n    dataset_patterns: Patterns | None = None,  # Kept for interface compatibility\n    load_versions: dict[str, str] | None = None,\n    save_version: str | None = None,\n    default_pattern: Patterns | None = None,  # Kept for interface compatibility\n    config_resolver: CatalogConfigResolver | None = None,\n) -&gt; None:\n    \"\"\"``DataCatalog`` stores instances of ``AbstractDataset``\n    implementations to provide ``load`` and ``save`` capabilities from\n    anywhere in the program. To use a ``DataCatalog``, you need to\n    instantiate it with a dictionary of datasets. Then it will act as a\n    single point of reference for your calls, relaying load and save\n    functions to the underlying datasets.\n\n    Args:\n        datasets: A dictionary of dataset names and dataset instances.\n        feed_dict: A feed dict with data to be added in memory.\n        dataset_patterns: A dictionary of dataset factory patterns\n            and corresponding dataset configuration. When fetched from catalog configuration\n            these patterns will be sorted by:\n            1. Decreasing specificity (number of characters outside the curly brackets)\n            2. Decreasing number of placeholders (number of curly bracket pairs)\n            3. Alphabetically\n            A pattern of specificity 0 is a catch-all pattern and will overwrite the default\n            pattern provided through the runners if it comes before \"default\" in the alphabet.\n            Such an overwriting pattern will emit a warning. The `\"{default}\"` name will\n            not emit a warning.\n        load_versions: A mapping between dataset names and versions\n            to load. Has no effect on datasets without enabled versioning.\n        save_version: Version string to be used for ``save`` operations\n            by all datasets with enabled versioning. It must: a) be a\n            case-insensitive string that conforms with operating system\n            filename limitations, b) always return the latest version when\n            sorted in lexicographical order.\n        default_pattern: A dictionary of the default catch-all pattern that overrides the default\n            pattern provided through the runners.\n        config_resolver: An instance of CatalogConfigResolver to resolve dataset patterns and configurations.\n\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; cars = CSVDataset(filepath=\"cars.csv\",\n        &gt;&gt;&gt;                   load_args=None,\n        &gt;&gt;&gt;                   save_args={\"index\": False})\n        &gt;&gt;&gt; catalog = DataCatalog(datasets={'cars': cars})\n    \"\"\"\n    warnings.warn(\n        \"`DataCatalog` has been deprecated and will be replaced by `KedroDataCatalog`, in Kedro 1.0.0.\"\n        \"Currently some `KedroDataCatalog` APIs have been retained for compatibility with `DataCatalog`, including \"\n        \"the `datasets` property and the `get_datasets`, `_get_datasets`, `add`,` list`, `add_feed_dict`, \"\n        \"and `shallow_copy` methods. These will be removed or replaced with updated alternatives in Kedro 1.0.0. \"\n        \"For more details, refer to the documentation: \"\n        \"https://docs.kedro.org/en/stable/data/index.html#kedrodatacatalog-experimental-feature\",\n        KedroDeprecationWarning,\n    )\n    self._config_resolver = config_resolver or CatalogConfigResolver()\n    # Kept to avoid breaking changes\n    if not config_resolver:\n        self._config_resolver._dataset_patterns = dataset_patterns or {}\n        self._config_resolver._default_pattern = default_pattern or {}\n\n    self._load_versions, self._save_version = _validate_versions(\n        datasets, load_versions or {}, save_version\n    )\n\n    self._datasets: dict[str, AbstractDataset] = {}\n    self.datasets: _FrozenDatasets | None = None\n\n    self.add_all(datasets or {})\n\n    self._use_rich_markup = _has_rich_handler()\n\n    if feed_dict:\n        self.add_feed_dict(feed_dict)\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog._config_resolver","title":"_config_resolver  <code>instance-attribute</code>","text":"<pre><code>_config_resolver = config_resolver or CatalogConfigResolver()\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog._datasets","title":"_datasets  <code>instance-attribute</code>","text":"<pre><code>_datasets = {}\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog._logger","title":"_logger  <code>property</code>","text":"<pre><code>_logger\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog._use_rich_markup","title":"_use_rich_markup  <code>instance-attribute</code>","text":"<pre><code>_use_rich_markup = _has_rich_handler()\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.config_resolver","title":"config_resolver  <code>property</code>","text":"<pre><code>config_resolver\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.datasets","title":"datasets  <code>instance-attribute</code>","text":"<pre><code>datasets = None\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.__contains__","title":"__contains__","text":"<pre><code>__contains__(dataset_name)\n</code></pre> <p>Check if an item is in the catalog as a materialised dataset or pattern</p> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def __contains__(self, dataset_name: str) -&gt; bool:\n    \"\"\"Check if an item is in the catalog as a materialised dataset or pattern\"\"\"\n    return (\n        dataset_name in self._datasets\n        or self._config_resolver.match_pattern(dataset_name) is not None\n    )\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.__eq__","title":"__eq__","text":"<pre><code>__eq__(other)\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def __eq__(self, other) -&gt; bool:  # type: ignore[no-untyped-def]\n    return (self._datasets, self._config_resolver.list_patterns()) == (\n        other._datasets,\n        other.config_resolver.list_patterns(),\n    )\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return self.datasets.__repr__()\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog._get_dataset","title":"_get_dataset","text":"<pre><code>_get_dataset(dataset_name, version=None, suggest=True)\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def _get_dataset(\n    self,\n    dataset_name: str,\n    version: Version | None = None,\n    suggest: bool = True,\n) -&gt; AbstractDataset:\n    ds_config = self._config_resolver.resolve_pattern(dataset_name)\n\n    if dataset_name not in self._datasets and ds_config:\n        ds = AbstractDataset.from_config(\n            dataset_name,\n            ds_config,\n            self._load_versions.get(dataset_name),\n            self._save_version,\n        )\n        self.add(dataset_name, ds)\n    if dataset_name not in self._datasets:\n        error_msg = f\"Dataset '{dataset_name}' not found in the catalog\"\n\n        # Flag to turn on/off fuzzy-matching which can be time consuming and\n        # slow down plugins like `kedro-viz`\n        if suggest:\n            matches = difflib.get_close_matches(dataset_name, self._datasets.keys())\n            if matches:\n                suggestions = \", \".join(matches)\n                error_msg += f\" - did you mean one of these instead: {suggestions}\"\n        raise DatasetNotFoundError(error_msg)\n\n    dataset = self._datasets[dataset_name]\n\n    if version and isinstance(dataset, AbstractVersionedDataset):\n        # we only want to return a similar-looking dataset,\n        # not modify the one stored in the current catalog\n        dataset = dataset._copy(_version=version)\n\n    return dataset\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.add","title":"add","text":"<pre><code>add(dataset_name, dataset, replace=False)\n</code></pre> <p>Adds a new <code>AbstractDataset</code> object to the <code>DataCatalog</code>.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>A unique dataset name which has not been registered yet.</p> </li> <li> <code>dataset</code>               (<code>AbstractDataset</code>)           \u2013            <p>A dataset object to be associated with the given data set name.</p> </li> <li> <code>replace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Specifies whether to replace an existing dataset with the same name is allowed.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetAlreadyExistsError</code>             \u2013            <p>When a dataset with the same name has already been registered.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt; catalog = DataCatalog(datasets={\n&gt;&gt;&gt;                   'cars': CSVDataset(filepath=\"cars.csv\")\n&gt;&gt;&gt;                  })\n&gt;&gt;&gt;\n&gt;&gt;&gt; catalog.add(\"boats\", CSVDataset(filepath=\"boats.csv\"))\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def add(\n    self,\n    dataset_name: str,\n    dataset: AbstractDataset,\n    replace: bool = False,\n) -&gt; None:\n    \"\"\"Adds a new ``AbstractDataset`` object to the ``DataCatalog``.\n\n    Args:\n        dataset_name: A unique dataset name which has not been\n            registered yet.\n        dataset: A dataset object to be associated with the given data\n            set name.\n        replace: Specifies whether to replace an existing dataset\n            with the same name is allowed.\n\n    Raises:\n        DatasetAlreadyExistsError: When a dataset with the same name\n            has already been registered.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; catalog = DataCatalog(datasets={\n        &gt;&gt;&gt;                   'cars': CSVDataset(filepath=\"cars.csv\")\n        &gt;&gt;&gt;                  })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; catalog.add(\"boats\", CSVDataset(filepath=\"boats.csv\"))\n    \"\"\"\n    if dataset_name in self._datasets:\n        if replace:\n            self._logger.warning(\"Replacing dataset '%s'\", dataset_name)\n        else:\n            raise DatasetAlreadyExistsError(\n                f\"Dataset '{dataset_name}' has already been registered\"\n            )\n    self._load_versions, self._save_version = _validate_versions(\n        {dataset_name: dataset}, self._load_versions, self._save_version\n    )\n    self._datasets[dataset_name] = dataset\n    self.datasets = _FrozenDatasets(self.datasets, {dataset_name: dataset})\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.add_all","title":"add_all","text":"<pre><code>add_all(datasets, replace=False)\n</code></pre> <p>Adds a group of new datasets to the <code>DataCatalog</code>.</p> <p>Parameters:</p> <ul> <li> <code>datasets</code>               (<code>dict[str, AbstractDataset]</code>)           \u2013            <p>A dictionary of dataset names and dataset instances.</p> </li> <li> <code>replace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Specifies whether to replace an existing dataset with the same name is allowed.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetAlreadyExistsError</code>             \u2013            <p>When a dataset with the same name has already been registered.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset, ParquetDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt; catalog = DataCatalog(datasets={\n&gt;&gt;&gt;                   \"cars\": CSVDataset(filepath=\"cars.csv\")\n&gt;&gt;&gt;                  })\n&gt;&gt;&gt; additional = {\n&gt;&gt;&gt;     \"planes\": ParquetDataset(\"planes.parq\"),\n&gt;&gt;&gt;     \"boats\": CSVDataset(filepath=\"boats.csv\")\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; catalog.add_all(additional)\n&gt;&gt;&gt;\n&gt;&gt;&gt; assert catalog.list() == [\"cars\", \"planes\", \"boats\"]\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def add_all(\n    self,\n    datasets: dict[str, AbstractDataset],\n    replace: bool = False,\n) -&gt; None:\n    \"\"\"Adds a group of new datasets to the ``DataCatalog``.\n\n    Args:\n        datasets: A dictionary of dataset names and dataset\n            instances.\n        replace: Specifies whether to replace an existing dataset\n            with the same name is allowed.\n\n    Raises:\n        DatasetAlreadyExistsError: When a dataset with the same name\n            has already been registered.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset, ParquetDataset\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; catalog = DataCatalog(datasets={\n        &gt;&gt;&gt;                   \"cars\": CSVDataset(filepath=\"cars.csv\")\n        &gt;&gt;&gt;                  })\n        &gt;&gt;&gt; additional = {\n        &gt;&gt;&gt;     \"planes\": ParquetDataset(\"planes.parq\"),\n        &gt;&gt;&gt;     \"boats\": CSVDataset(filepath=\"boats.csv\")\n        &gt;&gt;&gt; }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; catalog.add_all(additional)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; assert catalog.list() == [\"cars\", \"planes\", \"boats\"]\n    \"\"\"\n    for ds_name, ds in datasets.items():\n        self.add(ds_name, ds, replace)\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.add_feed_dict","title":"add_feed_dict","text":"<pre><code>add_feed_dict(feed_dict, replace=False)\n</code></pre> <p>Add datasets to the <code>DataCatalog</code> using the data provided through the <code>feed_dict</code>.</p> <p><code>feed_dict</code> is a dictionary where the keys represent dataset names and the values can either be raw data or Kedro datasets - instances of classes that inherit from <code>AbstractDataset</code>. If raw data is provided, it will be automatically wrapped in a <code>MemoryDataset</code> before being added to the <code>DataCatalog</code>.</p> <p>Parameters:</p> <ul> <li> <code>feed_dict</code>               (<code>dict[str, Any]</code>)           \u2013            <p>A dictionary with data to be added to the <code>DataCatalog</code>. Keys are dataset names and values can be raw data or instances of classes that inherit from <code>AbstractDataset</code>.</p> </li> <li> <code>replace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Specifies whether to replace an existing dataset with the same name in the <code>DataCatalog</code>.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; df = pd.DataFrame({\"col1\": [1, 2],\n&gt;&gt;&gt;                    \"col2\": [4, 5],\n&gt;&gt;&gt;                    \"col3\": [5, 6]})\n&gt;&gt;&gt;\n&gt;&gt;&gt; catalog = DataCatalog()\n&gt;&gt;&gt; catalog.add_feed_dict({\n&gt;&gt;&gt;     \"data_df\": df\n&gt;&gt;&gt; }, replace=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; assert catalog.load(\"data_df\").equals(df)\n&gt;&gt;&gt;\n&gt;&gt;&gt; csv_dataset = CSVDataset(filepath=\"test.csv\")\n&gt;&gt;&gt; csv_dataset.save(df)\n&gt;&gt;&gt; catalog.add_feed_dict({\"data_csv_dataset\": csv_dataset})\n&gt;&gt;&gt;\n&gt;&gt;&gt; assert catalog.load(\"data_csv_dataset\").equals(df)\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def add_feed_dict(self, feed_dict: dict[str, Any], replace: bool = False) -&gt; None:\n    \"\"\"Add datasets to the ``DataCatalog`` using the data provided through the `feed_dict`.\n\n    `feed_dict` is a dictionary where the keys represent dataset names and the values can either be raw data or\n    Kedro datasets - instances of classes that inherit from ``AbstractDataset``. If raw data is provided,\n    it will be automatically wrapped in a ``MemoryDataset`` before being added to the ``DataCatalog``.\n\n    Args:\n        feed_dict: A dictionary with data to be added to the ``DataCatalog``. Keys are dataset names and\n            values can be raw data or instances of classes that inherit from ``AbstractDataset``.\n        replace: Specifies whether to replace an existing dataset with the same name in the ``DataCatalog``.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df = pd.DataFrame({\"col1\": [1, 2],\n        &gt;&gt;&gt;                    \"col2\": [4, 5],\n        &gt;&gt;&gt;                    \"col3\": [5, 6]})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; catalog = DataCatalog()\n        &gt;&gt;&gt; catalog.add_feed_dict({\n        &gt;&gt;&gt;     \"data_df\": df\n        &gt;&gt;&gt; }, replace=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; assert catalog.load(\"data_df\").equals(df)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; csv_dataset = CSVDataset(filepath=\"test.csv\")\n        &gt;&gt;&gt; csv_dataset.save(df)\n        &gt;&gt;&gt; catalog.add_feed_dict({\"data_csv_dataset\": csv_dataset})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; assert catalog.load(\"data_csv_dataset\").equals(df)\n    \"\"\"\n    for ds_name, ds_data in feed_dict.items():\n        dataset = (\n            ds_data\n            if isinstance(ds_data, AbstractDataset)\n            else MemoryDataset(data=ds_data)  # type: ignore[abstract]\n        )\n        self.add(ds_name, dataset, replace)\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.confirm","title":"confirm","text":"<pre><code>confirm(name)\n</code></pre> <p>Confirm a dataset by its name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of the dataset.</p> </li> </ul> <p>Raises:     DatasetError: When the dataset does not have <code>confirm</code> method.</p> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def confirm(self, name: str) -&gt; None:\n    \"\"\"Confirm a dataset by its name.\n\n    Args:\n        name: Name of the dataset.\n    Raises:\n        DatasetError: When the dataset does not have `confirm` method.\n\n    \"\"\"\n    self._logger.info(\"Confirming dataset '%s'\", name)\n    dataset = self._get_dataset(name)\n\n    if hasattr(dataset, \"confirm\"):\n        dataset.confirm()\n    else:\n        raise DatasetError(f\"Dataset '{name}' does not have 'confirm' method\")\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.exists","title":"exists","text":"<pre><code>exists(name)\n</code></pre> <p>Checks whether registered dataset exists by calling its <code>exists()</code> method. Raises a warning and returns False if <code>exists()</code> is not implemented.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>A dataset to be checked.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>Whether the dataset output exists.</p> </li> </ul> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def exists(self, name: str) -&gt; bool:\n    \"\"\"Checks whether registered dataset exists by calling its `exists()`\n    method. Raises a warning and returns False if `exists()` is not\n    implemented.\n\n    Args:\n        name: A dataset to be checked.\n\n    Returns:\n        Whether the dataset output exists.\n\n    \"\"\"\n    try:\n        dataset = self._get_dataset(name)\n    except DatasetNotFoundError:\n        return False\n    return dataset.exists()\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(catalog, credentials=None, load_versions=None, save_version=None)\n</code></pre> <p>Create a <code>DataCatalog</code> instance from configuration. This is a factory method used to provide developers with a way to instantiate <code>DataCatalog</code> with configuration parsed from configuration files.</p> <p>Parameters:</p> <ul> <li> <code>catalog</code>               (<code>dict[str, dict[str, Any]] | None</code>)           \u2013            <p>A dictionary whose keys are the dataset names and the values are dictionaries with the constructor arguments for classes implementing <code>AbstractDataset</code>. The dataset class to be loaded is specified with the key <code>type</code> and their fully qualified class name. All <code>kedro.io</code> dataset can be specified by their class name only, i.e. their module name can be omitted.</p> </li> <li> <code>credentials</code>               (<code>dict[str, dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary containing credentials for different datasets. Use the <code>credentials</code> key in a <code>AbstractDataset</code> to refer to the appropriate credentials as shown in the example below.</p> </li> <li> <code>load_versions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A mapping between dataset names and versions to load. Has no effect on datasets without enabled versioning.</p> </li> <li> <code>save_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Version string to be used for <code>save</code> operations by all datasets with enabled versioning. It must: a) be a case-insensitive string that conforms with operating system filename limitations, b) always return the latest version when sorted in lexicographical order.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataCatalog</code>           \u2013            <p>An instantiated <code>DataCatalog</code> containing all specified</p> </li> <li> <code>DataCatalog</code>           \u2013            <p>datasets, created and ready to use.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>When the method fails to create any of the data sets from their config.</p> </li> <li> <code>DatasetNotFoundError</code>             \u2013            <p>When <code>load_versions</code> refers to a dataset that doesn't exist in the catalog.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; config = {\n&gt;&gt;&gt;     \"cars\": {\n&gt;&gt;&gt;         \"type\": \"pandas.CSVDataset\",\n&gt;&gt;&gt;         \"filepath\": \"cars.csv\",\n&gt;&gt;&gt;         \"save_args\": {\n&gt;&gt;&gt;             \"index\": False\n&gt;&gt;&gt;         }\n&gt;&gt;&gt;     },\n&gt;&gt;&gt;     \"boats\": {\n&gt;&gt;&gt;         \"type\": \"pandas.CSVDataset\",\n&gt;&gt;&gt;         \"filepath\": \"s3://aws-bucket-name/boats.csv\",\n&gt;&gt;&gt;         \"credentials\": \"boats_credentials\",\n&gt;&gt;&gt;         \"save_args\": {\n&gt;&gt;&gt;             \"index\": False\n&gt;&gt;&gt;         }\n&gt;&gt;&gt;     }\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; credentials = {\n&gt;&gt;&gt;     \"boats_credentials\": {\n&gt;&gt;&gt;         \"client_kwargs\": {\n&gt;&gt;&gt;             \"aws_access_key_id\": \"&lt;your key id&gt;\",\n&gt;&gt;&gt;             \"aws_secret_access_key\": \"&lt;your secret&gt;\"\n&gt;&gt;&gt;         }\n&gt;&gt;&gt;      }\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; catalog = DataCatalog.from_config(config, credentials)\n&gt;&gt;&gt;\n&gt;&gt;&gt; df = catalog.load(\"cars\")\n&gt;&gt;&gt; catalog.save(\"boats\", df)\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    catalog: dict[str, dict[str, Any]] | None,\n    credentials: dict[str, dict[str, Any]] | None = None,\n    load_versions: dict[str, str] | None = None,\n    save_version: str | None = None,\n) -&gt; DataCatalog:\n    \"\"\"Create a ``DataCatalog`` instance from configuration. This is a\n    factory method used to provide developers with a way to instantiate\n    ``DataCatalog`` with configuration parsed from configuration files.\n\n    Args:\n        catalog: A dictionary whose keys are the dataset names and\n            the values are dictionaries with the constructor arguments\n            for classes implementing ``AbstractDataset``. The dataset\n            class to be loaded is specified with the key ``type`` and their\n            fully qualified class name. All ``kedro.io`` dataset can be\n            specified by their class name only, i.e. their module name\n            can be omitted.\n        credentials: A dictionary containing credentials for different\n            datasets. Use the ``credentials`` key in a ``AbstractDataset``\n            to refer to the appropriate credentials as shown in the example\n            below.\n        load_versions: A mapping between dataset names and versions\n            to load. Has no effect on datasets without enabled versioning.\n        save_version: Version string to be used for ``save`` operations\n            by all datasets with enabled versioning. It must: a) be a\n            case-insensitive string that conforms with operating system\n            filename limitations, b) always return the latest version when\n            sorted in lexicographical order.\n\n    Returns:\n        An instantiated ``DataCatalog`` containing all specified\n        datasets, created and ready to use.\n\n    Raises:\n        DatasetError: When the method fails to create any of the data\n            sets from their config.\n        DatasetNotFoundError: When `load_versions` refers to a dataset that doesn't\n            exist in the catalog.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; config = {\n        &gt;&gt;&gt;     \"cars\": {\n        &gt;&gt;&gt;         \"type\": \"pandas.CSVDataset\",\n        &gt;&gt;&gt;         \"filepath\": \"cars.csv\",\n        &gt;&gt;&gt;         \"save_args\": {\n        &gt;&gt;&gt;             \"index\": False\n        &gt;&gt;&gt;         }\n        &gt;&gt;&gt;     },\n        &gt;&gt;&gt;     \"boats\": {\n        &gt;&gt;&gt;         \"type\": \"pandas.CSVDataset\",\n        &gt;&gt;&gt;         \"filepath\": \"s3://aws-bucket-name/boats.csv\",\n        &gt;&gt;&gt;         \"credentials\": \"boats_credentials\",\n        &gt;&gt;&gt;         \"save_args\": {\n        &gt;&gt;&gt;             \"index\": False\n        &gt;&gt;&gt;         }\n        &gt;&gt;&gt;     }\n        &gt;&gt;&gt; }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; credentials = {\n        &gt;&gt;&gt;     \"boats_credentials\": {\n        &gt;&gt;&gt;         \"client_kwargs\": {\n        &gt;&gt;&gt;             \"aws_access_key_id\": \"&lt;your key id&gt;\",\n        &gt;&gt;&gt;             \"aws_secret_access_key\": \"&lt;your secret&gt;\"\n        &gt;&gt;&gt;         }\n        &gt;&gt;&gt;      }\n        &gt;&gt;&gt; }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; catalog = DataCatalog.from_config(config, credentials)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df = catalog.load(\"cars\")\n        &gt;&gt;&gt; catalog.save(\"boats\", df)\n    \"\"\"\n    catalog = catalog or {}\n    datasets = {}\n    config_resolver = CatalogConfigResolver(catalog, credentials)\n    save_version = save_version or generate_timestamp()\n    load_versions = load_versions or {}\n\n    for ds_name in catalog:\n        if not config_resolver.is_pattern(ds_name):\n            datasets[ds_name] = AbstractDataset.from_config(\n                ds_name,\n                config_resolver.config.get(ds_name, {}),\n                load_versions.get(ds_name),\n                save_version,\n            )\n\n    missing_keys = [\n        ds_name\n        for ds_name in load_versions\n        if not (\n            ds_name in config_resolver.config\n            or config_resolver.match_pattern(ds_name)\n        )\n    ]\n    if missing_keys:\n        raise DatasetNotFoundError(\n            f\"'load_versions' keys [{', '.join(sorted(missing_keys))}] \"\n            f\"are not found in the catalog.\"\n        )\n\n    return cls(\n        datasets=datasets,\n        dataset_patterns=config_resolver._dataset_patterns,\n        load_versions=load_versions,\n        save_version=save_version,\n        default_pattern=config_resolver._default_pattern,\n        config_resolver=config_resolver,\n    )\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.list","title":"list","text":"<pre><code>list(regex_search=None)\n</code></pre> <p>List of all dataset names registered in the catalog. This can be filtered by providing an optional regular expression which will only return matching keys.</p> <p>Parameters:</p> <ul> <li> <code>regex_search</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional regular expression which can be provided to limit the datasets returned by a particular pattern.</p> </li> </ul> <p>Returns:     A list of dataset names available which match the     <code>regex_search</code> criteria (if provided). All dataset names are returned     by default.</p> <p>Raises:</p> <ul> <li> <code>SyntaxError</code>             \u2013            <p>When an invalid regex filter is provided.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; catalog = DataCatalog()\n&gt;&gt;&gt; # get datasets where the substring 'raw' is present\n&gt;&gt;&gt; raw_data = catalog.list(regex_search='raw')\n&gt;&gt;&gt; # get datasets which start with 'prm' or 'feat'\n&gt;&gt;&gt; feat_eng_data = catalog.list(regex_search='^(prm|feat)')\n&gt;&gt;&gt; # get datasets which end with 'time_series'\n&gt;&gt;&gt; models = catalog.list(regex_search='.+time_series$')\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def list(self, regex_search: str | None = None) -&gt; list[str]:\n    \"\"\"\n    List of all dataset names registered in the catalog.\n    This can be filtered by providing an optional regular expression\n    which will only return matching keys.\n\n    Args:\n        regex_search: An optional regular expression which can be provided\n            to limit the datasets returned by a particular pattern.\n    Returns:\n        A list of dataset names available which match the\n        `regex_search` criteria (if provided). All dataset names are returned\n        by default.\n\n    Raises:\n        SyntaxError: When an invalid regex filter is provided.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; catalog = DataCatalog()\n        &gt;&gt;&gt; # get datasets where the substring 'raw' is present\n        &gt;&gt;&gt; raw_data = catalog.list(regex_search='raw')\n        &gt;&gt;&gt; # get datasets which start with 'prm' or 'feat'\n        &gt;&gt;&gt; feat_eng_data = catalog.list(regex_search='^(prm|feat)')\n        &gt;&gt;&gt; # get datasets which end with 'time_series'\n        &gt;&gt;&gt; models = catalog.list(regex_search='.+time_series$')\n    \"\"\"\n\n    if regex_search is None:\n        return list(self._datasets.keys())\n\n    if not regex_search.strip():\n        self._logger.warning(\"The empty string will not match any datasets\")\n        return []\n\n    try:\n        pattern = re.compile(regex_search, flags=re.IGNORECASE)\n\n    except re.error as exc:\n        raise SyntaxError(\n            f\"Invalid regular expression provided: '{regex_search}'\"\n        ) from exc\n    return [ds_name for ds_name in self._datasets if pattern.search(ds_name)]\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.load","title":"load","text":"<pre><code>load(name, version=None)\n</code></pre> <p>Loads a registered dataset.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>A dataset to be loaded.</p> </li> <li> <code>version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional argument for concrete data version to be loaded. Works only with versioned datasets.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The loaded data as configured.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetNotFoundError</code>             \u2013            <p>When a dataset with the given name has not yet been registered.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro.io import DataCatalog\n&gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt; cars = CSVDataset(filepath=\"cars.csv\",\n&gt;&gt;&gt;                   load_args=None,\n&gt;&gt;&gt;                   save_args={\"index\": False})\n&gt;&gt;&gt; catalog = DataCatalog(datasets={'cars': cars})\n&gt;&gt;&gt;\n&gt;&gt;&gt; df = catalog.load(\"cars\")\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def load(self, name: str, version: str | None = None) -&gt; Any:\n    \"\"\"Loads a registered dataset.\n\n    Args:\n        name: A dataset to be loaded.\n        version: Optional argument for concrete data version to be loaded.\n            Works only with versioned datasets.\n\n    Returns:\n        The loaded data as configured.\n\n    Raises:\n        DatasetNotFoundError: When a dataset with the given name\n            has not yet been registered.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; from kedro.io import DataCatalog\n        &gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; cars = CSVDataset(filepath=\"cars.csv\",\n        &gt;&gt;&gt;                   load_args=None,\n        &gt;&gt;&gt;                   save_args={\"index\": False})\n        &gt;&gt;&gt; catalog = DataCatalog(datasets={'cars': cars})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df = catalog.load(\"cars\")\n    \"\"\"\n    load_version = Version(version, None) if version else None\n    dataset = self._get_dataset(name, version=load_version)\n\n    self._logger.info(\n        \"Loading data from %s (%s)...\",\n        name,\n        type(dataset).__name__,\n        extra={\"rich_format\": [\"dark_orange\"]},\n    )\n\n    result = dataset.load()\n\n    return result\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.release","title":"release","text":"<pre><code>release(name)\n</code></pre> <p>Release any cached data associated with a dataset</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>A dataset to be checked.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetNotFoundError</code>             \u2013            <p>When a dataset with the given name has not yet been registered.</p> </li> </ul> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def release(self, name: str) -&gt; None:\n    \"\"\"Release any cached data associated with a dataset\n\n    Args:\n        name: A dataset to be checked.\n\n    Raises:\n        DatasetNotFoundError: When a dataset with the given name\n            has not yet been registered.\n    \"\"\"\n    dataset = self._get_dataset(name)\n    dataset.release()\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.save","title":"save","text":"<pre><code>save(name, data)\n</code></pre> <p>Save data to a registered dataset.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>A dataset to be saved to.</p> </li> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>A data object to be saved as configured in the registered dataset.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetNotFoundError</code>             \u2013            <p>When a dataset with the given name has not yet been registered.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt; cars = CSVDataset(filepath=\"cars.csv\",\n&gt;&gt;&gt;                   load_args=None,\n&gt;&gt;&gt;                   save_args={\"index\": False})\n&gt;&gt;&gt; catalog = DataCatalog(datasets={'cars': cars})\n&gt;&gt;&gt;\n&gt;&gt;&gt; df = pd.DataFrame({'col1': [1, 2],\n&gt;&gt;&gt;                    'col2': [4, 5],\n&gt;&gt;&gt;                    'col3': [5, 6]})\n&gt;&gt;&gt; catalog.save(\"cars\", df)\n</code></pre> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def save(self, name: str, data: Any) -&gt; None:\n    \"\"\"Save data to a registered dataset.\n\n    Args:\n        name: A dataset to be saved to.\n        data: A data object to be saved as configured in the registered\n            dataset.\n\n    Raises:\n        DatasetNotFoundError: When a dataset with the given name\n            has not yet been registered.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; from kedro_datasets.pandas import CSVDataset\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; cars = CSVDataset(filepath=\"cars.csv\",\n        &gt;&gt;&gt;                   load_args=None,\n        &gt;&gt;&gt;                   save_args={\"index\": False})\n        &gt;&gt;&gt; catalog = DataCatalog(datasets={'cars': cars})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df = pd.DataFrame({'col1': [1, 2],\n        &gt;&gt;&gt;                    'col2': [4, 5],\n        &gt;&gt;&gt;                    'col3': [5, 6]})\n        &gt;&gt;&gt; catalog.save(\"cars\", df)\n    \"\"\"\n    dataset = self._get_dataset(name)\n\n    self._logger.info(\n        \"Saving data to %s (%s)...\",\n        name,\n        type(dataset).__name__,\n        extra={\"rich_format\": [\"dark_orange\"]},\n    )\n\n    dataset.save(data)\n</code></pre>"},{"location":"api/io/kedro.io.DataCatalog/#kedro.io.DataCatalog.shallow_copy","title":"shallow_copy","text":"<pre><code>shallow_copy(extra_dataset_patterns=None)\n</code></pre> <p>Returns a shallow copy of the current object.</p> <p>Returns:</p> <ul> <li> <code>DataCatalog</code>           \u2013            <p>Copy of the current object.</p> </li> </ul> Source code in <code>kedro/io/data_catalog.py</code> <pre><code>def shallow_copy(\n    self, extra_dataset_patterns: Patterns | None = None\n) -&gt; DataCatalog:\n    \"\"\"Returns a shallow copy of the current object.\n\n    Returns:\n        Copy of the current object.\n    \"\"\"\n    if extra_dataset_patterns:\n        self._config_resolver.add_runtime_patterns(extra_dataset_patterns)\n    return self.__class__(\n        datasets=self._datasets,\n        dataset_patterns=self._config_resolver._dataset_patterns,\n        default_pattern=self._config_resolver._default_pattern,\n        load_versions=self._load_versions,\n        save_version=self._save_version,\n        config_resolver=self._config_resolver,\n    )\n</code></pre>"},{"location":"api/io/kedro.io.DatasetAlreadyExistsError/","title":"DatasetAlreadyExistsError","text":""},{"location":"api/io/kedro.io.DatasetAlreadyExistsError/#kedro.io.DatasetAlreadyExistsError","title":"kedro.io.DatasetAlreadyExistsError","text":"<p>               Bases: <code>DatasetError</code></p> <p><code>DatasetAlreadyExistsError</code> raised by <code>`DataCatalog</code> and <code>KedroDataCatalog</code> classes in case of trying to add a dataset which already exists in the <code>DataCatalog</code>.</p>"},{"location":"api/io/kedro.io.DatasetError/","title":"DatasetError","text":""},{"location":"api/io/kedro.io.DatasetError/#kedro.io.DatasetError","title":"kedro.io.DatasetError","text":"<p>               Bases: <code>Exception</code></p> <p><code>DatasetError</code> raised by <code>AbstractDataset</code> implementations in case of failure of input/output methods.</p> <p><code>AbstractDataset</code> implementations should provide instructive information in case of failure.</p>"},{"location":"api/io/kedro.io.DatasetNotFoundError/","title":"DatasetNotFoundError","text":""},{"location":"api/io/kedro.io.DatasetNotFoundError/#kedro.io.DatasetNotFoundError","title":"kedro.io.DatasetNotFoundError","text":"<p>               Bases: <code>DatasetError</code></p> <p><code>DatasetNotFoundError</code> raised by <code>`DataCatalog</code> and <code>KedroDataCatalog</code> classes in case of trying to use a non-existing dataset.</p>"},{"location":"api/io/kedro.io.LambdaDataset/","title":"LambdaDataset","text":""},{"location":"api/io/kedro.io.LambdaDataset/#kedro.io.LambdaDataset","title":"kedro.io.LambdaDataset","text":"<pre><code>LambdaDataset(load, save, exists=None, release=None, metadata=None)\n</code></pre> <p>               Bases: <code>AbstractDataset</code></p> <p><code>LambdaDataset</code> loads and saves data to a dataset. It relies on delegating to specific implementation such as csv, sql, etc.</p> <p><code>LambdaDataset</code> class captures Exceptions while performing operations on composed <code>Dataset</code> implementations. The composed dataset is responsible for providing information on how to resolve the issue when possible. This information should be available through str(error).</p> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro.io import LambdaDataset\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; file_name = \"test.csv\"\n&gt;&gt;&gt; def load() -&gt; pd.DataFrame:\n&gt;&gt;&gt;     raise FileNotFoundError(\"'{}' csv file not found.\"\n&gt;&gt;&gt;                             .format(file_name))\n&gt;&gt;&gt; dataset = LambdaDataset(load, None)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>load</code>               (<code>Callable[[], Any] | None</code>)           \u2013            <p>Method to load data from a dataset.</p> </li> <li> <code>save</code>               (<code>Callable[[Any], None] | None</code>)           \u2013            <p>Method to save data to a dataset.</p> </li> <li> <code>exists</code>               (<code>Callable[[], bool] | None</code>, default:                   <code>None</code> )           \u2013            <p>Method to check whether output data already exists.</p> </li> <li> <code>release</code>               (<code>Callable[[], None] | None</code>, default:                   <code>None</code> )           \u2013            <p>Method to release any cached information.</p> </li> <li> <code>metadata</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Any arbitrary metadata. This is ignored by Kedro, but may be consumed by users or external plugins.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatasetError</code>             \u2013            <p>If a method is specified, but is not a Callable.</p> </li> </ul> Source code in <code>kedro/io/lambda_dataset.py</code> <pre><code>def __init__(\n    self,\n    load: Callable[[], Any] | None,\n    save: Callable[[Any], None] | None,\n    exists: Callable[[], bool] | None = None,\n    release: Callable[[], None] | None = None,\n    metadata: dict[str, Any] | None = None,\n):\n    \"\"\"Creates a new instance of ``LambdaDataset`` with references to the\n    required input/output dataset methods.\n\n    Args:\n        load: Method to load data from a dataset.\n        save: Method to save data to a dataset.\n        exists: Method to check whether output data already exists.\n        release: Method to release any cached information.\n        metadata: Any arbitrary metadata.\n            This is ignored by Kedro, but may be consumed by users or external plugins.\n\n    Raises:\n        DatasetError: If a method is specified, but is not a Callable.\n\n    \"\"\"\n\n    warnings.warn(\n        \"`LambdaDataset` has been deprecated and will be removed in Kedro 0.20.0.\",\n        KedroDeprecationWarning,\n    )\n\n    for name, value in [\n        (\"load\", load),\n        (\"save\", save),\n        (\"exists\", exists),\n        (\"release\", release),\n    ]:\n        if value is not None and not callable(value):\n            raise DatasetError(\n                f\"'{name}' function for LambdaDataset must be a Callable. \"\n                f\"Object of type '{value.__class__.__name__}' provided instead.\"\n            )\n\n    self.__load = load\n    self.__save = save\n    self.__exists = exists\n    self.__release = release\n    self.metadata = metadata\n</code></pre>"},{"location":"api/io/kedro.io.LambdaDataset/#kedro.io.LambdaDataset.__exists","title":"__exists  <code>instance-attribute</code>","text":"<pre><code>__exists = exists\n</code></pre>"},{"location":"api/io/kedro.io.LambdaDataset/#kedro.io.LambdaDataset.__load","title":"__load  <code>instance-attribute</code>","text":"<pre><code>__load = load\n</code></pre>"},{"location":"api/io/kedro.io.LambdaDataset/#kedro.io.LambdaDataset.__release","title":"__release  <code>instance-attribute</code>","text":"<pre><code>__release = release\n</code></pre>"},{"location":"api/io/kedro.io.LambdaDataset/#kedro.io.LambdaDataset.__save","title":"__save  <code>instance-attribute</code>","text":"<pre><code>__save = save\n</code></pre>"},{"location":"api/io/kedro.io.LambdaDataset/#kedro.io.LambdaDataset.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata = metadata\n</code></pre>"},{"location":"api/io/kedro.io.LambdaDataset/#kedro.io.LambdaDataset._describe","title":"_describe","text":"<pre><code>_describe()\n</code></pre> Source code in <code>kedro/io/lambda_dataset.py</code> <pre><code>def _describe(self) -&gt; dict[str, Any]:\n    def _to_str(func: Any) -&gt; str | None:\n        if not func:\n            return None\n        try:\n            return f\"&lt;{func.__module__}.{func.__name__}&gt;\"\n        except AttributeError:  # pragma: no cover\n            return str(func)\n\n    descr = {\n        \"load\": _to_str(self.__load),\n        \"save\": _to_str(self.__save),\n        \"exists\": _to_str(self.__exists),\n        \"release\": _to_str(self.__release),\n    }\n\n    return descr\n</code></pre>"},{"location":"api/io/kedro.io.LambdaDataset/#kedro.io.LambdaDataset._exists","title":"_exists","text":"<pre><code>_exists()\n</code></pre> Source code in <code>kedro/io/lambda_dataset.py</code> <pre><code>def _exists(self) -&gt; bool:\n    if not self.__exists:\n        return super()._exists()\n    return self.__exists()\n</code></pre>"},{"location":"api/io/kedro.io.LambdaDataset/#kedro.io.LambdaDataset._load","title":"_load","text":"<pre><code>_load()\n</code></pre> Source code in <code>kedro/io/lambda_dataset.py</code> <pre><code>def _load(self) -&gt; Any:\n    if not self.__load:\n        raise DatasetError(\n            \"Cannot load dataset. No 'load' function \"\n            \"provided when LambdaDataset was created.\"\n        )\n    return self.__load()\n</code></pre>"},{"location":"api/io/kedro.io.LambdaDataset/#kedro.io.LambdaDataset._release","title":"_release","text":"<pre><code>_release()\n</code></pre> Source code in <code>kedro/io/lambda_dataset.py</code> <pre><code>def _release(self) -&gt; None:\n    if not self.__release:\n        super()._release()\n    else:\n        self.__release()\n</code></pre>"},{"location":"api/io/kedro.io.LambdaDataset/#kedro.io.LambdaDataset._save","title":"_save","text":"<pre><code>_save(data)\n</code></pre> Source code in <code>kedro/io/lambda_dataset.py</code> <pre><code>def _save(self, data: Any) -&gt; None:\n    if not self.__save:\n        raise DatasetError(\n            \"Cannot save to dataset. No 'save' function \"\n            \"provided when LambdaDataset was created.\"\n        )\n    self.__save(data)\n</code></pre>"},{"location":"api/io/kedro.io.MemoryDataset/","title":"MemoryDataset","text":""},{"location":"api/io/kedro.io.MemoryDataset/#kedro.io.MemoryDataset","title":"kedro.io.MemoryDataset","text":"<pre><code>MemoryDataset(data=_EMPTY, copy_mode=None, metadata=None)\n</code></pre> <p>               Bases: <code>AbstractDataset</code></p> <p><code>MemoryDataset</code> loads and saves data from/to an in-memory Python object. The <code>_EPHEMERAL</code> attribute is set to True to indicate MemoryDataset's non-persistence.</p> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro.io import MemoryDataset\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.DataFrame({'col1': [1, 2], 'col2': [4, 5],\n&gt;&gt;&gt;                      'col3': [5, 6]})\n&gt;&gt;&gt; dataset = MemoryDataset(data=data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; loaded_data = dataset.load()\n&gt;&gt;&gt; assert loaded_data.equals(data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; new_data = pd.DataFrame({'col1': [1, 2], 'col2': [4, 5]})\n&gt;&gt;&gt; dataset.save(new_data)\n&gt;&gt;&gt; reloaded_data = dataset.load()\n&gt;&gt;&gt; assert reloaded_data.equals(new_data)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Any</code>, default:                   <code>_EMPTY</code> )           \u2013            <p>Python object containing the data.</p> </li> <li> <code>copy_mode</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The copy mode used to copy the data. Possible values are: \"deepcopy\", \"copy\" and \"assign\". If not provided, it is inferred based on the data type.</p> </li> <li> <code>metadata</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Any arbitrary metadata. This is ignored by Kedro, but may be consumed by users or external plugins.</p> </li> </ul> Source code in <code>kedro/io/memory_dataset.py</code> <pre><code>def __init__(\n    self,\n    data: Any = _EMPTY,\n    copy_mode: str | None = None,\n    metadata: dict[str, Any] | None = None,\n):\n    \"\"\"Creates a new instance of ``MemoryDataset`` pointing to the\n    provided Python object.\n\n    Args:\n        data: Python object containing the data.\n        copy_mode: The copy mode used to copy the data. Possible\n            values are: \"deepcopy\", \"copy\" and \"assign\". If not\n            provided, it is inferred based on the data type.\n        metadata: Any arbitrary metadata.\n            This is ignored by Kedro, but may be consumed by users or external plugins.\n    \"\"\"\n    self._data = _EMPTY\n    self._copy_mode = copy_mode\n    self.metadata = metadata\n    self._EPHEMERAL = True\n    if data is not _EMPTY:\n        self.save.__wrapped__(self, data)  # type: ignore[attr-defined]\n</code></pre>"},{"location":"api/io/kedro.io.MemoryDataset/#kedro.io.MemoryDataset._EPHEMERAL","title":"_EPHEMERAL  <code>instance-attribute</code>","text":"<pre><code>_EPHEMERAL = True\n</code></pre>"},{"location":"api/io/kedro.io.MemoryDataset/#kedro.io.MemoryDataset._copy_mode","title":"_copy_mode  <code>instance-attribute</code>","text":"<pre><code>_copy_mode = copy_mode\n</code></pre>"},{"location":"api/io/kedro.io.MemoryDataset/#kedro.io.MemoryDataset._data","title":"_data  <code>instance-attribute</code>","text":"<pre><code>_data = _EMPTY\n</code></pre>"},{"location":"api/io/kedro.io.MemoryDataset/#kedro.io.MemoryDataset.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata = metadata\n</code></pre>"},{"location":"api/io/kedro.io.MemoryDataset/#kedro.io.MemoryDataset._describe","title":"_describe","text":"<pre><code>_describe()\n</code></pre> Source code in <code>kedro/io/memory_dataset.py</code> <pre><code>def _describe(self) -&gt; dict[str, Any]:\n    if self._data is not _EMPTY:\n        return {\"data\": f\"&lt;{type(self._data).__name__}&gt;\"}\n    # the string representation of datasets leaves out __init__\n    # arguments that are empty/None, equivalent here is _EMPTY\n    return {\"data\": None}  # pragma: no cover\n</code></pre>"},{"location":"api/io/kedro.io.MemoryDataset/#kedro.io.MemoryDataset._exists","title":"_exists","text":"<pre><code>_exists()\n</code></pre> Source code in <code>kedro/io/memory_dataset.py</code> <pre><code>def _exists(self) -&gt; bool:\n    return self._data is not _EMPTY\n</code></pre>"},{"location":"api/io/kedro.io.MemoryDataset/#kedro.io.MemoryDataset._release","title":"_release","text":"<pre><code>_release()\n</code></pre> Source code in <code>kedro/io/memory_dataset.py</code> <pre><code>def _release(self) -&gt; None:\n    self._data = _EMPTY\n</code></pre>"},{"location":"api/io/kedro.io.MemoryDataset/#kedro.io.MemoryDataset.load","title":"load","text":"<pre><code>load()\n</code></pre> Source code in <code>kedro/io/memory_dataset.py</code> <pre><code>def load(self) -&gt; Any:\n    if self._data is _EMPTY:\n        raise DatasetError(\"Data for MemoryDataset has not been saved yet.\")\n\n    copy_mode = self._copy_mode or _infer_copy_mode(self._data)\n    data = _copy_with_mode(self._data, copy_mode=copy_mode)\n    return data\n</code></pre>"},{"location":"api/io/kedro.io.MemoryDataset/#kedro.io.MemoryDataset.save","title":"save","text":"<pre><code>save(data)\n</code></pre> Source code in <code>kedro/io/memory_dataset.py</code> <pre><code>def save(self, data: Any) -&gt; None:\n    copy_mode = self._copy_mode or _infer_copy_mode(data)\n    self._data = _copy_with_mode(data, copy_mode=copy_mode)\n</code></pre>"},{"location":"api/io/kedro.io.Version/","title":"Version","text":""},{"location":"api/io/kedro.io.Version/#kedro.io.Version","title":"kedro.io.Version","text":"<p>               Bases: <code>namedtuple('Version', ['load', 'save'])</code></p> <p>This namedtuple is used to provide load and save versions for versioned datasets. If <code>Version.load</code> is None, then the latest available version is loaded. If <code>Version.save</code> is None, then save version is formatted as YYYY-MM-DDThh.mm.ss.sssZ of the current timestamp.</p>"},{"location":"api/io/kedro.io.Version/#kedro.io.Version.__slots__","title":"__slots__  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>__slots__ = ()\n</code></pre>"},{"location":"api/io/kedro.io/","title":"kedro.io","text":"Name Type Description <code>AbstractDataset</code> Class Base class for all Kedro datasets. <code>AbstractVersionedDataset</code> Class Base class for versioned datasets. <code>CachedDataset</code> Class Dataset wrapper for caching data in memory. <code>DataCatalog</code> Class Manages datasets used in a Kedro pipeline. <code>LambdaDataset</code> Class Dataset wrapper for inline data transformations. <code>MemoryDataset</code> Class Dataset for storing data in memory. <code>Version</code> Class Represents dataset version information. <code>DatasetAlreadyExistsError</code> Exception Raised when a dataset already exists. <code>DatasetError</code> Exception General dataset-related error. <code>DatasetNotFoundError</code> Exception Raised when a dataset is not found."},{"location":"api/io/kedro.io/#kedro.io","title":"kedro.io","text":"<p><code>kedro.io</code> provides functionality to read and write to a number of datasets. At the core of the library is the <code>AbstractDataset</code> class.</p>"},{"location":"api/ipython/kedro.ipython.load_ipython_extension/","title":"load_ipython_extension","text":""},{"location":"api/ipython/kedro.ipython.load_ipython_extension/#kedro.ipython.load_ipython_extension","title":"kedro.ipython.load_ipython_extension","text":"<pre><code>load_ipython_extension(ipython)\n</code></pre> <p>Main entry point when %load_ext kedro.ipython is executed, either manually or automatically through <code>kedro ipython</code> or <code>kedro jupyter lab/notebook</code>. IPython will look for this function specifically. See https://ipython.readthedocs.io/en/stable/config/extensions/index.html</p> Source code in <code>kedro/ipython/__init__.py</code> <pre><code>def load_ipython_extension(ipython: InteractiveShell) -&gt; None:\n    \"\"\"\n    Main entry point when %load_ext kedro.ipython is executed, either manually or\n    automatically through `kedro ipython` or `kedro jupyter lab/notebook`.\n    IPython will look for this function specifically.\n    See https://ipython.readthedocs.io/en/stable/config/extensions/index.html\n    \"\"\"\n    ipython.register_magic_function(func=magic_reload_kedro, magic_name=\"reload_kedro\")  # type: ignore[call-arg]\n    logger.info(\"Registered line magic '%reload_kedro'\")\n    ipython.register_magic_function(func=magic_load_node, magic_name=\"load_node\")  # type: ignore[call-arg]\n    logger.info(\"Registered line magic '%load_node'\")\n\n    if _find_kedro_project(Path.cwd()) is None:\n        logger.warning(\n            \"Kedro extension was registered but couldn't find a Kedro project. \"\n            \"Make sure you run '%reload_kedro &lt;project_root&gt;'.\"\n        )\n        return\n\n    reload_kedro()\n</code></pre>"},{"location":"api/ipython/kedro.ipython.magic_load_node/","title":"magic_load_node","text":""},{"location":"api/ipython/kedro.ipython.magic_load_node/#kedro.ipython.magic_load_node","title":"kedro.ipython.magic_load_node","text":"<pre><code>magic_load_node(args)\n</code></pre> <p>The line magic %load_node . Currently, this feature is only available for Jupyter Notebook (&gt;7.0), Jupyter Lab, IPython, and VSCode Notebook. This line magic will generate code in multiple cells to load datasets from <code>DataCatalog</code>, import relevant functions and modules, node function definition and a function call. If generating code is not possible, it will print the code instead. Source code in <code>kedro/ipython/__init__.py</code> <pre><code>@typing.no_type_check\n@magic_arguments()\n@argument(\n    \"node\",\n    type=str,\n    help=(\"Name of the Node.\"),\n    nargs=\"?\",\n    default=None,\n)\ndef magic_load_node(args: str) -&gt; None:\n    \"\"\"The line magic %load_node &lt;node_name&gt;.\n    Currently, this feature is only available for Jupyter Notebook (&gt;7.0), Jupyter Lab, IPython,\n    and VSCode Notebook. This line magic will generate code in multiple cells to load\n    datasets from `DataCatalog`, import relevant functions and modules, node function\n    definition and a function call. If generating code is not possible, it will print\n    the code instead.\n    \"\"\"\n    parameters = parse_argstring(magic_load_node, args)\n    node_name = parameters.node\n\n    cells = _load_node(node_name, pipelines)\n\n    run_environment = _guess_run_environment()\n\n    if run_environment in (\"ipython\", \"vscode\", \"jupyter\"):\n        # Combine multiple cells into one for IPython or VSCode or Jupyter\n        combined_cell = \"\\n\\n\".join(cells)\n        _create_cell_with_text(combined_cell)\n    else:\n        # For other environments or if detection fails, just print the cells\n        _print_cells(cells)\n</code></pre>"},{"location":"api/ipython/kedro.ipython.magic_reload_kedro/","title":"magic_reload_kedro","text":""},{"location":"api/ipython/kedro.ipython.magic_reload_kedro/#kedro.ipython.magic_reload_kedro","title":"kedro.ipython.magic_reload_kedro","text":"<pre><code>magic_reload_kedro(line, local_ns=None, conf_source=None)\n</code></pre> <p>The <code>%reload_kedro</code> IPython line magic. See https://docs.kedro.org/en/stable/notebooks_and_ipython/kedro_and_notebooks.html#reload-kedro-line-magic for more.</p> Source code in <code>kedro/ipython/__init__.py</code> <pre><code>@typing.no_type_check\n@needs_local_scope\n@magic_arguments()\n@argument(\n    \"path\",\n    type=str,\n    help=(\n        \"Path to the project root directory. If not given, use the previously set\"\n        \"project root.\"\n    ),\n    nargs=\"?\",\n    default=None,\n)\n@argument(\"-e\", \"--env\", type=str, default=None, help=ENV_HELP)\n@argument(\n    \"--params\",\n    type=lambda value: _split_params(None, None, value),\n    default=None,\n    help=PARAMS_ARG_HELP,\n)\n@argument(\"--conf-source\", type=str, default=None, help=CONF_SOURCE_HELP)\ndef magic_reload_kedro(\n    line: str,\n    local_ns: dict[str, Any] | None = None,\n    conf_source: str | None = None,\n) -&gt; None:\n    \"\"\"\n    The `%reload_kedro` IPython line magic.\n    See https://docs.kedro.org/en/stable/notebooks_and_ipython/kedro_and_notebooks.html#reload-kedro-line-magic\n    for more.\n    \"\"\"\n    args = parse_argstring(magic_reload_kedro, line)\n    reload_kedro(args.path, args.env, args.params, local_ns, args.conf_source)\n</code></pre>"},{"location":"api/ipython/kedro.ipython/","title":"Overview","text":"Name Type Description <code>load_ipython_extension</code> Function Main entry point when <code>%load_ext kedro.ipython</code> is executed, either manually or automatically through Kedro IPython or Kedro Jupyter Lab/Notebook. <code>magic_load_node</code> Function . <code>magic_reload_kedro</code> Function . <code>reload_kedro</code> Function Function that underlies the <code>%reload_kedro</code> Line magic."},{"location":"api/ipython/kedro.ipython/#kedro.ipython","title":"kedro.ipython","text":"<p>This script creates an IPython extension to load Kedro-related variables in local scope.</p>"},{"location":"api/ipython/kedro.ipython.reload_kedro/","title":"reload_kedro","text":""},{"location":"api/ipython/kedro.ipython.reload_kedro/#kedro.ipython.reload_kedro","title":"kedro.ipython.reload_kedro","text":"<pre><code>reload_kedro(path=None, env=None, extra_params=None, local_namespace=None, conf_source=None)\n</code></pre> <p>Function that underlies the %reload_kedro Line magic. This should not be imported or run directly but instead invoked through %reload_kedro.</p> Source code in <code>kedro/ipython/__init__.py</code> <pre><code>def reload_kedro(\n    path: str | None = None,\n    env: str | None = None,\n    extra_params: dict[str, Any] | None = None,\n    local_namespace: dict[str, Any] | None = None,\n    conf_source: str | None = None,\n) -&gt; None:  # pragma: no cover\n    \"\"\"Function that underlies the %reload_kedro Line magic. This should not be imported\n    or run directly but instead invoked through %reload_kedro.\"\"\"\n\n    project_path = _resolve_project_path(path, local_namespace)\n\n    metadata = bootstrap_project(project_path)\n    _remove_cached_modules(metadata.package_name)\n    configure_project(metadata.package_name)\n\n    session = KedroSession.create(\n        project_path,\n        env=env,\n        extra_params=extra_params,\n        conf_source=conf_source,\n    )\n    context = session.load_context()\n    catalog = context.catalog\n\n    get_ipython().push(  # type: ignore[no-untyped-call]\n        variables={\n            \"context\": context,\n            \"catalog\": catalog,\n            \"session\": session,\n            \"pipelines\": pipelines,\n        }\n    )\n\n    logger.info(\"Kedro project %s\", str(metadata.project_name))\n    logger.info(\n        \"Defined global variable 'context', 'session', 'catalog' and 'pipelines'\"\n    )\n\n    for line_magic in load_entry_points(\"line_magic\"):\n        register_line_magic(needs_local_scope(line_magic))  # type: ignore[no-untyped-call]\n        logger.info(\"Registered line magic '%s'\", line_magic.__name__)  # type: ignore[attr-defined]\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/","title":"Pipeline","text":""},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline","title":"kedro.pipeline.Pipeline","text":"<pre><code>Pipeline(nodes, *, tags=None)\n</code></pre> <p>A <code>Pipeline</code> defined as a collection of <code>Node</code> objects. This class treats nodes as part of a graph representation and provides inputs, outputs and execution order.</p> <p>Parameters:</p> <ul> <li> <code>nodes</code>               (<code>Iterable[Node | Pipeline]</code>)           \u2013            <p>The iterable of nodes the <code>Pipeline</code> will be made of. If you provide pipelines among the list of nodes, those pipelines will be expanded and all their nodes will become part of this new pipeline.</p> </li> <li> <code>tags</code>               (<code>str | Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional set of tags to be applied to all the pipeline nodes.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When an empty list of nodes is provided, or when not all nodes have unique names.</p> </li> <li> <code>CircularDependencyError</code>             \u2013            <p>When visiting all the nodes is not possible due to the existence of a circular dependency.</p> </li> <li> <code>OutputNotUniqueError</code>             \u2013            <p>When multiple <code>Node</code> instances produce the same output.</p> </li> <li> <code>ConfirmNotUniqueError</code>             \u2013            <p>When multiple <code>Node</code> instances attempt to confirm the same dataset.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; from kedro.pipeline import Pipeline\n&gt;&gt;&gt; from kedro.pipeline import node\n&gt;&gt;&gt;\n&gt;&gt;&gt; # In the following scenario first_ds and second_ds\n&gt;&gt;&gt; # are datasets provided by io. Pipeline will pass these\n&gt;&gt;&gt; # datasets to first_node function and provides the result\n&gt;&gt;&gt; # to the second_node as input.\n&gt;&gt;&gt;\n&gt;&gt;&gt; def first_node(first_ds, second_ds):\n&gt;&gt;&gt;     return dict(third_ds=first_ds+second_ds)\n&gt;&gt;&gt;\n&gt;&gt;&gt; def second_node(third_ds):\n&gt;&gt;&gt;     return third_ds\n&gt;&gt;&gt;\n&gt;&gt;&gt; pipeline = Pipeline([\n&gt;&gt;&gt;     node(first_node, ['first_ds', 'second_ds'], ['third_ds']),\n&gt;&gt;&gt;     node(second_node, dict(third_ds='third_ds'), 'fourth_ds')])\n&gt;&gt;&gt;\n&gt;&gt;&gt; pipeline.describe()\n&gt;&gt;&gt;\n</code></pre> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def __init__(\n    self,\n    nodes: Iterable[Node | Pipeline],\n    *,\n    tags: str | Iterable[str] | None = None,\n):\n    \"\"\"Initialise ``Pipeline`` with a list of ``Node`` instances.\n\n    Args:\n        nodes: The iterable of nodes the ``Pipeline`` will be made of. If you\n            provide pipelines among the list of nodes, those pipelines will\n            be expanded and all their nodes will become part of this\n            new pipeline.\n        tags: Optional set of tags to be applied to all the pipeline nodes.\n\n    Raises:\n        ValueError:\n            When an empty list of nodes is provided, or when not all\n            nodes have unique names.\n        CircularDependencyError:\n            When visiting all the nodes is not\n            possible due to the existence of a circular dependency.\n        OutputNotUniqueError:\n            When multiple ``Node`` instances produce the same output.\n        ConfirmNotUniqueError:\n            When multiple ``Node`` instances attempt to confirm the same\n            dataset.\n    Example:\n    ::\n\n        &gt;&gt;&gt; from kedro.pipeline import Pipeline\n        &gt;&gt;&gt; from kedro.pipeline import node\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # In the following scenario first_ds and second_ds\n        &gt;&gt;&gt; # are datasets provided by io. Pipeline will pass these\n        &gt;&gt;&gt; # datasets to first_node function and provides the result\n        &gt;&gt;&gt; # to the second_node as input.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; def first_node(first_ds, second_ds):\n        &gt;&gt;&gt;     return dict(third_ds=first_ds+second_ds)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; def second_node(third_ds):\n        &gt;&gt;&gt;     return third_ds\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; pipeline = Pipeline([\n        &gt;&gt;&gt;     node(first_node, ['first_ds', 'second_ds'], ['third_ds']),\n        &gt;&gt;&gt;     node(second_node, dict(third_ds='third_ds'), 'fourth_ds')])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; pipeline.describe()\n        &gt;&gt;&gt;\n\n    \"\"\"\n    if nodes is None:\n        raise ValueError(\n            \"'nodes' argument of 'Pipeline' is None. It must be an \"\n            \"iterable of nodes and/or pipelines instead.\"\n        )\n    nodes_list = list(nodes)  # in case it's a generator\n    _validate_duplicate_nodes(nodes_list)\n\n    nodes_chain = list(\n        chain.from_iterable(\n            [[n] if isinstance(n, Node) else n.nodes for n in nodes_list]\n        )\n    )\n    _validate_transcoded_inputs_outputs(nodes_chain)\n    _tags = set(_to_list(tags))\n\n    if _tags:\n        tagged_nodes = [n.tag(_tags) for n in nodes_chain]\n    else:\n        tagged_nodes = nodes_chain\n\n    self._nodes_by_name = {node.name: node for node in tagged_nodes}\n    _validate_unique_outputs(tagged_nodes)\n    _validate_unique_confirms(tagged_nodes)\n\n    # input -&gt; nodes with input\n    self._nodes_by_input: dict[str, set[Node]] = defaultdict(set)\n    for node in tagged_nodes:\n        for input_ in node.inputs:\n            self._nodes_by_input[_strip_transcoding(input_)].add(node)\n\n    # output -&gt; node with output\n    self._nodes_by_output: dict[str, Node] = {}\n    for node in tagged_nodes:\n        for output in node.outputs:\n            self._nodes_by_output[_strip_transcoding(output)] = node\n\n    self._nodes = tagged_nodes\n    self._toposorter = TopologicalSorter(self.node_dependencies)\n\n    # test for circular dependencies without executing the toposort for efficiency\n    try:\n        self._toposorter.prepare()\n    except CycleError as exc:\n        loop = list(set(exc.args[1]))\n        message = f\"Circular dependencies exist among the following {len(loop)} item(s): {loop}\"\n        raise CircularDependencyError(message) from exc\n\n    self._toposorted_nodes: list[Node] = []\n    self._toposorted_groups: list[list[Node]] = []\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline._nodes","title":"_nodes  <code>instance-attribute</code>","text":"<pre><code>_nodes = tagged_nodes\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline._nodes_by_input","title":"_nodes_by_input  <code>instance-attribute</code>","text":"<pre><code>_nodes_by_input = defaultdict(set)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline._nodes_by_name","title":"_nodes_by_name  <code>instance-attribute</code>","text":"<pre><code>_nodes_by_name = {name: nodefor node in tagged_nodes}\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline._nodes_by_output","title":"_nodes_by_output  <code>instance-attribute</code>","text":"<pre><code>_nodes_by_output = {}\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline._toposorted_groups","title":"_toposorted_groups  <code>instance-attribute</code>","text":"<pre><code>_toposorted_groups = []\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline._toposorted_nodes","title":"_toposorted_nodes  <code>instance-attribute</code>","text":"<pre><code>_toposorted_nodes = []\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline._toposorter","title":"_toposorter  <code>instance-attribute</code>","text":"<pre><code>_toposorter = TopologicalSorter(node_dependencies)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.grouped_nodes","title":"grouped_nodes  <code>property</code>","text":"<pre><code>grouped_nodes\n</code></pre> <p>Return a list of the pipeline nodes in topologically ordered groups, i.e. if node A needs to be run before node B, it will appear in an earlier group.</p> <p>Returns:</p> <ul> <li> <code>list[list[Node]]</code>           \u2013            <p>The pipeline nodes in topologically ordered groups.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.grouped_nodes_by_namespace","title":"grouped_nodes_by_namespace  <code>property</code>","text":"<pre><code>grouped_nodes_by_namespace\n</code></pre> <p>Return a dictionary of the pipeline nodes grouped by top-level namespace with information about the nodes, their type, and dependencies. The structure of the dictionary is: {'node_name/namespace_name' : {'name': 'node_name/namespace_name','type': 'namespace' or 'node','nodes': [list of nodes],'dependencies': [list of dependencies]}} This property is intended to be used by deployment plugins to group nodes by namespace.</p>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.node_dependencies","title":"node_dependencies  <code>property</code>","text":"<pre><code>node_dependencies\n</code></pre> <p>All dependencies of nodes where the first Node has a direct dependency on the second Node.</p> <p>Returns:</p> <ul> <li> <code>dict[Node, set[Node]]</code>           \u2013            <p>Dictionary where keys are nodes and values are sets made up of</p> </li> <li> <code>dict[Node, set[Node]]</code>           \u2013            <p>their parent nodes. Independent nodes have this as empty sets.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.nodes","title":"nodes  <code>property</code>","text":"<pre><code>nodes\n</code></pre> <p>Return a list of the pipeline nodes in topological order, i.e. if node A needs to be run before node B, it will appear earlier in the list.</p> <p>Returns:</p> <ul> <li> <code>list[Node]</code>           \u2013            <p>The list of all pipeline nodes in topological order.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def __add__(self, other: Any) -&gt; Pipeline:\n    if not isinstance(other, Pipeline):\n        return NotImplemented\n    return Pipeline(set(self._nodes + other._nodes))\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.__and__","title":"__and__","text":"<pre><code>__and__(other)\n</code></pre> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def __and__(self, other: Any) -&gt; Pipeline:\n    if not isinstance(other, Pipeline):\n        return NotImplemented\n    return Pipeline(set(self._nodes) &amp; set(other._nodes))\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.__or__","title":"__or__","text":"<pre><code>__or__(other)\n</code></pre> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def __or__(self, other: Any) -&gt; Pipeline:\n    if not isinstance(other, Pipeline):\n        return NotImplemented\n    return Pipeline(set(self._nodes + other._nodes))\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.__radd__","title":"__radd__","text":"<pre><code>__radd__(other)\n</code></pre> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def __radd__(self, other: Any) -&gt; Pipeline:\n    if isinstance(other, int) and other == 0:\n        return self\n    return self.__add__(other)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Pipeline ([node1, ..., node10 ...], name='pipeline_name')</p> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def __repr__(self) -&gt; str:  # pragma: no cover\n    \"\"\"Pipeline ([node1, ..., node10 ...], name='pipeline_name')\"\"\"\n    max_nodes_to_display = 10\n\n    nodes_reprs = [repr(node) for node in self.nodes[:max_nodes_to_display]]\n    if len(self.nodes) &gt; max_nodes_to_display:\n        nodes_reprs.append(\"...\")\n    sep = \",\\n\"\n    nodes_reprs_str = f\"[\\n{sep.join(nodes_reprs)}\\n]\" if nodes_reprs else \"[]\"\n    constructor_repr = f\"({nodes_reprs_str})\"\n    return f\"{self.__class__.__name__}{constructor_repr}\"\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.__sub__","title":"__sub__","text":"<pre><code>__sub__(other)\n</code></pre> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def __sub__(self, other: Any) -&gt; Pipeline:\n    if not isinstance(other, Pipeline):\n        return NotImplemented\n    return Pipeline(set(self._nodes) - set(other._nodes))\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline._get_nodes_with_inputs_transcode_compatible","title":"_get_nodes_with_inputs_transcode_compatible","text":"<pre><code>_get_nodes_with_inputs_transcode_compatible(datasets)\n</code></pre> <p>Retrieves nodes that use the given <code>datasets</code> as inputs. If provided a name, but no format, for a transcoded dataset, it includes all nodes that use inputs with that name, otherwise it matches to the fully-qualified name only (i.e. name@format).</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>if any of the given datasets do not exist in the <code>Pipeline</code> object</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[Node]</code>           \u2013            <p>Set of <code>Nodes</code> that use the given datasets as inputs.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def _get_nodes_with_inputs_transcode_compatible(\n    self, datasets: set[str]\n) -&gt; set[Node]:\n    \"\"\"Retrieves nodes that use the given `datasets` as inputs.\n    If provided a name, but no format, for a transcoded dataset, it\n    includes all nodes that use inputs with that name, otherwise it\n    matches to the fully-qualified name only (i.e. name@format).\n\n    Raises:\n        ValueError: if any of the given datasets do not exist in the\n            ``Pipeline`` object\n\n    Returns:\n        Set of ``Nodes`` that use the given datasets as inputs.\n    \"\"\"\n    missing = sorted(\n        datasets - self.datasets() - self._transcode_compatible_names()\n    )\n    if missing:\n        raise ValueError(f\"Pipeline does not contain datasets named {missing}\")\n\n    relevant_nodes = set()\n    for input_ in datasets:\n        if _strip_transcoding(input_) == input_:\n            relevant_nodes.update(self._nodes_by_input[_strip_transcoding(input_)])\n        else:\n            for node_ in self._nodes_by_input[_strip_transcoding(input_)]:\n                if input_ in node_.inputs:\n                    relevant_nodes.add(node_)\n    return relevant_nodes\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline._get_nodes_with_outputs_transcode_compatible","title":"_get_nodes_with_outputs_transcode_compatible","text":"<pre><code>_get_nodes_with_outputs_transcode_compatible(datasets)\n</code></pre> <p>Retrieves nodes that output to the given <code>datasets</code>. If provided a name, but no format, for a transcoded dataset, it includes the node that outputs to that name, otherwise it matches to the fully-qualified name only (i.e. name@format).</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>if any of the given datasets do not exist in the <code>Pipeline</code> object</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[Node]</code>           \u2013            <p>Set of <code>Nodes</code> that output to the given datasets.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def _get_nodes_with_outputs_transcode_compatible(\n    self, datasets: set[str]\n) -&gt; set[Node]:\n    \"\"\"Retrieves nodes that output to the given `datasets`.\n    If provided a name, but no format, for a transcoded dataset, it\n    includes the node that outputs to that name, otherwise it matches\n    to the fully-qualified name only (i.e. name@format).\n\n    Raises:\n        ValueError: if any of the given datasets do not exist in the\n            ``Pipeline`` object\n\n    Returns:\n        Set of ``Nodes`` that output to the given datasets.\n    \"\"\"\n    missing = sorted(\n        datasets - self.datasets() - self._transcode_compatible_names()\n    )\n    if missing:\n        raise ValueError(f\"Pipeline does not contain datasets named {missing}\")\n\n    relevant_nodes = set()\n    for output in datasets:\n        if _strip_transcoding(output) in self._nodes_by_output:\n            node_with_output = self._nodes_by_output[_strip_transcoding(output)]\n            if (\n                _strip_transcoding(output) == output\n                or output in node_with_output.outputs\n            ):\n                relevant_nodes.add(node_with_output)\n\n    return relevant_nodes\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline._remove_intermediates","title":"_remove_intermediates","text":"<pre><code>_remove_intermediates(datasets)\n</code></pre> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def _remove_intermediates(self, datasets: set[str]) -&gt; set[str]:\n    intermediate = {_strip_transcoding(i) for i in self.all_inputs()} &amp; {\n        _strip_transcoding(o) for o in self.all_outputs()\n    }\n    return {d for d in datasets if _strip_transcoding(d) not in intermediate}\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline._transcode_compatible_names","title":"_transcode_compatible_names","text":"<pre><code>_transcode_compatible_names()\n</code></pre> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def _transcode_compatible_names(self) -&gt; set[str]:\n    return {_strip_transcoding(ds) for ds in self.datasets()}\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.all_inputs","title":"all_inputs","text":"<pre><code>all_inputs()\n</code></pre> <p>All inputs for all nodes in the pipeline.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>All node input names as a Set.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def all_inputs(self) -&gt; set[str]:\n    \"\"\"All inputs for all nodes in the pipeline.\n\n    Returns:\n        All node input names as a Set.\n\n    \"\"\"\n    return set.union(set(), *(node.inputs for node in self._nodes))\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.all_outputs","title":"all_outputs","text":"<pre><code>all_outputs()\n</code></pre> <p>All outputs of all nodes in the pipeline.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>All node outputs.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def all_outputs(self) -&gt; set[str]:\n    \"\"\"All outputs of all nodes in the pipeline.\n\n    Returns:\n        All node outputs.\n\n    \"\"\"\n    return set.union(set(), *(node.outputs for node in self._nodes))\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.datasets","title":"datasets","text":"<pre><code>datasets()\n</code></pre> <p>The names of all datasets used by the <code>Pipeline</code>, including inputs and outputs.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>The set of all pipeline datasets.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def datasets(self) -&gt; set[str]:\n    \"\"\"The names of all datasets used by the ``Pipeline``,\n    including inputs and outputs.\n\n    Returns:\n        The set of all pipeline datasets.\n\n    \"\"\"\n    return self.all_outputs() | self.all_inputs()\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.describe","title":"describe","text":"<pre><code>describe(names_only=True)\n</code></pre> <p>Obtain the order of execution and expected free input variables in a loggable pre-formatted string. The order of nodes matches the order of execution given by the topological sort.</p> <p>Parameters:</p> <ul> <li> <code>names_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>The flag to describe names_only pipeline with just node names.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; pipeline = Pipeline([ ... ])\n&gt;&gt;&gt;\n&gt;&gt;&gt; logger = logging.getLogger(__name__)\n&gt;&gt;&gt;\n&gt;&gt;&gt; logger.info(pipeline.describe())\n</code></pre> <p>After invocation the following will be printed as an info level log statement: ::</p> <pre><code>#### Pipeline execution order ####\nInputs: C, D\n\nfunc1([C]) -&gt; [A]\nfunc2([D]) -&gt; [B]\nfunc3([A, D]) -&gt; [E]\n\nOutputs: B, E\n##################################\n</code></pre> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The pipeline description as a formatted string.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def describe(self, names_only: bool = True) -&gt; str:\n    \"\"\"Obtain the order of execution and expected free input variables in\n    a loggable pre-formatted string. The order of nodes matches the order\n    of execution given by the topological sort.\n\n    Args:\n        names_only: The flag to describe names_only pipeline with just\n            node names.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; pipeline = Pipeline([ ... ])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; logger = logging.getLogger(__name__)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; logger.info(pipeline.describe())\n\n    After invocation the following will be printed as an info level log\n    statement:\n    ::\n\n        #### Pipeline execution order ####\n        Inputs: C, D\n\n        func1([C]) -&gt; [A]\n        func2([D]) -&gt; [B]\n        func3([A, D]) -&gt; [E]\n\n        Outputs: B, E\n        ##################################\n\n    Returns:\n        The pipeline description as a formatted string.\n\n    \"\"\"\n\n    def set_to_string(set_of_strings: set[str]) -&gt; str:\n        \"\"\"Convert set to a string but return 'None' in case of an empty\n        set.\n        \"\"\"\n        return \", \".join(sorted(set_of_strings)) if set_of_strings else \"None\"\n\n    nodes_as_string = \"\\n\".join(\n        node.name if names_only else str(node) for node in self.nodes\n    )\n\n    str_representation = (\n        \"#### Pipeline execution order ####\\n\"\n        \"Inputs: {0}\\n\\n\"\n        \"{1}\\n\\n\"\n        \"Outputs: {2}\\n\"\n        \"##################################\"\n    )\n\n    return str_representation.format(\n        set_to_string(self.inputs()), nodes_as_string, set_to_string(self.outputs())\n    )\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.filter","title":"filter","text":"<pre><code>filter(tags=None, from_nodes=None, to_nodes=None, node_names=None, from_inputs=None, to_outputs=None, node_namespace=None)\n</code></pre> <p>Creates a new <code>Pipeline</code> object with the nodes that meet all of the specified filtering conditions.</p> <p>The new pipeline object is the intersection of pipelines that meet each filtering condition. This is distinct from chaining multiple filters together.</p> <p>Parameters:</p> <ul> <li> <code>tags</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of node tags which should be used to lookup the nodes of the new <code>Pipeline</code>.</p> </li> <li> <code>from_nodes</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of node names which should be used as a starting point of the new <code>Pipeline</code>.</p> </li> <li> <code>to_nodes</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of node names which should be used as an end point of the new <code>Pipeline</code>.</p> </li> <li> <code>node_names</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of node names which should be selected for the new <code>Pipeline</code>.</p> </li> <li> <code>from_inputs</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of inputs which should be used as a starting point of the new <code>Pipeline</code></p> </li> <li> <code>to_outputs</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of outputs which should be the final outputs of the new <code>Pipeline</code>.</p> </li> <li> <code>node_namespace</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>One node namespace which should be used to select nodes in the new <code>Pipeline</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> object with nodes that meet all of the specified filtering conditions.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>The filtered <code>Pipeline</code> has no nodes.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; pipeline = Pipeline(\n&gt;&gt;&gt;     [\n&gt;&gt;&gt;         node(func, \"A\", \"B\", name=\"node1\"),\n&gt;&gt;&gt;         node(func, \"B\", \"C\", name=\"node2\"),\n&gt;&gt;&gt;         node(func, \"C\", \"D\", name=\"node3\"),\n&gt;&gt;&gt;     ]\n&gt;&gt;&gt; )\n&gt;&gt;&gt; pipeline.filter(node_names=[\"node1\", \"node3\"], from_inputs=[\"A\"])\n&gt;&gt;&gt; # Gives a new pipeline object containing node1 and node3.\n</code></pre> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def filter(  # noqa: PLR0913\n    self,\n    tags: Iterable[str] | None = None,\n    from_nodes: Iterable[str] | None = None,\n    to_nodes: Iterable[str] | None = None,\n    node_names: Iterable[str] | None = None,\n    from_inputs: Iterable[str] | None = None,\n    to_outputs: Iterable[str] | None = None,\n    node_namespace: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Creates a new ``Pipeline`` object with the nodes that meet all of the\n    specified filtering conditions.\n\n    The new pipeline object is the intersection of pipelines that meet each\n    filtering condition. This is distinct from chaining multiple filters together.\n\n    Args:\n        tags: A list of node tags which should be used to lookup\n            the nodes of the new ``Pipeline``.\n        from_nodes: A list of node names which should be used as a\n            starting point of the new ``Pipeline``.\n        to_nodes:  A list of node names which should be used as an\n            end point of the new ``Pipeline``.\n        node_names: A list of node names which should be selected for the\n            new ``Pipeline``.\n        from_inputs: A list of inputs which should be used as a starting point\n            of the new ``Pipeline``\n        to_outputs: A list of outputs which should be the final outputs of\n            the new ``Pipeline``.\n        node_namespace: One node namespace which should be used to select\n            nodes in the new ``Pipeline``.\n\n    Returns:\n        A new ``Pipeline`` object with nodes that meet all of the specified\n            filtering conditions.\n\n    Raises:\n        ValueError: The filtered ``Pipeline`` has no nodes.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; pipeline = Pipeline(\n        &gt;&gt;&gt;     [\n        &gt;&gt;&gt;         node(func, \"A\", \"B\", name=\"node1\"),\n        &gt;&gt;&gt;         node(func, \"B\", \"C\", name=\"node2\"),\n        &gt;&gt;&gt;         node(func, \"C\", \"D\", name=\"node3\"),\n        &gt;&gt;&gt;     ]\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; pipeline.filter(node_names=[\"node1\", \"node3\"], from_inputs=[\"A\"])\n        &gt;&gt;&gt; # Gives a new pipeline object containing node1 and node3.\n    \"\"\"\n    # Use [node_namespace] so only_nodes_with_namespace can follow the same\n    # *filter_args pattern as the other filtering methods, which all take iterables.\n    node_namespace_iterable = [node_namespace] if node_namespace else None\n\n    filter_methods = {\n        self.only_nodes_with_tags: tags,\n        self.from_nodes: from_nodes,\n        self.to_nodes: to_nodes,\n        self.only_nodes: node_names,\n        self.from_inputs: from_inputs,\n        self.to_outputs: to_outputs,\n        self.only_nodes_with_namespace: node_namespace_iterable,\n    }\n\n    subset_pipelines = {\n        filter_method(*filter_args)  # type: ignore\n        for filter_method, filter_args in filter_methods.items()\n        if filter_args\n    }\n\n    # Intersect all the pipelines subsets. We apply each filter to the original\n    # pipeline object (self) rather than incrementally chaining filter methods\n    # together. Hence the order of filtering does not affect the outcome, and the\n    # resultant pipeline is unambiguously defined.\n    # If this were not the case then, for example,\n    # pipeline.filter(node_names=[\"node1\", \"node3\"], from_inputs=[\"A\"])\n    # would give different outcomes depending on the order of filter methods:\n    # only_nodes and then from_inputs would give node1, while only_nodes and then\n    # from_inputs would give node1 and node3.\n    filtered_pipeline = Pipeline(self._nodes)\n    for subset_pipeline in subset_pipelines:\n        filtered_pipeline &amp;= subset_pipeline\n\n    if not filtered_pipeline.nodes:\n        raise ValueError(\n            \"Pipeline contains no nodes after applying all provided filters. \"\n            \"Please ensure that at least one pipeline with nodes has been defined.\"\n        )\n    return filtered_pipeline\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.from_inputs","title":"from_inputs","text":"<pre><code>from_inputs(*inputs)\n</code></pre> <p>Create a new <code>Pipeline</code> object with the nodes which depend directly or transitively on the provided inputs. If provided a name, but no format, for a transcoded input, it includes all the nodes that use inputs with that name, otherwise it matches to the fully-qualified name only (i.e. name@format).</p> <p>Parameters:</p> <ul> <li> <code>*inputs</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of inputs which should be used as a starting point of the new <code>Pipeline</code></p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised when any of the given inputs do not exist in the <code>Pipeline</code> object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> object, containing a subset of the nodes of the current one such that only nodes depending directly or transitively on the provided inputs are being copied.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def from_inputs(self, *inputs: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` object with the nodes which depend\n    directly or transitively on the provided inputs.\n    If provided a name, but no format, for a transcoded input, it\n    includes all the nodes that use inputs with that name, otherwise it\n    matches to the fully-qualified name only (i.e. name@format).\n\n    Args:\n        *inputs: A list of inputs which should be used as a starting point\n            of the new ``Pipeline``\n\n    Raises:\n        ValueError: Raised when any of the given inputs do not exist in the\n            ``Pipeline`` object.\n\n    Returns:\n        A new ``Pipeline`` object, containing a subset of the\n            nodes of the current one such that only nodes depending\n            directly or transitively on the provided inputs are being\n            copied.\n\n    \"\"\"\n    starting = set(inputs)\n    result: set[Node] = set()\n    next_nodes = self._get_nodes_with_inputs_transcode_compatible(starting)\n\n    while next_nodes:\n        result |= next_nodes\n        outputs = set(chain.from_iterable(node.outputs for node in next_nodes))\n        starting = outputs\n\n        next_nodes = set(\n            chain.from_iterable(\n                self._nodes_by_input[_strip_transcoding(input_)]\n                for input_ in starting\n            )\n        )\n\n    return Pipeline(result)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.from_nodes","title":"from_nodes","text":"<pre><code>from_nodes(*node_names)\n</code></pre> <p>Create a new <code>Pipeline</code> object with the nodes which depend directly or transitively on the provided nodes.</p> <p>Parameters:</p> <ul> <li> <code>*node_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of node_names which should be used as a starting point of the new <code>Pipeline</code>.</p> </li> </ul> <p>Raises:     ValueError: Raised when any of the given names do not exist in the         <code>Pipeline</code> object. Returns:     A new <code>Pipeline</code> object, containing a subset of the nodes of         the current one such that only nodes depending directly or         transitively on the provided nodes are being copied.</p> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def from_nodes(self, *node_names: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` object with the nodes which depend\n    directly or transitively on the provided nodes.\n\n    Args:\n        *node_names: A list of node_names which should be used as a\n            starting point of the new ``Pipeline``.\n    Raises:\n        ValueError: Raised when any of the given names do not exist in the\n            ``Pipeline`` object.\n    Returns:\n        A new ``Pipeline`` object, containing a subset of the nodes of\n            the current one such that only nodes depending directly or\n            transitively on the provided nodes are being copied.\n\n    \"\"\"\n\n    res = self.only_nodes(*node_names)\n    res += self.from_inputs(*map(_strip_transcoding, res.all_outputs()))\n    return res\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.inputs","title":"inputs","text":"<pre><code>inputs()\n</code></pre> <p>The names of free inputs that must be provided at runtime so that the pipeline is runnable. Does not include intermediate inputs which are produced and consumed by the inner pipeline nodes. Resolves transcoded names where necessary.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>The set of free input names needed by the pipeline.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def inputs(self) -&gt; set[str]:\n    \"\"\"The names of free inputs that must be provided at runtime so that\n    the pipeline is runnable. Does not include intermediate inputs which\n    are produced and consumed by the inner pipeline nodes. Resolves\n    transcoded names where necessary.\n\n    Returns:\n        The set of free input names needed by the pipeline.\n\n    \"\"\"\n    return self._remove_intermediates(self.all_inputs())\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.only_nodes","title":"only_nodes","text":"<pre><code>only_nodes(*node_names)\n</code></pre> <p>Create a new <code>Pipeline</code> which will contain only the specified nodes by name.</p> <p>Parameters:</p> <ul> <li> <code>*node_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>One or more node names. The returned <code>Pipeline</code> will only contain these nodes.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When some invalid node name is given.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code>, containing only <code>nodes</code>.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def only_nodes(self, *node_names: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` which will contain only the specified\n    nodes by name.\n\n    Args:\n        *node_names: One or more node names. The returned ``Pipeline``\n            will only contain these nodes.\n\n    Raises:\n        ValueError: When some invalid node name is given.\n\n    Returns:\n        A new ``Pipeline``, containing only ``nodes``.\n\n    \"\"\"\n    unregistered_nodes = set(node_names) - set(self._nodes_by_name.keys())\n    if unregistered_nodes:\n        # check if unregistered nodes are available under namespace\n        namespaces = []\n        for unregistered_node in unregistered_nodes:\n            namespaces.extend(\n                [\n                    node_name\n                    for node_name in self._nodes_by_name.keys()\n                    if node_name.endswith(f\".{unregistered_node}\")\n                ]\n            )\n        if namespaces:\n            raise ValueError(\n                f\"Pipeline does not contain nodes named {list(unregistered_nodes)}. \"\n                f\"Did you mean: {namespaces}?\"\n            )\n        raise ValueError(\n            f\"Pipeline does not contain nodes named {list(unregistered_nodes)}.\"\n        )\n\n    nodes = [self._nodes_by_name[name] for name in node_names]\n    return Pipeline(nodes)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.only_nodes_with_inputs","title":"only_nodes_with_inputs","text":"<pre><code>only_nodes_with_inputs(*inputs)\n</code></pre> <p>Create a new <code>Pipeline</code> object with the nodes which depend directly on the provided inputs. If provided a name, but no format, for a transcoded input, it includes all the nodes that use inputs with that name, otherwise it matches to the fully-qualified name only (i.e. name@format).</p> <p>Parameters:</p> <ul> <li> <code>*inputs</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of inputs which should be used as a starting point of the new <code>Pipeline</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised when any of the given inputs do not exist in the <code>Pipeline</code> object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> object, containing a subset of the nodes of the current one such that only nodes depending directly on the provided inputs are being copied.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def only_nodes_with_inputs(self, *inputs: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` object with the nodes which depend\n    directly on the provided inputs.\n    If provided a name, but no format, for a transcoded input, it\n    includes all the nodes that use inputs with that name, otherwise it\n    matches to the fully-qualified name only (i.e. name@format).\n\n    Args:\n        *inputs: A list of inputs which should be used as a starting\n            point of the new ``Pipeline``.\n\n    Raises:\n        ValueError: Raised when any of the given inputs do not exist in the\n            ``Pipeline`` object.\n\n    Returns:\n        A new ``Pipeline`` object, containing a subset of the\n            nodes of the current one such that only nodes depending\n            directly on the provided inputs are being copied.\n\n    \"\"\"\n    starting = set(inputs)\n    nodes = self._get_nodes_with_inputs_transcode_compatible(starting)\n\n    return Pipeline(nodes)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.only_nodes_with_namespace","title":"only_nodes_with_namespace","text":"<pre><code>only_nodes_with_namespace(node_namespace)\n</code></pre> <p>Creates a new <code>Pipeline</code> containing only nodes with the specified namespace.</p> <p>Parameters:</p> <ul> <li> <code>node_namespace</code>               (<code>str</code>)           \u2013            <p>One node namespace.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When pipeline contains no nodes with the specified namespace.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> containing nodes with the specified namespace.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def only_nodes_with_namespace(self, node_namespace: str) -&gt; Pipeline:\n    \"\"\"Creates a new ``Pipeline`` containing only nodes with the specified\n    namespace.\n\n    Args:\n        node_namespace: One node namespace.\n\n    Raises:\n        ValueError: When pipeline contains no nodes with the specified namespace.\n\n    Returns:\n        A new ``Pipeline`` containing nodes with the specified namespace.\n    \"\"\"\n    nodes = [\n        n\n        for n in self._nodes\n        if n.namespace and n.namespace.startswith(node_namespace)\n    ]\n    if not nodes:\n        raise ValueError(\n            f\"Pipeline does not contain nodes with namespace '{node_namespace}'\"\n        )\n    return Pipeline(nodes)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.only_nodes_with_outputs","title":"only_nodes_with_outputs","text":"<pre><code>only_nodes_with_outputs(*outputs)\n</code></pre> <p>Create a new <code>Pipeline</code> object with the nodes which are directly required to produce the provided outputs. If provided a name, but no format, for a transcoded dataset, it includes all the nodes that output to that name, otherwise it matches to the fully-qualified name only (i.e. name@format).</p> <p>Parameters:</p> <ul> <li> <code>*outputs</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of outputs which should be the final outputs of the new <code>Pipeline</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised when any of the given outputs do not exist in the <code>Pipeline</code> object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> object, containing a subset of the nodes of the</p> </li> <li> <code>Pipeline</code>           \u2013            <p>current one such that only nodes which are directly required to</p> </li> <li> <code>Pipeline</code>           \u2013            <p>produce the provided outputs are being copied.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def only_nodes_with_outputs(self, *outputs: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` object with the nodes which are directly\n    required to produce the provided outputs.\n    If provided a name, but no format, for a transcoded dataset, it\n    includes all the nodes that output to that name, otherwise it matches\n    to the fully-qualified name only (i.e. name@format).\n\n    Args:\n        *outputs: A list of outputs which should be the final outputs\n            of the new ``Pipeline``.\n\n    Raises:\n        ValueError: Raised when any of the given outputs do not exist in the\n            ``Pipeline`` object.\n\n    Returns:\n        A new ``Pipeline`` object, containing a subset of the nodes of the\n        current one such that only nodes which are directly required to\n        produce the provided outputs are being copied.\n    \"\"\"\n    starting = set(outputs)\n    nodes = self._get_nodes_with_outputs_transcode_compatible(starting)\n\n    return Pipeline(nodes)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.only_nodes_with_tags","title":"only_nodes_with_tags","text":"<pre><code>only_nodes_with_tags(*tags)\n</code></pre> <p>Creates a new <code>Pipeline</code> object with the nodes which contain any of the provided tags. The resulting <code>Pipeline</code> is empty if no tags are provided.</p> <p>Parameters:</p> <ul> <li> <code>*tags</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of node tags which should be used to lookup the nodes of the new <code>Pipeline</code>.</p> </li> </ul> <p>Returns:     Pipeline: A new <code>Pipeline</code> object, containing a subset of the         nodes of the current one such that only nodes containing any         of the tags provided are being copied.</p> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def only_nodes_with_tags(self, *tags: str) -&gt; Pipeline:\n    \"\"\"Creates a new ``Pipeline`` object with the nodes which contain *any*\n    of the provided tags. The resulting ``Pipeline`` is empty if no tags\n    are provided.\n\n    Args:\n        *tags: A list of node tags which should be used to lookup\n            the nodes of the new ``Pipeline``.\n    Returns:\n        Pipeline: A new ``Pipeline`` object, containing a subset of the\n            nodes of the current one such that only nodes containing *any*\n            of the tags provided are being copied.\n    \"\"\"\n    unique_tags = set(tags)\n    nodes = [node for node in self._nodes if unique_tags &amp; node.tags]\n    return Pipeline(nodes)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.outputs","title":"outputs","text":"<pre><code>outputs()\n</code></pre> <p>The names of outputs produced when the whole pipeline is run. Does not include intermediate outputs that are consumed by other pipeline nodes. Resolves transcoded names where necessary.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>The set of final pipeline outputs.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def outputs(self) -&gt; set[str]:\n    \"\"\"The names of outputs produced when the whole pipeline is run.\n    Does not include intermediate outputs that are consumed by\n    other pipeline nodes. Resolves transcoded names where necessary.\n\n    Returns:\n        The set of final pipeline outputs.\n\n    \"\"\"\n    return self._remove_intermediates(self.all_outputs())\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.tag","title":"tag","text":"<pre><code>tag(tags)\n</code></pre> <p>Tags all the nodes in the pipeline.</p> <p>Parameters:</p> <ul> <li> <code>tags</code>               (<code>str | Iterable[str]</code>)           \u2013            <p>The tags to be added to the nodes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>New <code>Pipeline</code> object with nodes tagged.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def tag(self, tags: str | Iterable[str]) -&gt; Pipeline:\n    \"\"\"Tags all the nodes in the pipeline.\n\n    Args:\n        tags: The tags to be added to the nodes.\n\n    Returns:\n        New ``Pipeline`` object with nodes tagged.\n    \"\"\"\n    nodes = [n.tag(tags) for n in self._nodes]\n    return Pipeline(nodes)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.to_json","title":"to_json","text":"<pre><code>to_json()\n</code></pre> <p>Return a json representation of the pipeline.</p> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Return a json representation of the pipeline.\"\"\"\n    transformed = [\n        {\n            \"name\": n.name,\n            \"inputs\": list(n.inputs),\n            \"outputs\": list(n.outputs),\n            \"tags\": list(n.tags),\n        }\n        for n in self._nodes\n    ]\n    pipeline_versioned = {\n        \"kedro_version\": kedro.__version__,\n        \"pipeline\": transformed,\n    }\n\n    return json.dumps(pipeline_versioned)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.to_nodes","title":"to_nodes","text":"<pre><code>to_nodes(*node_names)\n</code></pre> <p>Create a new <code>Pipeline</code> object with the nodes required directly or transitively by the provided nodes.</p> <p>Parameters:</p> <ul> <li> <code>*node_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of node_names which should be used as an end point of the new <code>Pipeline</code>.</p> </li> </ul> <p>Raises:     ValueError: Raised when any of the given names do not exist in the         <code>Pipeline</code> object. Returns:     A new <code>Pipeline</code> object, containing a subset of the nodes of the         current one such that only nodes required directly or         transitively by the provided nodes are being copied.</p> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def to_nodes(self, *node_names: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` object with the nodes required directly\n    or transitively by the provided nodes.\n\n    Args:\n        *node_names: A list of node_names which should be used as an\n            end point of the new ``Pipeline``.\n    Raises:\n        ValueError: Raised when any of the given names do not exist in the\n            ``Pipeline`` object.\n    Returns:\n        A new ``Pipeline`` object, containing a subset of the nodes of the\n            current one such that only nodes required directly or\n            transitively by the provided nodes are being copied.\n\n    \"\"\"\n\n    res = self.only_nodes(*node_names)\n    res += self.to_outputs(*map(_strip_transcoding, res.all_inputs()))\n    return res\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.Pipeline/#kedro.pipeline.Pipeline.to_outputs","title":"to_outputs","text":"<pre><code>to_outputs(*outputs)\n</code></pre> <p>Create a new <code>Pipeline</code> object with the nodes which are directly or transitively required to produce the provided outputs. If provided a name, but no format, for a transcoded dataset, it includes all the nodes that output to that name, otherwise it matches to the fully-qualified name only (i.e. name@format).</p> <p>Parameters:</p> <ul> <li> <code>*outputs</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of outputs which should be the final outputs of the new <code>Pipeline</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised when any of the given outputs do not exist in the <code>Pipeline</code> object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> object, containing a subset of the nodes of the</p> </li> <li> <code>Pipeline</code>           \u2013            <p>current one such that only nodes which are directly or transitively</p> </li> <li> <code>Pipeline</code>           \u2013            <p>required to produce the provided outputs are being copied.</p> </li> </ul> Source code in <code>kedro/pipeline/pipeline.py</code> <pre><code>def to_outputs(self, *outputs: str) -&gt; Pipeline:\n    \"\"\"Create a new ``Pipeline`` object with the nodes which are directly\n    or transitively required to produce the provided outputs.\n    If provided a name, but no format, for a transcoded dataset, it\n    includes all the nodes that output to that name, otherwise it matches\n    to the fully-qualified name only (i.e. name@format).\n\n    Args:\n        *outputs: A list of outputs which should be the final outputs of\n            the new ``Pipeline``.\n\n    Raises:\n        ValueError: Raised when any of the given outputs do not exist in the\n            ``Pipeline`` object.\n\n\n    Returns:\n        A new ``Pipeline`` object, containing a subset of the nodes of the\n        current one such that only nodes which are directly or transitively\n        required to produce the provided outputs are being copied.\n\n    \"\"\"\n    starting = set(outputs)\n    result: set[Node] = set()\n    next_nodes = self._get_nodes_with_outputs_transcode_compatible(starting)\n\n    while next_nodes:\n        result |= next_nodes\n        inputs = set(chain.from_iterable(node.inputs for node in next_nodes))\n        starting = inputs\n\n        next_nodes = {\n            self._nodes_by_output[_strip_transcoding(output)]\n            for output in starting\n            if _strip_transcoding(output) in self._nodes_by_output\n        }\n\n    return Pipeline(result)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline/","title":"kedro.pipeline","text":"Name Type Description <code>kedro.pipeline.node</code> Function A decorator to define a node in a Kedro pipeline. <code>kedro.pipeline.modular_pipeline.pipeline</code> Function A helper to create modular pipelines. <code>kedro.pipeline.Pipeline</code> Class Represents a Kedro pipeline. <code>kedro.pipeline.node.Node</code> Class Represents a single node in a Kedro pipeline. <code>kedro.pipeline.modular_pipeline.ModularPipelineError</code> Exception Raised for errors in modular pipelines."},{"location":"api/pipeline/kedro.pipeline/#kedro.pipeline","title":"kedro.pipeline","text":"<p><code>kedro.pipeline</code> provides functionality to define and execute data-driven pipelines.</p>"},{"location":"api/pipeline/kedro.pipeline.modular_pipeline.ModularPipelineError/","title":"ModularPipelineError","text":""},{"location":"api/pipeline/kedro.pipeline.modular_pipeline.ModularPipelineError/#kedro.pipeline.modular_pipeline.ModularPipelineError","title":"kedro.pipeline.modular_pipeline.ModularPipelineError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a modular pipeline is not adapted and integrated appropriately using the helper.</p>"},{"location":"api/pipeline/kedro.pipeline.modular_pipeline.pipeline/","title":"modular_pipeline.pipeline","text":""},{"location":"api/pipeline/kedro.pipeline.modular_pipeline.pipeline/#kedro.pipeline.modular_pipeline.pipeline","title":"kedro.pipeline.modular_pipeline.pipeline","text":"<pre><code>pipeline(pipe, *, inputs=None, outputs=None, parameters=None, tags=None, namespace=None)\n</code></pre> <p>Create a <code>Pipeline</code> from a collection of nodes and/or <code>Pipeline</code>\\s.</p> <p>Parameters:</p> <ul> <li> <code>pipe</code>               (<code>Iterable[Node | Pipeline] | Pipeline</code>)           \u2013            <p>The nodes the <code>Pipeline</code> will be made of. If you provide pipelines among the list of nodes, those pipelines will be expanded and all their nodes will become part of this new pipeline.</p> </li> <li> <code>inputs</code>               (<code>str | set[str] | dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A name or collection of input names to be exposed as connection points to other pipelines upstream. This is optional; if not provided, the pipeline inputs are automatically inferred from the pipeline structure. When str or set[str] is provided, the listed input names will stay the same as they are named in the provided pipeline. When dict[str, str] is provided, current input names will be mapped to new names. Must only refer to the pipeline's free inputs.</p> </li> <li> <code>outputs</code>               (<code>str | set[str] | dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A name or collection of names to be exposed as connection points to other pipelines downstream. This is optional; if not provided, the pipeline outputs are automatically inferred from the pipeline structure. When str or set[str] is provided, the listed output names will stay the same as they are named in the provided pipeline. When dict[str, str] is provided, current output names will be mapped to new names. Can refer to both the pipeline's free outputs, as well as intermediate results that need to be exposed.</p> </li> <li> <code>parameters</code>               (<code>str | set[str] | dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A name or collection of parameters to namespace. When str or set[str] are provided, the listed parameter names will stay the same as they are named in the provided pipeline. When dict[str, str] is provided, current parameter names will be mapped to new names. The parameters can be specified without the <code>params:</code> prefix.</p> </li> <li> <code>tags</code>               (<code>str | Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional set of tags to be applied to all the pipeline nodes.</p> </li> <li> <code>namespace</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A prefix to give to all dataset names, except those explicitly named with the <code>inputs</code>/<code>outputs</code> arguments, and parameter references (<code>params:</code> and <code>parameters</code>).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ModularPipelineError</code>             \u2013            <p>When inputs, outputs or parameters are incorrectly specified, or they do not exist on the original pipeline.</p> </li> <li> <code>ValueError</code>             \u2013            <p>When underlying pipeline nodes inputs/outputs are not any of the expected types (str, dict, list, or None).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>           \u2013            <p>A new <code>Pipeline</code> object.</p> </li> </ul> Source code in <code>kedro/pipeline/modular_pipeline.py</code> <pre><code>def pipeline(  # noqa: PLR0913\n    pipe: Iterable[Node | Pipeline] | Pipeline,\n    *,\n    inputs: str | set[str] | dict[str, str] | None = None,\n    outputs: str | set[str] | dict[str, str] | None = None,\n    parameters: str | set[str] | dict[str, str] | None = None,\n    tags: str | Iterable[str] | None = None,\n    namespace: str | None = None,\n) -&gt; Pipeline:\n    r\"\"\"Create a ``Pipeline`` from a collection of nodes and/or ``Pipeline``\\s.\n\n    Args:\n        pipe: The nodes the ``Pipeline`` will be made of. If you\n            provide pipelines among the list of nodes, those pipelines will\n            be expanded and all their nodes will become part of this\n            new pipeline.\n        inputs: A name or collection of input names to be exposed as connection points\n            to other pipelines upstream. This is optional; if not provided, the\n            pipeline inputs are automatically inferred from the pipeline structure.\n            When str or set[str] is provided, the listed input names will stay\n            the same as they are named in the provided pipeline.\n            When dict[str, str] is provided, current input names will be\n            mapped to new names.\n            Must only refer to the pipeline's free inputs.\n        outputs: A name or collection of names to be exposed as connection points\n            to other pipelines downstream. This is optional; if not provided, the\n            pipeline outputs are automatically inferred from the pipeline structure.\n            When str or set[str] is provided, the listed output names will stay\n            the same as they are named in the provided pipeline.\n            When dict[str, str] is provided, current output names will be\n            mapped to new names.\n            Can refer to both the pipeline's free outputs, as well as\n            intermediate results that need to be exposed.\n        parameters: A name or collection of parameters to namespace.\n            When str or set[str] are provided, the listed parameter names will stay\n            the same as they are named in the provided pipeline.\n            When dict[str, str] is provided, current parameter names will be\n            mapped to new names.\n            The parameters can be specified without the `params:` prefix.\n        tags: Optional set of tags to be applied to all the pipeline nodes.\n        namespace: A prefix to give to all dataset names,\n            except those explicitly named with the `inputs`/`outputs`\n            arguments, and parameter references (`params:` and `parameters`).\n\n    Raises:\n        ModularPipelineError: When inputs, outputs or parameters are incorrectly\n            specified, or they do not exist on the original pipeline.\n        ValueError: When underlying pipeline nodes inputs/outputs are not\n            any of the expected types (str, dict, list, or None).\n\n    Returns:\n        A new ``Pipeline`` object.\n    \"\"\"\n    if isinstance(pipe, Pipeline):\n        # To ensure that we are always dealing with a *copy* of pipe.\n        pipe = Pipeline([pipe], tags=tags)\n    else:\n        pipe = Pipeline(pipe, tags=tags)\n\n    if not any([inputs, outputs, parameters, namespace]):\n        return pipe\n\n    inputs = _get_dataset_names_mapping(inputs)\n    outputs = _get_dataset_names_mapping(outputs)\n    parameters = _get_param_names_mapping(parameters)\n\n    _validate_datasets_exist(inputs.keys(), outputs.keys(), parameters.keys(), pipe)\n    _validate_inputs_outputs(inputs.keys(), outputs.keys(), pipe)\n\n    mapping = {**inputs, **outputs, **parameters}\n\n    def _prefix_dataset(name: str) -&gt; str:\n        return f\"{namespace}.{name}\"\n\n    def _prefix_param(name: str) -&gt; str:\n        _, param_name = name.split(\"params:\")\n        return f\"params:{namespace}.{param_name}\"\n\n    def _is_transcode_base_in_mapping(name: str) -&gt; bool:\n        base_name, _ = _transcode_split(name)\n        return base_name in mapping\n\n    def _map_transcode_base(name: str) -&gt; str:\n        base_name, transcode_suffix = _transcode_split(name)\n        return TRANSCODING_SEPARATOR.join((mapping[base_name], transcode_suffix))\n\n    def _rename(name: str) -&gt; str:\n        rules = [\n            # if name mapped to new name, update with new name\n            (lambda n: n in mapping, lambda n: mapping[n]),\n            # if name refers to the set of all \"parameters\", leave as is\n            (_is_all_parameters, lambda n: n),\n            # if transcode base is mapped to a new name, update with new base\n            (_is_transcode_base_in_mapping, _map_transcode_base),\n            # if name refers to a single parameter and a namespace is given, apply prefix\n            (lambda n: bool(namespace) and _is_single_parameter(n), _prefix_param),\n            # if namespace given for a dataset, prefix name using that namespace\n            (lambda n: bool(namespace), _prefix_dataset),\n        ]\n\n        for predicate, processor in rules:\n            if predicate(name):  # type: ignore[no-untyped-call]\n                processor_name: str = processor(name)  # type: ignore[no-untyped-call]\n                return processor_name\n\n        # leave name as is\n        return name\n\n    def _process_dataset_names(\n        datasets: str | list[str] | dict[str, str] | None,\n    ) -&gt; str | list[str] | dict[str, str] | None:\n        if datasets is None:\n            return None\n        if isinstance(datasets, str):\n            return _rename(datasets)\n        if isinstance(datasets, list):\n            return [_rename(name) for name in datasets]\n        if isinstance(datasets, dict):\n            return {key: _rename(value) for key, value in datasets.items()}\n\n        raise ValueError(  # pragma: no cover\n            f\"Unexpected input {datasets} of type {type(datasets)}\"\n        )\n\n    def _copy_node(node: Node) -&gt; Node:\n        new_namespace = node.namespace\n        if namespace:\n            new_namespace = (\n                f\"{namespace}.{node.namespace}\" if node.namespace else namespace\n            )\n\n        node_copy: Node = node._copy(\n            inputs=_process_dataset_names(node._inputs),\n            outputs=_process_dataset_names(node._outputs),\n            namespace=new_namespace,\n        )\n        return node_copy\n\n    new_nodes = [_copy_node(n) for n in pipe.nodes]\n\n    return Pipeline(new_nodes, tags=tags)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/","title":"Node","text":""},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node","title":"kedro.pipeline.node.Node","text":"<pre><code>Node(func, inputs, outputs, *, name=None, tags=None, confirms=None, namespace=None)\n</code></pre> <p><code>Node</code> is an auxiliary class facilitating the operations required to run user-provided functions as part of Kedro pipelines.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable</code>)           \u2013            <p>A function that corresponds to the node logic. The function should have at least one input or output.</p> </li> <li> <code>inputs</code>               (<code>str | list[str] | dict[str, str] | None</code>)           \u2013            <p>The name or the list of the names of variables used as inputs to the function. The number of names should match the number of arguments in the definition of the provided function. When dict[str, str] is provided, variable names will be mapped to function argument names.</p> </li> <li> <code>outputs</code>               (<code>str | list[str] | dict[str, str] | None</code>)           \u2013            <p>The name or the list of the names of variables used as outputs of the function. The number of names should match the number of outputs returned by the provided function. When dict[str, str] is provided, variable names will be mapped to the named outputs the function returns.</p> </li> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional node name to be used when displaying the node in logs or any other visualisations. Valid node name must contain only letters, digits, hyphens, underscores and/or fullstops.</p> </li> <li> <code>tags</code>               (<code>str | Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional set of tags to be applied to the node. Valid node tag must contain only letters, digits, hyphens, underscores and/or fullstops.</p> </li> <li> <code>confirms</code>               (<code>str | list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional name or the list of the names of the datasets that should be confirmed. This will result in calling <code>confirm()</code> method of the corresponding dataset instance. Specified dataset names do not necessarily need to be present in the node <code>inputs</code> or <code>outputs</code>.</p> </li> <li> <code>namespace</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional node namespace.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised in the following cases: a) When the provided arguments do not conform to the format suggested by the type hint of the argument. b) When the node produces multiple outputs with the same name. c) When an input has the same name as an output. d) When the given node name violates the requirements: it must contain only letters, digits, hyphens, underscores and/or fullstops.</p> </li> </ul> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    func: Callable,\n    inputs: str | list[str] | dict[str, str] | None,\n    outputs: str | list[str] | dict[str, str] | None,\n    *,\n    name: str | None = None,\n    tags: str | Iterable[str] | None = None,\n    confirms: str | list[str] | None = None,\n    namespace: str | None = None,\n):\n    \"\"\"Create a node in the pipeline by providing a function to be called\n    along with variable names for inputs and/or outputs.\n\n    Args:\n        func: A function that corresponds to the node logic.\n            The function should have at least one input or output.\n        inputs: The name or the list of the names of variables used as\n            inputs to the function. The number of names should match\n            the number of arguments in the definition of the provided\n            function. When dict[str, str] is provided, variable names\n            will be mapped to function argument names.\n        outputs: The name or the list of the names of variables used\n            as outputs of the function. The number of names should match\n            the number of outputs returned by the provided function.\n            When dict[str, str] is provided, variable names will be mapped\n            to the named outputs the function returns.\n        name: Optional node name to be used when displaying the node in\n            logs or any other visualisations. Valid node name must contain\n            only letters, digits, hyphens, underscores and/or fullstops.\n        tags: Optional set of tags to be applied to the node. Valid node tag must\n            contain only letters, digits, hyphens, underscores and/or fullstops.\n        confirms: Optional name or the list of the names of the datasets\n            that should be confirmed. This will result in calling\n            ``confirm()`` method of the corresponding dataset instance.\n            Specified dataset names do not necessarily need to be present\n            in the node ``inputs`` or ``outputs``.\n        namespace: Optional node namespace.\n\n    Raises:\n        ValueError: Raised in the following cases:\n            a) When the provided arguments do not conform to\n            the format suggested by the type hint of the argument.\n            b) When the node produces multiple outputs with the same name.\n            c) When an input has the same name as an output.\n            d) When the given node name violates the requirements:\n            it must contain only letters, digits, hyphens, underscores\n            and/or fullstops.\n\n    \"\"\"\n    if not callable(func):\n        raise ValueError(\n            _node_error_message(\n                f\"first argument must be a function, not '{type(func).__name__}'.\"\n            )\n        )\n\n    if inputs and not isinstance(inputs, (list, dict, str)):\n        raise ValueError(\n            _node_error_message(\n                f\"'inputs' type must be one of [String, List, Dict, None], \"\n                f\"not '{type(inputs).__name__}'.\"\n            )\n        )\n\n    for _input in _to_list(inputs):\n        if not isinstance(_input, str):\n            raise ValueError(\n                _node_error_message(\n                    f\"names of variables used as inputs to the function \"\n                    f\"must be of 'String' type, but {_input} from {inputs} \"\n                    f\"is '{type(_input)}'.\"\n                )\n            )\n\n    if outputs and not isinstance(outputs, (list, dict, str)):\n        raise ValueError(\n            _node_error_message(\n                f\"'outputs' type must be one of [String, List, Dict, None], \"\n                f\"not '{type(outputs).__name__}'.\"\n            )\n        )\n\n    for _output in _to_list(outputs):\n        if not isinstance(_output, str):\n            raise ValueError(\n                _node_error_message(\n                    f\"names of variables used as outputs of the function \"\n                    f\"must be of 'String' type, but {_output} from {outputs} \"\n                    f\"is '{type(_output)}'.\"\n                )\n            )\n\n    if not inputs and not outputs:\n        raise ValueError(\n            _node_error_message(\"it must have some 'inputs' or 'outputs'.\")\n        )\n\n    self._validate_inputs(func, inputs)\n\n    self._func = func\n    self._inputs = inputs\n    # The type of _outputs is picked up as possibly being None, however the checks above prevent that\n    # ever being the case. Mypy doesn't get that though, so it complains about the assignment of outputs to\n    # _outputs with different types.\n    self._outputs: str | list[str] | dict[str, str] = outputs  # type: ignore[assignment]\n    if name and not re.match(r\"[\\w\\.-]+$\", name):\n        raise ValueError(\n            f\"'{name}' is not a valid node name. It must contain only \"\n            f\"letters, digits, hyphens, underscores and/or fullstops.\"\n        )\n    self._name = name\n    self._namespace = namespace\n    self._tags = set(_to_list(tags))\n    for tag in self._tags:\n        if not re.match(r\"[\\w\\.-]+$\", tag):\n            raise ValueError(\n                f\"'{tag}' is not a valid node tag. It must contain only \"\n                f\"letters, digits, hyphens, underscores and/or fullstops.\"\n            )\n\n    self._validate_unique_outputs()\n    self._validate_inputs_dif_than_outputs()\n    self._confirms = confirms\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._confirms","title":"_confirms  <code>instance-attribute</code>","text":"<pre><code>_confirms = confirms\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._func","title":"_func  <code>instance-attribute</code>","text":"<pre><code>_func = func\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._func_name","title":"_func_name  <code>property</code>","text":"<pre><code>_func_name\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._inputs","title":"_inputs  <code>instance-attribute</code>","text":"<pre><code>_inputs = inputs\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._logger","title":"_logger  <code>property</code>","text":"<pre><code>_logger\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._name","title":"_name  <code>instance-attribute</code>","text":"<pre><code>_name = name\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._namespace","title":"_namespace  <code>instance-attribute</code>","text":"<pre><code>_namespace = namespace\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._outputs","title":"_outputs  <code>instance-attribute</code>","text":"<pre><code>_outputs = outputs\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._tags","title":"_tags  <code>instance-attribute</code>","text":"<pre><code>_tags = set(_to_list(tags))\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._unique_key","title":"_unique_key  <code>property</code>","text":"<pre><code>_unique_key\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.confirms","title":"confirms  <code>property</code>","text":"<pre><code>confirms\n</code></pre> <p>Return dataset names to confirm as a list.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>Dataset names to confirm as a list.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.func","title":"func  <code>property</code> <code>writable</code>","text":"<pre><code>func\n</code></pre> <p>Exposes the underlying function of the node.</p> <p>Returns:</p> <ul> <li> <code>Callable</code>           \u2013            <p>Return the underlying function of the node.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.inputs","title":"inputs  <code>property</code>","text":"<pre><code>inputs\n</code></pre> <p>Return node inputs as a list, in the order required to bind them properly to the node's function.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>Node input names as a list.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Node's name.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Node's name if provided or the name of its function.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.namespace","title":"namespace  <code>property</code>","text":"<pre><code>namespace\n</code></pre> <p>Node's namespace.</p> <p>Returns:</p> <ul> <li> <code>str | None</code>           \u2013            <p>String representing node's namespace, typically from outer to inner scopes.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs\n</code></pre> <p>Return node outputs as a list preserving the original order     if possible.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>Node output names as a list.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.short_name","title":"short_name  <code>property</code>","text":"<pre><code>short_name\n</code></pre> <p>Node's name.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Returns a short, user-friendly name that is not guaranteed to be unique.</p> </li> <li> <code>str</code>           \u2013            <p>The namespace is stripped out of the node name.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.tags","title":"tags  <code>property</code>","text":"<pre><code>tags\n</code></pre> <p>Return the tags assigned to the node.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>Return the set of all assigned tags to the node.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs)\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def __call__(self, **kwargs: Any) -&gt; dict[str, Any]:\n    return self.run(inputs=kwargs)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.__eq__","title":"__eq__","text":"<pre><code>__eq__(other)\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    if not isinstance(other, Node):\n        return NotImplemented\n    return self._unique_key == other._unique_key\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def __hash__(self) -&gt; int:\n    return hash(self._unique_key)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.__lt__","title":"__lt__","text":"<pre><code>__lt__(other)\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def __lt__(self, other: Any) -&gt; bool:\n    if not isinstance(other, Node):\n        return NotImplemented\n    return self._unique_key &lt; other._unique_key\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def __repr__(self) -&gt; str:  # pragma: no cover\n    return (\n        f\"Node({self._func_name}, {self._inputs!r}, {self._outputs!r}, \"\n        f\"{self._name!r})\"\n    )\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def __str__(self) -&gt; str:\n    def _set_to_str(xset: set | list[str]) -&gt; str:\n        return f\"[{';'.join(xset)}]\"\n\n    out_str = _set_to_str(self.outputs) if self._outputs else \"None\"\n    in_str = _set_to_str(self.inputs) if self._inputs else \"None\"\n\n    prefix = self._name + \": \" if self._name else \"\"\n    return prefix + f\"{self._func_name}({in_str}) -&gt; {out_str}\"\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._copy","title":"_copy","text":"<pre><code>_copy(**overwrite_params)\n</code></pre> <p>Helper function to copy the node, replacing some values.</p> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def _copy(self, **overwrite_params: Any) -&gt; Node:\n    \"\"\"\n    Helper function to copy the node, replacing some values.\n    \"\"\"\n    params = {\n        \"func\": self._func,\n        \"inputs\": self._inputs,\n        \"outputs\": self._outputs,\n        \"name\": self._name,\n        \"namespace\": self._namespace,\n        \"tags\": self._tags,\n        \"confirms\": self._confirms,\n    }\n    params.update(overwrite_params)\n    return Node(**params)  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._outputs_to_dictionary","title":"_outputs_to_dictionary","text":"<pre><code>_outputs_to_dictionary(outputs)\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def _outputs_to_dictionary(self, outputs: Any) -&gt; dict[str, Any]:\n    def _from_dict() -&gt; dict[str, Any]:\n        result, iterator = outputs, None\n        # generator functions are lazy and we need a peek into their first output\n        if inspect.isgenerator(outputs):\n            (result,), iterator = spy(outputs)\n\n        # The type of _outputs is picked up as possibly not being a dict, but _from_dict is only called when\n        # it is a dictionary and so the calls to .keys and .values will work even though Mypy doesn't pick that up.\n        keys = list(self._outputs.keys())  # type: ignore[union-attr]\n        names = list(self._outputs.values())  # type: ignore[union-attr]\n        if not isinstance(result, dict):\n            raise ValueError(\n                f\"Failed to save outputs of node {self}.\\n\"\n                f\"The node output is a dictionary, whereas the \"\n                f\"function output is {type(result)}.\"\n            )\n        if set(keys) != set(result.keys()):\n            raise ValueError(\n                f\"Failed to save outputs of node {self!s}.\\n\"\n                f\"The node's output keys {set(result.keys())} \"\n                f\"do not match with the returned output's keys {set(keys)}.\"\n            )\n        if iterator:\n            exploded = map(lambda x: tuple(x[k] for k in keys), iterator)\n            result = unzip(exploded)\n        else:\n            # evaluate this eagerly so we can reuse variable name\n            result = tuple(result[k] for k in keys)\n        return dict(zip(names, result))\n\n    def _from_list() -&gt; dict:\n        result, iterator = outputs, None\n        # generator functions are lazy and we need a peek into their first output\n        if inspect.isgenerator(outputs):\n            (result,), iterator = spy(outputs)\n\n        if not isinstance(result, (list, tuple)):\n            raise ValueError(\n                f\"Failed to save outputs of node {self!s}.\\n\"\n                f\"The node definition contains a list of \"\n                f\"outputs {self._outputs}, whereas the node function \"\n                f\"returned a '{type(result).__name__}'.\"\n            )\n        if len(result) != len(self._outputs):\n            raise ValueError(\n                f\"Failed to save outputs of node {self!s}.\\n\"\n                f\"The node function returned {len(result)} output(s), \"\n                f\"whereas the node definition contains {len(self._outputs)} \"\n                f\"output(s).\"\n            )\n\n        if iterator:\n            result = unzip(iterator)\n        return dict(zip(self._outputs, result))\n\n    if self._outputs is None:\n        return {}\n    if isinstance(self._outputs, str):\n        return {self._outputs: outputs}\n    if isinstance(self._outputs, dict):\n        return _from_dict()\n    return _from_list()\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._process_inputs_for_bind","title":"_process_inputs_for_bind  <code>staticmethod</code>","text":"<pre><code>_process_inputs_for_bind(inputs)\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>@staticmethod\ndef _process_inputs_for_bind(\n    inputs: str | list[str] | dict[str, str] | None,\n) -&gt; tuple[list[str], dict[str, str]]:\n    # Safeguard that we do not mutate list inputs\n    inputs = copy.copy(inputs)\n    args: list[str] = []\n    kwargs: dict[str, str] = {}\n    if isinstance(inputs, str):\n        args = [inputs]\n    elif isinstance(inputs, list):\n        args = inputs\n    elif isinstance(inputs, dict):\n        kwargs = inputs\n    return args, kwargs\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._run_with_dict","title":"_run_with_dict","text":"<pre><code>_run_with_dict(inputs, node_inputs)\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def _run_with_dict(\n    self, inputs: dict[str, Any], node_inputs: dict[str, str]\n) -&gt; Any:\n    # Node inputs and provided run inputs should completely overlap\n    if set(node_inputs.values()) != set(inputs.keys()):\n        raise ValueError(\n            f\"Node {self!s} expected {len(set(node_inputs.values()))} input(s) \"\n            f\"{sorted(set(node_inputs.values()))}, \"\n            f\"but got the following {len(inputs)} input(s) instead: \"\n            f\"{sorted(inputs.keys())}.\"\n        )\n    kwargs = {arg: inputs[alias] for arg, alias in node_inputs.items()}\n    return self._func(**kwargs)\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._run_with_list","title":"_run_with_list","text":"<pre><code>_run_with_list(inputs, node_inputs)\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def _run_with_list(self, inputs: dict[str, Any], node_inputs: list[str]) -&gt; Any:\n    # Node inputs and provided run inputs should completely overlap\n    if set(node_inputs) != set(inputs.keys()):\n        raise ValueError(\n            f\"Node {self!s} expected {len(node_inputs)} input(s) {node_inputs}, \"\n            f\"but got the following {len(inputs)} input(s) instead: \"\n            f\"{sorted(inputs.keys())}.\"\n        )\n    # Ensure the function gets the inputs in the correct order\n    return self._func(*(inputs[item] for item in node_inputs))\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._run_with_no_inputs","title":"_run_with_no_inputs","text":"<pre><code>_run_with_no_inputs(inputs)\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def _run_with_no_inputs(self, inputs: dict[str, Any]) -&gt; Any:\n    if inputs:\n        raise ValueError(\n            f\"Node {self!s} expected no inputs, \"\n            f\"but got the following {len(inputs)} input(s) instead: \"\n            f\"{sorted(inputs.keys())}.\"\n        )\n\n    return self._func()\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._run_with_one_input","title":"_run_with_one_input","text":"<pre><code>_run_with_one_input(inputs, node_input)\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def _run_with_one_input(self, inputs: dict[str, Any], node_input: str) -&gt; Any:\n    if len(inputs) != 1 or node_input not in inputs:\n        raise ValueError(\n            f\"Node {self!s} expected one input named '{node_input}', \"\n            f\"but got the following {len(inputs)} input(s) instead: \"\n            f\"{sorted(inputs.keys())}.\"\n        )\n\n    return self._func(inputs[node_input])\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._validate_inputs","title":"_validate_inputs","text":"<pre><code>_validate_inputs(func, inputs)\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def _validate_inputs(\n    self, func: Callable, inputs: None | str | list[str] | dict[str, str]\n) -&gt; None:\n    # inspect does not support built-in Python functions written in C.\n    # Thus we only validate func if it is not built-in.\n    if not inspect.isbuiltin(func):\n        args, kwargs = self._process_inputs_for_bind(inputs)\n        try:\n            inspect.signature(func, follow_wrapped=False).bind(*args, **kwargs)\n        except Exception as exc:\n            func_args = inspect.signature(\n                func, follow_wrapped=False\n            ).parameters.keys()\n            func_name = _get_readable_func_name(func)\n\n            raise TypeError(\n                f\"Inputs of '{func_name}' function expected {list(func_args)}, \"\n                f\"but got {inputs}\"\n            ) from exc\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._validate_inputs_dif_than_outputs","title":"_validate_inputs_dif_than_outputs","text":"<pre><code>_validate_inputs_dif_than_outputs()\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def _validate_inputs_dif_than_outputs(self) -&gt; None:\n    common_in_out = set(map(_strip_transcoding, self.inputs)).intersection(\n        set(map(_strip_transcoding, self.outputs))\n    )\n    if common_in_out:\n        raise ValueError(\n            f\"Failed to create node {self}.\\n\"\n            f\"A node cannot have the same inputs and outputs even if they are transcoded: \"\n            f\"{common_in_out}\"\n        )\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node._validate_unique_outputs","title":"_validate_unique_outputs","text":"<pre><code>_validate_unique_outputs()\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def _validate_unique_outputs(self) -&gt; None:\n    cnt = Counter(self.outputs)\n    diff = {k for k in cnt if cnt[k] &gt; 1}\n    if diff:\n        raise ValueError(\n            f\"Failed to create node {self} due to duplicate \"\n            f\"output(s) {diff}.\\nNode outputs must be unique.\"\n        )\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.run","title":"run","text":"<pre><code>run(inputs=None)\n</code></pre> <p>Run this node using the provided inputs and return its results in a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>inputs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary of inputs as specified at the creation of the node.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>In the following cases: a) The node function inputs are incompatible with the node input definition. Example 1: node definition input is a list of 2 DataFrames, whereas only 1 was provided or 2 different ones were provided. b) The node function outputs are incompatible with the node output definition. Example 1: node function definition is a dictionary, whereas function returns a list. Example 2: node definition output is a list of 5 strings, whereas the function returns a list of 4 objects.</p> </li> <li> <code>Exception</code>             \u2013            <p>Any exception thrown during execution of the node.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>All produced node outputs are returned in a dictionary, where the</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>keys are defined by the node outputs.</p> </li> </ul> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def run(self, inputs: dict[str, Any] | None = None) -&gt; dict[str, Any]:\n    \"\"\"Run this node using the provided inputs and return its results\n    in a dictionary.\n\n    Args:\n        inputs: Dictionary of inputs as specified at the creation of\n            the node.\n\n    Raises:\n        ValueError: In the following cases:\n            a) The node function inputs are incompatible with the node\n            input definition.\n            Example 1: node definition input is a list of 2\n            DataFrames, whereas only 1 was provided or 2 different ones\n            were provided.\n            b) The node function outputs are incompatible with the node\n            output definition.\n            Example 1: node function definition is a dictionary,\n            whereas function returns a list.\n            Example 2: node definition output is a list of 5\n            strings, whereas the function returns a list of 4 objects.\n        Exception: Any exception thrown during execution of the node.\n\n    Returns:\n        All produced node outputs are returned in a dictionary, where the\n        keys are defined by the node outputs.\n\n    \"\"\"\n    self._logger.info(\"Running node: %s\", str(self))\n\n    outputs = None\n\n    if not (inputs is None or isinstance(inputs, dict)):\n        raise ValueError(\n            f\"Node.run() expects a dictionary or None, \"\n            f\"but got {type(inputs)} instead\"\n        )\n\n    try:\n        inputs = {} if inputs is None else inputs\n        if not self._inputs:\n            outputs = self._run_with_no_inputs(inputs)\n        elif isinstance(self._inputs, str):\n            outputs = self._run_with_one_input(inputs, self._inputs)\n        elif isinstance(self._inputs, list):\n            outputs = self._run_with_list(inputs, self._inputs)\n        elif isinstance(self._inputs, dict):\n            outputs = self._run_with_dict(inputs, self._inputs)\n\n        return self._outputs_to_dictionary(outputs)\n\n    # purposely catch all exceptions\n    except Exception as exc:\n        self._logger.error(\n            \"Node %s failed with error: \\n%s\",\n            str(self),\n            str(exc),\n            extra={\"markup\": True},\n        )\n        raise exc\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node.Node/#kedro.pipeline.node.Node.tag","title":"tag","text":"<pre><code>tag(tags)\n</code></pre> <p>Create a new <code>Node</code> which is an exact copy of the current one,     but with more tags added to it.</p> <p>Parameters:</p> <ul> <li> <code>tags</code>               (<code>str | Iterable[str]</code>)           \u2013            <p>The tags to be added to the new node.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Node</code>           \u2013            <p>A copy of the current <code>Node</code> object with the tags added.</p> </li> </ul> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def tag(self, tags: str | Iterable[str]) -&gt; Node:\n    \"\"\"Create a new ``Node`` which is an exact copy of the current one,\n        but with more tags added to it.\n\n    Args:\n        tags: The tags to be added to the new node.\n\n    Returns:\n        A copy of the current ``Node`` object with the tags added.\n\n    \"\"\"\n    return self._copy(tags=self.tags | set(_to_list(tags)))\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node/","title":"node","text":""},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node","title":"kedro.pipeline.node","text":"<p>This module provides user-friendly functions for creating nodes as parts of Kedro pipelines.</p>"},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node.Node","title":"Node","text":"<pre><code>Node(func, inputs, outputs, *, name=None, tags=None, confirms=None, namespace=None)\n</code></pre> <p><code>Node</code> is an auxiliary class facilitating the operations required to run user-provided functions as part of Kedro pipelines.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable</code>)           \u2013            <p>A function that corresponds to the node logic. The function should have at least one input or output.</p> </li> <li> <code>inputs</code>               (<code>str | list[str] | dict[str, str] | None</code>)           \u2013            <p>The name or the list of the names of variables used as inputs to the function. The number of names should match the number of arguments in the definition of the provided function. When dict[str, str] is provided, variable names will be mapped to function argument names.</p> </li> <li> <code>outputs</code>               (<code>str | list[str] | dict[str, str] | None</code>)           \u2013            <p>The name or the list of the names of variables used as outputs of the function. The number of names should match the number of outputs returned by the provided function. When dict[str, str] is provided, variable names will be mapped to the named outputs the function returns.</p> </li> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional node name to be used when displaying the node in logs or any other visualisations. Valid node name must contain only letters, digits, hyphens, underscores and/or fullstops.</p> </li> <li> <code>tags</code>               (<code>str | Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional set of tags to be applied to the node. Valid node tag must contain only letters, digits, hyphens, underscores and/or fullstops.</p> </li> <li> <code>confirms</code>               (<code>str | list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional name or the list of the names of the datasets that should be confirmed. This will result in calling <code>confirm()</code> method of the corresponding dataset instance. Specified dataset names do not necessarily need to be present in the node <code>inputs</code> or <code>outputs</code>.</p> </li> <li> <code>namespace</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional node namespace.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised in the following cases: a) When the provided arguments do not conform to the format suggested by the type hint of the argument. b) When the node produces multiple outputs with the same name. c) When an input has the same name as an output. d) When the given node name violates the requirements: it must contain only letters, digits, hyphens, underscores and/or fullstops.</p> </li> </ul> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    func: Callable,\n    inputs: str | list[str] | dict[str, str] | None,\n    outputs: str | list[str] | dict[str, str] | None,\n    *,\n    name: str | None = None,\n    tags: str | Iterable[str] | None = None,\n    confirms: str | list[str] | None = None,\n    namespace: str | None = None,\n):\n    \"\"\"Create a node in the pipeline by providing a function to be called\n    along with variable names for inputs and/or outputs.\n\n    Args:\n        func: A function that corresponds to the node logic.\n            The function should have at least one input or output.\n        inputs: The name or the list of the names of variables used as\n            inputs to the function. The number of names should match\n            the number of arguments in the definition of the provided\n            function. When dict[str, str] is provided, variable names\n            will be mapped to function argument names.\n        outputs: The name or the list of the names of variables used\n            as outputs of the function. The number of names should match\n            the number of outputs returned by the provided function.\n            When dict[str, str] is provided, variable names will be mapped\n            to the named outputs the function returns.\n        name: Optional node name to be used when displaying the node in\n            logs or any other visualisations. Valid node name must contain\n            only letters, digits, hyphens, underscores and/or fullstops.\n        tags: Optional set of tags to be applied to the node. Valid node tag must\n            contain only letters, digits, hyphens, underscores and/or fullstops.\n        confirms: Optional name or the list of the names of the datasets\n            that should be confirmed. This will result in calling\n            ``confirm()`` method of the corresponding dataset instance.\n            Specified dataset names do not necessarily need to be present\n            in the node ``inputs`` or ``outputs``.\n        namespace: Optional node namespace.\n\n    Raises:\n        ValueError: Raised in the following cases:\n            a) When the provided arguments do not conform to\n            the format suggested by the type hint of the argument.\n            b) When the node produces multiple outputs with the same name.\n            c) When an input has the same name as an output.\n            d) When the given node name violates the requirements:\n            it must contain only letters, digits, hyphens, underscores\n            and/or fullstops.\n\n    \"\"\"\n    if not callable(func):\n        raise ValueError(\n            _node_error_message(\n                f\"first argument must be a function, not '{type(func).__name__}'.\"\n            )\n        )\n\n    if inputs and not isinstance(inputs, (list, dict, str)):\n        raise ValueError(\n            _node_error_message(\n                f\"'inputs' type must be one of [String, List, Dict, None], \"\n                f\"not '{type(inputs).__name__}'.\"\n            )\n        )\n\n    for _input in _to_list(inputs):\n        if not isinstance(_input, str):\n            raise ValueError(\n                _node_error_message(\n                    f\"names of variables used as inputs to the function \"\n                    f\"must be of 'String' type, but {_input} from {inputs} \"\n                    f\"is '{type(_input)}'.\"\n                )\n            )\n\n    if outputs and not isinstance(outputs, (list, dict, str)):\n        raise ValueError(\n            _node_error_message(\n                f\"'outputs' type must be one of [String, List, Dict, None], \"\n                f\"not '{type(outputs).__name__}'.\"\n            )\n        )\n\n    for _output in _to_list(outputs):\n        if not isinstance(_output, str):\n            raise ValueError(\n                _node_error_message(\n                    f\"names of variables used as outputs of the function \"\n                    f\"must be of 'String' type, but {_output} from {outputs} \"\n                    f\"is '{type(_output)}'.\"\n                )\n            )\n\n    if not inputs and not outputs:\n        raise ValueError(\n            _node_error_message(\"it must have some 'inputs' or 'outputs'.\")\n        )\n\n    self._validate_inputs(func, inputs)\n\n    self._func = func\n    self._inputs = inputs\n    # The type of _outputs is picked up as possibly being None, however the checks above prevent that\n    # ever being the case. Mypy doesn't get that though, so it complains about the assignment of outputs to\n    # _outputs with different types.\n    self._outputs: str | list[str] | dict[str, str] = outputs  # type: ignore[assignment]\n    if name and not re.match(r\"[\\w\\.-]+$\", name):\n        raise ValueError(\n            f\"'{name}' is not a valid node name. It must contain only \"\n            f\"letters, digits, hyphens, underscores and/or fullstops.\"\n        )\n    self._name = name\n    self._namespace = namespace\n    self._tags = set(_to_list(tags))\n    for tag in self._tags:\n        if not re.match(r\"[\\w\\.-]+$\", tag):\n            raise ValueError(\n                f\"'{tag}' is not a valid node tag. It must contain only \"\n                f\"letters, digits, hyphens, underscores and/or fullstops.\"\n            )\n\n    self._validate_unique_outputs()\n    self._validate_inputs_dif_than_outputs()\n    self._confirms = confirms\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node.Node.confirms","title":"confirms  <code>property</code>","text":"<pre><code>confirms\n</code></pre> <p>Return dataset names to confirm as a list.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>Dataset names to confirm as a list.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node.Node.func","title":"func  <code>property</code> <code>writable</code>","text":"<pre><code>func\n</code></pre> <p>Exposes the underlying function of the node.</p> <p>Returns:</p> <ul> <li> <code>Callable</code>           \u2013            <p>Return the underlying function of the node.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node.Node.inputs","title":"inputs  <code>property</code>","text":"<pre><code>inputs\n</code></pre> <p>Return node inputs as a list, in the order required to bind them properly to the node's function.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>Node input names as a list.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node.Node.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Node's name.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Node's name if provided or the name of its function.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node.Node.namespace","title":"namespace  <code>property</code>","text":"<pre><code>namespace\n</code></pre> <p>Node's namespace.</p> <p>Returns:</p> <ul> <li> <code>str | None</code>           \u2013            <p>String representing node's namespace, typically from outer to inner scopes.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node.Node.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs\n</code></pre> <p>Return node outputs as a list preserving the original order     if possible.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>Node output names as a list.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node.Node.short_name","title":"short_name  <code>property</code>","text":"<pre><code>short_name\n</code></pre> <p>Node's name.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Returns a short, user-friendly name that is not guaranteed to be unique.</p> </li> <li> <code>str</code>           \u2013            <p>The namespace is stripped out of the node name.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node.Node.tags","title":"tags  <code>property</code>","text":"<pre><code>tags\n</code></pre> <p>Return the tags assigned to the node.</p> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>Return the set of all assigned tags to the node.</p> </li> </ul>"},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node.Node.run","title":"run","text":"<pre><code>run(inputs=None)\n</code></pre> <p>Run this node using the provided inputs and return its results in a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>inputs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary of inputs as specified at the creation of the node.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>In the following cases: a) The node function inputs are incompatible with the node input definition. Example 1: node definition input is a list of 2 DataFrames, whereas only 1 was provided or 2 different ones were provided. b) The node function outputs are incompatible with the node output definition. Example 1: node function definition is a dictionary, whereas function returns a list. Example 2: node definition output is a list of 5 strings, whereas the function returns a list of 4 objects.</p> </li> <li> <code>Exception</code>             \u2013            <p>Any exception thrown during execution of the node.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>All produced node outputs are returned in a dictionary, where the</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>keys are defined by the node outputs.</p> </li> </ul> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def run(self, inputs: dict[str, Any] | None = None) -&gt; dict[str, Any]:\n    \"\"\"Run this node using the provided inputs and return its results\n    in a dictionary.\n\n    Args:\n        inputs: Dictionary of inputs as specified at the creation of\n            the node.\n\n    Raises:\n        ValueError: In the following cases:\n            a) The node function inputs are incompatible with the node\n            input definition.\n            Example 1: node definition input is a list of 2\n            DataFrames, whereas only 1 was provided or 2 different ones\n            were provided.\n            b) The node function outputs are incompatible with the node\n            output definition.\n            Example 1: node function definition is a dictionary,\n            whereas function returns a list.\n            Example 2: node definition output is a list of 5\n            strings, whereas the function returns a list of 4 objects.\n        Exception: Any exception thrown during execution of the node.\n\n    Returns:\n        All produced node outputs are returned in a dictionary, where the\n        keys are defined by the node outputs.\n\n    \"\"\"\n    self._logger.info(\"Running node: %s\", str(self))\n\n    outputs = None\n\n    if not (inputs is None or isinstance(inputs, dict)):\n        raise ValueError(\n            f\"Node.run() expects a dictionary or None, \"\n            f\"but got {type(inputs)} instead\"\n        )\n\n    try:\n        inputs = {} if inputs is None else inputs\n        if not self._inputs:\n            outputs = self._run_with_no_inputs(inputs)\n        elif isinstance(self._inputs, str):\n            outputs = self._run_with_one_input(inputs, self._inputs)\n        elif isinstance(self._inputs, list):\n            outputs = self._run_with_list(inputs, self._inputs)\n        elif isinstance(self._inputs, dict):\n            outputs = self._run_with_dict(inputs, self._inputs)\n\n        return self._outputs_to_dictionary(outputs)\n\n    # purposely catch all exceptions\n    except Exception as exc:\n        self._logger.error(\n            \"Node %s failed with error: \\n%s\",\n            str(self),\n            str(exc),\n            extra={\"markup\": True},\n        )\n        raise exc\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node.Node.tag","title":"tag","text":"<pre><code>tag(tags)\n</code></pre> <p>Create a new <code>Node</code> which is an exact copy of the current one,     but with more tags added to it.</p> <p>Parameters:</p> <ul> <li> <code>tags</code>               (<code>str | Iterable[str]</code>)           \u2013            <p>The tags to be added to the new node.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Node</code>           \u2013            <p>A copy of the current <code>Node</code> object with the tags added.</p> </li> </ul> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def tag(self, tags: str | Iterable[str]) -&gt; Node:\n    \"\"\"Create a new ``Node`` which is an exact copy of the current one,\n        but with more tags added to it.\n\n    Args:\n        tags: The tags to be added to the new node.\n\n    Returns:\n        A copy of the current ``Node`` object with the tags added.\n\n    \"\"\"\n    return self._copy(tags=self.tags | set(_to_list(tags)))\n</code></pre>"},{"location":"api/pipeline/kedro.pipeline.node/#kedro.pipeline.node.node","title":"node","text":"<pre><code>node(func, inputs, outputs, *, name=None, tags=None, confirms=None, namespace=None)\n</code></pre> <p>Create a node in the pipeline by providing a function to be called along with variable names for inputs and/or outputs.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable</code>)           \u2013            <p>A function that corresponds to the node logic. The function should have at least one input or output.</p> </li> <li> <code>inputs</code>               (<code>str | list[str] | dict[str, str] | None</code>)           \u2013            <p>The name or the list of the names of variables used as inputs to the function. The number of names should match the number of arguments in the definition of the provided function. When dict[str, str] is provided, variable names will be mapped to function argument names.</p> </li> <li> <code>outputs</code>               (<code>str | list[str] | dict[str, str] | None</code>)           \u2013            <p>The name or the list of the names of variables used as outputs to the function. The number of names should match the number of outputs returned by the provided function. When dict[str, str] is provided, variable names will be mapped to the named outputs the function returns.</p> </li> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional node name to be used when displaying the node in logs or any other visualisations.</p> </li> <li> <code>tags</code>               (<code>str | Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional set of tags to be applied to the node.</p> </li> <li> <code>confirms</code>               (<code>str | list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional name or the list of the names of the datasets that should be confirmed. This will result in calling <code>confirm()</code> method of the corresponding dataset instance. Specified dataset names do not necessarily need to be present in the node <code>inputs</code> or <code>outputs</code>.</p> </li> <li> <code>namespace</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional node namespace.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Node</code>           \u2013            <p>A Node object with mapped inputs, outputs and function.</p> </li> </ul> <p>Example: ::</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; def clean_data(cars: pd.DataFrame,\n&gt;&gt;&gt;                boats: pd.DataFrame) -&gt; dict[str, pd.DataFrame]:\n&gt;&gt;&gt;     return dict(cars_df=cars.dropna(), boats_df=boats.dropna())\n&gt;&gt;&gt;\n&gt;&gt;&gt; def halve_dataframe(data: pd.DataFrame) -&gt; List[pd.DataFrame]:\n&gt;&gt;&gt;     return np.array_split(data, 2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; nodes = [\n&gt;&gt;&gt;     node(clean_data,\n&gt;&gt;&gt;          inputs=['cars2017', 'boats2017'],\n&gt;&gt;&gt;          outputs=dict(cars_df='clean_cars2017',\n&gt;&gt;&gt;                       boats_df='clean_boats2017')),\n&gt;&gt;&gt;     node(halve_dataframe,\n&gt;&gt;&gt;          'clean_cars2017',\n&gt;&gt;&gt;          ['train_cars2017', 'test_cars2017']),\n&gt;&gt;&gt;     node(halve_dataframe,\n&gt;&gt;&gt;          dict(data='clean_boats2017'),\n&gt;&gt;&gt;          ['train_boats2017', 'test_boats2017'])\n&gt;&gt;&gt; ]\n</code></pre> Source code in <code>kedro/pipeline/node.py</code> <pre><code>def node(  # noqa: PLR0913\n    func: Callable,\n    inputs: str | list[str] | dict[str, str] | None,\n    outputs: str | list[str] | dict[str, str] | None,\n    *,\n    name: str | None = None,\n    tags: str | Iterable[str] | None = None,\n    confirms: str | list[str] | None = None,\n    namespace: str | None = None,\n) -&gt; Node:\n    \"\"\"Create a node in the pipeline by providing a function to be called\n    along with variable names for inputs and/or outputs.\n\n    Args:\n        func: A function that corresponds to the node logic. The function\n            should have at least one input or output.\n        inputs: The name or the list of the names of variables used as inputs\n            to the function. The number of names should match the number of\n            arguments in the definition of the provided function. When\n            dict[str, str] is provided, variable names will be mapped to\n            function argument names.\n        outputs: The name or the list of the names of variables used as outputs\n            to the function. The number of names should match the number of\n            outputs returned by the provided function. When dict[str, str]\n            is provided, variable names will be mapped to the named outputs the\n            function returns.\n        name: Optional node name to be used when displaying the node in logs or\n            any other visualisations.\n        tags: Optional set of tags to be applied to the node.\n        confirms: Optional name or the list of the names of the datasets\n            that should be confirmed. This will result in calling ``confirm()``\n            method of the corresponding dataset instance. Specified dataset\n            names do not necessarily need to be present in the node ``inputs``\n            or ``outputs``.\n        namespace: Optional node namespace.\n\n    Returns:\n        A Node object with mapped inputs, outputs and function.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; def clean_data(cars: pd.DataFrame,\n        &gt;&gt;&gt;                boats: pd.DataFrame) -&gt; dict[str, pd.DataFrame]:\n        &gt;&gt;&gt;     return dict(cars_df=cars.dropna(), boats_df=boats.dropna())\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; def halve_dataframe(data: pd.DataFrame) -&gt; List[pd.DataFrame]:\n        &gt;&gt;&gt;     return np.array_split(data, 2)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; nodes = [\n        &gt;&gt;&gt;     node(clean_data,\n        &gt;&gt;&gt;          inputs=['cars2017', 'boats2017'],\n        &gt;&gt;&gt;          outputs=dict(cars_df='clean_cars2017',\n        &gt;&gt;&gt;                       boats_df='clean_boats2017')),\n        &gt;&gt;&gt;     node(halve_dataframe,\n        &gt;&gt;&gt;          'clean_cars2017',\n        &gt;&gt;&gt;          ['train_cars2017', 'test_cars2017']),\n        &gt;&gt;&gt;     node(halve_dataframe,\n        &gt;&gt;&gt;          dict(data='clean_boats2017'),\n        &gt;&gt;&gt;          ['train_boats2017', 'test_boats2017'])\n        &gt;&gt;&gt; ]\n    \"\"\"\n    return Node(\n        func,\n        inputs,\n        outputs,\n        name=name,\n        tags=tags,\n        confirms=confirms,\n        namespace=namespace,\n    )\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/","title":"AbstractRunner","text":""},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner","title":"kedro.runner.AbstractRunner","text":"<pre><code>AbstractRunner(is_async=False, extra_dataset_patterns=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p><code>AbstractRunner</code> is the base class for all <code>Pipeline</code> runner implementations.</p> <p>Parameters:</p> <ul> <li> <code>is_async</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the node inputs and outputs are loaded and saved asynchronously with threads. Defaults to False.</p> </li> <li> <code>extra_dataset_patterns</code>               (<code>dict[str, dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra dataset factory patterns to be added to the catalog during the run. This is used to set the default datasets on the Runner instances.</p> </li> </ul> Source code in <code>kedro/runner/runner.py</code> <pre><code>def __init__(\n    self,\n    is_async: bool = False,\n    extra_dataset_patterns: dict[str, dict[str, Any]] | None = None,\n):\n    \"\"\"Instantiates the runner class.\n\n    Args:\n        is_async: If True, the node inputs and outputs are loaded and saved\n            asynchronously with threads. Defaults to False.\n        extra_dataset_patterns: Extra dataset factory patterns to be added to the catalog\n            during the run. This is used to set the default datasets on the Runner instances.\n\n    \"\"\"\n    self._is_async = is_async\n    self._extra_dataset_patterns = extra_dataset_patterns\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._extra_dataset_patterns","title":"_extra_dataset_patterns  <code>instance-attribute</code>","text":"<pre><code>_extra_dataset_patterns = extra_dataset_patterns\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._is_async","title":"_is_async  <code>instance-attribute</code>","text":"<pre><code>_is_async = is_async\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._logger","title":"_logger  <code>property</code>","text":"<pre><code>_logger\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._get_executor","title":"_get_executor  <code>abstractmethod</code>","text":"<pre><code>_get_executor(max_workers)\n</code></pre> <p>Abstract method to provide the correct executor (e.g., ThreadPoolExecutor, ProcessPoolExecutor or None if running sequentially).</p> Source code in <code>kedro/runner/runner.py</code> <pre><code>@abstractmethod  # pragma: no cover\ndef _get_executor(self, max_workers: int) -&gt; Executor | None:\n    \"\"\"Abstract method to provide the correct executor (e.g., ThreadPoolExecutor, ProcessPoolExecutor or None if running sequentially).\"\"\"\n    pass\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._get_required_workers_count","title":"_get_required_workers_count","text":"<pre><code>_get_required_workers_count(pipeline)\n</code></pre> Source code in <code>kedro/runner/runner.py</code> <pre><code>def _get_required_workers_count(self, pipeline: Pipeline) -&gt; int:\n    return 1\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._raise_runtime_error","title":"_raise_runtime_error  <code>staticmethod</code>","text":"<pre><code>_raise_runtime_error(todo_nodes, done_nodes, ready, done)\n</code></pre> Source code in <code>kedro/runner/runner.py</code> <pre><code>@staticmethod\ndef _raise_runtime_error(\n    todo_nodes: set[Node],\n    done_nodes: set[Node],\n    ready: set[Node],\n    done: set[Future[Node]] | None,\n) -&gt; None:\n    debug_data = {\n        \"todo_nodes\": todo_nodes,\n        \"done_nodes\": done_nodes,\n        \"ready_nodes\": ready,\n        \"done_futures\": done,\n    }\n    debug_data_str = \"\\n\".join(f\"{k} = {v}\" for k, v in debug_data.items())\n    raise RuntimeError(\n        f\"Unable to schedule new tasks although some nodes \"\n        f\"have not been run:\\n{debug_data_str}\"\n    )\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._release_datasets","title":"_release_datasets  <code>staticmethod</code>","text":"<pre><code>_release_datasets(node, catalog, load_counts, pipeline)\n</code></pre> <p>Decrement dataset load counts and release any datasets we've finished with</p> Source code in <code>kedro/runner/runner.py</code> <pre><code>@staticmethod\ndef _release_datasets(\n    node: Node, catalog: CatalogProtocol, load_counts: dict, pipeline: Pipeline\n) -&gt; None:\n    \"\"\"Decrement dataset load counts and release any datasets we've finished with\"\"\"\n    for dataset in node.inputs:\n        load_counts[dataset] -= 1\n        if load_counts[dataset] &lt; 1 and dataset not in pipeline.inputs():\n            catalog.release(dataset)\n    for dataset in node.outputs:\n        if load_counts[dataset] &lt; 1 and dataset not in pipeline.outputs():\n            catalog.release(dataset)\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._run","title":"_run  <code>abstractmethod</code>","text":"<pre><code>_run(pipeline, catalog, hook_manager=None, session_id=None)\n</code></pre> <p>The abstract interface for running pipelines, assuming that the  inputs have already been checked and normalized by run().  This contains the Common pipeline execution logic using an executor.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> from which to fetch data.</p> </li> <li> <code>hook_manager</code>               (<code>PluginManager | None</code>, default:                   <code>None</code> )           \u2013            <p>The <code>PluginManager</code> to activate hooks.</p> </li> <li> <code>session_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The id of the session.</p> </li> </ul> Source code in <code>kedro/runner/runner.py</code> <pre><code>@abstractmethod  # pragma: no cover\ndef _run(\n    self,\n    pipeline: Pipeline,\n    catalog: CatalogProtocol,\n    hook_manager: PluginManager | None = None,\n    session_id: str | None = None,\n) -&gt; None:\n    \"\"\"The abstract interface for running pipelines, assuming that the\n     inputs have already been checked and normalized by run().\n     This contains the Common pipeline execution logic using an executor.\n\n    Args:\n        pipeline: The ``Pipeline`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` from which to fetch data.\n        hook_manager: The ``PluginManager`` to activate hooks.\n        session_id: The id of the session.\n    \"\"\"\n\n    nodes = pipeline.nodes\n\n    self._validate_catalog(catalog, pipeline)\n    self._validate_nodes(nodes)\n    self._set_manager_datasets(catalog, pipeline)\n\n    load_counts = Counter(chain.from_iterable(n.inputs for n in pipeline.nodes))\n    node_dependencies = pipeline.node_dependencies\n    todo_nodes = set(node_dependencies.keys())\n    done_nodes: set[Node] = set()\n    futures = set()\n    done = None\n    max_workers = self._get_required_workers_count(pipeline)\n\n    pool = self._get_executor(max_workers)\n    if pool is None:\n        for exec_index, node in enumerate(nodes):\n            try:\n                Task(\n                    node=node,\n                    catalog=catalog,\n                    hook_manager=hook_manager,\n                    is_async=self._is_async,\n                    session_id=session_id,\n                ).execute()\n                done_nodes.add(node)\n            except Exception:\n                self._suggest_resume_scenario(pipeline, done_nodes, catalog)\n                raise\n            self._logger.info(\"Completed node: %s\", node.name)\n            self._logger.info(\n                \"Completed %d out of %d tasks\", len(done_nodes), len(nodes)\n            )\n            self._release_datasets(node, catalog, load_counts, pipeline)\n\n        return  # Exit early since everything runs sequentially\n\n    with pool as executor:\n        while True:\n            ready = {n for n in todo_nodes if node_dependencies[n] &lt;= done_nodes}\n            todo_nodes -= ready\n            for node in ready:\n                task = Task(\n                    node=node,\n                    catalog=catalog,\n                    hook_manager=hook_manager,\n                    is_async=self._is_async,\n                    session_id=session_id,\n                )\n                if isinstance(executor, ProcessPoolExecutor):\n                    task.parallel = True\n                futures.add(executor.submit(task))\n            if not futures:\n                if todo_nodes:\n                    self._raise_runtime_error(todo_nodes, done_nodes, ready, done)\n                break\n            done, futures = wait(futures, return_when=FIRST_COMPLETED)\n            for future in done:\n                try:\n                    node = future.result()\n                except Exception:\n                    self._suggest_resume_scenario(pipeline, done_nodes, catalog)\n                    raise\n                done_nodes.add(node)\n                self._logger.info(\"Completed node: %s\", node.name)\n                self._logger.info(\n                    \"Completed %d out of %d tasks\", len(done_nodes), len(nodes)\n                )\n                self._release_datasets(node, catalog, load_counts, pipeline)\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._set_manager_datasets","title":"_set_manager_datasets","text":"<pre><code>_set_manager_datasets(catalog, pipeline)\n</code></pre> Source code in <code>kedro/runner/runner.py</code> <pre><code>def _set_manager_datasets(\n    self, catalog: CatalogProtocol, pipeline: Pipeline\n) -&gt; None:\n    # Set up any necessary manager datasets here\n    pass\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._suggest_resume_scenario","title":"_suggest_resume_scenario","text":"<pre><code>_suggest_resume_scenario(pipeline, done_nodes, catalog)\n</code></pre> <p>Suggest a command to the user to resume a run after it fails. The run should be started from the point closest to the failure for which persisted input exists.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>the <code>Pipeline</code> of the run.</p> </li> <li> <code>done_nodes</code>               (<code>Iterable[Node]</code>)           \u2013            <p>the <code>Node</code>s that executed successfully.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>an implemented instance of <code>CatalogProtocol</code> of the run.</p> </li> </ul> Source code in <code>kedro/runner/runner.py</code> <pre><code>def _suggest_resume_scenario(\n    self,\n    pipeline: Pipeline,\n    done_nodes: Iterable[Node],\n    catalog: CatalogProtocol,\n) -&gt; None:\n    \"\"\"\n    Suggest a command to the user to resume a run after it fails.\n    The run should be started from the point closest to the failure\n    for which persisted input exists.\n\n    Args:\n        pipeline: the ``Pipeline`` of the run.\n        done_nodes: the ``Node``s that executed successfully.\n        catalog: an implemented instance of ``CatalogProtocol`` of the run.\n\n    \"\"\"\n    remaining_nodes = set(pipeline.nodes) - set(done_nodes)\n\n    postfix = \"\"\n    if done_nodes:\n        start_node_names = _find_nodes_to_resume_from(\n            pipeline=pipeline,\n            unfinished_nodes=remaining_nodes,\n            catalog=catalog,\n        )\n        start_nodes_str = \",\".join(sorted(start_node_names))\n        postfix += f'  --from-nodes \"{start_nodes_str}\"'\n\n    if not postfix:\n        self._logger.warning(\n            \"No nodes ran. Repeat the previous command to attempt a new run.\"\n        )\n    else:\n        self._logger.warning(\n            f\"There are {len(remaining_nodes)} nodes that have not run.\\n\"\n            \"You can resume the pipeline run from the nearest nodes with \"\n            \"persisted inputs by adding the following \"\n            f\"argument to your previous command:\\n{postfix}\"\n        )\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._validate_catalog","title":"_validate_catalog","text":"<pre><code>_validate_catalog(catalog, pipeline)\n</code></pre> Source code in <code>kedro/runner/runner.py</code> <pre><code>def _validate_catalog(self, catalog: CatalogProtocol, pipeline: Pipeline) -&gt; None:\n    # Add catalog validation logic here if needed\n    pass\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._validate_max_workers","title":"_validate_max_workers  <code>classmethod</code>","text":"<pre><code>_validate_max_workers(max_workers)\n</code></pre> <p>Validates and returns the number of workers. Sets to os.cpu_count() or 1 if max_workers is None, and limits max_workers to 61 on Windows.</p> <p>Parameters:</p> <ul> <li> <code>max_workers</code>               (<code>int | None</code>)           \u2013            <p>Desired number of workers. If None, defaults to os.cpu_count() or 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>A valid number of workers to use.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If max_workers is set and is not positive.</p> </li> </ul> Source code in <code>kedro/runner/runner.py</code> <pre><code>@classmethod\ndef _validate_max_workers(cls, max_workers: int | None) -&gt; int:\n    \"\"\"\n    Validates and returns the number of workers. Sets to os.cpu_count() or 1 if max_workers is None,\n    and limits max_workers to 61 on Windows.\n\n    Args:\n        max_workers: Desired number of workers. If None, defaults to os.cpu_count() or 1.\n\n    Returns:\n        A valid number of workers to use.\n\n    Raises:\n        ValueError: If max_workers is set and is not positive.\n    \"\"\"\n    if max_workers is None:\n        max_workers = os.cpu_count() or 1\n        if sys.platform == \"win32\":\n            max_workers = min(_MAX_WINDOWS_WORKERS, max_workers)\n    elif max_workers &lt;= 0:\n        raise ValueError(\"max_workers should be positive\")\n\n    return max_workers\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner._validate_nodes","title":"_validate_nodes","text":"<pre><code>_validate_nodes(node)\n</code></pre> Source code in <code>kedro/runner/runner.py</code> <pre><code>def _validate_nodes(self, node: Iterable[Node]) -&gt; None:\n    # Add node validation logic here if needed\n    pass\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner.run","title":"run","text":"<pre><code>run(pipeline, catalog, hook_manager=None, session_id=None)\n</code></pre> <p>Run the <code>Pipeline</code> using the datasets provided by <code>catalog</code> and save results back to the same objects.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> from which to fetch data.</p> </li> <li> <code>hook_manager</code>               (<code>PluginManager | None</code>, default:                   <code>None</code> )           \u2013            <p>The <code>PluginManager</code> to activate hooks.</p> </li> <li> <code>session_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The id of the session.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised when <code>Pipeline</code> inputs cannot be satisfied.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Any node outputs that cannot be processed by the catalog.</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>These are returned in a dictionary, where the keys are defined</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>by the node outputs.</p> </li> </ul> Source code in <code>kedro/runner/runner.py</code> <pre><code>def run(\n    self,\n    pipeline: Pipeline,\n    catalog: CatalogProtocol,\n    hook_manager: PluginManager | None = None,\n    session_id: str | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Run the ``Pipeline`` using the datasets provided by ``catalog``\n    and save results back to the same objects.\n\n    Args:\n        pipeline: The ``Pipeline`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` from which to fetch data.\n        hook_manager: The ``PluginManager`` to activate hooks.\n        session_id: The id of the session.\n\n    Raises:\n        ValueError: Raised when ``Pipeline`` inputs cannot be satisfied.\n\n    Returns:\n        Any node outputs that cannot be processed by the catalog.\n        These are returned in a dictionary, where the keys are defined\n        by the node outputs.\n\n    \"\"\"\n    # Check which datasets used in the pipeline are in the catalog or match\n    # a pattern in the catalog, not including extra dataset patterns\n    # Run a warm-up to materialize all datasets in the catalog before run\n    warmed_up_ds = []\n    for ds in pipeline.datasets():\n        if ds in catalog:\n            warmed_up_ds.append(ds)\n            _ = catalog._get_dataset(ds)\n\n    # Check if there are any input datasets that aren't in the catalog and\n    # don't match a pattern in the catalog.\n    unsatisfied = pipeline.inputs() - set(warmed_up_ds)\n\n    if unsatisfied:\n        raise ValueError(\n            f\"Pipeline input(s) {unsatisfied} not found in the {catalog.__class__.__name__}\"\n        )\n\n    # Register the default dataset pattern with the catalog\n    # TODO: replace with catalog.config_resolver.add_runtime_patterns() when removing old catalog\n    catalog = catalog.shallow_copy(\n        extra_dataset_patterns=self._extra_dataset_patterns\n    )\n\n    hook_or_null_manager = hook_manager or _NullPluginManager()\n\n    # Check which datasets used in the pipeline are in the catalog or match\n    # a pattern in the catalog, including added extra_dataset_patterns\n    registered_ds = [ds for ds in pipeline.datasets() if ds in catalog]\n\n    if self._is_async:\n        self._logger.info(\n            \"Asynchronous mode is enabled for loading and saving data\"\n        )\n\n    self._run(pipeline, catalog, hook_or_null_manager, session_id)  # type: ignore[arg-type]\n\n    self._logger.info(\"Pipeline execution completed successfully.\")\n\n    # Identify MemoryDataset in the catalog\n    memory_datasets = {\n        ds_name\n        for ds_name, ds in catalog._datasets.items()\n        if isinstance(ds, MemoryDataset) or isinstance(ds, SharedMemoryDataset)\n    }\n\n    # Check if there's any output datasets that aren't in the catalog and don't match a pattern\n    # in the catalog and include MemoryDataset.\n    free_outputs = pipeline.outputs() - (set(registered_ds) - memory_datasets)\n\n    run_output = {ds_name: catalog.load(ds_name) for ds_name in free_outputs}\n\n    # Remove runtime patterns after run, so they do not affect further runs\n    if self._extra_dataset_patterns:\n        catalog.config_resolver.remove_runtime_patterns(\n            self._extra_dataset_patterns\n        )\n\n    return run_output\n</code></pre>"},{"location":"api/runner/kedro.runner.AbstractRunner/#kedro.runner.AbstractRunner.run_only_missing","title":"run_only_missing","text":"<pre><code>run_only_missing(pipeline, catalog, hook_manager)\n</code></pre> <p>Run only the missing outputs from the <code>Pipeline</code> using the datasets provided by <code>catalog</code>, and save results back to the same objects.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> from which to fetch data.</p> </li> <li> <code>hook_manager</code>               (<code>PluginManager</code>)           \u2013            <p>The <code>PluginManager</code> to activate hooks.</p> </li> </ul> <p>Raises:     ValueError: Raised when <code>Pipeline</code> inputs cannot be         satisfied.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Any node outputs that cannot be processed by the</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>catalog. These are returned in a dictionary, where</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>the keys are defined by the node outputs.</p> </li> </ul> Source code in <code>kedro/runner/runner.py</code> <pre><code>def run_only_missing(\n    self, pipeline: Pipeline, catalog: CatalogProtocol, hook_manager: PluginManager\n) -&gt; dict[str, Any]:\n    \"\"\"Run only the missing outputs from the ``Pipeline`` using the\n    datasets provided by ``catalog``, and save results back to the\n    same objects.\n\n    Args:\n        pipeline: The ``Pipeline`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` from which to fetch data.\n        hook_manager: The ``PluginManager`` to activate hooks.\n    Raises:\n        ValueError: Raised when ``Pipeline`` inputs cannot be\n            satisfied.\n\n    Returns:\n        Any node outputs that cannot be processed by the\n        catalog. These are returned in a dictionary, where\n        the keys are defined by the node outputs.\n\n    \"\"\"\n    free_outputs = pipeline.outputs() - set(catalog.list())\n    missing = {ds for ds in catalog.list() if not catalog.exists(ds)}\n    to_build = free_outputs | missing\n    to_rerun = pipeline.only_nodes_with_outputs(*to_build) + pipeline.from_inputs(\n        *to_build\n    )\n\n    # We also need any missing datasets that are required to run the\n    # `to_rerun` pipeline, including any chains of missing datasets.\n    unregistered_ds = pipeline.datasets() - set(catalog.list())\n    output_to_unregistered = pipeline.only_nodes_with_outputs(*unregistered_ds)\n    input_from_unregistered = to_rerun.inputs() &amp; unregistered_ds\n    to_rerun += output_to_unregistered.to_outputs(*input_from_unregistered)\n\n    return self.run(to_rerun, catalog, hook_manager)\n</code></pre>"},{"location":"api/runner/kedro.runner.ParallelRunner/","title":"ParallelRunner","text":""},{"location":"api/runner/kedro.runner.ParallelRunner/#kedro.runner.ParallelRunner","title":"kedro.runner.ParallelRunner","text":"<pre><code>ParallelRunner(max_workers=None, is_async=False, extra_dataset_patterns=None)\n</code></pre> <p>               Bases: <code>AbstractRunner</code></p> <p><code>ParallelRunner</code> is an <code>AbstractRunner</code> implementation. It can be used to run the <code>Pipeline</code> in parallel groups formed by toposort. Please note that this <code>runner</code> implementation validates dataset using the <code>_validate_catalog</code> method, which checks if any of the datasets are single process only using the <code>_SINGLE_PROCESS</code> dataset attribute.</p> <p>Parameters:</p> <ul> <li> <code>max_workers</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of worker processes to spawn. If not set, calculated automatically based on the pipeline configuration and CPU core count. On windows machines, the max_workers value cannot be larger than 61 and will be set to min(61, max_workers).</p> </li> <li> <code>is_async</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the node inputs and outputs are loaded and saved asynchronously with threads. Defaults to False.</p> </li> <li> <code>extra_dataset_patterns</code>               (<code>dict[str, dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra dataset factory patterns to be added to the catalog during the run. This is used to set the default datasets to SharedMemoryDataset for <code>ParallelRunner</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>bad parameters passed</p> </li> </ul> Source code in <code>kedro/runner/parallel_runner.py</code> <pre><code>def __init__(\n    self,\n    max_workers: int | None = None,\n    is_async: bool = False,\n    extra_dataset_patterns: dict[str, dict[str, Any]] | None = None,\n):\n    \"\"\"\n    Instantiates the runner by creating a Manager.\n\n    Args:\n        max_workers: Number of worker processes to spawn. If not set,\n            calculated automatically based on the pipeline configuration\n            and CPU core count. On windows machines, the max_workers value\n            cannot be larger than 61 and will be set to min(61, max_workers).\n        is_async: If True, the node inputs and outputs are loaded and saved\n            asynchronously with threads. Defaults to False.\n        extra_dataset_patterns: Extra dataset factory patterns to be added to the catalog\n            during the run. This is used to set the default datasets to SharedMemoryDataset\n            for `ParallelRunner`.\n\n    Raises:\n        ValueError: bad parameters passed\n    \"\"\"\n    default_dataset_pattern = {\"{default}\": {\"type\": \"SharedMemoryDataset\"}}\n    self._extra_dataset_patterns = extra_dataset_patterns or default_dataset_pattern\n    super().__init__(\n        is_async=is_async, extra_dataset_patterns=self._extra_dataset_patterns\n    )\n    self._manager = ParallelRunnerManager()\n    self._manager.start()\n\n    self._max_workers = self._validate_max_workers(max_workers)\n</code></pre>"},{"location":"api/runner/kedro.runner.ParallelRunner/#kedro.runner.ParallelRunner._extra_dataset_patterns","title":"_extra_dataset_patterns  <code>instance-attribute</code>","text":"<pre><code>_extra_dataset_patterns = extra_dataset_patterns or default_dataset_pattern\n</code></pre>"},{"location":"api/runner/kedro.runner.ParallelRunner/#kedro.runner.ParallelRunner._manager","title":"_manager  <code>instance-attribute</code>","text":"<pre><code>_manager = ParallelRunnerManager()\n</code></pre>"},{"location":"api/runner/kedro.runner.ParallelRunner/#kedro.runner.ParallelRunner._max_workers","title":"_max_workers  <code>instance-attribute</code>","text":"<pre><code>_max_workers = _validate_max_workers(max_workers)\n</code></pre>"},{"location":"api/runner/kedro.runner.ParallelRunner/#kedro.runner.ParallelRunner.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> Source code in <code>kedro/runner/parallel_runner.py</code> <pre><code>def __del__(self) -&gt; None:\n    self._manager.shutdown()\n</code></pre>"},{"location":"api/runner/kedro.runner.ParallelRunner/#kedro.runner.ParallelRunner._get_executor","title":"_get_executor","text":"<pre><code>_get_executor(max_workers)\n</code></pre> Source code in <code>kedro/runner/parallel_runner.py</code> <pre><code>def _get_executor(self, max_workers: int) -&gt; Executor:\n    return ProcessPoolExecutor(max_workers=max_workers)\n</code></pre>"},{"location":"api/runner/kedro.runner.ParallelRunner/#kedro.runner.ParallelRunner._get_required_workers_count","title":"_get_required_workers_count","text":"<pre><code>_get_required_workers_count(pipeline)\n</code></pre> <p>Calculate the max number of processes required for the pipeline, limit to the number of CPU cores.</p> Source code in <code>kedro/runner/parallel_runner.py</code> <pre><code>def _get_required_workers_count(self, pipeline: Pipeline) -&gt; int:\n    \"\"\"\n    Calculate the max number of processes required for the pipeline,\n    limit to the number of CPU cores.\n    \"\"\"\n    # Number of nodes is a safe upper-bound estimate.\n    # It's also safe to reduce it by the number of layers minus one,\n    # because each layer means some nodes depend on other nodes\n    # and they can not run in parallel.\n    # It might be not a perfect solution, but good enough and simple.\n    required_processes = len(pipeline.nodes) - len(pipeline.grouped_nodes) + 1\n\n    return min(required_processes, self._max_workers)\n</code></pre>"},{"location":"api/runner/kedro.runner.ParallelRunner/#kedro.runner.ParallelRunner._run","title":"_run","text":"<pre><code>_run(pipeline, catalog, hook_manager=None, session_id=None)\n</code></pre> <p>The method implementing parallel pipeline running.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> from which to fetch data.</p> </li> <li> <code>hook_manager</code>               (<code>PluginManager | None</code>, default:                   <code>None</code> )           \u2013            <p>The <code>PluginManager</code> to activate hooks.</p> </li> <li> <code>session_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The id of the session.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AttributeError</code>             \u2013            <p>When the provided pipeline is not suitable for parallel execution.</p> </li> <li> <code>RuntimeError</code>             \u2013            <p>If the runner is unable to schedule the execution of all pipeline nodes.</p> </li> <li> <code>Exception</code>             \u2013            <p>In case of any downstream node failure.</p> </li> </ul> Source code in <code>kedro/runner/parallel_runner.py</code> <pre><code>def _run(\n    self,\n    pipeline: Pipeline,\n    catalog: CatalogProtocol,\n    hook_manager: PluginManager | None = None,\n    session_id: str | None = None,\n) -&gt; None:\n    \"\"\"The method implementing parallel pipeline running.\n\n    Args:\n        pipeline: The ``Pipeline`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` from which to fetch data.\n        hook_manager: The ``PluginManager`` to activate hooks.\n        session_id: The id of the session.\n\n    Raises:\n        AttributeError: When the provided pipeline is not suitable for\n            parallel execution.\n        RuntimeError: If the runner is unable to schedule the execution of\n            all pipeline nodes.\n        Exception: In case of any downstream node failure.\n\n    \"\"\"\n    if not self._is_async:\n        self._logger.info(\n            \"Using synchronous mode for loading and saving data. Use the --async flag \"\n            \"for potential performance gains. https://docs.kedro.org/en/stable/nodes_and_pipelines/run_a_pipeline.html#load-and-save-asynchronously\"\n        )\n\n    super()._run(\n        pipeline=pipeline,\n        catalog=catalog,\n        session_id=session_id,\n    )\n</code></pre>"},{"location":"api/runner/kedro.runner.ParallelRunner/#kedro.runner.ParallelRunner._set_manager_datasets","title":"_set_manager_datasets","text":"<pre><code>_set_manager_datasets(catalog, pipeline)\n</code></pre> Source code in <code>kedro/runner/parallel_runner.py</code> <pre><code>def _set_manager_datasets(\n    self, catalog: CatalogProtocol, pipeline: Pipeline\n) -&gt; None:\n    for dataset in pipeline.datasets():\n        try:\n            catalog.exists(dataset)\n        except DatasetNotFoundError:\n            pass\n    for name, ds in catalog._datasets.items():\n        if isinstance(ds, SharedMemoryDataset):\n            ds.set_manager(self._manager)\n</code></pre>"},{"location":"api/runner/kedro.runner.ParallelRunner/#kedro.runner.ParallelRunner._validate_catalog","title":"_validate_catalog  <code>classmethod</code>","text":"<pre><code>_validate_catalog(catalog, pipeline)\n</code></pre> <p>Ensure that all datasets are serialisable and that we do not have any non proxied memory datasets being used as outputs as their content will not be synchronized across threads.</p> Source code in <code>kedro/runner/parallel_runner.py</code> <pre><code>@classmethod\ndef _validate_catalog(cls, catalog: CatalogProtocol, pipeline: Pipeline) -&gt; None:\n    \"\"\"Ensure that all datasets are serialisable and that we do not have\n    any non proxied memory datasets being used as outputs as their content\n    will not be synchronized across threads.\n    \"\"\"\n\n    datasets = catalog._datasets\n\n    unserialisable = []\n    for name, dataset in datasets.items():\n        if getattr(dataset, \"_SINGLE_PROCESS\", False):  # SKIP_IF_NO_SPARK\n            unserialisable.append(name)\n            continue\n        try:\n            ForkingPickler.dumps(dataset)\n        except (AttributeError, PicklingError):\n            unserialisable.append(name)\n\n    if unserialisable:\n        raise AttributeError(\n            f\"The following datasets cannot be used with multiprocessing: \"\n            f\"{sorted(unserialisable)}\\nIn order to utilize multiprocessing you \"\n            f\"need to make sure all datasets are serialisable, i.e. datasets \"\n            f\"should not make use of lambda functions, nested functions, closures \"\n            f\"etc.\\nIf you are using custom decorators ensure they are correctly \"\n            f\"decorated using functools.wraps().\"\n        )\n\n    memory_datasets = []\n    for name, dataset in datasets.items():\n        if (\n            name in pipeline.all_outputs()\n            and isinstance(dataset, MemoryDataset)\n            and not isinstance(dataset, BaseProxy)\n        ):\n            memory_datasets.append(name)\n\n    if memory_datasets:\n        raise AttributeError(\n            f\"The following datasets are memory datasets: \"\n            f\"{sorted(memory_datasets)}\\n\"\n            f\"ParallelRunner does not support output to externally created \"\n            f\"MemoryDatasets\"\n        )\n</code></pre>"},{"location":"api/runner/kedro.runner.ParallelRunner/#kedro.runner.ParallelRunner._validate_nodes","title":"_validate_nodes  <code>classmethod</code>","text":"<pre><code>_validate_nodes(nodes)\n</code></pre> <p>Ensure all tasks are serialisable.</p> Source code in <code>kedro/runner/parallel_runner.py</code> <pre><code>@classmethod\ndef _validate_nodes(cls, nodes: Iterable[Node]) -&gt; None:\n    \"\"\"Ensure all tasks are serialisable.\"\"\"\n    unserialisable = []\n    for node in nodes:\n        try:\n            ForkingPickler.dumps(node)\n        except (AttributeError, PicklingError):\n            unserialisable.append(node)\n\n    if unserialisable:\n        raise AttributeError(\n            f\"The following nodes cannot be serialised: {sorted(unserialisable)}\\n\"\n            f\"In order to utilize multiprocessing you need to make sure all nodes \"\n            f\"are serialisable, i.e. nodes should not include lambda \"\n            f\"functions, nested functions, closures, etc.\\nIf you \"\n            f\"are using custom decorators ensure they are correctly decorated using \"\n            f\"functools.wraps().\"\n        )\n</code></pre>"},{"location":"api/runner/kedro.runner.SequentialRunner/","title":"SequentialRunner","text":""},{"location":"api/runner/kedro.runner.SequentialRunner/#kedro.runner.SequentialRunner","title":"kedro.runner.SequentialRunner","text":"<pre><code>SequentialRunner(is_async=False, extra_dataset_patterns=None)\n</code></pre> <p>               Bases: <code>AbstractRunner</code></p> <p><code>SequentialRunner</code> is an <code>AbstractRunner</code> implementation. It can be used to run the <code>Pipeline</code> in a sequential manner using a topological sort of provided nodes.</p> <p>Parameters:</p> <ul> <li> <code>is_async</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the node inputs and outputs are loaded and saved asynchronously with threads. Defaults to False.</p> </li> <li> <code>extra_dataset_patterns</code>               (<code>dict[str, dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra dataset factory patterns to be added to the catalog during the run. This is used to set the default datasets to MemoryDataset for <code>SequentialRunner</code>.</p> </li> </ul> Source code in <code>kedro/runner/sequential_runner.py</code> <pre><code>def __init__(\n    self,\n    is_async: bool = False,\n    extra_dataset_patterns: dict[str, dict[str, Any]] | None = None,\n):\n    \"\"\"Instantiates the runner class.\n\n    Args:\n        is_async: If True, the node inputs and outputs are loaded and saved\n            asynchronously with threads. Defaults to False.\n        extra_dataset_patterns: Extra dataset factory patterns to be added to the catalog\n            during the run. This is used to set the default datasets to MemoryDataset\n            for `SequentialRunner`.\n\n    \"\"\"\n    default_dataset_pattern = {\"{default}\": {\"type\": \"MemoryDataset\"}}\n    self._extra_dataset_patterns = extra_dataset_patterns or default_dataset_pattern\n    super().__init__(\n        is_async=is_async, extra_dataset_patterns=self._extra_dataset_patterns\n    )\n</code></pre>"},{"location":"api/runner/kedro.runner.SequentialRunner/#kedro.runner.SequentialRunner._extra_dataset_patterns","title":"_extra_dataset_patterns  <code>instance-attribute</code>","text":"<pre><code>_extra_dataset_patterns = extra_dataset_patterns or default_dataset_pattern\n</code></pre>"},{"location":"api/runner/kedro.runner.SequentialRunner/#kedro.runner.SequentialRunner._get_executor","title":"_get_executor","text":"<pre><code>_get_executor(max_workers)\n</code></pre> Source code in <code>kedro/runner/sequential_runner.py</code> <pre><code>def _get_executor(self, max_workers: int) -&gt; None:\n    return None\n</code></pre>"},{"location":"api/runner/kedro.runner.SequentialRunner/#kedro.runner.SequentialRunner._run","title":"_run","text":"<pre><code>_run(pipeline, catalog, hook_manager=None, session_id=None)\n</code></pre> <p>The method implementing sequential pipeline running.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> from which to fetch data.</p> </li> <li> <code>hook_manager</code>               (<code>PluginManager | None</code>, default:                   <code>None</code> )           \u2013            <p>The <code>PluginManager</code> to activate hooks.</p> </li> <li> <code>session_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The id of the session.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>Exception</code>             \u2013            <p>in case of any downstream node failure.</p> </li> </ul> Source code in <code>kedro/runner/sequential_runner.py</code> <pre><code>def _run(\n    self,\n    pipeline: Pipeline,\n    catalog: CatalogProtocol,\n    hook_manager: PluginManager | None = None,\n    session_id: str | None = None,\n) -&gt; None:\n    \"\"\"The method implementing sequential pipeline running.\n\n    Args:\n        pipeline: The ``Pipeline`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` from which to fetch data.\n        hook_manager: The ``PluginManager`` to activate hooks.\n        session_id: The id of the session.\n\n    Raises:\n        Exception: in case of any downstream node failure.\n    \"\"\"\n    if not self._is_async:\n        self._logger.info(\n            \"Using synchronous mode for loading and saving data. Use the --async flag \"\n            \"for potential performance gains. https://docs.kedro.org/en/stable/nodes_and_pipelines/run_a_pipeline.html#load-and-save-asynchronously\"\n        )\n    super()._run(\n        pipeline=pipeline,\n        catalog=catalog,\n        hook_manager=hook_manager,\n        session_id=session_id,\n    )\n</code></pre>"},{"location":"api/runner/kedro.runner.ThreadRunner/","title":"ThreadRunner","text":""},{"location":"api/runner/kedro.runner.ThreadRunner/#kedro.runner.ThreadRunner","title":"kedro.runner.ThreadRunner","text":"<pre><code>ThreadRunner(max_workers=None, is_async=False, extra_dataset_patterns=None)\n</code></pre> <p>               Bases: <code>AbstractRunner</code></p> <p><code>ThreadRunner</code> is an <code>AbstractRunner</code> implementation. It can be used to run the <code>Pipeline</code> in parallel groups formed by toposort using threads.</p> <p>Parameters:</p> <ul> <li> <code>max_workers</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of worker processes to spawn. If not set, calculated automatically based on the pipeline configuration and CPU core count.</p> </li> <li> <code>is_async</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, set to False, because <code>ThreadRunner</code> doesn't support loading and saving the node inputs and outputs asynchronously with threads. Defaults to False.</p> </li> <li> <code>extra_dataset_patterns</code>               (<code>dict[str, dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra dataset factory patterns to be added to the catalog during the run. This is used to set the default datasets to MemoryDataset for <code>ThreadRunner</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>bad parameters passed</p> </li> </ul> Source code in <code>kedro/runner/thread_runner.py</code> <pre><code>def __init__(\n    self,\n    max_workers: int | None = None,\n    is_async: bool = False,\n    extra_dataset_patterns: dict[str, dict[str, Any]] | None = None,\n):\n    \"\"\"\n    Instantiates the runner.\n\n    Args:\n        max_workers: Number of worker processes to spawn. If not set,\n            calculated automatically based on the pipeline configuration\n            and CPU core count.\n        is_async: If True, set to False, because `ThreadRunner`\n            doesn't support loading and saving the node inputs and\n            outputs asynchronously with threads. Defaults to False.\n        extra_dataset_patterns: Extra dataset factory patterns to be added to the catalog\n            during the run. This is used to set the default datasets to MemoryDataset\n            for `ThreadRunner`.\n\n    Raises:\n        ValueError: bad parameters passed\n    \"\"\"\n    if is_async:\n        warnings.warn(\n            \"'ThreadRunner' doesn't support loading and saving the \"\n            \"node inputs and outputs asynchronously with threads. \"\n            \"Setting 'is_async' to False.\"\n        )\n    default_dataset_pattern = {\"{default}\": {\"type\": \"MemoryDataset\"}}\n    self._extra_dataset_patterns = extra_dataset_patterns or default_dataset_pattern\n    super().__init__(\n        is_async=False, extra_dataset_patterns=self._extra_dataset_patterns\n    )\n\n    self._max_workers = self._validate_max_workers(max_workers)\n</code></pre>"},{"location":"api/runner/kedro.runner.ThreadRunner/#kedro.runner.ThreadRunner._extra_dataset_patterns","title":"_extra_dataset_patterns  <code>instance-attribute</code>","text":"<pre><code>_extra_dataset_patterns = extra_dataset_patterns or default_dataset_pattern\n</code></pre>"},{"location":"api/runner/kedro.runner.ThreadRunner/#kedro.runner.ThreadRunner._max_workers","title":"_max_workers  <code>instance-attribute</code>","text":"<pre><code>_max_workers = _validate_max_workers(max_workers)\n</code></pre>"},{"location":"api/runner/kedro.runner.ThreadRunner/#kedro.runner.ThreadRunner._get_executor","title":"_get_executor","text":"<pre><code>_get_executor(max_workers)\n</code></pre> Source code in <code>kedro/runner/thread_runner.py</code> <pre><code>def _get_executor(self, max_workers: int) -&gt; Executor:\n    return ThreadPoolExecutor(max_workers=max_workers)\n</code></pre>"},{"location":"api/runner/kedro.runner.ThreadRunner/#kedro.runner.ThreadRunner._get_required_workers_count","title":"_get_required_workers_count","text":"<pre><code>_get_required_workers_count(pipeline)\n</code></pre> <p>Calculate the max number of processes required for the pipeline</p> Source code in <code>kedro/runner/thread_runner.py</code> <pre><code>def _get_required_workers_count(self, pipeline: Pipeline) -&gt; int:\n    \"\"\"\n    Calculate the max number of processes required for the pipeline\n    \"\"\"\n    # Number of nodes is a safe upper-bound estimate.\n    # It's also safe to reduce it by the number of layers minus one,\n    # because each layer means some nodes depend on other nodes\n    # and they can not run in parallel.\n    # It might be not a perfect solution, but good enough and simple.\n    required_threads = len(pipeline.nodes) - len(pipeline.grouped_nodes) + 1\n\n    return (\n        min(required_threads, self._max_workers)\n        if self._max_workers\n        else required_threads\n    )\n</code></pre>"},{"location":"api/runner/kedro.runner.ThreadRunner/#kedro.runner.ThreadRunner._run","title":"_run","text":"<pre><code>_run(pipeline, catalog, hook_manager=None, session_id=None)\n</code></pre> <p>The method implementing threaded pipeline running.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The <code>Pipeline</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> from which to fetch data.</p> </li> <li> <code>hook_manager</code>               (<code>PluginManager | None</code>, default:                   <code>None</code> )           \u2013            <p>The <code>PluginManager</code> to activate hooks.</p> </li> <li> <code>session_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The id of the session.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>Exception</code>             \u2013            <p>in case of any downstream node failure.</p> </li> </ul> Source code in <code>kedro/runner/thread_runner.py</code> <pre><code>def _run(\n    self,\n    pipeline: Pipeline,\n    catalog: CatalogProtocol,\n    hook_manager: PluginManager | None = None,\n    session_id: str | None = None,\n) -&gt; None:\n    \"\"\"The method implementing threaded pipeline running.\n\n    Args:\n        pipeline: The ``Pipeline`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` from which to fetch data.\n        hook_manager: The ``PluginManager`` to activate hooks.\n        session_id: The id of the session.\n\n    Raises:\n        Exception: in case of any downstream node failure.\n\n    \"\"\"\n    super()._run(\n        pipeline=pipeline,\n        catalog=catalog,\n        hook_manager=hook_manager,\n        session_id=session_id,\n    )\n</code></pre>"},{"location":"api/runner/kedro.runner/","title":"kedro.runner","text":"Name Type Description <code>kedro.runner.run_node</code> Function Runs a single node in isolation. <code>kedro.runner.AbstractRunner</code> Class Abstract base class for all Kedro runners. <code>kedro.runner.SequentialRunner</code> Class Runs nodes sequentially in a pipeline. <code>kedro.runner.ParallelRunner</code> Class Runs nodes in parallel in a pipeline. <code>kedro.runner.ThreadRunner</code> Class Runs nodes in parallel using threads."},{"location":"api/runner/kedro.runner/#kedro.runner","title":"kedro.runner","text":"<p><code>kedro.runner</code> provides runners that are able to execute <code>Pipeline</code> instances.</p>"},{"location":"api/runner/kedro.runner.run_node/","title":"run_node","text":""},{"location":"api/runner/kedro.runner.run_node/#kedro.runner.run_node","title":"kedro.runner.run_node","text":"<pre><code>run_node(node, catalog, hook_manager, is_async=False, session_id=None)\n</code></pre> <p>Run a single <code>Node</code> with inputs from and outputs to the <code>catalog</code>.</p> <p>Parameters:</p> <ul> <li> <code>node</code>               (<code>Node</code>)           \u2013            <p>The <code>Node</code> to run.</p> </li> <li> <code>catalog</code>               (<code>CatalogProtocol</code>)           \u2013            <p>An implemented instance of <code>CatalogProtocol</code> containing the node's inputs and outputs.</p> </li> <li> <code>hook_manager</code>               (<code>PluginManager</code>)           \u2013            <p>The <code>PluginManager</code> to activate hooks.</p> </li> <li> <code>is_async</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the node inputs and outputs are loaded and saved asynchronously with threads. Defaults to False.</p> </li> <li> <code>session_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The session id of the pipeline run.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raised if is_async is set to True for nodes wrapping generator functions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Node</code>           \u2013            <p>The node argument.</p> </li> </ul> Source code in <code>kedro/runner/runner.py</code> <pre><code>def run_node(\n    node: Node,\n    catalog: CatalogProtocol,\n    hook_manager: PluginManager,\n    is_async: bool = False,\n    session_id: str | None = None,\n) -&gt; Node:\n    \"\"\"Run a single `Node` with inputs from and outputs to the `catalog`.\n\n    Args:\n        node: The ``Node`` to run.\n        catalog: An implemented instance of ``CatalogProtocol`` containing the node's inputs and outputs.\n        hook_manager: The ``PluginManager`` to activate hooks.\n        is_async: If True, the node inputs and outputs are loaded and saved\n            asynchronously with threads. Defaults to False.\n        session_id: The session id of the pipeline run.\n\n    Raises:\n        ValueError: Raised if is_async is set to True for nodes wrapping\n            generator functions.\n\n    Returns:\n        The node argument.\n\n    \"\"\"\n    warnings.warn(\n        \"`run_node()` has been deprecated and will be removed in Kedro 0.20.0\",\n        KedroDeprecationWarning,\n    )\n\n    if is_async and inspect.isgeneratorfunction(node.func):\n        raise ValueError(\n            f\"Async data loading and saving does not work with \"\n            f\"nodes wrapping generator functions. Please make \"\n            f\"sure you don't use `yield` anywhere \"\n            f\"in node {node!s}.\"\n        )\n\n    task = Task(\n        node=node,\n        catalog=catalog,\n        hook_manager=hook_manager,\n        is_async=is_async,\n        session_id=session_id,\n    )\n    node = task.execute()\n    return node\n</code></pre>"},{"location":"pages/","title":"Home","text":""},{"location":"pages/#welcome-to-kedros-award-winning-documentation","title":"Welcome to Kedro\\'s award-winning documentation!","text":""},{"location":"pages/#learn-about-kedro","title":"Learn about Kedro","text":"<ul> <li>Introduction</li> <li>Get Started</li> <li>Course</li> </ul>"},{"location":"pages/#tutorial-and-basic-kedro-usage","title":"Tutorial and Basic Kedro Usage","text":"<ul> <li>Spaceflights Tutorial</li> <li>Visualisation</li> <li>Notebooks and IPython</li> <li>Resources</li> </ul>"},{"location":"pages/#kedro-projects","title":"Kedro Projects","text":"<ul> <li>Starters</li> <li>Configuration</li> <li>Data</li> <li>Nodes and Pipelines</li> <li>Telemetry</li> </ul>"},{"location":"pages/#integrations","title":"Integrations","text":"<ul> <li>PySpark</li> <li>MLflow</li> <li>DVC Versioning</li> <li>DeltaLake Versioning</li> <li>Iceberg Versioning</li> </ul>"},{"location":"pages/#development","title":"Development","text":"<ul> <li>Set Up VSCode</li> <li>Set Up PyCharm</li> <li>Debugging</li> <li>Automated Testing</li> <li>Linting</li> </ul>"},{"location":"pages/#advanced-usage","title":"Advanced Usage","text":"<ul> <li>Kedro Project Setup</li> <li>Extend Kedro</li> <li>Hooks</li> <li>Logging</li> <li>Deployment</li> </ul>"},{"location":"pages/#contribute-to-kedro","title":"Contribute to Kedro","text":"<ul> <li>Contribution Guide</li> </ul>"},{"location":"pages/#cli-reference","title":"CLI Reference","text":"<ul> <li>Commands Reference</li> </ul>"},{"location":"pages/#api-documentation","title":"API documentation","text":""},{"location":"pages/#indices-and-tables","title":"Indices and tables","text":"<ul> <li><code>genindex</code>{.interpreted-text role=\"ref\"}</li> <li><code>modindex</code>{.interpreted-text role=\"ref\"}</li> </ul>"},{"location":"pages/configuration/","title":"Configuration","text":"<ul> <li>Configuration Basics</li> <li>Credentials</li> <li>Parameters</li> <li>Config Loader Migration</li> <li>Advanced Configuration</li> </ul>"},{"location":"pages/configuration/advanced_configuration/","title":"Advanced configuration","text":"<p>The documentation on configuration describes how to satisfy most common requirements of standard Kedro project configuration:</p> <p>By default, Kedro is set up to use the {py:class}<code>~kedro.config.OmegaConfigLoader</code> class.</p>"},{"location":"pages/configuration/advanced_configuration/#advanced-configuration-for-kedro-projects","title":"Advanced configuration for Kedro projects","text":"<p>This page also contains a set of guidance for advanced configuration requirements of standard Kedro projects:</p> <ul> <li>How to use a custom config loader</li> <li>How to change which configuration files are loaded</li> <li>How to ensure non default configuration files get loaded</li> <li>How to bypass the configuration loading rules</li> <li>How to do templating with the <code>OmegaConfigLoader</code></li> <li>How to load a data catalog with templating in code?</li> <li>How to use global variables with the <code>OmegaConfigLoader</code></li> <li>How to override configuration with runtime parameters with the <code>OmegaConfigLoader</code></li> <li>How to use resolvers in the <code>OmegaConfigLoader</code></li> <li>How to load credentials through environment variables with <code>OmegaConfigLoader</code></li> <li>How to change the merge strategy used by <code>OmegaConfigLoader</code></li> </ul>"},{"location":"pages/configuration/advanced_configuration/#how-to-use-a-custom-configuration-loader","title":"How to use a custom configuration loader","text":"<p>You can implement a custom configuration loader by extending the {py:class}<code>~kedro.config.AbstractConfigLoader</code> class:</p> <pre><code>from kedro.config import AbstractConfigLoader\n\n\nclass CustomConfigLoader(AbstractConfigLoader):\n    def __init__(\n        self,\n        conf_source: str,\n        env: str = None,\n        runtime_params: Dict[str, Any] = None,\n    ):\n        super().__init__(\n            conf_source=conf_source, env=env, runtime_params=runtime_params\n        )\n\n        # Custom implementation\n</code></pre> <p>To use this custom configuration loader, set it as the project configuration loader in <code>src/&lt;package_name&gt;/settings.py</code> as follows:</p> <pre><code>from package_name.custom_configloader import CustomConfigLoader\n\nCONFIG_LOADER_CLASS = CustomConfigLoader\n</code></pre>"},{"location":"pages/configuration/advanced_configuration/#how-to-change-which-configuration-files-are-loaded","title":"How to change which configuration files are loaded","text":"<p>If you want to change the patterns that the configuration loader uses to find the files to load you need to set the <code>CONFIG_LOADER_ARGS</code> variable in <code>src/&lt;package_name&gt;/settings.py</code>. For example, if your <code>parameters</code> files are using a <code>params</code> naming convention instead of <code>parameters</code> (e.g. <code>params.yml</code>) you need to update <code>CONFIG_LOADER_ARGS</code> as follows:</p> <pre><code>CONFIG_LOADER_ARGS = {\n    \"config_patterns\": {\n        \"parameters\": [\"params*\", \"params*/**\", \"**/params*\"],\n    }\n}\n</code></pre> <p>By changing this setting, the default behaviour for loading parameters will be replaced, while the other configuration patterns will remain in their default state.</p>"},{"location":"pages/configuration/advanced_configuration/#how-to-ensure-non-default-configuration-files-get-loaded","title":"How to ensure non default configuration files get loaded","text":"<p>You can add configuration patterns to match files other than <code>parameters</code>, <code>credentials</code>, and <code>catalog</code> by setting the <code>CONFIG_LOADER_ARGS</code> variable in <code>src/&lt;package_name&gt;/settings.py</code>. For example, if you want to load Spark configuration files you need to update <code>CONFIG_LOADER_ARGS</code> as follows:</p> <pre><code>CONFIG_LOADER_ARGS = {\n    \"config_patterns\": {\n        \"spark\": [\"spark*/\"],\n    }\n}\n</code></pre>"},{"location":"pages/configuration/advanced_configuration/#how-to-bypass-the-configuration-loading-rules","title":"How to bypass the configuration loading rules","text":"<p>You can bypass the configuration patterns and set configuration directly on the instance of a config loader class. You can bypass the default configuration (catalog, parameters and credentials) as well as additional configuration.</p> <p>For example, you can use hooks to load external credentials.</p> <p>Alternatively, if you are using config loader as a standalone component, you can override configuration as follows:</p> <p>```{code-block} python :lineno-start: 10 :emphasize-lines: 8</p> <p>from kedro.config import OmegaConfigLoader from kedro.framework.project import settings</p> <p>conf_path = str(project_path / settings.CONF_SOURCE) conf_loader = OmegaConfigLoader(conf_source=conf_path)</p>"},{"location":"pages/configuration/advanced_configuration/#bypass-configuration-patterns-by-setting-the-key-and-values-directly-on-the-config-loader-instance","title":"Bypass configuration patterns by setting the key and values directly on the config loader instance.","text":"<p>conf_loader[\"catalog\"] = {\"catalog_config\": \"something_new\"}</p> <pre><code>\n### How to do templating with the `OmegaConfigLoader`\n#### Parameters\nTemplating or [variable interpolation](https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#variable-interpolation), as it's called in `OmegaConf`, for parameters works out of the box if the template values are within the parameter files or the name of the file that contains the template values follows the same config pattern specified for parameters.\nBy default, the config pattern for parameters is: `[\"parameters*\", \"parameters*/**\", \"**/parameters*\"]`.\nSuppose you have one parameters file called `parameters.yml` containing parameters with `omegaconf` placeholders like this:\n\n```yaml\nmodel_options:\n  test_size: ${data.size}\n  random_state: 3\n</code></pre> <p>and a file containing the template values called <code>parameters_globals.yml</code>:</p> <pre><code>data:\n  size: 0.2\n</code></pre> <p>Since both of the file names (<code>parameters.yml</code> and <code>parameters_globals.yml</code>) match the config pattern for parameters, the <code>OmegaConfigLoader</code> will load the files and resolve the placeholders correctly.</p>"},{"location":"pages/configuration/advanced_configuration/#catalog","title":"Catalog","text":"<p>From Kedro <code>0.18.10</code> templating also works for catalog files. To enable templating in the catalog you need to ensure that the template values are within the catalog files or the name of the file that contains the template values follows the same config pattern specified for catalogs. By default, the config pattern for catalogs is: <code>[\"catalog*\", \"catalog*/**\", \"**/catalog*\"]</code>.</p> <p>Additionally, any template values in the catalog need to start with an underscore <code>_</code>. This is because of how catalog entries are validated. Templated values will neither trigger a key duplication error nor appear in the resulting configuration dictionary.</p> <p>Suppose you have one catalog file called <code>catalog.yml</code> containing entries with <code>omegaconf</code> placeholders like this:</p> <pre><code>companies:\n  type: ${_pandas.type}\n  filepath: data/01_raw/companies.csv\n</code></pre> <p>and a file containing the template values called <code>catalog_globals.yml</code>:</p> <pre><code>_pandas:\n  type: pandas.CSVDataset\n</code></pre> <p>Since both of the file names (<code>catalog.yml</code> and <code>catalog_globals.yml</code>) match the config pattern for catalogs, the <code>OmegaConfigLoader</code> will load the files and resolve the placeholders correctly.</p>"},{"location":"pages/configuration/advanced_configuration/#other-configuration-files","title":"Other configuration files","text":"<p>It's also possible to use variable interpolation in configuration files other than parameters and catalog, such as custom spark or mlflow configuration. This works in the same way as variable interpolation in parameter files. You can still use the underscore for the templated values if you want, but it's not mandatory like it is for catalog files.</p>"},{"location":"pages/configuration/advanced_configuration/#how-to-load-a-data-catalog-with-templating-in-code","title":"How to load a data catalog with templating in code?","text":"<p>You can use the <code>OmegaConfigLoader</code> to directly load a data catalog that contains templating in code. Under the hood the <code>OmegaConfigLoader</code> will resolve any templates, so no further steps are required to load catalog entries properly.</p> <pre><code># Example catalog with templating\ncompanies:\n  type: ${_dataset_type}\n  filepath: data/01_raw/companies.csv\n\n_dataset_type: pandas.CSVDataset\n</code></pre> <pre><code>from kedro.config import OmegaConfigLoader\nfrom kedro.framework.project import settings\n\n# Instantiate an `OmegaConfigLoader` instance with the location of your project configuration.\nconf_path = str(project_path / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\n\nconf_catalog = conf_loader[\"catalog\"]\n# conf_catalog[\"companies\"]\n# Will result in: {'type': 'pandas.CSVDataset', 'filepath': 'data/01_raw/companies.csv'}\n</code></pre>"},{"location":"pages/configuration/advanced_configuration/#how-to-use-global-variables-with-the-omegaconfigloader","title":"How to use global variables with the <code>OmegaConfigLoader</code>","text":"<p>From Kedro <code>0.18.13</code>, you can use variable interpolation in your configurations using \"globals\" with <code>OmegaConfigLoader</code>. The benefit of using globals over regular variable interpolation is that the global variables are shared across different configuration types, such as catalog and parameters. By default, these global variables are assumed to be in files called <code>globals.yml</code> in any of your environments. If you want to configure the naming patterns for the files that contain your global variables, you can do so by overwriting the <code>globals</code> key in <code>config_patterns</code>. You can also bypass the configuration loading to directly set the global variables in <code>OmegaConfigLoader</code>.</p> <p>Suppose you have global variables located in the file <code>conf/base/globals.yml</code>:</p> <pre><code>my_global_value: 45\ndataset_type:\n  csv: pandas.CSVDataset\n</code></pre> <p>You can access these global variables in your catalog or parameters config files with a <code>globals</code> resolver like this: <code>conf/base/parameters.yml</code>:</p> <pre><code>my_param : \"${globals:my_global_value}\"\n</code></pre> <p><code>conf/base/catalog.yml</code>:</p> <pre><code>companies:\n  filepath: data/01_raw/companies.csv\n  type: \"${globals:dataset_type.csv}\"\n</code></pre> <p>You can also provide a default value to be used in case the global variable does not exist:</p> <pre><code>my_param: \"${globals: nonexistent_global, 23}\"\n</code></pre> <p>If there are duplicate keys in the globals files in your base and runtime environments, the values in the runtime environment overwrite the values in your base environment.</p> <p>(runtime-params)=</p>"},{"location":"pages/configuration/advanced_configuration/#how-to-override-configuration-with-runtime-parameters-with-the-omegaconfigloader","title":"How to override configuration with runtime parameters with the <code>OmegaConfigLoader</code>","text":"<p>Kedro allows you to specify runtime parameters for the <code>kedro run</code> command with the <code>--params</code> CLI option. These runtime parameters are added to the <code>KedroContext</code> and merged with parameters from the configuration files to be used in your project's pipelines and nodes. From Kedro <code>0.18.14</code>, you can use the <code>runtime_params</code> resolver to indicate that you want to override values of certain keys in your configuration with runtime parameters provided through the CLI option. This resolver can be used across different configuration types, such as parameters, catalog, and more, except for \"globals\".</p> <p>Consider this <code>parameters.yml</code> file:</p> <pre><code>model_options:\n  random_state: \"${runtime_params:random}\"\n</code></pre> <p>This will allow you to pass a runtime parameter named <code>random</code> through the CLI to specify the value of <code>model_options.random_state</code> in your project's parameters:</p> <pre><code>kedro run --params random=3\n</code></pre> <p>You can also specify a default value to be used in case the runtime parameter is not specified with the <code>kedro run</code> command. Consider this catalog entry:</p> <pre><code>companies:\n  type: pandas.CSVDataset\n  filepath: \"${runtime_params:folder, 'data/01_raw'}/companies.csv\"\n</code></pre> <p>If the <code>folder</code> parameter is not passed through the CLI <code>--params</code> option with <code>kedro run</code>, the default value <code>'data/01_raw/'</code> is used for the <code>filepath</code>.</p> <pre><code>When manually instantiating `OmegaConfigLoader` in code, runtime parameters passed via the CLI `--params` option will not be available to the resolver. This occurs because the manually created config loader instance doesn't have access to the runtime parameters provided through the CLI.\nIf you need to access runtime parameters in code that manually instantiates `OmegaConfigLoader`, you should instead use the Kedro context to access parameters.\n</code></pre>"},{"location":"pages/configuration/advanced_configuration/#how-to-use-globals-and-runtime_params","title":"How to use <code>globals</code> and <code>runtime_params</code>","text":"<p>As mentioned above, <code>runtime_params</code> are not designed to override <code>globals</code> configuration. This is done to avoid unexplicit overrides and to simplify parameter resolutions. Thus, <code>globals</code> has only one entry point - the <code>yaml</code> file.</p> <p>However, you can use <code>globals</code> and <code>runtime_params</code> by specifying <code>globals</code> as a default value to be used in case the runtime parameter is not passed.</p> <p>Consider this <code>parameters.yml</code>:</p> <pre><code>model_options:\n  random_state: \"${runtime_params:random, ${globals:my_global_value}}\"\n</code></pre> <p>and this <code>globals.yml</code> file:</p> <pre><code>my_global_value: 4\n</code></pre> <p>This will allow you to pass a runtime parameter named <code>random</code> through the CLI to specify the value of <code>model_options.random_state</code> in your project's parameters:</p> <pre><code>kedro run --params random=3\n</code></pre> <p>If the <code>random</code> parameter is not passed through the CLI <code>--params</code> option with <code>kedro run</code>, then <code>my_global_value</code> from <code>globals.yml</code> is used for the <code>model_options.random_state</code>.</p>"},{"location":"pages/configuration/advanced_configuration/#how-to-use-resolvers-in-the-omegaconfigloader","title":"How to use resolvers in the <code>OmegaConfigLoader</code>","text":"<p>Instead of hard-coding values in your configuration files, you can also dynamically compute them using <code>OmegaConf</code>'s resolvers functionality. You use resolvers to define custom logic to calculate values of parameters or catalog entries, or inject these values from elsewhere. To use this feature with Kedro, pass a <code>dict</code> of custom resolvers to <code>OmegaConfigLoader</code> through <code>CONFIG_LOADER_ARGS</code> in your project's <code>src/&lt;package_name&gt;/settings.py</code>. The example below illustrates this:</p> <pre><code>import polars as pl\nfrom datetime import date\n\n\ndef date_today():\n    return date.today()\n\n\nCONFIG_LOADER_ARGS = {\n    \"custom_resolvers\": {\n        \"add\": lambda *my_list: sum(my_list),\n        \"polars\": lambda x: getattr(pl, x),\n        \"today\": date_today,\n    }\n}\n</code></pre> <p>These custom resolvers are then registered using <code>OmegaConf.register_new_resolver()</code> under the hood and can be used in any of the configuration files in your project. For example, you can use the <code>add</code> or the <code>today</code> resolver defined above in your <code>parameters.yml</code> like this:</p> <pre><code>model_options:\n  test_size: \"${add:1,2,3}\"\n  random_state: 3\n\ndate: \"${today:}\"\n</code></pre> <p>The values of these parameters will be computed at access time and will be passed on to your nodes. Resolvers can also be used in your <code>catalog.yml</code>. In the example below, we use the <code>polars</code> resolver defined above to pass non-primitive types to the catalog entry.</p> <pre><code>my_polars_dataset:\n  type: polars.CSVDataset\n  filepath: data/01_raw/my_dataset.csv\n  load_args:\n    dtypes:\n      product_age: \"${polars:Float64}\"\n      group_identifier: \"${polars:Utf8}\"\n    try_parse_dates: true\n</code></pre> <p><code>OmegaConf</code> also comes with some built-in resolvers that you can use with the <code>OmegaConfigLoader</code> in Kedro. All built-in resolvers except for <code>oc.env</code> are enabled by default. <code>oc.env</code> is only turned on for loading credentials. You can, however, turn this on for all configurations through your project's <code>src/&lt;package_name&gt;/settings.py</code> in a similar way:</p> <pre><code>This is an advanced feature and should be used with caution. We do not recommend using environment variables for configurations other than credentials.\n</code></pre> <pre><code>from omegaconf.resolvers import oc\n\nCONFIG_LOADER_ARGS = {\n    \"custom_resolvers\": {\n        \"oc.env\": oc.env,\n    }\n}\n</code></pre>"},{"location":"pages/configuration/advanced_configuration/#how-to-load-credentials-through-environment-variables","title":"How to load credentials through environment variables","text":"<p>The {py:class}<code>~kedro.config.OmegaConfigLoader</code> enables you to load credentials from environment variables. To achieve this you have to use the <code>OmegaConfigLoader</code> and the <code>omegaconf</code> <code>oc.env</code> resolver. You can use the <code>oc.env</code> resolver to access credentials from environment variables in your <code>credentials.yml</code>:</p> <pre><code>dev_s3:\n  client_kwargs:\n    aws_access_key_id: ${oc.env:AWS_ACCESS_KEY_ID}\n    aws_secret_access_key: ${oc.env:AWS_SECRET_ACCESS_KEY}\n</code></pre> <pre><code>Note that you can only use the resolver in `credentials.yml` and not in catalog or parameter files. This is because we do not encourage the usage of environment variables for anything other than credentials.\n</code></pre>"},{"location":"pages/configuration/advanced_configuration/#how-to-change-the-merge-strategy-used-by-omegaconfigloader","title":"How to change the merge strategy used by <code>OmegaConfigLoader</code>","text":"<p>By default, <code>OmegaConfigLoader</code> merges configuration in different environments as well as runtime parameters in a destructive way. This means that whatever configuration resides in your overriding environment (<code>local</code> by default) takes precedence when the same top-level key is present in the base and overriding environment. Any configuration for that key besides that given in the overriding environment is discarded. The same behaviour applies to runtime parameters overriding any configuration in the <code>base</code> environment. You can change the merge strategy for each configuration type in your project's <code>src/&lt;package_name&gt;/settings.py</code>. The accepted merging strategies are <code>soft</code> and <code>destructive</code>.</p> <pre><code>from kedro.config import OmegaConfigLoader\n\nCONFIG_LOADER_CLASS = OmegaConfigLoader\n\nCONFIG_LOADER_ARGS = {\n    \"merge_strategy\": {\n        \"parameters\": \"soft\",\n        \"spark\": \"destructive\",\n        \"mlflow\": \"soft\",\n    }\n}\n</code></pre> <p>If no merge strategy is defined, the default destructive strategy will be applied. Note that this merge strategy setting only applies to configuration files in different environments. When files are part of the same environment, they are always merged in a soft way. An error is thrown when files in the same environment contain the same top-level keys.</p>"},{"location":"pages/configuration/advanced_configuration/#advanced-configuration-without-a-full-kedro-project","title":"Advanced configuration without a full Kedro project","text":"<p>In some cases, you may only want to use the <code>OmegaConfigLoader</code> without a Kedro project. By default, a Kedro project has a <code>base</code> and <code>local</code> environment. However, when you use the <code>OmegaConfigLoader</code> directly, it assumes no environment. You may find it useful to add Kedro to your existing notebooks.</p>"},{"location":"pages/configuration/advanced_configuration/#read-configuration","title":"Read configuration","text":"<p>The config loader can work without a Kedro project structure.</p> <pre><code>tree .\n.\n\u2514\u2500\u2500 parameters.yml\n</code></pre> <p>Consider the following <code>parameters.yml</code> file and example Python script:</p> <pre><code>learning_rate: 0.01\ntrain_test_ratio: 0.7\n</code></pre> <pre><code>from kedro.config import OmegaConfigLoader\nconfig_loader = OmegaConfigLoader(conf_source=\".\")\n\n# Optionally, you can also use environments\n# config_loader = OmegaConfigLoader(conf_source=\".\", base_env=\"base\", default_run_env=\"local\")\n\nprint(config_loader[\"parameters\"])\n</code></pre> <p>If you run it from the same directory where <code>parameters.yml</code> placed it gives the following output:</p> <pre><code>{'learning_rate': 0.01, 'train_test_ratio': 0.7}\n</code></pre> <p>For the full list of features, please refer to configuration_basics and advanced_configuration</p>"},{"location":"pages/configuration/advanced_configuration/#how-to-use-custom-resolvers-with-omegaconfigloader","title":"How to use Custom Resolvers with <code>OmegaConfigLoader</code>","text":"<p>You can register custom resolvers to use non-primitive types for parameters.</p> <p>Consider the following <code>parameters.yml</code> file an example of Python script for registering a custom resolver:</p> <pre><code>polars_float64: \"${polars: Float64}\"\ntoday: \"${today:}\"\n</code></pre> <pre><code>import polars as pl\nfrom datetime import date\n\nfrom kedro.config import OmegaConfigLoader\n\ncustom_resolvers = {\"polars\": lambda x: getattr(pl, x),\n                    \"today\": lambda: date.today()}\n\n# Register custom resolvers\nconfig_loader = OmegaConfigLoader(conf_source=\".\", custom_resolvers=custom_resolvers)\n\nprint(config_loader[\"parameters\"])\n</code></pre> <p>If you run it from the same directory where <code>parameters.yml</code> placed it gives the following output:</p> <pre><code>{'polars_float64': Float64, 'today': datetime.date(2023, 11, 23)}\n</code></pre>"},{"location":"pages/configuration/config_loader_migration/","title":"Migration guide for config loaders","text":"<p>The <code>ConfigLoader</code> and <code>TemplatedConfigLoader</code> classes have been deprecated since Kedro <code>0.18.12</code> and were removed in Kedro <code>0.19.0</code>. To use that release or later, you must adopt the {py:class}<code>~kedro.config.OmegaConfigLoader</code>. This migration guide outlines the primary distinctions between the old loaders and the <code>OmegaConfigLoader</code>, providing step-by-step instructions on updating your code base to utilise the new class effectively.</p>"},{"location":"pages/configuration/config_loader_migration/#configloader-to-omegaconfigloader","title":"<code>ConfigLoader</code> to <code>OmegaConfigLoader</code>","text":""},{"location":"pages/configuration/config_loader_migration/#1-install-the-required-library","title":"1. Install the required library","text":"<p>The <code>OmegaConfigLoader</code> was introduced in Kedro <code>0.18.5</code> and is based on OmegaConf. To use it you need Kedro (version <code>0.18.5</code> or later) and <code>omegaconf</code> installed. You can install both using <code>pip</code>:</p> <pre><code>pip install kedro==0.18.5\n</code></pre> <p>This would be the minimum required Kedro version which includes <code>omegaconf</code> as a dependency. Or you can run:</p> <pre><code>pip install -U kedro\n</code></pre> <p>This command installs the most recent version of Kedro which also includes <code>omegaconf</code> as a dependency.</p>"},{"location":"pages/configuration/config_loader_migration/#2-use-the-omegaconfigloader","title":"2. Use the <code>OmegaConfigLoader</code>","text":"<p>To use <code>OmegaConfigLoader</code> in your project, set the <code>CONFIG_LOADER_CLASS</code> constant in your <code>src/&lt;package_name&gt;/settings.py</code>:</p> <pre><code>+ from kedro.config import OmegaConfigLoader  # new import\n\n+ CONFIG_LOADER_CLASS = OmegaConfigLoader\n</code></pre>"},{"location":"pages/configuration/config_loader_migration/#3-import-statements","title":"3. Import statements","text":"<p>Replace the import statement for <code>ConfigLoader</code> with the one for <code>OmegaConfigLoader</code>:</p> <pre><code>- from kedro.config import ConfigLoader\n\n+ from kedro.config import OmegaConfigLoader\n</code></pre>"},{"location":"pages/configuration/config_loader_migration/#4-file-format-support","title":"4. File format support","text":"<p><code>OmegaConfigLoader</code> supports only <code>yaml</code> and <code>json</code> file formats. Make sure that all your configuration files are in one of these formats. If you previously used other formats with <code>ConfigLoader</code>, convert them to <code>yaml</code> or <code>json</code>.</p>"},{"location":"pages/configuration/config_loader_migration/#5-load-configuration","title":"5. Load configuration","text":"<p>The method to load the configuration using <code>OmegaConfigLoader</code> differs slightly from that used by <code>ConfigLoader</code>, which allowed users to access configuration through the <code>.get()</code> method and required patterns as argument. When you migrate to use <code>OmegaConfigLoader</code> it  requires you to fetch configuration through a configuration key that points to configuration patterns specified in the loader class or provided in the <code>CONFIG_LOADER_ARGS</code> in <code>settings.py</code>.</p> <pre><code>- conf_path = str(project_path / settings.CONF_SOURCE)\n- conf_loader = ConfigLoader(conf_source=conf_path, env=\"local\")\n- catalog = conf_loader.get(\"catalog*\")\n\n+ conf_path = str(project_path / settings.CONF_SOURCE)\n+ config_loader = OmegaConfigLoader(conf_source=conf_path, env=\"local\")\n+ catalog = config_loader[\"catalog\"]\n</code></pre> <p>In this example, <code>\"catalog\"</code> is the key to the default catalog patterns specified in the <code>OmegaConfigLoader</code> class.</p>"},{"location":"pages/configuration/config_loader_migration/#6-exception-handling","title":"6. Exception handling","text":"<p>For error and exception handling, most errors are the same. Those you need to be aware of that are different between the original <code>ConfigLoader</code> and <code>OmegaConfigLoader</code> are as follows: * <code>OmegaConfigLoader</code> throws a <code>MissingConfigException</code> when configuration paths don't exist, rather than the <code>ValueError</code> used in <code>ConfigLoader</code>. * In <code>OmegaConfigLoader</code>, if there is bad syntax in your configuration files, it will trigger a <code>ParserError</code> instead of a <code>BadConfigException</code> used in <code>ConfigLoader</code>.</p>"},{"location":"pages/configuration/config_loader_migration/#templatedconfigloader-to-omegaconfigloader","title":"<code>TemplatedConfigLoader</code> to <code>OmegaConfigLoader</code>","text":""},{"location":"pages/configuration/config_loader_migration/#1-install-the-required-library_1","title":"1. Install the required library","text":"<p>The <code>OmegaConfigLoader</code> was introduced in Kedro <code>0.18.5</code> and is based on OmegaConf. Features that replace <code>TemplatedConfigLoader</code> functionality have been released in later versions, so we recommend users install Kedro version <code>0.18.13</code> or later to properly replace the <code>TemplatedConfigLoader</code> with <code>OmegaConfigLoader</code>. You can install both this Kedro version and <code>omegaconf</code> using <code>pip</code>:</p> <pre><code>pip install \"kedro&gt;=0.18.13\"\n</code></pre> <p>This would be the minimum required Kedro version which includes <code>omegaconf</code> as a dependency and the necessary functionality to replace <code>TemplatedConfigLoader</code>. Or you can run:</p> <pre><code>pip install -U kedro\n</code></pre> <p>This command installs the most recent version of Kedro which also includes <code>omegaconf</code> as a dependency.</p>"},{"location":"pages/configuration/config_loader_migration/#2-use-the-omegaconfigloader_1","title":"2. Use the <code>OmegaConfigLoader</code>","text":"<p>To use <code>OmegaConfigLoader</code> in your project, set the <code>CONFIG_LOADER_CLASS</code> constant in your <code>src/&lt;package_name&gt;/settings.py</code>:</p> <pre><code>+ from kedro.config import OmegaConfigLoader  # new import\n\n+ CONFIG_LOADER_CLASS = OmegaConfigLoader\n</code></pre>"},{"location":"pages/configuration/config_loader_migration/#3-import-statements_1","title":"3. Import statements","text":"<p>Replace the import statement for <code>TemplatedConfigLoader</code> with the one for <code>OmegaConfigLoader</code>:</p> <pre><code>- from kedro.config import TemplatedConfigLoader\n+ from kedro.config import OmegaConfigLoader\n</code></pre>"},{"location":"pages/configuration/config_loader_migration/#4-file-format-support_1","title":"4. File format support","text":"<p><code>OmegaConfigLoader</code> supports only <code>yaml</code> and <code>json</code> file formats. Make sure that all your configuration files are in one of these formats. If you were using other formats with <code>TemplatedConfigLoader</code>, convert them to <code>yaml</code> or <code>json</code>.</p>"},{"location":"pages/configuration/config_loader_migration/#5-load-configuration_1","title":"5. Load configuration","text":"<p>The method to load the configuration using <code>OmegaConfigLoader</code> differs slightly from that used by <code>TemplatedConfigLoader</code>, which allowed users to access configuration through the <code>.get()</code> method and required patterns as argument. When you migrate to use <code>OmegaConfigLoader</code> it  requires you to fetch configuration through a configuration key that points to configuration patterns specified in the loader class or provided in the <code>CONFIG_LOADER_ARGS</code> in <code>settings.py</code>.</p> <pre><code>- conf_path = str(project_path / settings.CONF_SOURCE)\n- conf_loader = TemplatedConfigLoader(conf_source=conf_path, env=\"local\")\n- catalog = conf_loader.get(\"catalog*\")\n\n+ conf_path = str(project_path / settings.CONF_SOURCE)\n+ config_loader = OmegaConfigLoader(conf_source=conf_path, env=\"local\")\n+ catalog = config_loader[\"catalog\"] # note the key accessor syntax\n</code></pre> <p>In this example, the <code>\"catalog\"</code> key points to the default catalog patterns specified in the <code>OmegaConfigLoader</code> class.</p>"},{"location":"pages/configuration/config_loader_migration/#6-templating-of-values","title":"6. Templating of values","text":"<p>Templating of values is done through native variable interpolation in <code>OmegaConfigLoader</code>. Where in <code>TemplatedConfigLoader</code> it was necessary to provide the template values in a <code>globals</code> file or dictionary, in <code>OmegaConfigLoader</code> you can provide these values within the same file that has the placeholders or a file that has a name that follows the same config pattern specified. The variable interpolation is scoped to a specific configuration type and environment. If you want to share templated values across configuration types and environments, you will need to use globals.</p> <p>Suppose you are migrating a templated catalog file from using <code>TemplatedConfigLoader</code> to <code>OmegaConfigLoader</code> you would do the following: 1. Rename <code>conf/base/globals.yml</code> to match the patterns specified for catalog (<code>[\"catalog*\", \"catalog*/**\", \"**/catalog*\"]</code>), for example <code>conf/base/catalog_globals.yml</code> 2. Add an underscore <code>_</code> to any catalog template values. This is needed because of how catalog entries are validated.</p> <pre><code>- bucket_name: \"my_s3_bucket\"\n+ _bucket_name: \"my_s3_bucket\" # kedro requires `_` to mark templatable keys\n- key_prefix: \"my/key/prefix/\"\n+ _key_prefix: \"my/key/prefix/\"\n\n- datasets:\n+ _datasets:\n    csv: \"pandas.CSVDataset\"\n    spark: \"spark.SparkDataset\"\n\n</code></pre> <ol> <li>Update <code>catalog.yml</code> with the underscores <code>_</code> at the beginning of the templated value names.</li> </ol> <pre><code>raw_boat_data:\n-   type: \"${datasets.spark}\"\n+   type: \"${_datasets.spark}\"\n-   filepath: \"s3a://${bucket_name}/${key_prefix}/raw/boats.csv\"\n+   filepath: \"s3a://${_bucket_name}/${_key_prefix}/raw/boats.csv\"\n    file_format: parquet\n\nraw_car_data:\n-    type: \"${datasets.csv}\"\n+    type: \"${_datasets.csv}\"\n-    filepath: \"s3://${bucket_name}/data/${key_prefix}/raw/cars.csv\"\n+    filepath: \"s3://${_bucket_name}/data/${_key_prefix}/raw/cars.csv\"\n</code></pre>"},{"location":"pages/configuration/config_loader_migration/#providing-default-values-for-templates-via-ocselect","title":"Providing default values for templates via <code>oc.select</code>","text":"<p>To provide a default for any template values you have to use the omegaconf <code>oc.select</code> resolver.</p> <pre><code>boats:\n  users:\n    - fred\n-    - \"${write_only_user|ron}\"\n+    - \"${oc.select:write_only_user,ron}\"\n</code></pre>"},{"location":"pages/configuration/config_loader_migration/#7-globals","title":"7. Globals","text":"<p>If you want to share variables across configuration types, for example parameters and catalog, and environments you need to use the custom globals resolver with the <code>OmegaConfigLoader</code>. The <code>OmegaConfigLoader</code> requires global values to be provided in a <code>globals.yml</code> file. Note that using a <code>globals_dict</code> to provide globals is not supported with <code>OmegaConfigLoader</code>. The following section explains the differences between using globals with <code>TemplatedConfigLoader</code> and the <code>OmegaConfigLoader</code>.</p> <p>Let's assume your project contains a <code>conf/base/globals.yml</code> file with the following contents:</p> <pre><code>bucket_name: \"my_s3_bucket\"\nkey_prefix: \"my/key/prefix/\"\n\ndatasets:\n    csv: \"pandas.CSVDataset\"\n    spark: \"spark.SparkDataset\"\n\nfolders:\n    raw: \"01_raw\"\n    int: \"02_intermediate\"\n    pri: \"03_primary\"\n    fea: \"04_feature\"\n</code></pre> <p>You no longer need to set <code>CONFIG_LOADER_ARGS</code> variable in <code>src/&lt;package_name&gt;/settings.py</code> to find this <code>globals.yml</code> file, because the <code>OmegaConfigLoader</code> is configured to pick up files named <code>globals.yml</code> by default.</p> <pre><code>- CONFIG_LOADER_ARGS = {\"globals_pattern\": \"*globals.yml\"}\n</code></pre> <p>The globals templating in your catalog configuration will need to be updated to use the globals resolver as follows:</p> <pre><code>raw_boat_data:\n-   type: \"${datasets.spark}\"\n+   type: \"${globals:datasets.spark}\"  # nested paths into global dict are allowed\n-   filepath: \"s3a://${bucket_name}/${key_prefix}/${folders.raw}/boats.csv\"\n+   filepath: \"s3a://${globals:bucket_name}/${globals:key_prefix}/${globals:folders.raw}/boats.csv\"\n    file_format: parquet\n\nraw_car_data:\n-   type: \"${datasets.csv}\"\n+   type: \"${globals:datasets.csv}\"\n-   filepath: \"s3://${bucket_name}/data/${key_prefix}/${folders.raw}/${filename|cars.csv}\"  # default to 'cars.csv' if the 'filename' key is not found in the global dict\n+   filepath: \"s3://${globals:bucket_name}/data/${globals:key_prefix}/${globals:folders.raw}/${globals:filename,'cars.csv'}\"  # default to 'cars.csv' if the 'filename' key is not found in the global dict\n</code></pre>"},{"location":"pages/configuration/config_loader_migration/#8-deprecation-of-jinja2","title":"8. Deprecation of Jinja2","text":"<p><code>OmegaConfigLoader</code> does not support Jinja2 syntax in configuration. However, users can achieve similar functionality with the <code>OmegaConfigLoader</code> in combination with dataset factories. The following example shows how you can rewrite your Jinja2 configuration to work with <code>OmegaConfigLoader</code>:</p> <pre><code># catalog.yml\n- {% for speed in ['fast', 'slow'] %}\n- {{ speed }}-trains:\n+ \"{speed}-trains\":\n    type: MemoryDataset\n\n- {{ speed }}-cars:\n+ \"{speed}-cars\":\n    type: pandas.CSVDataset\n-    filepath: s3://${bucket_name}/{{ speed }}-cars.csv\n+    filepath: s3://${bucket_name}/{speed}-cars.csv\n    save_args:\n        index: true\n\n- {% endfor %}\n</code></pre>"},{"location":"pages/configuration/config_loader_migration/#9-exception-handling","title":"9. Exception handling","text":"<p>For error and exception handling, most errors are the same. Those you need to be aware of that are different between the original <code>TemplatedConfigLoader</code> and <code>OmegaConfigLoader</code> are as follows: * For missing template values <code>OmegaConfigLoader</code> throws <code>omegaconf.errors.InterpolationKeyError</code>.</p>"},{"location":"pages/configuration/configuration_basics/","title":"Configuration","text":"<p>This section contains detailed information about Kedro project configuration, which you can use to store settings for your project such as parameters, credentials, the data catalog, and logging information.</p> <p>Kedro makes use of a configuration loader to load any project configuration files, which is {py:class}<code>~kedro.config.OmegaConfigLoader</code> by default since Kedro 0.19.0.</p> <pre><code>`ConfigLoader` and `TemplatedConfigLoader` have been removed in Kedro `0.19.0`. Refer to the [migration guide for config loaders](./config_loader_migration.md) for instructions on how to update your code base to use `OmegaConfigLoader`.\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#omegaconfigloader","title":"OmegaConfigLoader","text":"<p>OmegaConf is a Python library designed to handle and manage settings. It serves as a YAML-based hierarchical system to organise configurations, which can be structured to accommodate various sources, allowing you to merge settings from multiple locations.</p> <p>From Kedro 0.18.5 you can use the {py:class}<code>~kedro.config.OmegaConfigLoader</code> which uses <code>OmegaConf</code> to load data.</p> <p><code>OmegaConfigLoader</code> can load <code>YAML</code> and <code>JSON</code> files. Acceptable file extensions are <code>.yml</code>, <code>.yaml</code>, and <code>.json</code>. By default, any configuration files used by the config loaders in Kedro are <code>.yml</code> files.</p>"},{"location":"pages/configuration/configuration_basics/#omegaconf-vs-kedros-omegaconfigloader","title":"<code>OmegaConf</code> vs. Kedro's <code>OmegaConfigLoader</code>","text":"<p><code>OmegaConf</code> is a configuration management library in Python that allows you to manage hierarchical configurations. Kedro's <code>OmegaConfigLoader</code> uses <code>OmegaConf</code> for handling configurations. This means that when you work with <code>OmegaConfigLoader</code> in Kedro, you are using the capabilities of <code>OmegaConf</code> without directly interacting with it.</p> <p><code>OmegaConfigLoader</code> in Kedro is designed to handle more complex configuration setups commonly used in Kedro projects. It automates the process of merging configuration files, such as those for catalogs, and accounts for different environments to make it convenient to manage configurations in a structured way.</p> <p>When you need to load configurations manually, such as for exploration in a notebook, you have two options: 1. Use the <code>OmegaConfigLoader</code> class provided by Kedro. 2. Directly use the <code>OmegaConf</code> library.</p> <p>Kedro's <code>OmegaConfigLoader</code> is designed to handle complex project environments. If your use case involves loading only one configuration file and is straightforward, it may be simpler to use <code>OmegaConf</code> directly.</p> <pre><code>from omegaconf import OmegaConf\n\nparameters = OmegaConf.load(\"/path/to/parameters.yml\")\n</code></pre> <p>When your configuration files are complex and contain credentials or templating, Kedro's <code>OmegaConfigLoader</code> is more suitable, as described in more detail in How to load a data catalog with credentials in code? and How to load a data catalog with templating in code?.</p> <p>In summary, while both <code>OmegaConf</code> and Kedro's <code>OmegaConfigLoader</code> provide ways to manage configurations, your choice depends on the complexity of your configuration and whether you are working within the context of the Kedro framework.</p>"},{"location":"pages/configuration/configuration_basics/#configuration-source","title":"Configuration source","text":"<p>The configuration source folder is <code>conf</code> by default. We recommend that you keep all configuration files in the default <code>conf</code> folder of a Kedro project.</p>"},{"location":"pages/configuration/configuration_basics/#configuration-environments","title":"Configuration environments","text":"<p>A configuration environment is a way of organising your configuration settings for different stages of your data pipeline. For example, you might have different settings for development, testing, and production environments.</p> <p>By default, Kedro projects have a <code>base</code> and a <code>local</code> environment.</p>"},{"location":"pages/configuration/configuration_basics/#base","title":"Base","text":"<p>In Kedro, the base configuration environment refers to the default configuration settings that are used as the foundation for all other configuration environments.</p> <p>The <code>base</code> folder contains the default settings that are used across your pipelines, unless they are overridden by a specific environment.</p> <pre><code>Do not put private access credentials in the base configuration folder or any other configuration environment folder that is stored in version control.\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#local","title":"Local","text":"<p>The <code>local</code> configuration environment folder should be used for configuration that is either user-specific (e.g. IDE configuration) or protected (e.g. security keys).</p> <pre><code>Do not add any local configuration to version control.\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#configuration-loading","title":"Configuration loading","text":"<p>Kedro-specific configuration (e.g., <code>DataCatalog</code> configuration for I/O) is loaded using a configuration loader class, by default, this is {py:class}<code>~kedro.config.OmegaConfigLoader</code>. When you interact with Kedro through the command line, e.g. by running <code>kedro run</code>, Kedro loads all project configuration in the configuration source through this configuration loader.</p> <p>The loader recursively scans for configuration files inside the <code>conf</code> folder, firstly in <code>conf/base</code> (<code>base</code> being the default environment) and then in <code>conf/local</code> (<code>local</code> being the designated overriding environment).</p> <p>Kedro merges configuration information and returns a configuration dictionary according to the following rules:</p> <ul> <li>If any two configuration files (exception for parameters) located inside the same environment path (such as <code>conf/base/</code>) contain the same top-level key, the configuration loader raises a <code>ValueError</code> indicating that duplicates are not allowed.</li> <li>If two configuration files contain the same top-level key but are in different environment paths (for example, one in <code>conf/base/</code>, another in <code>conf/local/</code>) then the last loaded path (<code>conf/local/</code>) takes precedence as the key value. <code>OmegaConfigLoader.__getitem__</code> does not raise any errors but a <code>DEBUG</code> level log message is emitted with information on the overridden keys.</li> <li>If any two parameter configuration files contain the same top-level key, the configuration loader checks the sub-keys for duplicates. If there are any, it raises a <code>ValueError</code> indicating that duplicates are not allowed.</li> </ul> <p>When using any of the configuration loaders, any top-level keys that start with <code>_</code> are considered hidden (or reserved) and are ignored. Those keys will neither trigger a key duplication error nor appear in the resulting configuration dictionary. However, you can still use such keys, for example, as YAML anchors and aliases or to enable templating in the catalog when using the <code>OmegaConfigLoader</code>.</p>"},{"location":"pages/configuration/configuration_basics/#configuration-file-names","title":"Configuration file names","text":"<p>Configuration files will be matched according to file name and type rules. Suppose the config loader needs to fetch the catalog configuration, it will search according to the following rules:</p> <ul> <li>Either of the following is true:</li> <li>filename starts with <code>catalog</code></li> <li>file is located in a subfolder whose name is prefixed with <code>catalog</code></li> <li>And file extension is one of the following: <code>yaml</code>, <code>yml</code>, or <code>json</code></li> </ul>"},{"location":"pages/configuration/configuration_basics/#configuration-patterns","title":"Configuration patterns","text":"<p>Under the hood, the Kedro configuration loader loads files based on regex patterns that specify the naming convention for configuration files. These patterns are specified by <code>config_patterns</code> in the configuration loader classes.</p> <p>By default, those patterns are set as follows for the configuration of catalog, parameters, logging, credentials:</p> <pre><code>config_patterns = {\n    \"catalog\": [\"catalog*\", \"catalog*/**\", \"**/catalog*\"],\n    \"parameters\": [\"parameters*\", \"parameters*/**\", \"**/parameters*\"],\n    \"credentials\": [\"credentials*\", \"credentials*/**\", \"**/credentials*\"],\n    \"logging\": [\"logging*\", \"logging*/**\", \"**/logging*\"],\n}\n</code></pre> <p>If you want to change the way configuration is loaded, you can either customise the config patterns or bypass the configuration loading as described in the advanced configuration chapter.</p>"},{"location":"pages/configuration/configuration_basics/#how-to-use-kedro-configuration","title":"How to use Kedro configuration","text":"<p>This section contains a set of guidance for the most common configuration requirements of standard Kedro projects:</p> <ul> <li>How to change the setting for a configuration source folder</li> <li>How to change the configuration source folder at runtime</li> <li>How to read configuration from a compressed file</li> <li>How to read configuration from remote storage</li> <li>How to access configuration in code</li> <li>How to load a data catalog with credentials in code?</li> <li>How to specify additional configuration environments</li> <li>How to change the default overriding environment</li> <li>How to use only one configuration environment</li> </ul>"},{"location":"pages/configuration/configuration_basics/#how-to-change-the-setting-for-a-configuration-source-folder","title":"How to change the setting for a configuration source folder","text":"<p>To store the Kedro project configuration in a different folder to <code>conf</code>, change the configuration source by setting the <code>CONF_SOURCE</code> variable in <code>src/&lt;package_name&gt;/settings.py</code> as follows:</p> <pre><code>CONF_SOURCE = \"new_conf\"\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#how-to-change-the-configuration-source-folder-at-runtime","title":"How to change the configuration source folder at runtime","text":"<p>Specify a source folder for the configuration files at runtime using the <code>kedro run</code> CLI command with the <code>--conf-source</code> flag as follows:</p> <pre><code>kedro run --conf-source=&lt;path-to-new-conf-folder&gt;\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#how-to-read-configuration-from-a-compressed-file","title":"How to read configuration from a compressed file","text":"<p>You can read configuration from a compressed file in <code>tar.gz</code> or <code>zip</code> format by using the {py:class}<code>~kedro.config.OmegaConfigLoader</code>.</p> <p>How to reference a <code>tar.gz</code> file:</p> <p>```bash kedro run --conf-source=.tar.gz <pre><code>\nHow to reference a `zip` file:\n\n```bash\nkedro run --conf-source=&lt;path-to-compressed-file&gt;.zip\n</code></pre> <p>To compress your configuration you can use Kedro's <code>kedro package</code> command which builds the package into the <code>dist/</code> folder of your project, and creates a <code>.whl</code> file, as well as a <code>tar.gz</code> file containing the project configuration. The compressed version of the config files excludes any files inside your <code>local</code> folder.</p> <p>Alternatively you can run the command below to create a <code>tar.gz</code> file:</p> <pre><code>tar --exclude=local/*.yml -czf &lt;my_conf_name&gt;.tar.gz --directory=&lt;path-to-conf-dir&gt; &lt;conf-dir&gt;\n</code></pre> <p>Or the following command to create a <code>zip</code> file:</p> <pre><code>zip -x &lt;conf-dir&gt;/local/** -r &lt;my_conf_name&gt;.zip &lt;conf-dir&gt;\n</code></pre> <p>For both the <code>tar.gz</code> and <code>zip</code> file, the following structure is expected:</p> <pre><code>&lt;conf_dir&gt;\n\u251c\u2500\u2500 base               &lt;-- the files inside may be different, but this is an example of a standard Kedro structure.\n\u2502   \u2514\u2500\u2500 parameters.yml\n\u2502   \u2514\u2500\u2500 catalog.yml\n\u2514\u2500\u2500 local              &lt;-- the top level local folder is required, but no files should be inside when distributed.\n\u2514\u2500\u2500 README.md          &lt;-- optional but included with the default Kedro conf structure.\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#how-to-read-configuration-from-remote-storage","title":"How to read configuration from remote storage","text":"<p>You can read configuration from remote storage locations using cloud storage protocols. This allows you to separate your configuration from your code and securely store different configurations for various environments or tenants.</p> <p>Supported protocols include: - Amazon S3 (<code>s3://</code>) - Azure Blob Storage (<code>abfs://</code>, <code>abfss://</code>) - Google Cloud Storage (<code>gs://</code>, <code>gcs://</code>) - HTTP/HTTPS (<code>http://</code>, <code>https://</code>) - And other protocols supported by <code>fsspec</code></p> <p>To use a remote configuration source, specify the URL when running your Kedro pipeline:</p> <pre><code># Amazon S3\nkedro run --conf-source=s3://my-bucket/configs/\n\n# Azure Blob Storage\nkedro run --conf-source=abfs://container@account/configs/\n\n# Google Cloud Storage\nkedro run --conf-source=gs://my-bucket/configs/\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#authentication-for-remote-configuration","title":"Authentication for remote configuration","text":"<p>Authentication for remote configuration sources must be set up through environment variables or other credential mechanisms provided by the cloud platform. Unlike datasets in the data catalog, you cannot use credentials from <code>credentials.yml</code> for remote configuration sources since those credentials would be part of the configuration you're trying to access.</p> <p>Examples of authentication setup:</p> <p>Amazon S3:</p> <pre><code>  export AWS_ACCESS_KEY_ID=your_access_key\n  export AWS_SECRET_ACCESS_KEY=your_secret_key\n  kedro run --conf-source=s3://my-bucket/configs/\n</code></pre> <p>Google Cloud Storage:</p> <pre><code>gcloud auth application-default login\nkedro run --conf-source=gs://my-bucket/configs/\n</code></pre> <p>Azure Blob Storage:</p> <pre><code>export AZURE_STORAGE_ACCOUNT=your_account_name\nexport AZURE_STORAGE_KEY=your_account_key\nkedro run --conf-source=abfs://container@account/configs/\n</code></pre> <p>For more detailed authentication instructions, refer to the documentation of your cloud provider.</p> <p>The remote storage should maintain the same configuration structure as local configuration, with appropriate <code>base</code> and environment folders:</p> <pre><code>s3://my-bucket/configs/\n\u251c\u2500\u2500 base/\n\u2502   \u251c\u2500\u2500 catalog.yml\n\u2502   \u2514\u2500\u2500 parameters.yml\n\u2514\u2500\u2500 prod/\n    \u2514\u2500\u2500 parameters.yml\n</code></pre> <pre><code>While Kedro supports reading configuration from compressed files (.tar.gz, .zip) and from cloud storage separately, it does not currently support reading compressed files directly from cloud storage (e.g., s3://my-bucket/configs.tar.gz).\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#how-to-access-configuration-in-code","title":"How to access configuration in code","text":"<p>To directly access configuration in code, for example to debug, you can do so as follows:</p> <pre><code>from kedro.config import OmegaConfigLoader\nfrom kedro.framework.project import settings\n\n# Instantiate an `OmegaConfigLoader` instance with the location of your project configuration.\nconf_path = str(project_path / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\n\n# This line shows how to access the catalog configuration. You can access other configuration in the same way.\nconf_catalog = conf_loader[\"catalog\"]\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#how-to-load-a-data-catalog-with-credentials-in-code","title":"How to load a data catalog with credentials in code?","text":"<pre><code>We do not recommend that you load and manipulate a data catalog directly in a Kedro node. Nodes are designed to be pure functions and thus should remain agnostic of I/O.\n</code></pre> <p>Assuming your project contains a catalog and credentials file, each located in <code>base</code> and <code>local</code> environments respectively, you can use the <code>OmegaConfigLoader</code> to load these configurations, and pass them to a <code>DataCatalog</code> object to access the catalog entries with resolved credentials.</p> <pre><code>from kedro.config import OmegaConfigLoader\nfrom kedro.framework.project import settings\nfrom kedro.io import DataCatalog\n\n# Instantiate an `OmegaConfigLoader` instance with the location of your project configuration.\nconf_path = str(project_path / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(\n    conf_source=conf_path, base_env=\"base\", default_run_env=\"local\"\n)\n\n# These lines show how to access the catalog and credentials configurations.\nconf_catalog = conf_loader[\"catalog\"]\nconf_credentials = conf_loader[\"credentials\"]\n\n# Fetch the catalog with resolved credentials from the configuration.\ncatalog = DataCatalog.from_config(catalog=conf_catalog, credentials=conf_credentials)\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#how-to-specify-additional-configuration-environments","title":"How to specify additional configuration environments","text":"<p>In addition to the two built-in <code>local</code> and <code>base</code> configuration environments, you can create your own. Your project loads <code>conf/base/</code> as the bottom-level configuration environment but allows you to overwrite it with any other environments that you create, such as <code>conf/server/</code> or <code>conf/test/</code>. To use additional configuration environments, run the following command:</p> <pre><code>kedro run --env=&lt;your-environment&gt;\n</code></pre> <p>If no <code>env</code> option is specified, this will default to using the <code>local</code> environment to overwrite <code>conf/base</code>.</p> <p>If you set the <code>KEDRO_ENV</code> environment variable to the name of your environment, Kedro will load that environment for your <code>kedro run</code>, <code>kedro ipython</code>, <code>kedro jupyter notebook</code> and <code>kedro jupyter lab</code> sessions:</p> <pre><code>export KEDRO_ENV=&lt;your-environment&gt;\n</code></pre> <pre><code>If you both specify the `KEDRO_ENV` environment variable and provide the `--env` argument to a CLI command, the CLI argument takes precedence.\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#how-to-change-the-default-overriding-environment","title":"How to change the default overriding environment","text":"<p>By default, <code>local</code> is the overriding environment for <code>base</code>. To change the folder, customise the configuration loader argument settings in <code>src/&lt;package_name&gt;/settings.py</code> and set the <code>CONFIG_LOADER_ARGS</code> key to have a new <code>default_run_env</code> value.</p> <p>For example, if you want to override <code>base</code> with configuration in a custom environment called <code>prod</code>, you change the configuration loader arguments in <code>settings.py</code> as follows:</p> <pre><code>CONFIG_LOADER_ARGS = {\"default_run_env\": \"prod\"}\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#how-to-use-only-one-configuration-environment","title":"How to use only one configuration environment","text":"<p>Customise the configuration loader arguments in <code>settings.py</code> as follows if your project does not have any other environments apart from <code>base</code> (i.e. no <code>local</code> environment to default to):</p> <pre><code>CONFIG_LOADER_ARGS = {\"default_run_env\": \"base\"}\n</code></pre>"},{"location":"pages/configuration/configuration_basics/#how-to-use-kedro-without-the-rich-library","title":"How to use Kedro without the rich library","text":"<p>If you prefer not to have the <code>rich</code> library in your Kedro project, you have the option to uninstall it. However, it's important to note that versions of the <code>cookiecutter</code> library above 2.3 have a dependency on rich. You will need to downgrade <code>cookiecutter</code> to a version below 2.3 to have Kedro work without <code>rich</code>.</p> <p>To uninstall the rich library, run:</p> <pre><code>pip uninstall rich\n</code></pre> <p>To downgrade cookiecutter to a version that does not require rich, you can specify a version below 2.3. For example:</p> <pre><code>pip install cookiecutter==2.2.0\n</code></pre> <p>These changes will affect the visual appearance and formatting of Kedro's logging, prompts, and the output of the <code>kedro ipython</code> command. While using a version of <code>cookiecutter</code> below 2.3, the appearance of the prompts will be plain even with <code>rich</code> installed.</p>"},{"location":"pages/configuration/credentials/","title":"Credentials","text":"<p>For security reasons, we strongly recommend that you do not commit any credentials or other secrets to version control. Kedro is set up so that, by default, if a file inside the <code>conf</code> folder (and its subfolders) contains <code>credentials</code> in its name, it will be ignored by git.</p> <p>Credentials configuration can be used on its own directly in code or fed into the <code>DataCatalog</code>. If you would rather store your credentials in environment variables instead of a file, you can use the <code>OmegaConfigLoader</code> to load credentials from environment variables as described in the advanced configuration chapter.</p>"},{"location":"pages/configuration/credentials/#how-to-load-credentials-in-code","title":"How to load credentials in code","text":"<p>Credentials configuration can be loaded the same way as any other project configuration using the configuration loader class <code>OmegaConfigLoader</code>.</p> <pre><code>from pathlib import Path\n\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.framework.project import settings\n\n# Substitute &lt;project_root&gt; with the [root folder for your project](https://docs.kedro.org/en/stable/tutorial/spaceflights_tutorial.html#terminology)\nconf_path = str(Path(&lt;project_root&gt;) / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\ncredentials = conf_loader[\"credentials\"]\n</code></pre> <p>This loads configuration files from <code>conf/base</code> and <code>conf/local</code> whose filenames start with <code>credentials</code>, or that are located inside a folder with a name that starts with <code>credentials</code>.</p> <p>Calling <code>conf_loader[key]</code> in the example above throws a <code>MissingConfigException</code> error if no configuration files match the given key. But if this is a valid workflow for your application, you can handle it as follows:</p> <pre><code>from pathlib import Path\n\nfrom kedro.config import OmegaConfigLoader, MissingConfigException\nfrom kedro.framework.project import settings\n\nconf_path = str(Path(&lt;project_root&gt;) / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\n\ntry:\n    credentials = conf_loader[\"credentials\"]\nexcept MissingConfigException:\n    credentials = {}\n</code></pre> <pre><code>The `kedro.framework.context.KedroContext` class uses the approach above to load project credentials.\n</code></pre>"},{"location":"pages/configuration/credentials/#how-to-work-with-aws-credentials","title":"How to work with AWS credentials","text":"<p>When you work with AWS credentials on datasets, you are not required to store AWS credentials in the project configuration files. Instead, you can specify them using environment variables <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and, optionally, <code>AWS_SESSION_TOKEN</code>. See the official AWS documentation for more details.</p>"},{"location":"pages/configuration/parameters/","title":"Parameters","text":"<p>Project parameters in Kedro are defined inside the <code>conf</code> folder in a file that has a filename starting with <code>parameters</code>, or are located inside a folder with name starting with <code>parameters</code>. By default, in a new Kedro project, parameters are defined in the <code>parameters.yml</code> file, which is located in the project's <code>conf/base</code> directory. This file contains a dictionary of key-value pairs, where each key is a parameter name and each value is the corresponding parameter value. These parameters can serve as input to nodes and are used when running the pipeline. By using parameters, you can make your Kedro pipelines more flexible and easier to configure, since you can change the behaviour of your nodes by modifying the <code>parameters.yml</code> file.</p>"},{"location":"pages/configuration/parameters/#how-to-use-parameters","title":"How to use parameters","text":"<p>If you have a group of parameters that determine the hyperparameters of your model, you can define them in a single location such as <code>conf/base/parameters.yml</code>. This way, you can keep all your modifications in a centralised location and avoid making changes across multiple parts of your code.</p> <pre><code>step_size: 1\nlearning_rate: 0.01\n</code></pre> <p>You can now use the <code>params:</code> prefix to reference these parameters in the <code>node</code> definition:</p> <pre><code>def increase_volume(volume, step):\n    return volume + step\n\n\n# in pipeline definition\nnode(\n    func=increase_volume,\n    inputs=[\"input_volume\", \"params:step_size\"],\n    outputs=\"output_volume\",\n)\n</code></pre> <p>You can also group your parameters into nested structures and, using the same method above, load them by top-level key:</p> <pre><code>step_size: 1\nmodel_params:\n    learning_rate: 0.01\n    test_data_ratio: 0.2\n    number_of_train_iterations: 10000\n</code></pre> <pre><code>def train_model(data, model):\n    lr = model[\"learning_rate\"]\n    test_data_ratio = model[\"test_data_ratio\"]\n    iterations = model[\"number_of_train_iterations\"]\n    ...\n\n\n# in pipeline definition\nnode(\n    func=train_model,\n    inputs=[\"input_data\", \"params:model_params\"],\n    outputs=\"output_data\",\n)\n</code></pre> <p>Alternatively, you can also pass <code>parameters</code> to the node inputs and get access to the entire collection of values inside the node function.</p> <pre><code>def increase_volume(volume, params):\n    step = params[\"step_size\"]\n    return volume + step\n\n\n# in pipeline definition\nnode(\n    func=increase_volume, inputs=[\"input_volume\", \"parameters\"], outputs=\"output_volume\"\n)\n</code></pre> <p>In both cases, under the hood parameters are added to the Data Catalog through the method {py:meth}<code>add_feed_dict() &lt;kedro.io.DataCatalog.add_feed_dict&gt;</code> in {py:class}<code>~kedro.io.DataCatalog</code>, where they live as <code>MemoryDataset</code>s. This method is also what the {py:class}<code>~kedro.framework.context.KedroContext</code> class uses when instantiating the catalog.</p> <pre><code>You can use `add_feed_dict()` to inject any other entries into your `DataCatalog` as per your use case.\n</code></pre>"},{"location":"pages/configuration/parameters/#how-to-load-parameters-in-code","title":"How to load parameters in code","text":"<p>Parameters project configuration can be loaded by the configuration loader class, which is <code>OmegaConfigLoader</code> by default.</p> <pre><code>from kedro.config import OmegaConfigLoader\nfrom kedro.framework.project import settings\n\nconf_path = str(project_path / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\nparameters = conf_loader[\"parameters\"]\n</code></pre> <p>This loads configuration files from any subdirectories in <code>conf</code> that have a filename starting with <code>parameters</code>, or are located inside a folder with name starting with <code>parameters</code>.</p> <p>Calling <code>conf_loader[key]</code> in the example above will throw a <code>MissingConfigException</code> error if no configuration files match the given key. But if this is a valid workflow for your application, you can handle it as follows:</p> <pre><code>from kedro.config import OmegaConfigLoader, MissingConfigException\nfrom kedro.framework.project import settings\n\nconf_path = str(project_path / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\n\ntry:\n    parameters = conf_loader[\"parameters\"]\nexcept MissingConfigException:\n    parameters = {}\n</code></pre> <pre><code>The `kedro.framework.context.KedroContext` class uses the approach above to load project parameters.\n</code></pre> <p>Parameters can then be used on their own or fed in as function inputs.</p>"},{"location":"pages/configuration/parameters/#how-to-specify-parameters-at-runtime","title":"How to specify parameters at runtime","text":"<p>Kedro also allows you to specify runtime parameters for the <code>kedro run</code> CLI command. Use the <code>--params</code> command line option and specify a comma-separated list of key-value pairs that will be added to {py:class}<code>~kedro.framework.context.KedroContext</code> parameters and made available to pipeline nodes.</p> <p>Each key-value pair is split on the first equals sign. The following example is a valid command:</p> <pre><code>kedro run --params=param_key1=value1,param_key2=2.0\n</code></pre> <p>Values provided in the CLI take precedence and overwrite parameters specified in configuration files. By default, runtime parameters get merged destructively, meaning that any configuration for that key besides that given in the runtime parameters is discarded. This section describes how to change the merging strategy.</p> <p>For example, if you have the following parameters in your <code>base</code> and <code>local</code> environments:</p> <pre><code># base/parameters.yml\nmodel_options:\n  model_params:\n    learning_date: \"2023-11-01\"\n    training_date: \"2023-11-01\"\n    data_ratio: 14\n\ndata_options:\n  step_size: 123123\n</code></pre> <pre><code># local/parameters.yml\nfeatures:\n    rate: 123\n</code></pre> <p>And you provide the following parameter at runtime:</p> <pre><code>kedro run --params=\"model_options.model_params.training_date=2011-11-11\"\n</code></pre> <p>The final merged result will be:</p> <pre><code>model_options:\n  model_params:\n    training_date: \"2011-11-11\"\n\ndata_options:\n  step_size: 123123\n\nfeatures:\n    rate: 123\n</code></pre> <ul> <li>Parameter keys are always treated as strings.</li> <li>Parameter values are converted to a float or an integer number if the corresponding conversion succeeds; otherwise, they are also treated as string.</li> </ul> <p>If any extra parameter key or value contains spaces, wrap the whole option contents in quotes:</p> <pre><code>kedro run --params=\"key1=value with spaces,key2=value\"\n</code></pre> <p>Since key-value pairs are split on the first equals sign, values can contain equals signs, but keys cannot.</p> <p>To override not just parameters but other configurations, such as catalog entries or file paths, or to specify upfront that certain parameters must be set at runtime, use <code>$runtime_params</code> with the <code>OmegaConfigLoader</code>. Introduced in Kedro <code>0.18.14</code>, this feature allows dynamic overrides of various configuration types using the <code>--params</code> CLI option.  It\u2019s particularly useful for scenarios like switching data sources or fine-tuning runtime settings. Learn more about <code>$runtime_params</code>.</p>"},{"location":"pages/configuration/telemetry/","title":"Anonymous Telemetry","text":"<p>To help the Kedro Project maintainers improve the software, Kedro can capture anonymised telemetry. This data is collected with the sole purpose of improving Kedro by understanding feature usage. Importantly, we do not store personal information about you or sensitive data from your project, and this process is never utilized for marketing or promotional purposes. Participation in this program is optional, and it is enabled by default. Kedro will continue working as normal if you opt-out.</p> <p>The Kedro Project's telemetry has been reviewed and approved under the Telemetry Data Collection and Usage Policy of LF Projects, LLC.</p> <p>Kedro collects anonymous telemetry through the Kedro-Telemetry plugin, which is installed as one of Kedro\u2019s dependencies.</p>"},{"location":"pages/configuration/telemetry/#collected-data-fields","title":"Collected data fields:","text":"<ul> <li>Unique user identifier(UUID): The UUID is a randomly generated anonymous identifier, stored within an OS-specific configuration folder for Kedro, named <code>telemetry.toml</code>. If a UUID does not already exist, the telemetry plugin generates a new one, stores it, and then uses this UUID in subsequent telemetry events.</li> <li>CLI Command (Masked Arguments): The command used, with sensitive arguments masked for privacy. Example Input: <code>kedro run --pipeline=ds --env=test</code> What we receive: <code>kedro run --pipeline ***** --env *****</code></li> <li>Project UUID: The hash of project UUID (randomly generated anonymous project identifier) and the package name. If project UUID does not already exist, the telemetry plugin generates a new one, stores it in <code>pyproject.toml</code>, and then joins this project UUID with the package name, hashes the joined result and uses it in subsequent telemetry events.</li> <li>Kedro Project Version: The version of Kedro being used.</li> <li>Kedro-Telemetry Version: The version of the Kedro-Telemetry plugin.</li> <li>Python Version: The version of Python in use.</li> <li>Operating System: The operating system on which Kedro is running.</li> <li>Tools Selected and Example Pipeline: The tools chosen and example pipeline inclusion during the <code>kedro new</code> command execution, if applicable.</li> <li>Number of Datasets, Nodes, and Pipelines: Quantitative data about the project structure.</li> </ul> <p>For technical information on how the telemetry collection works, you can browse the source code of <code>kedro-telemetry</code>.</p>"},{"location":"pages/configuration/telemetry/#how-do-i-withdraw-consent","title":"How do I withdraw consent?","text":"<p>To withdraw consent, you have a few options:</p> <ol> <li> <p>Set Environment Variables:    Set the environment variables <code>DO_NOT_TRACK</code> or <code>KEDRO_DISABLE_TELEMETRY</code> to any value. The presence of any of these environment variables will disable telemetry for all Kedro projects in that environment and will override any consent specified in the <code>.telemetry</code> file of the specific project.</p> </li> <li> <p>CLI Option When Creating a New Project:    When creating a new project, you can use the command:</p> </li> </ol> <p><code>console    kedro new --telemetry=no</code>    This will create a new project with a <code>.telemetry</code> file in its root folder, containing <code>consent: false</code>. This file will be used when executing Kedro commands within that project folder. Note that telemetry data about the execution of the <code>kedro new</code> command will still be sent if telemetry has not been disabled using environment variables.</p> <pre><code>The `.telemetry` file should not be committed to `git` or packaged in deployment. In `kedro&gt;=0.17.4` the file is git-ignored.\n</code></pre> <ol> <li> <p>Modify or Create the <code>.telemetry</code> file manually:    If the <code>.telemetry</code> file exists in the root folder of your Kedro project, set the <code>consent</code> variable to <code>false</code>. If the file does not exist, create it with the following content:      <code>yaml      consent: false</code></p> </li> <li> <p>Uninstall the plugin:    Remove the <code>kedro-telemetry</code> plugin:</p> </li> </ol> <p><code>console    pip uninstall kedro-telemetry</code></p> <pre><code>This is a last resort option, as it will break the dependencies of Kedro (for example, `pip check` will report issues).\n</code></pre>"},{"location":"pages/contribution/","title":"Contribute to Kedro","text":"<p>We welcome any and all contributions to Kedro, at whatever level you can manage. For example, you could:</p> <ul> <li>Join the community on Slack to answer questions</li> <li>Review Kedro's GitHub issues or raise your own issue to report a bug or feature request</li> <li>Start a conversation about the Kedro project on GitHub discussions</li> <li>Make a pull request on the <code>awesome-kedro</code> GitHub repo to update the curated list of Kedro community content</li> <li>Report a bug or propose a new feature on GitHub issues</li> <li>View the Kedro security policy to report a security vulnerability.</li> <li>Review other contributors' PRs</li> <li>Contribute code, for example to fix a bug or add a feature</li> <li>Contribute to the documentation</li> <li>Write a blog post for blog.kedro.org</li> </ul> <p>There is further information about contributing to Kedro, including our developer guidelines, on the Kedro wiki.</p> <pre><code>:hidden:\n\ntechnical_steering_committee\n</code></pre>"},{"location":"pages/contribution/technical_steering_committee/","title":"Kedro's Technical Steering Committee","text":"<p>Kedro is an incubating project within LF AI &amp; Data.</p> <p>The term \"Technical Steering Committee\" (TSC) describes the group of Kedro maintainers. We list Kedro's current and past maintainers on this page.</p> <p>The TSC is responsible for the project's future development; you can read about our duties in our Technical Charter. We accept new members into the TSC to fuel Kedro's continued development.</p> <p>On this page we describe:</p> <ul> <li>Responsibilities of a maintainer</li> <li>Requirements to become a maintainer</li> <li>Current maintainers</li> <li>Past maintainers</li> <li>Application process</li> <li>Voting process</li> </ul>"},{"location":"pages/contribution/technical_steering_committee/#responsibilities-of-a-maintainer","title":"Responsibilities of a maintainer","text":""},{"location":"pages/contribution/technical_steering_committee/#product-development","title":"Product development","text":"<ul> <li>Be available for at least one full day per week to help with product development</li> <li>Attend community meetings to discuss the project plans and roadmap</li> <li>Be proactive about project maintenance including security, updates, CI/CD, builds and infrastructure</li> <li>Give priority to the work following the product roadmap to move the project forward</li> </ul>"},{"location":"pages/contribution/technical_steering_committee/#community-management","title":"Community management","text":"<ul> <li>Ensure that ongoing pull requests are moving forward at the right pace or closing them</li> <li> <p>Guide the community to use our various communication channels:</p> </li> <li> <p>GitHub issues for feature requests and bug reports</p> </li> <li>GitHub discussions to discuss the future of the Kedro project</li> <li>Slack for questions and to support other users</li> </ul>"},{"location":"pages/contribution/technical_steering_committee/#requirements-to-become-a-maintainer","title":"Requirements to become a maintainer","text":"<p>Just contributing does not make you a maintainer; you need to demonstrate commitment to Kedro's long-term success by working with existing maintainers for a period of time.</p> <p>We look for commitment markers who can do the following:</p> <ul> <li>Write high-quality code and collaborate with the team and community</li> <li>Understand the project's code base and internals</li> <li>Make pull requests from our backlog or roadmap; maintainers need to work towards a common goal</li> <li>Learn how the team works, including processes for testing, quality standards and code review</li> <li>Show evidence of already having started pull requests and code reviews under the guidance of maintainers; including asking   for help where needed</li> <li>Show excitement about the future of Kedro</li> <li>Build a collaborative relationship with the existing team</li> </ul>"},{"location":"pages/contribution/technical_steering_committee/#current-maintainers","title":"Current maintainers","text":"Name Organisation Ankita Katiyar QuantumBlack, AI by McKinsey Deepyaman Datta Dagster Labs Dmitry Sorokin QuantumBlack, AI by McKinsey Huong Nguyen QuantumBlack, AI by McKinsey Ivan Danov QuantumBlack, AI by McKinsey Jitendra Gundaniya QuantumBlack, AI by McKinsey Joel Schwarzmann Aneira Health Juan Luis Cano QuantumBlack, AI by McKinsey Laura Couto QuantumBlack, AI by McKinsey Marcin Zab\u0142ocki Printify, Inc. Merel Theisen QuantumBlack, AI by McKinsey Nok Lam Chan QuantumBlack, AI by McKinsey Rashida Kanchwala QuantumBlack, AI by McKinsey Ravi Kumar Pilla QuantumBlack, AI by McKinsey Sajid Alam QuantumBlack, AI by McKinsey Simon Brugman ING Stephanie Kaiser QuantumBlack, AI by McKinsey Tynan DeBold QuantumBlack, AI by McKinsey Yetunde Dada QuantumBlack, AI by McKinsey Yolan Honor\u00e9-Roug\u00e9 Soci\u00e9t\u00e9 G\u00e9n\u00e9rale Assurances"},{"location":"pages/contribution/technical_steering_committee/#past-maintainers","title":"Past maintainers","text":"<p>Kedro was originally designed by Aris Valtazanos and Nikolaos Tsaousis at QuantumBlack to solve challenges they faced in their project work. Their work was later turned into an internal product by Peteris Erins, Ivan Danov, Nikolaos Kaltsas, Meisam Emamjome and Nikolaos Tsaousis.</p> <p>Former core team members with significant contributions include Ahdra Merali, Amanda Koh, Andrew Mackay, Andrii Ivaniuk, Anton Kirilenko, Antony Milne, Cvetanka Nechevska, Dmitrii Deriabin, Gabriel Comym, Gordon Wrigley, Hamza Oza, Ignacio Paricio, Jannic Holzer, Jo Stichbury, Jiri Klein, Kiyohito Kunii, La\u00eds Carvalho, Liam Brummitt, Lim Hoang, Lorena B\u0103lan, Mehdi Naderi Varandi, Nasef Khan, Nero Okwa, Richard Westenra, Susanna Wong, Vladimir Nikolic and Zain Patel.</p>"},{"location":"pages/contribution/technical_steering_committee/#application-process","title":"Application process","text":"<p>Every quarter year, existing maintainers will collect a list of contributors that have shown regular activity on the project over the prior months and want to become maintainers. From this list, maintainer candidates are selected and proposed for a vote.</p> <p>Following a successful vote, candidates are added to the <code>kedro-developers</code> team on the Kedro GitHub organisation and the <code>kedro-team</code> channel on the Kedro Slack organisation, and listed as Kedro maintainers.</p>"},{"location":"pages/contribution/technical_steering_committee/#voting-process","title":"Voting process","text":"<p>Voting can change project maintainers and decide on the future of Kedro. The TSC leads the process as voting maintainers of Kedro. The voting period is one week and via a GitHub discussion or through a pull request.</p>"},{"location":"pages/contribution/technical_steering_committee/#other-issues-or-proposals","title":"Other issues or proposals","text":"<p>Kedro's GitHub discussions section is used to host votes on issues, proposals and changes affecting the future of Kedro, including amendments to our ways of working described on this page. These votes require a 1/2 majority.</p>"},{"location":"pages/contribution/technical_steering_committee/#adding-or-removing-maintainers","title":"Adding or removing maintainers","text":"<p>The decision to add or remove a maintainer is made based on TSC members votes in that pull request. Additions and removals of maintainers require a 2/3 majority.</p> <p>The act of adding or removing maintainers onto the list requires a pull request against the \"Current maintainers\" section of this page.</p>"},{"location":"pages/course/","title":"Learn Kedro with hands-on video","text":"<p>If you like to learn from video, you can follow our hands-on course \"Introduction to Kedro: Building Maintainable Data Pipelines\" on YouTube.</p> <p>The course is structured into sections and these are each broken into short videos that cover specific Kedro topics. You'll walk through the spaceflights tutorial and get hands-on with the example. Along the way, you'll learn key Kedro concepts like datasets and the Kedro Data Catalog, nodes and pipelines, and configuration management.</p>"},{"location":"pages/course/#who-is-this-course-for","title":"Who is this course for?","text":"<p>This course is for data scientists, data engineers and machine learning engineers. You can be junior, mid-level or senior in your field of work. You're likely to be hands-on with projects, or a decision-maker who regularly makes design and implementation choices about Python data products.</p> <p>We assume you know these concepts:</p> <ul> <li>Python basics (coding on Jupyter and other notebook interfaces)</li> <li>Manipulating data with pandas</li> <li>Visualising insights</li> <li>Command line basics</li> </ul> <p>We don't assume knowledge of software engineering in Python, so the course contains information about reusability principles, how to create a Python package, and how to use version control.</p> <p>Please note that we do expect users to have Git installed, as it is a prerequisite for the <code>kedro new</code> flow, which is used when creating a new project.</p>"},{"location":"pages/course/#what-youll-learn","title":"What you'll learn","text":"<p>In short, you'll learn answers to the following:</p> <ul> <li>Introduction to Kedro</li> <li>What is Kedro? How does it help you create maintainable, reusable data science code?</li> <li>How does Kedro fit into the data science ecosystem?</li> <li>What do you need to do to create a Kedro project?</li> <li>How can you refactor a Jupyter notebook to a Kedro project?</li> <li>How do you package Python code as a library?</li> <li>How do you work with Kedro projects in VS Code?</li> <li>What are namespaces and dataset factories?</li> <li>What is needed to deploy a Kedro project using container solutions like Docker and open source orchestrators like Airflow?</li> <li>What are Kedro plugins?</li> <li>How can you contribute to Kedro?</li> </ul> <p>You don't need to register for the course and you can skip around the sections to find help on a particular area as you pick up the skills needed to build your own Kedro projects.</p>"},{"location":"pages/course/#index-of-videos","title":"Index of videos","text":"<p>Introduction to Kedro: Building Maintainable Data Pipelines is split into the following videos:</p>"},{"location":"pages/course/#part-0-introduction","title":"Part 0: Introduction","text":"<ol> <li>Data science in production: the good, the bad and the ugly</li> <li>What is Kedro?</li> <li>Kedro and data orchestrators</li> <li>How does Kedro fit into the data science ecosystem?</li> </ol>"},{"location":"pages/course/#part-1-get-started-with-kedro","title":"Part 1: Get started with Kedro","text":"<ol> <li>Create a Kedro project from scratch?</li> <li>The spaceflights starter</li> <li>Use Kedro from Jupyter notebook</li> <li>Set up the Kedro Data Catalog</li> <li>Explore the spaceflights data</li> <li>Refactor your data processing code into functions</li> <li>Create your first data pipeline with Kedro</li> <li>Assemble your nodes into a Kedro pipeline</li> <li>Run your Kedro pipeline</li> <li>Visualise your data pipeline with Kedro-Viz</li> </ol>"},{"location":"pages/course/#part-2-make-complex-kedro-pipelines","title":"Part 2: Make complex Kedro pipelines","text":"<ol> <li>Merge different dataframes in Kedro</li> <li>Predict prices using machine learning</li> <li>Refactor your data science code into functions</li> <li>How to work with parameters in Kedro</li> <li>Create a Kedro pipeline with parameters</li> <li>Reuse your Kedro pipeline using namespaces</li> <li>Kedro pipeline runners</li> <li>Create Kedro datasets dynamically using factories</li> </ol>"},{"location":"pages/course/#part-3-ship-your-kedro-project-to-production","title":"Part 3: Ship your Kedro project to production","text":"<ol> <li>Define your own Kedro environments</li> <li>Use S3 and MinIO cloud storage with Kedro</li> <li>Package your Kedro project into a Python wheel</li> <li>Turn your Kedro project into a Docker container</li> <li>Deploy your Kedro project to Apache Airflow</li> </ol>"},{"location":"pages/course/#part-4-where-next","title":"Part 4: Where next?","text":"<p>Continue your Kedro journey</p>"},{"location":"pages/data/","title":"Data Catalog","text":"<p>In a Kedro project, the Data Catalog is a registry of all data sources available for use by the project. The catalog is stored in a YAML file (<code>catalog.yml</code>) that maps the names of node inputs and outputs as keys in the <code>DataCatalog</code> class.</p> <p>The {py:mod}<code>kedro-datasets &lt;kedro-datasets:kedro_datasets&gt;</code> package offers built-in datasets for common file types and file systems.</p>"},{"location":"pages/data/#introduction-to-the-data-catalog","title":"Introduction to the Data Catalog","text":"<p>We first introduce the basic sections of <code>catalog.yml</code>, which is the file used to register data sources for a Kedro project.</p> <ul> <li>Introduction to the Data Catalog</li> </ul>"},{"location":"pages/data/#examples-of-data-catalog-yaml","title":"Examples of data catalog YAML","text":"<p>The following page offers a range of examples of YAML specification for various Data Catalog use cases:</p> <ul> <li>Data Catalog YAML Examples</li> </ul> <p>Once you are familiar with the format of <code>catalog.yml</code>, you may find your catalog gets repetitive if you need to load multiple datasets with similar configuration. From Kedro 0.18.12 you can use dataset factories to generalise the configuration and reduce the number of similar catalog entries. This works by matching datasets used in your project\u2019s pipelines to dataset factory patterns and is explained in a new page about Kedro dataset factories:</p> <ul> <li>Kedro Dataset Factories</li> </ul>"},{"location":"pages/data/#advanced-concepts","title":"Advanced concepts","text":"<p>Further pages describe more advanced concepts:</p> <ul> <li>Advanced: Access the Data Catalog in code</li> <li>Advanced: Partitioned and incremental datasets</li> </ul> <p>This section on handing data with Kedro concludes with an advanced use case, illustrated with a tutorial that explains how to create your own custom dataset:</p> <ul> <li>Advanced: Tutorial to create a custom datase</li> </ul>"},{"location":"pages/data/#kedrodatacatalog-experimental-feature","title":"<code>KedroDataCatalog</code> (experimental feature)","text":"<p>As of Kedro 0.19.9, you can explore a new experimental feature \u2014 the <code>KedroDataCatalog</code>, an enhanced alternative to <code>DataCatalog</code>.</p> <p>At present, <code>KedroDataCatalog</code> replicates the functionality of <code>DataCatalog</code> and is fully compatible with the Kedro <code>run</code> command. It introduces several API improvements: * Simplified dataset access: <code>_FrozenDatasets</code> has been replaced with a public <code>get</code> method to retrieve datasets. * Added dict-like interface: You can now use a dictionary-like syntax to retrieve, set, and iterate over datasets.</p> <p>For more details and examples of how to use <code>KedroDataCatalog</code>, see the Kedro Data Catalog page.</p> <ul> <li>KedroDataCatalog (experimental feature)</li> </ul> <p>The documentation for <code>DataCatalog</code> remains relevant as <code>KedroDataCatalog</code> retains its core functionality with some enhancements.</p> <p>Note <code>KedroDataCatalog</code> is under active development and may undergo breaking changes in future releases. While we encourage you to try it out, please be aware of potential modifications as we continue to improve it. Additionally, all upcoming catalog-related features will be introduced through <code>KedroDataCatalog</code> before it replaces <code>DataCatalog</code>.</p> <p>We value your feedback \u2014 let us know if you have any thoughts or suggestions regarding <code>KedroDataCatalog</code> or potential new features via our Slack channel.</p>"},{"location":"pages/data/advanced_data_catalog_usage/","title":"Advanced: Access the Data Catalog in code","text":"<p>You can define a Data Catalog in two ways. Most use cases can be through a YAML configuration file as illustrated previously, but it is possible to access the Data Catalog programmatically through {py:class}<code>~kedro.io.DataCatalog</code> using an API that allows you to configure data sources in code and use the IO module within notebooks.</p> <pre><code>Datasets are not included in the core Kedro package from Kedro version **`0.19.0`**. Import them from the [`kedro-datasets`](https://github.com/kedro-org/kedro-plugins/tree/main/kedro-datasets) package instead.\nFrom version **`2.0.0`** of `kedro-datasets`, all dataset names have changed to replace the capital letter \"S\" in \"DataSet\" with a lower case \"s\". For example, `CSVDataSet` is now `CSVDataset`.\n</code></pre>"},{"location":"pages/data/advanced_data_catalog_usage/#how-to-configure-the-data-catalog","title":"How to configure the Data Catalog","text":"<p>To use the <code>DataCatalog</code> API, construct a <code>DataCatalog</code> object programmatically in a file like <code>catalog.py</code>.</p> <p>In the following code, we use several pre-built data loaders documented in the {py:mod}<code>kedro-datasets documentation &lt;kedro-datasets:kedro_datasets&gt;</code>.</p> <pre><code>from kedro.io import DataCatalog\nfrom kedro_datasets.pandas import (\n    CSVDataset,\n    SQLTableDataset,\n    SQLQueryDataset,\n    ParquetDataset,\n)\n\ncatalog =  DataCatalog(\n    {\n        \"bikes\": CSVDataset(filepath=\"../data/01_raw/bikes.csv\"),\n        \"cars\": CSVDataset(filepath=\"../data/01_raw/cars.csv\", load_args=dict(sep=\",\")),\n        \"cars_table\": SQLTableDataset(\n            table_name=\"cars\", credentials=dict(con=\"sqlite:///kedro.db\")\n        ),\n        \"scooters_query\": SQLQueryDataset(\n            sql=\"select * from cars where gear=4\",\n            credentials=dict(con=\"sqlite:///kedro.db\"),\n        ),\n        \"ranked\": ParquetDataset(filepath=\"ranked.parquet\"),\n    }\n)\n</code></pre> <p>When using <code>SQLTableDataset</code> or <code>SQLQueryDataset</code> you must provide a <code>con</code> key containing SQLAlchemy compatible database connection string. In the example above we pass it as part of <code>credentials</code> argument. Alternative to <code>credentials</code> is to put <code>con</code> into <code>load_args</code> and <code>save_args</code> (<code>SQLTableDataset</code> only).</p>"},{"location":"pages/data/advanced_data_catalog_usage/#how-to-view-the-available-data-sources","title":"How to view the available data sources","text":"<p>To review the <code>DataCatalog</code>:</p> <pre><code>catalog.list()\n</code></pre>"},{"location":"pages/data/advanced_data_catalog_usage/#how-to-load-datasets-programmatically","title":"How to load datasets programmatically","text":"<p>To access each dataset by its name:</p> <pre><code>cars = catalog.load(\"cars\")  # data is now loaded as a DataFrame in 'cars'\ngear = cars[\"gear\"].values\n</code></pre> <p>The following steps happened behind the scenes when <code>load</code> was called:</p> <ul> <li>The value <code>cars</code> was located in the Data Catalog</li> <li>The corresponding <code>AbstractDataset</code> object was retrieved</li> <li>The <code>load</code> method of this dataset was called</li> <li>This <code>load</code> method delegated the loading to the underlying pandas <code>read_csv</code> function</li> </ul>"},{"location":"pages/data/advanced_data_catalog_usage/#how-to-save-data-programmatically","title":"How to save data programmatically","text":"<pre><code>This pattern is not recommended unless you are using platform notebook environments (Sagemaker, Databricks etc) or writing unit/integration tests for your Kedro pipeline. Use the YAML approach in preference.\n</code></pre>"},{"location":"pages/data/advanced_data_catalog_usage/#how-to-save-data-to-memory","title":"How to save data to memory","text":"<p>To save data using an API similar to that used to load data:</p> <pre><code>from kedro.io import MemoryDataset\n\nmemory = MemoryDataset(data=None)\ncatalog.add(\"cars_cache\", memory)\ncatalog.save(\"cars_cache\", \"Memory can store anything.\")\ncatalog.load(\"cars_cache\")\n</code></pre>"},{"location":"pages/data/advanced_data_catalog_usage/#how-to-save-data-to-a-sql-database-for-querying","title":"How to save data to a SQL database for querying","text":"<p>To put the data in a SQLite database:</p> <pre><code>import os\n\n# This cleans up the database in case it exists at this point\ntry:\n    os.remove(\"kedro.db\")\nexcept FileNotFoundError:\n    pass\n\ncatalog.save(\"cars_table\", cars)\n\n# rank scooters by their mpg\nranked = catalog.load(\"scooters_query\")[[\"brand\", \"mpg\"]]\n</code></pre>"},{"location":"pages/data/advanced_data_catalog_usage/#how-to-save-data-in-parquet","title":"How to save data in Parquet","text":"<p>To save the processed data in Parquet format:</p> <pre><code>catalog.save(\"ranked\", ranked)\n</code></pre> <pre><code>Saving `None` to a dataset is not allowed!\n</code></pre>"},{"location":"pages/data/advanced_data_catalog_usage/#how-to-access-a-dataset-with-credentials","title":"How to access a dataset with credentials","text":"<p>Before instantiating the <code>DataCatalog</code>, Kedro will first attempt to read the credentials from the project configuration. The resulting dictionary is then passed into <code>DataCatalog.from_config()</code> as the <code>credentials</code> argument.</p> <p>Let's assume that the project contains the file <code>conf/local/credentials.yml</code> with the following contents:</p> <pre><code>dev_s3:\n  client_kwargs:\n    aws_access_key_id: key\n    aws_secret_access_key: secret\n\nscooters_credentials:\n  con: sqlite:///kedro.db\n\nmy_gcp_credentials:\n  id_token: key\n</code></pre> <p>Your code will look as follows:</p> <pre><code>CSVDataset(\n    filepath=\"s3://test_bucket/data/02_intermediate/company/motorbikes.csv\",\n    load_args=dict(sep=\",\", skiprows=5, skipfooter=1, na_values=[\"#NA\", \"NA\"]),\n    credentials=dict(key=\"token\", secret=\"key\"),\n)\n</code></pre>"},{"location":"pages/data/advanced_data_catalog_usage/#how-to-version-a-dataset-using-the-code-api","title":"How to version a dataset using the Code API","text":"<p>In an earlier section of the documentation we described how Kedro enables dataset and ML model versioning.</p> <p>If you require programmatic control over load and save versions of a specific dataset, you can instantiate <code>Version</code> and pass it as a parameter to the dataset initialisation:</p> <pre><code>from kedro.io import DataCatalog, Version\nfrom kedro_datasets.pandas import CSVDataset\nimport pandas as pd\n\ndata1 = pd.DataFrame({\"col1\": [1, 2], \"col2\": [4, 5], \"col3\": [5, 6]})\ndata2 = pd.DataFrame({\"col1\": [7], \"col2\": [8], \"col3\": [9]})\nversion = Version(\n    load=None,  # load the latest available version\n    save=None,  # generate save version automatically on each save operation\n)\n\ntest_dataset = CSVDataset(\n    filepath=\"data/01_raw/test.csv\", save_args={\"index\": False}, version=version\n)\ncatalog =  DataCatalog({\"test_dataset\": test_dataset})\n\n# save the dataset to data/01_raw/test.csv/&lt;version&gt;/test.csv\ncatalog.save(\"test_dataset\", data1)\n# save the dataset into a new file data/01_raw/test.csv/&lt;version&gt;/test.csv\ncatalog.save(\"test_dataset\", data2)\n\n# load the latest version from data/test.csv/*/test.csv\nreloaded = catalog.load(\"test_dataset\")\nassert data2.equals(reloaded)\n</code></pre> <p>In the example above, we do not fix any versions. The behaviour of load and save operations becomes slightly different when we set a version:</p> <pre><code>version = Version(\n    load=\"my_exact_version\",  # load exact version\n    save=\"my_exact_version\",  # save to exact version\n)\n\ntest_dataset = CSVDataset(\n    filepath=\"data/01_raw/test.csv\", save_args={\"index\": False}, version=version\n)\ncatalog =  DataCatalog({\"test_dataset\": test_dataset})\n\n# save the dataset to data/01_raw/test.csv/my_exact_version/test.csv\ncatalog.save(\"test_dataset\", data1)\n# load from data/01_raw/test.csv/my_exact_version/test.csv\nreloaded = catalog.load(\"test_dataset\")\nassert data1.equals(reloaded)\n\n# raises DatasetError since the path\n# data/01_raw/test.csv/my_exact_version/test.csv already exists\ncatalog.save(\"test_dataset\", data2)\n</code></pre> <p>We do not recommend passing exact load or save versions, since it might lead to inconsistencies between operations. For example, if versions for load and save operations do not match, a save operation would result in a <code>UserWarning</code>.</p> <p>Imagine a simple pipeline with two nodes, where B takes the output from A. If you specify the load-version of the data for B to be <code>my_data_2023_08_16.csv</code>, the data that A produces (<code>my_data_20230818.csv</code>) is not used.</p> <pre><code>Node_A -&gt; my_data_20230818.csv\nmy_data_2023_08_16.csv -&gt; Node B\n</code></pre> <p>In code:</p> <pre><code>version = Version(\n    load=\"my_data_2023_08_16.csv\",  # load exact version\n    save=\"my_data_20230818.csv\",  # save to exact version\n)\n\ntest_dataset = CSVDataset(\n    filepath=\"data/01_raw/test.csv\", save_args={\"index\": False}, version=version\n)\ncatalog =  DataCatalog({\"test_dataset\": test_dataset})\n\ncatalog.save(\"test_dataset\", data1)  # emits a UserWarning due to version inconsistency\n\n# raises DatasetError since the data/01_raw/test.csv/exact_load_version/test.csv\n# file does not exist\nreloaded = catalog.load(\"test_dataset\")\n</code></pre>"},{"location":"pages/data/data_catalog/","title":"Introduction to the Data Catalog","text":"<p>In a Kedro project, the Data Catalog is a registry of all data sources available for use by the project. It is specified with a YAML catalog file that maps the names of node inputs and outputs as keys in the <code>DataCatalog</code> class.</p> <p>This page introduces the basic sections of <code>catalog.yml</code>, which is the file Kedro uses to register data sources for a project.</p> <pre><code>Datasets are not included in the core Kedro package from Kedro version **`0.19.0`**. Import them from the [`kedro-datasets`](https://github.com/kedro-org/kedro-plugins/tree/main/kedro-datasets) package instead.\nFrom version **`2.0.0`** of `kedro-datasets`, all dataset names have changed to replace the capital letter \"S\" in \"DataSet\" with a lower case \"s\". For example, `CSVDataSet` is now `CSVDataset`.\n</code></pre>"},{"location":"pages/data/data_catalog/#the-basics-of-catalogyml","title":"The basics of <code>catalog.yml</code>","text":"<p>A separate page of Data Catalog YAML examples  gives further examples of how to work with <code>catalog.yml</code>, but here we revisit the basic <code>catalog.yml</code> introduced by the spaceflights tutorial.</p> <p>The example below registers two <code>csv</code> datasets, and an <code>xlsx</code> dataset. The minimum details needed to load and save a file within a local file system are the key, which is name of the dataset, the type of data to indicate the dataset to use (<code>type</code>) and the file's location (<code>filepath</code>).</p> <pre><code>companies:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/companies.csv\n\nreviews:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/reviews.csv\n\nshuttles:\n  type: pandas.ExcelDataset\n  filepath: data/01_raw/shuttles.xlsx\n  load_args:\n    engine: openpyxl # Use modern Excel engine (the default since Kedro 0.18.0)\n</code></pre>"},{"location":"pages/data/data_catalog/#configuring-dataset-parameters-in-catalogyml","title":"Configuring dataset parameters in <code>catalog.yml</code>","text":"<p>The dataset configuration in <code>catalog.yml</code> is defined as follows: 1. The top-level key is the dataset name used as a dataset identifier in the catalog - <code>shuttles</code>, <code>weather</code> in the example below. 2. The next level includes multiple keys. The first one is the mandatory key - <code>type</code> which defines the type of dataset to use. The rest of the keys are dataset parameters and vary depending on the implementation. To get the extensive list of dataset parameters, see {py:mod}<code>The kedro-datasets package documentation &lt;kedro-datasets:kedro_datasets&gt;</code> and navigate to the <code>__init__</code> method of the target dataset. 3. Some dataset parameters can be further configured depending on the libraries underlying the dataset implementation. In the example below, a configuration of the <code>shuttles</code> dataset includes the <code>load_args</code> parameter which is defined by the <code>pandas</code> option for loading CSV files. While the <code>save_args</code> parameter in a configuration of the <code>weather</code> dataset is defined by the <code>snowpark</code> <code>saveAsTable</code> method. To get the extensive list of dataset parameters, see {py:mod}<code>The kedro-datasets package documentation &lt;kedro-datasets:kedro_datasets&gt;</code> and navigate to the target parameter in the <code>__init__</code> definition for the dataset. For those parameters we provide a reference to the underlying library configuration parameters. For example, under the <code>load_args</code> parameter section for pandas.ExcelDataset you can find a reference to the pandas.read_excel method defining the full set of the parameters accepted.</p> <pre><code>Kedro datasets delegate any of the `load_args` / `save_args` directly to the underlying implementation.\n</code></pre> <p>The example below showcases the configuration of two datasets - <code>shuttles</code> of type pandas.ExcelDataset and <code>weather</code> of type snowflake.SnowparkTableDataset.</p> <pre><code>shuttles: # Dataset name\n  type: pandas.ExcelDataset # Dataset type\n  filepath: data/01_raw/shuttles.xlsx # pandas.ExcelDataset parameter\n  load_args: # pandas.ExcelDataset parameter\n    engine: openpyxl # Pandas option for loading CSV files\n\nweather: # Dataset name\n  type: snowflake.SnowparkTableDataset # Dataset type\n  table_name: \"weather_data\"\n  database: \"meteorology\"\n  schema: \"observations\"\n  credentials: snowflake_client\n  save_args: # snowflake.SnowparkTableDataset parameter\n    mode: overwrite # Snowpark saveAsTable input option\n    column_order: name\n    table_type: ''\n</code></pre>"},{"location":"pages/data/data_catalog/#dataset-type","title":"Dataset <code>type</code>","text":"<p>Kedro supports a range of connectors, for CSV files, Excel spreadsheets, Parquet files, Feather files, HDF5 files, JSON documents, pickled objects, SQL tables, SQL queries, and more. They are supported using libraries such as pandas, PySpark, NetworkX, and Matplotlib.</p> <p>{py:mod}<code>The kedro-datasets package documentation &lt;kedro-datasets:kedro_datasets&gt;</code> contains a comprehensive list of all available file types.</p>"},{"location":"pages/data/data_catalog/#dataset-filepath","title":"Dataset <code>filepath</code>","text":"<p>Kedro relies on <code>fsspec</code> to read and save data from a variety of data stores including local file systems, network file systems, cloud object stores, and Hadoop. When specifying a storage location in <code>filepath:</code>, you should provide a URL using the general form <code>protocol://path/to/data</code>.  If no protocol is provided, the local file system is assumed (which is the same as <code>file://</code>).</p> <p>The following protocols are available:</p> <ul> <li>Local or Network File System: <code>file://</code> - the local file system is default in the absence of any protocol, it also permits relative paths.</li> <li>Hadoop File System (HDFS): <code>hdfs://user@server:port/path/to/data</code> - Hadoop Distributed File System, for resilient, replicated files within a cluster.</li> <li>Amazon S3: <code>s3://my-bucket-name/path/to/data</code> - Amazon S3 remote binary store, often used with Amazon EC2,   using the library s3fs.</li> <li>S3 Compatible Storage: <code>s3://my-bucket-name/path/_to/data</code> - for example, MinIO, using the s3fs library.</li> <li>Google Cloud Storage: <code>gcs://</code> - Google Cloud Storage, typically used with Google Compute   resource using gcsfs (in development).</li> <li>Azure Blob Storage / Azure Data Lake Storage Gen2: <code>abfs://</code> - Azure Blob Storage, typically used when working on an Azure environment.</li> <li>HTTP(s): <code>http://</code> or <code>https://</code> for reading data directly from HTTP web servers.</li> </ul> <p><code>fsspec</code> also provides other file systems, such as SSH, FTP and WebHDFS. See the fsspec documentation for more information.</p>"},{"location":"pages/data/data_catalog/#additional-settings-in-catalogyml","title":"Additional settings in <code>catalog.yml</code>","text":"<p>This section explains the additional settings available within <code>catalog.yml</code>.</p>"},{"location":"pages/data/data_catalog/#load-save-and-filesystem-arguments","title":"Load, save and filesystem arguments","text":"<p>The Kedro Data Catalog also accepts different groups of <code>*_args</code> parameters that serve different purposes:</p> <ul> <li><code>load_args</code> and <code>save_args</code>: Configure how a third-party library loads/saves data from/to a file. In the spaceflights example above, <code>load_args</code>, is passed to the excel file read method (<code>pd.read_excel</code>) as a keyword argument. Although not specified here, the equivalent output is <code>save_args</code> and the value would be passed to <code>pd.DataFrame.to_excel</code> method.</li> </ul> <p>For example, to load or save a CSV on a local file system, using specified load/save arguments:</p> <pre><code>cars:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/company/cars.csv\n  load_args:\n    sep: ','\n  save_args:\n    index: False\n    date_format: '%Y-%m-%d %H:%M'\n    decimal: .\n</code></pre> <ul> <li><code>fs_args</code>: Configures the interaction with a filesystem. All the top-level parameters of <code>fs_args</code> (except <code>open_args_load</code> and <code>open_args_save</code>) will be passed to an underlying filesystem class.</li> </ul> <p>For example, to provide the <code>project</code> value to the underlying filesystem class (<code>GCSFileSystem</code>) to interact with Google Cloud Storage:</p> <pre><code>test_dataset:\n  type: ...\n  fs_args:\n    project: test_project\n</code></pre> <p>The <code>open_args_load</code> and <code>open_args_save</code> parameters are passed to the filesystem's <code>open</code> method to configure how a dataset file (on a specific filesystem) is opened during a load or save operation, respectively.</p> <p>For example, to load data from a local binary file using <code>utf-8</code> encoding:</p> <pre><code>test_dataset:\n  type: ...\n  fs_args:\n    open_args_load:\n      mode: \"rb\"\n      encoding: \"utf-8\"\n</code></pre> <p>If you want to save a file in append mode instead of overwrite you can use the <code>open_args_save</code> <code>mode</code> parameter:</p> <pre><code>test_dataset:\n  type: ...\n  fs_args:\n    open_args_save:\n      mode: \"a\"\n</code></pre> <pre><code>Default load, save and filesystem arguments are defined inside the specific dataset implementations as `DEFAULT_LOAD_ARGS`, `DEFAULT_SAVE_ARGS`, and `DEFAULT_FS_ARGS` respectively.\nYou can check those in {py:mod}`the dataset API documentation &lt;kedro-datasets:kedro_datasets&gt;`.\n</code></pre>"},{"location":"pages/data/data_catalog/#dataset-access-credentials","title":"Dataset access credentials","text":"<p>The Data Catalog also works with the <code>credentials.yml</code> file in <code>conf/local/</code>, allowing you to specify usernames and passwords required to load certain datasets.</p> <p>Before instantiating the <code>DataCatalog</code>, Kedro will first attempt to read the credentials from the project configuration. The resulting dictionary is then passed into <code>DataCatalog.from_config()</code> as the <code>credentials</code> argument.</p> <p>Let's assume that the project contains the file <code>conf/local/credentials.yml</code> with the following contents:</p> <pre><code>dev_s3:\n  client_kwargs:\n    aws_access_key_id: key\n    aws_secret_access_key: secret\n</code></pre> <p>and the Data Catalog is specified in <code>catalog.yml</code> as follows:</p> <pre><code>motorbikes:\n  type: pandas.CSVDataset\n  filepath: s3://your_bucket/data/02_intermediate/company/motorbikes.csv\n  credentials: dev_s3\n  load_args:\n    sep: ','\n</code></pre> <p>In the example above, the <code>catalog.yml</code> file contains references to credentials keys <code>dev_s3</code>. The Data Catalog first reads <code>dev_s3</code> from the received <code>credentials</code> dictionary, and then passes its values into the dataset as a <code>credentials</code> argument to <code>__init__</code>.</p>"},{"location":"pages/data/data_catalog/#dataset-versioning","title":"Dataset versioning","text":"<p>Kedro enables dataset and ML model versioning through the <code>versioned</code> definition. For example:</p> <pre><code>cars:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/company/cars.csv\n  versioned: True\n</code></pre> <p>In this example, <code>filepath</code> is used as the basis of a folder that stores versions of the <code>cars</code> dataset. Each time a new version is created by a pipeline run it is stored within <code>data/01_raw/company/cars.csv/&lt;version&gt;/cars.csv</code>, where <code>&lt;version&gt;</code> corresponds to a version string formatted as <code>YYYY-MM-DDThh.mm.ss.sssZ</code>.</p> <p>By default, <code>kedro run</code> loads the latest version of the dataset. However, you can also specify a particular versioned dataset with <code>--load-version</code> flag as follows:</p> <pre><code>kedro run --load-versions=cars:YYYY-MM-DDThh.mm.ss.sssZ\n</code></pre> <p>where <code>--load-versions</code> is dataset name and version timestamp separated by <code>:</code>.</p> <p>A dataset offers versioning support if it extends the {py:class}<code>~kedro.io.AbstractVersionedDataset</code> class to accept a version keyword argument as part of the constructor and adapt the <code>_save</code> and <code>_load</code> method to use the versioned data path obtained from <code>_get_save_path</code> and <code>_get_load_path</code> respectively.</p> <p>To verify whether a dataset can undergo versioning, you should examine the dataset class code to inspect its inheritance (you can find contributed datasets within the <code>kedro-datasets</code> repository). Check if the dataset class inherits from the <code>AbstractVersionedDataset</code>. For instance, if you encounter a class like <code>CSVDataset(AbstractVersionedDataset[pd.DataFrame, pd.DataFrame])</code>, this indicates that the dataset is set up to support versioning.</p> <pre><code>Note that HTTP(S) is a supported file system in the dataset implementations, but if you use it, you can't also use versioning.\n</code></pre>"},{"location":"pages/data/data_catalog/#use-the-data-catalog-within-kedro-configuration","title":"Use the Data Catalog within Kedro configuration","text":"<p>Kedro configuration enables you to organise your project for different stages of your data pipeline. For example, you might need different Data Catalog settings for development, testing, and production environments.</p> <p>By default, Kedro has a <code>base</code> and a <code>local</code> folder for configuration. The Data Catalog configuration is loaded using a configuration loader class which recursively scans for configuration files inside the <code>conf</code> folder, firstly in <code>conf/base</code> and then in <code>conf/local</code> (which is the designated overriding environment). Kedro merges the configuration information and returns a configuration dictionary according to rules set out in the configuration documentation.</p> <p>In summary, if you need to configure your datasets for different environments, you can create both <code>conf/base/catalog.yml</code> and <code>conf/local/catalog.yml</code>. For instance, you can use the <code>catalog.yml</code> file in <code>conf/base/</code> to register the locations of datasets that would run in production, while adding a second version of <code>catalog.yml</code> in <code>conf/local/</code> to register the locations of sample datasets while you are using them for prototyping data pipeline(s).</p> <p>To illustrate this, consider the following catalog entry for a dataset named <code>cars</code> in <code>conf/base/catalog.yml</code>, which points to a csv file stored in a bucket on AWS S3:</p> <pre><code>cars:\n  filepath: s3://my_bucket/cars.csv\n  type: pandas.CSVDataset\n ```\nYou can overwrite this catalog entry in `conf/local/catalog.yml` to point to a locally stored file instead:\n```yaml\ncars:\n  filepath: data/01_raw/cars.csv\n  type: pandas.CSVDataset\n</code></pre> <p>In your pipeline code, when the <code>cars</code> dataset is used, it will use the overwritten catalog entry from <code>conf/local/catalog.yml</code> and rely on Kedro to detect which definition of <code>cars</code> dataset to use in your pipeline.</p>"},{"location":"pages/data/data_catalog_yaml_examples/","title":"Data Catalog YAML examples","text":"<p>This page contains a set of examples to help you structure your YAML configuration file in <code>conf/base/catalog.yml</code> or <code>conf/local/catalog.yml</code>.</p> <pre><code>Datasets are not included in the core Kedro package from Kedro version **`0.19.0`**. Import them from the [`kedro-datasets`](https://github.com/kedro-org/kedro-plugins/tree/main/kedro-datasets) package instead.\nFrom version **`2.0.0`** of `kedro-datasets`, all dataset names have changed to replace the capital letter \"S\" in \"DataSet\" with a lower case \"s\". For example, `CSVDataSet` is now `CSVDataset`.\n</code></pre> <p>```{contents} Table of Contents :depth: 3</p> <pre><code>\n## Load data from a local binary file using `utf-8` encoding\n\nThe `open_args_load` and `open_args_save` parameters are passed to the filesystem `open` method to configure how a dataset file (on a specific filesystem) is opened during a load or save operation respectively.\n\n```yaml\ntest_dataset:\n  type: ...\n  fs_args:\n    open_args_load:\n      mode: \"rb\"\n      encoding: \"utf-8\"\n</code></pre> <p><code>load_args</code> and <code>save_args</code> configure how a third-party library (e.g. <code>pandas</code> for <code>CSVDataset</code>) loads/saves data from/to a file.</p>"},{"location":"pages/data/data_catalog_yaml_examples/#save-data-to-a-csv-file-without-row-names-index-using-utf-8-encoding","title":"Save data to a CSV file without row names (index) using <code>utf-8</code> encoding","text":"<pre><code>test_dataset:\n  type: pandas.CSVDataset\n  ...\n  save_args:\n    index: False\n    encoding: \"utf-8\"\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#loadsave-a-csv-file-fromto-a-local-file-system","title":"Load/save a CSV file from/to a local file system","text":"<pre><code>bikes:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/bikes.csv\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#loadsave-a-csv-on-a-local-file-system-using-specified-loadsave-arguments","title":"Load/save a CSV on a local file system, using specified load/save arguments","text":"<pre><code>cars:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/company/cars.csv\n  load_args:\n    sep: ','\n  save_args:\n    index: False\n    date_format: '%Y-%m-%d %H:%M'\n    decimal: .\n\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#loadsave-a-compressed-csv-on-a-local-file-system","title":"Load/save a compressed CSV on a local file system","text":"<pre><code>boats:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/company/boats.csv.gz\n  load_args:\n    sep: ','\n    compression: 'gzip'\n  fs_args:\n    open_args_load:\n      mode: 'rb'\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#load-a-csv-file-from-a-specific-s3-bucket-using-credentials-and-load-arguments","title":"Load a CSV file from a specific S3 bucket, using credentials and load arguments","text":"<pre><code>motorbikes:\n  type: pandas.CSVDataset\n  filepath: s3://your_bucket/data/02_intermediate/company/motorbikes.csv\n  credentials: dev_s3\n  load_args:\n    sep: ','\n    skiprows: 5\n    skipfooter: 1\n    na_values: ['#NA', NA]\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#loadsave-a-pickle-file-fromto-a-local-file-system","title":"Load/save a pickle file from/to a local file system","text":"<pre><code>airplanes:\n  type: pickle.PickleDataset\n  filepath: data/06_models/airplanes.pkl\n  backend: pickle\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#load-an-excel-file-from-google-cloud-storage","title":"Load an Excel file from Google Cloud Storage","text":"<p>The example includes the <code>project</code> value for the underlying filesystem class (<code>GCSFileSystem</code>) within Google Cloud Storage (GCS)</p> <pre><code>rockets:\n  type: pandas.ExcelDataset\n  filepath: gcs://your_bucket/data/02_intermediate/company/motorbikes.xlsx\n  fs_args:\n    project: my-project\n  credentials: my_gcp_credentials\n  save_args:\n    sheet_name: Sheet1\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#load-a-multi-sheet-excel-file-from-a-local-file-system","title":"Load a multi-sheet Excel file from a local file system","text":"<pre><code>trains:\n  type: pandas.ExcelDataset\n  filepath: data/02_intermediate/company/trains.xlsx\n  load_args:\n    sheet_name: [Sheet1, Sheet2, Sheet3]\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#save-an-image-created-with-matplotlib-on-google-cloud-storage","title":"Save an image created with Matplotlib on Google Cloud Storage","text":"<pre><code>results_plot:\n  type: matplotlib.MatplotlibWriter\n  filepath: gcs://your_bucket/data/08_results/plots/output_1.jpeg\n  fs_args:\n    project: my-project\n  credentials: my_gcp_credentials\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#loadsave-an-hdf-file-on-local-file-system-storage-using-specified-loadsave-arguments","title":"Load/save an HDF file on local file system storage, using specified load/save arguments","text":"<pre><code>skateboards:\n  type: pandas.HDFDataset\n  filepath: data/02_intermediate/skateboards.hdf\n  key: name\n  load_args:\n    columns: [brand, length]\n  save_args:\n    mode: w  # Overwrite even when the file already exists\n    dropna: True\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#loadsave-a-parquet-file-on-local-file-system-storage-using-specified-loadsave-arguments","title":"Load/save a parquet file on local file system storage, using specified load/save arguments","text":"<pre><code>trucks:\n  type: pandas.ParquetDataset\n  filepath: data/02_intermediate/trucks.parquet\n  load_args:\n    columns: [name, gear, disp, wt]\n    categories: list\n    index: name\n  save_args:\n    compression: GZIP\n    file_scheme: hive\n    has_nulls: False\n    partition_on: [name]\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#loadsave-a-spark-table-on-s3-using-specified-loadsave-arguments","title":"Load/save a Spark table on S3, using specified load/save arguments","text":"<pre><code>weather:\n  type: spark.SparkDataset\n  filepath: s3a://your_bucket/data/01_raw/weather*\n  credentials: dev_s3\n  file_format: csv\n  load_args:\n    header: True\n    inferSchema: True\n  save_args:\n    sep: '|'\n    header: True\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#loadsave-a-sql-table-using-credentials-a-database-connection-and-specified-loadsave-arguments","title":"Load/save a SQL table using credentials, a database connection, and specified load/save arguments","text":"<pre><code>scooters:\n  type: pandas.SQLTableDataset\n  credentials: scooters_credentials\n  table_name: scooters\n  load_args:\n    index_col: [name]\n    columns: [name, gear]\n  save_args:\n    if_exists: replace\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#load-a-sql-table-with-credentials-and-a-database-connection-and-apply-a-sql-query-to-the-table","title":"Load a SQL table with credentials and a database connection, and apply a SQL query to the table","text":"<pre><code>scooters_query:\n  type: pandas.SQLQueryDataset\n  credentials: scooters_credentials\n  sql: select * from cars where gear=4\n  load_args:\n    index_col: [name]\n</code></pre> <p>When you use {class}<code>pandas.SQLTableDataset&lt;kedro-datasets:kedro_datasets.pandas.SQLTableDataset&gt;</code>, or {class}<code>pandas.SQLQueryDataset&lt;kedro-datasets:kedro_datasets.pandas.SQLQueryDataset&gt;</code> you must provide a database connection string. In the above example, we pass it using the <code>scooters_credentials</code> key from the credentials.</p> <p><code>scooters_credentials</code> must have a top-level key <code>con</code> containing a SQLAlchemy compatible connection string. As an alternative to credentials, you could explicitly put <code>con</code> into <code>load_args</code> and <code>save_args</code> (<code>pandas.SQLTableDataset</code> only).</p>"},{"location":"pages/data/data_catalog_yaml_examples/#load-data-from-an-api-endpoint","title":"Load data from an API endpoint","text":"<p>This example uses US corn yield data from USDA.</p> <pre><code>us_corn_yield_data:\n  type: api.APIDataset\n  url: https://quickstats.nass.usda.gov\n  credentials: usda_credentials\n  params:\n    key: SOME_TOKEN\n    format: JSON\n    commodity_desc: CORN\n    statisticcat_des: YIELD\n    agg_level_desc: STATE\n    year: 2000\n</code></pre> <p><code>usda_credentials</code> will be passed as the <code>auth</code> argument in the <code>requests</code> library. Specify the username and password as a list in your <code>credentials.yml</code> file as follows:</p> <pre><code>usda_credentials:\n  - username\n  - password\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#load-data-from-minio-s3-compatible-storage","title":"Load data from MinIO (S3-compatible storage)","text":"<pre><code>test:\n  type: pandas.CSVDataset\n  filepath: s3://your_bucket/test.csv # assume `test.csv` is uploaded to the MinIO server.\n  credentials: dev_minio\n</code></pre> <p>In <code>credentials.yml</code>, define the <code>key</code>, <code>secret</code> and the <code>endpoint_url</code> as follows:</p> <pre><code>dev_minio:\n  key: token\n  secret: key\n  client_kwargs:\n    endpoint_url : 'http://localhost:9000'\n</code></pre> <pre><code>The easiest way to setup MinIO is to run a Docker image. After the following command, you can access the MinIO server with `http://localhost:9000` and create a bucket and add files as if it is on S3.\n</code></pre> <p><code>docker run -p 9000:9000 -e \"MINIO_ACCESS_KEY=token\" -e \"MINIO_SECRET_KEY=key\" minio/minio server /data</code></p>"},{"location":"pages/data/data_catalog_yaml_examples/#load-a-model-saved-as-a-pickle-from-azure-blob-storage","title":"Load a model saved as a pickle from Azure Blob Storage","text":"<pre><code>ml_model:\n  type: pickle.PickleDataset\n  filepath: \"abfs://models/ml_models.pickle\"\n  versioned: True\n  credentials: dev_abs\n</code></pre> <p>In the <code>credentials.yml</code> file, define the <code>account_name</code> and <code>account_key</code>:</p> <pre><code>dev_abs:\n  account_name: accountname\n  account_key: key\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#load-a-csv-file-stored-in-a-remote-location-through-ssh","title":"Load a CSV file stored in a remote location through SSH","text":"<pre><code>This example requires [Paramiko](https://www.paramiko.org) to be installed (`pip install paramiko`).\n</code></pre> <pre><code>cool_dataset:\n  type: pandas.CSVDataset\n  filepath: \"sftp:///path/to/remote_cluster/cool_data.csv\"\n  credentials: cluster_credentials\n</code></pre> <p>All parameters required to establish the SFTP connection can be defined through <code>fs_args</code> or in the <code>credentials.yml</code> file as follows:</p> <pre><code>cluster_credentials:\n  username: my_username\n  host: host_address\n  port: 22\n  password: password\n</code></pre> <p>The list of all available parameters is given in the Paramiko documentation.</p>"},{"location":"pages/data/data_catalog_yaml_examples/#load-multiple-datasets-with-similar-configuration-using-yaml-anchors","title":"Load multiple datasets with similar configuration using YAML anchors","text":"<p>Different datasets might use the same file format, share the same load and save arguments, and be stored in the same folder. YAML has a built-in syntax for factorising parts of a YAML file, which means that you can decide what is generalisable across your datasets, so that you need not spend time copying and pasting dataset configurations in the <code>catalog.yml</code> file.</p> <p>You can see this in the following example:</p> <pre><code>_csv: &amp;csv\n  type: spark.SparkDataset\n  file_format: csv\n  load_args:\n    sep: ','\n    na_values: ['#NA', NA]\n    header: True\n    inferSchema: False\n\ncars:\n  &lt;&lt;: *csv\n  filepath: s3a://data/01_raw/cars.csv\n\ntrucks:\n  &lt;&lt;: *csv\n  filepath: s3a://data/01_raw/trucks.csv\n\nbikes:\n  &lt;&lt;: *csv\n  filepath: s3a://data/01_raw/bikes.csv\n  load_args:\n    header: False\n</code></pre> <p>The syntax <code>&amp;csv</code> names the following block <code>csv</code> and the syntax <code>&lt;&lt;: *csv</code> inserts the contents of the block named <code>csv</code>. Locally declared keys entirely override inserted ones as seen in <code>bikes</code>.</p> <pre><code>It's important that the name of the template entry starts with a `_` so Kedro knows not to try and instantiate it as a dataset.\n</code></pre> <p>You can also nest reusable YAML syntax:</p> <pre><code>_csv: &amp;csv\n  type: spark.SparkDataset\n  file_format: csv\n  load_args: &amp;csv_load_args\n    header: True\n    inferSchema: False\n\nairplanes:\n  &lt;&lt;: *csv\n  filepath: s3a://data/01_raw/airplanes.csv\n  load_args:\n    &lt;&lt;: *csv_load_args\n    sep: ;\n</code></pre> <p>In this example, the default <code>csv</code> configuration is inserted into <code>airplanes</code> and then the <code>load_args</code> block is overridden. Normally, that would replace the whole dictionary. In order to extend <code>load_args</code>, the defaults for that block are then re-inserted.</p>"},{"location":"pages/data/data_catalog_yaml_examples/#read-the-same-file-using-different-datasets-with-transcoding","title":"Read the same file using different datasets with transcoding","text":"<p>You might come across a situation where you would like to read the same file using two different <code>Dataset</code> implementations. You can achieve this by using transcoding to define separate <code>DataCatalog</code> entries that point to the same <code>filepath</code>.</p>"},{"location":"pages/data/data_catalog_yaml_examples/#how-to-use-transcoding","title":"How to use transcoding","text":"<p>Consider an example with Parquet files. Parquet files can be loaded with both the <code>pandas.ParquetDataset</code>, and the <code>spark.SparkDataset</code> directly. This conversion is typical when coordinating a <code>Spark</code> to <code>pandas</code> workflow.</p> <p>To load the same file as both a <code>pandas.ParquetDataset</code> and a <code>spark.SparkDataset</code>, define two <code>DataCatalog</code> entries for the same dataset in your <code>conf/base/catalog.yml</code>:</p> <pre><code>my_dataframe@spark:\n  type: spark.SparkDataset\n  filepath: data/02_intermediate/data.parquet\n  file_format: parquet\n\nmy_dataframe@pandas:\n  type: pandas.ParquetDataset\n  filepath: data/02_intermediate/data.parquet\n</code></pre> <p>When using transcoding you must ensure the filepaths defined for each catalog entry share the same format (for example: CSV, JSON, Parquet). These entries can then be used in the pipeline as follows:</p> <pre><code>pipeline(\n    [\n        node(name=\"my_func1_node\", func=my_func1, inputs=\"spark_input\", outputs=\"my_dataframe@spark\"),\n        node(name=\"my_func2_node\", func=my_func2, inputs=\"my_dataframe@pandas\", outputs=\"pipeline_output\"),\n    ]\n)\n</code></pre> <p>In this example, Kedro understands that <code>my_dataframe</code> is the same dataset in its <code>spark.SparkDataset</code> and <code>pandas.ParquetDataset</code> formats and resolves the node execution order.</p> <p>In the pipeline, Kedro uses the <code>spark.SparkDataset</code> implementation for saving and <code>pandas.ParquetDataset</code> for loading, so the first node outputs a <code>pyspark.sql.DataFrame</code>, while the second node receives a <code>pandas.Dataframe</code>.</p>"},{"location":"pages/data/data_catalog_yaml_examples/#how-not-to-use-transcoding","title":"How not to use transcoding","text":"<p>Kedro pipelines automatically resolve the node execution order and check to ensure there are no circular dependencies in the pipeline. It is during this process that the transcoded datasets are resolved and the transcoding notation <code>@...</code> is stripped. This means within the pipeline the datasets <code>my_dataframe@spark</code> and <code>my_dataframe@pandas</code> are considered to be one <code>my_dataframe</code> dataset. The <code>DataCatalog</code>, however, treats transcoded entries as separate datasets, as they are only resolved as part of the pipeline resolution process. This results in differences between your defined pipeline in <code>pipeline.py</code> and the resolved pipeline that is run by Kedro, and these differences may lead to unintended behaviours. Thus, it is important to be aware of this when using transcoding.</p> <pre><code>Below are some examples where transcoding may produce unwanted side effects and raise errors.\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#defining-a-node-with-the-same-inputs-and-outputs","title":"Defining a node with the same inputs and outputs","text":"<p>Consider the following pipeline:</p> <pre><code>pipeline(\n    [\n        node(name=\"my_func1_node\", func=my_func1, inputs=\"my_dataframe@pandas\", outputs=\"my_dataframe@spark\"),\n    ]\n)\n</code></pre> <p>During the pipeline resolution, the node above is defined as having the dataset <code>my_dataset</code> as both its input and output. As a node cannot have the same inputs and outputs, trying to run this pipeline will fail with the following error:</p> <pre><code>ValueError: Failed to create node my_func1([my_dataframe@pandas]) -&gt; [my_dataframe@spark].\nA node cannot have the same inputs and outputs even if they are transcoded: {'my_dataframe'}\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#defining-several-nodes-that-share-the-same-output","title":"Defining several nodes that share the same output","text":"<p>Consider the following pipeline:</p> <pre><code>pipeline(\n    [\n        node(name=\"my_func1_node\", func=my_func1, inputs=\"spark_input\", outputs=\"my_dataframe@spark\"),\n        node(name=\"my_func2_node\", func=my_func2, inputs=\"pandas_input\", outputs=\"my_dataframe@pandas\"),\n    ]\n)\n</code></pre> <p>When this pipeline is resolved, both nodes are defined as returning the same output <code>my_dataset</code>, which is not allowed. Running the pipeline will fail with the following error:</p> <pre><code>kedro.pipeline.pipeline.OutputNotUniqueError: Output(s) ['my_dataframe'] are returned by more than one nodes. Node outputs must be unique.\n</code></pre>"},{"location":"pages/data/data_catalog_yaml_examples/#creating-pipelines-with-hidden-dependencies","title":"Creating pipelines with hidden dependencies","text":"<p>Consider the following pipeline:</p> <pre><code>pipeline(\n    [\n        node(name=\"my_func1_node\", func=my_func1, inputs=\"my_dataframe@spark\", outputs=\"spark_output\"),\n        node(name=\"my_func2_node\", func=my_func2, inputs=\"pandas_input\", outputs=\"my_dataframe@pandas\"),\n        node(name=\"my_func3_node\", func=my_func3, inputs=\"my_dataframe@pandas\", outputs=\"pandas_output\"),\n    ]\n)\n</code></pre> <p>In this example, there is a single dependency between the nodes <code>my_func3_node</code> and <code>my_func2_node</code>. However, when this pipeline is resolved there are some hidden dependencies that will restrict the node execution order. We can expose them by removing the transcoding notation:</p> <pre><code>resolved_pipeline(\n    [\n        node(name=\"my_func1_node\", func=my_func1, inputs=\"my_dataframe\", outputs=\"spark_output\"),\n        node(name=\"my_func2_node\", func=my_func2, inputs=\"pandas_input\", outputs=\"my_dataframe\"),\n        node(name=\"my_func3_node\", func=my_func3, inputs=\"my_dataframe\", outputs=\"pandas_output\"),\n    ]\n)\n</code></pre> <p>When the node order is resolved, we can see that the node <code>my_func1_node</code> is treated as dependent on the node <code>my_func2_node</code>. This pipeline will still run without any errors, but one should be careful about creating hidden dependencies as they can decrease performance, for example, when using the <code>ParallelRunner</code>.</p>"},{"location":"pages/data/data_catalog_yaml_examples/#create-a-data-catalog-yaml-configuration-file-via-the-cli","title":"Create a Data Catalog YAML configuration file via the CLI","text":"<p>You can use the <code>kedro catalog create</code> command to create a Data Catalog YAML configuration.</p> <p>This creates a <code>&lt;conf_root&gt;/&lt;env&gt;/catalog/&lt;pipeline_name&gt;.yml</code> configuration file with <code>MemoryDataset</code> datasets for each dataset in a registered pipeline if it is missing from the <code>DataCatalog</code>.</p> <pre><code># &lt;conf_root&gt;/&lt;env&gt;/catalog/&lt;pipeline_name&gt;.yml\nrockets:\n  type: MemoryDataset\nscooters:\n  type: MemoryDataset\n</code></pre>"},{"location":"pages/data/how_to_create_a_custom_dataset/","title":"Advanced: Tutorial to create a custom dataset","text":"<p>{py:mod}<code>Kedro supports many datasets &lt;kedro-datasets:kedro_datasets&gt;</code> out of the box, but you may find that you need to create a custom dataset. For example, you may need to handle a proprietary data format or filesystem in your pipeline, or perhaps you have found a particular use case for a dataset that Kedro does not support. This tutorial explains how to create a custom dataset to read and save image data.</p>"},{"location":"pages/data/how_to_create_a_custom_dataset/#abstractdataset","title":"AbstractDataset","text":"<p>If you are a contributor and would like to submit a new dataset, you must extend the {py:class}<code>~kedro.io.AbstractDataset</code> interface or {py:class}<code>~kedro.io.AbstractVersionedDataset</code> interface if you plan to support versioning. It requires subclasses to implement the <code>load</code> and <code>save</code> methods while providing wrappers that enrich the corresponding methods with uniform error handling. It also requires subclasses to override <code>_describe</code>, which is used in logging the internal information about the instances of your custom <code>AbstractDataset</code> implementation.</p>"},{"location":"pages/data/how_to_create_a_custom_dataset/#scenario","title":"Scenario","text":"<p>In this example, we use a Kaggle dataset of Pok\u00e9mon images and types to train a model to classify the type of a given Pok\u00e9mon, e.g. Water, Fire, Bug, etc., based on its appearance. To train the model, we read the Pok\u00e9mon images from PNG files into <code>numpy</code> arrays before further manipulation in the Kedro pipeline. To work with PNG images out of the box, in this example we create an <code>ImageDataset</code> to read and save image data.</p>"},{"location":"pages/data/how_to_create_a_custom_dataset/#project-setup","title":"Project setup","text":"<p>We assume that you have already installed Kedro. Now create a project (feel free to name your project as you like, but here we will assume the project's repository name is <code>kedro-pokemon</code>).</p> <p>Log into your Kaggle account to download the Pok\u00e9mon dataset and unzip it into <code>data/01_raw</code>, within a subfolder named <code>pokemon-images-and-types</code>. The data comprises a single <code>pokemon.csv</code> file plus a subfolder of images.</p> <p>The dataset will use Pillow for generic image processing functionality, to ensure that it can work with a range of different image formats, not just PNG.</p> <p>To install Pillow:</p> <pre><code>pip install Pillow\n</code></pre> <p>Consult the Pillow documentation if you experience problems with the installation.</p>"},{"location":"pages/data/how_to_create_a_custom_dataset/#the-anatomy-of-a-dataset","title":"The anatomy of a dataset","text":"<p>At the minimum, a valid Kedro dataset needs to subclass the base {py:class}<code>~kedro.io.AbstractDataset</code> and provide an implementation for the following abstract methods:</p> <ul> <li><code>load</code></li> <li><code>save</code></li> <li><code>_describe</code></li> </ul> <p><code>AbstractDataset</code> is generically typed with an input data type for saving data, and an output data type for loading data. This typing is optional however, and defaults to <code>Any</code> type.</p> <p>The <code>_EPHEMERAL</code> boolean attribute in <code>AbstractDataset</code> indicates if a dataset is persistent. For example, in the case of {py:class}<code>~kedro.io.MemoryDataset</code>, which is not persistent, it is set to True. By default, <code>_EPHEMERAL</code> is set to False.</p> <pre><code>The parameter to specify the location of the data file/folder must be called either `filename`, `filepath`, or `path` in the constructor function of the custom dataset class to comply with the Kedro convention.\n</code></pre> <p>Here is an example skeleton for <code>ImageDataset</code>:</p> Click to expand <pre><code>from typing import Any, Dict\n\nimport numpy as np\n\nfrom kedro.io import AbstractDataset\n\n\nclass ImageDataset(AbstractDataset[np.ndarray, np.ndarray]):\n    \"\"\"``ImageDataset`` loads / save image data from a given filepath as `numpy` array using Pillow.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; ImageDataset(filepath='/img/file/path.png')\n    \"\"\"\n\n    def __init__(self, filepath: str):\n        \"\"\"Creates a new instance of ImageDataset to load / save image data at the given filepath.\n\n        Args:\n            filepath: The location of the image file to load / save data.\n        \"\"\"\n        self._filepath = filepath\n\n    def load(self) -&gt; np.ndarray:\n        \"\"\"Loads data from the image file.\n\n        Returns:\n            Data from the image file as a numpy array.\n        \"\"\"\n        ...\n\n    def save(self, data: np.ndarray) -&gt; None:\n        \"\"\"Saves image data to the specified filepath\"\"\"\n        ...\n\n    def _describe(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns a dict that describes the attributes of the dataset\"\"\"\n        ...\n</code></pre> <p>Create a subfolder called <code>datasets</code> in <code>src/kedro_pokemon/</code> to store the dataset definition <code>image_dataset.py</code>, adding <code>__init__.py</code> to make Python treat the directory as a package that you can import from:</p> <pre><code>src/kedro_pokemon/datasets\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 image_dataset.py\n</code></pre>"},{"location":"pages/data/how_to_create_a_custom_dataset/#implement-the-load-method-with-fsspec","title":"Implement the <code>load</code> method with <code>fsspec</code>","text":"<p>Many of the built-in Kedro datasets rely on fsspec as a consistent interface to different data sources, as described earlier in the section about the Data Catalog. In this example, it's particularly convenient to use <code>fsspec</code> in conjunction with <code>Pillow</code> to read image data, since it allows the dataset to work flexibly with different image locations and formats.</p> <p>Here is the implementation of the <code>load</code> method using <code>fsspec</code> and <code>Pillow</code> to read the data of a single image into a <code>numpy</code> array:</p> Click to expand <pre><code>from pathlib import PurePosixPath\nfrom typing import Any, Dict\n\nimport fsspec\nimport numpy as np\nfrom PIL import Image\n\nfrom kedro.io import AbstractDataset\nfrom kedro.io.core import get_filepath_str, get_protocol_and_path\n\n\nclass ImageDataset(AbstractDataset[np.ndarray, np.ndarray]):\n    def __init__(self, filepath: str):\n        \"\"\"Creates a new instance of ImageDataset to load / save image data for given filepath.\n\n        Args:\n            filepath: The location of the image file to load / save data.\n        \"\"\"\n        # parse the path and protocol (e.g. file, http, s3, etc.)\n        protocol, path = get_protocol_and_path(filepath)\n        self._protocol = protocol\n        self._filepath = PurePosixPath(path)\n        self._fs = fsspec.filesystem(self._protocol)\n\n    def load(self) -&gt; np.ndarray:\n        \"\"\"Loads data from the image file.\n\n        Returns:\n            Data from the image file as a numpy array\n        \"\"\"\n        # using get_filepath_str ensures that the protocol and path are appended correctly for different filesystems\n        load_path = get_filepath_str(self._filepath, self._protocol)\n        with self._fs.open(load_path) as f:\n            image = Image.open(f).convert(\"RGBA\")\n            return np.asarray(image)\n\n    ...\n</code></pre> <p>To test this out, let's add a dataset to the data catalog to load Pikachu's image.</p> <pre><code># in conf/base/catalog.yml\n\npikachu:\n  type: kedro_pokemon.datasets.image_dataset.ImageDataset\n  filepath: data/01_raw/pokemon-images-and-types/images/images/pikachu.png\n  # Note: the duplicated `images` path is part of the original Kaggle dataset\n</code></pre> <p>Then launch an IPython session with <code>kedro ipython</code> to preview the data:</p> <pre><code># read data image into a numpy array\nIn [1]: image = context.catalog.load('pikachu')\n\n# then re-show the image using Pillow's Image API.\nIn [2]: from PIL import Image\nIn [3]: Image.fromarray(image).show()\n</code></pre>"},{"location":"pages/data/how_to_create_a_custom_dataset/#implement-the-save-method-with-fsspec","title":"Implement the <code>save</code> method with <code>fsspec</code>","text":"<p>Similarly, we can implement the <code>_save</code> method as follows:</p> <pre><code>class ImageDataset(AbstractDataset[np.ndarray, np.ndarray]):\n    def save(self, data: np.ndarray) -&gt; None:\n        \"\"\"Saves image data to the specified filepath.\"\"\"\n        # using get_filepath_str ensures that the protocol and path are appended correctly for different filesystems\n        save_path = get_filepath_str(self._filepath, self._protocol)\n        with self._fs.open(save_path, \"wb\") as f:\n            image = Image.fromarray(data)\n            image.save(f)\n</code></pre> <p>Let's try it out in IPython:</p> <pre><code>In [1]: image = context.catalog.load('pikachu')\nIn [2]: context.catalog.save('pikachu', data=image)\n</code></pre> <p>You can open the file to verify that the data was written back correctly.</p>"},{"location":"pages/data/how_to_create_a_custom_dataset/#implement-the-_describe-method","title":"Implement the <code>_describe</code> method","text":"<p>The <code>_describe</code> method is used for printing purposes. The convention in Kedro is for the method to return a dictionary describing the attributes of the dataset.</p> <pre><code>class ImageDataset(AbstractDataset[np.ndarray, np.ndarray]):\n    def _describe(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n        return dict(filepath=self._filepath, protocol=self._protocol)\n</code></pre>"},{"location":"pages/data/how_to_create_a_custom_dataset/#the-complete-example","title":"The complete example","text":"<p>Here is the full implementation of our basic <code>ImageDataset</code>:</p> Click to expand <pre><code>from pathlib import PurePosixPath\nfrom typing import Any, Dict\n\nimport fsspec\nimport numpy as np\nfrom PIL import Image\n\nfrom kedro.io import AbstractDataset\nfrom kedro.io.core import get_filepath_str, get_protocol_and_path\n\n\nclass ImageDataset(AbstractDataset[np.ndarray, np.ndarray]):\n    \"\"\"``ImageDataset`` loads / save image data from a given filepath as `numpy` array using Pillow.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; ImageDataset(filepath='/img/file/path.png')\n    \"\"\"\n\n    def __init__(self, filepath: str):\n        \"\"\"Creates a new instance of ImageDataset to load / save image data for given filepath.\n\n        Args:\n            filepath: The location of the image file to load / save data.\n        \"\"\"\n        protocol, path = get_protocol_and_path(filepath)\n        self._protocol = protocol\n        self._filepath = PurePosixPath(path)\n        self._fs = fsspec.filesystem(self._protocol)\n\n    def load(self) -&gt; np.ndarray:\n        \"\"\"Loads data from the image file.\n\n        Returns:\n            Data from the image file as a numpy array\n        \"\"\"\n        load_path = get_filepath_str(self._filepath, self._protocol)\n        with self._fs.open(load_path, mode=\"r\") as f:\n            image = Image.open(f).convert(\"RGBA\")\n            return np.asarray(image)\n\n    def save(self, data: np.ndarray) -&gt; None:\n        \"\"\"Saves image data to the specified filepath.\"\"\"\n        save_path = get_filepath_str(self._filepath, self._protocol)\n        with self._fs.open(save_path, mode=\"wb\") as f:\n            image = Image.fromarray(data)\n            image.save(f)\n\n    def _describe(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n        return dict(filepath=self._filepath, protocol=self._protocol)\n</code></pre>"},{"location":"pages/data/how_to_create_a_custom_dataset/#integration-with-partitioneddataset","title":"Integration with <code>PartitionedDataset</code>","text":"<p>Currently, the <code>ImageDataset</code> only works with a single image, but this example needs to load all Pokemon images from the raw data directory for further processing.</p> <p>Kedro's {class}<code>PartitionedDataset&lt;kedro-datasets:kedro_datasets.partitions.PartitionedDataset&gt;</code> is a convenient way to load multiple separate data files of the same underlying dataset type into a directory.</p> <p>To use <code>PartitionedDataset</code> with <code>ImageDataset</code> to load all Pokemon PNG images, add this to the data catalog YAML so that <code>PartitionedDataset</code> loads all PNG files from the data directory using <code>ImageDataset</code>:</p> <pre><code># in conf/base/catalog.yml\n\npokemon:\n  type: partitions.PartitionedDataset\n  dataset: kedro_pokemon.datasets.image_dataset.ImageDataset\n  path: data/01_raw/pokemon-images-and-types/images/images\n  filename_suffix: \".png\"\n</code></pre> <p>Let's try it out in the IPython console:</p> <pre><code>In [1]: images = context.catalog.load('pokemon')\nIn [2]: len(images)\nOut[2]: 721\n</code></pre> <p>Verify the number of <code>.png</code> files in the data directory (it should be <code>721</code>):</p> <pre><code>$ ls -la data/01_raw/pokemon-images-and-types/images/images/*.png | wc -l\n    721\n</code></pre>"},{"location":"pages/data/how_to_create_a_custom_dataset/#versioning","title":"Versioning","text":""},{"location":"pages/data/how_to_create_a_custom_dataset/#how-to-implement-versioning-in-your-dataset","title":"How to implement versioning in your dataset","text":"<pre><code>Versioning doesn't work with `PartitionedDataset`. You can't use both of them at the same time.\n</code></pre> <p>To add versioning support to the new dataset we need to extend the  {py:class}<code>~kedro.io.AbstractVersionedDataset</code> to:</p> <ul> <li>Accept a <code>version</code> keyword argument as part of the constructor</li> <li>Adapt the <code>load</code> and <code>save</code> method to use the versioned data path obtained from <code>_get_load_path</code> and <code>_get_save_path</code> respectively</li> </ul> <p>The following amends the full implementation of our basic <code>ImageDataset</code>. It now loads and saves data to and from a versioned subfolder (<code>data/01_raw/pokemon-images-and-types/images/images/pikachu.png/&lt;version&gt;/pikachu.png</code> with <code>version</code> being a datetime-formatted string <code>YYYY-MM-DDThh.mm.ss.sssZ</code> by default):</p> Click to expand <pre><code>from pathlib import PurePosixPath\nfrom typing import Any, Dict\n\nimport fsspec\nimport numpy as np\nfrom PIL import Image\n\nfrom kedro.io import AbstractVersionedDataset\nfrom kedro.io.core import get_filepath_str, get_protocol_and_path, Version\n\n\nclass ImageDataset(AbstractVersionedDataset[np.ndarray, np.ndarray]):\n    \"\"\"``ImageDataset`` loads / save image data from a given filepath as `numpy` array using Pillow.\n\n    Example:\n    ::\n\n        &gt;&gt;&gt; ImageDataset(filepath='/img/file/path.png')\n    \"\"\"\n\n    def __init__(self, filepath: str, version: Version = None):\n        \"\"\"Creates a new instance of ImageDataset to load / save image data for given filepath.\n\n        Args:\n            filepath: The location of the image file to load / save data.\n            version: The version of the dataset being saved and loaded.\n        \"\"\"\n        protocol, path = get_protocol_and_path(filepath)\n        self._protocol = protocol\n        self._fs = fsspec.filesystem(self._protocol)\n\n        super().__init__(\n            filepath=PurePosixPath(path),\n            version=version,\n            exists_function=self._fs.exists,\n            glob_function=self._fs.glob,\n        )\n\n    def load(self) -&gt; np.ndarray:\n        \"\"\"Loads data from the image file.\n\n        Returns:\n            Data from the image file as a numpy array\n        \"\"\"\n        load_path = get_filepath_str(self._get_load_path(), self._protocol)\n        with self._fs.open(load_path, mode=\"r\") as f:\n            image = Image.open(f).convert(\"RGBA\")\n            return np.asarray(image)\n\n    def save(self, data: np.ndarray) -&gt; None:\n        \"\"\"Saves image data to the specified filepath.\"\"\"\n        save_path = get_filepath_str(self._get_save_path(), self._protocol)\n        with self._fs.open(save_path, mode=\"wb\") as f:\n            image = Image.fromarray(data)\n            image.save(f)\n\n    def _describe(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n        return dict(\n            filepath=self._filepath, version=self._version, protocol=self._protocol\n        )\n</code></pre> <p>The difference between the original <code>ImageDataset</code> and the versioned <code>ImageDataset</code> is as follows:</p> Click to expand <pre><code> from pathlib import PurePosixPath\n from typing import Any, Dict\n\n import fsspec\n import numpy as np\n from PIL import Image\n\n-from kedro.io import AbstractDataset\n-from kedro.io.core import get_filepath_str, get_protocol_and_path\n+from kedro.io import AbstractVersionedDataset\n+from kedro.io.core import get_filepath_str, get_protocol_and_path, Version\n\n\n-class ImageDataset(AbstractDataset[np.ndarray, np.ndarray]):\n+class ImageDataset(AbstractVersionedDataset[np.ndarray, np.ndarray]):\n     \"\"\"``ImageDataset`` loads / save image data from a given filepath as `numpy` array using Pillow.\n\n     Example:\n     ::\n\n         &gt;&gt;&gt; ImageDataset(filepath='/img/file/path.png')\n     \"\"\"\n\n-    def __init__(self, filepath: str):\n+    def __init__(self, filepath: str, version: Version = None):\n         \"\"\"Creates a new instance of ImageDataset to load / save image data for given filepath.\n\n         Args:\n             filepath: The location of the image file to load / save data.\n+            version: The version of the dataset being saved and loaded.\n         \"\"\"\n         protocol, path = get_protocol_and_path(filepath)\n         self._protocol = protocol\n-        self._filepath = PurePosixPath(path)\n         self._fs = fsspec.filesystem(self._protocol)\n\n+        super().__init__(\n+            filepath=PurePosixPath(path),\n+            version=version,\n+            exists_function=self._fs.exists,\n+            glob_function=self._fs.glob,\n+        )\n+\n     def load(self) -&gt; np.ndarray:\n         \"\"\"Loads data from the image file.\n\n         Returns:\n             Data from the image file as a numpy array\n         \"\"\"\n-        load_path = get_filepath_str(self._filepath, self._protocol)\n+        load_path = get_filepath_str(self._get_load_path(), self._protocol)\n         with self._fs.open(load_path, mode=\"r\") as f:\n             image = Image.open(f).convert(\"RGBA\")\n             return np.asarray(image)\n\n     def save(self, data: np.ndarray) -&gt; None:\n         \"\"\"Saves image data to the specified filepath.\"\"\"\n-        save_path = get_filepath_str(self._filepath, self._protocol)\n+        save_path = get_filepath_str(self._get_save_path(), self._protocol)\n         with self._fs.open(save_path, mode=\"wb\") as f:\n             image = Image.fromarray(data)\n             image.save(f)\n\n     def _describe(self) -&gt; Dict[str, Any]:\n         \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n-        return dict(filepath=self._filepath, protocol=self._protocol)\n+        return dict(\n+            filepath=self._filepath, version=self._version, protocol=self._protocol\n+        )\n</code></pre> <p>To test the code, you need to enable versioning support in the data catalog:</p> <pre><code># in conf/base/catalog.yml\n\npikachu:\n  type: kedro_pokemon.datasets.image_dataset.ImageDataset\n  filepath: data/01_raw/pokemon-images-and-types/images/images/pikachu.png\n  versioned: true\n</code></pre> <pre><code>Using an HTTP(S)-based `filepath` with `versioned: true` is NOT supported.\n</code></pre> <p>Create an initial version of the data by creating an example first version (e.g. <code>2020-02-22T00.00.00.000Z</code>):</p> <pre><code>$ mv data/01_raw/pokemon-images-and-types/images/images/pikachu.png data/01_raw/pokemon-images-and-types/images/images/pikachu.png.backup\n$ mkdir -p data/01_raw/pokemon-images-and-types/images/images/pikachu.png/2020-02-22T00.00.00.000Z/\n$ mv data/01_raw/pokemon-images-and-types/images/images/pikachu.png.backup data/01_raw/pokemon-images-and-types/images/images/pikachu.png/2020-02-22T00.00.00.000Z/pikachu.png\n</code></pre> <p>The directory structure should look like the following:</p> <pre><code>data/01_raw/pokemon-images-and-types/images/images/pikachu.png\n\u2514\u2500\u2500 2020-02-22T00.00.00.000Z/\n    \u2514\u2500\u2500 pikachu.png\n</code></pre> <p>Launch an IPython shell to test load/save of the versioned data:</p> <pre><code># loading works as Kedro automatically find the latest available version inside `pikachu.png` directory\nIn [1]: img = context.catalog.load('pikachu')\n# then saving it should work as well\nIn [2]: context.catalog.save('pikachu', data=img)\n</code></pre> <p>Inspect the content of the data directory to find a new version of the data, written by <code>save</code>.</p>"},{"location":"pages/data/how_to_create_a_custom_dataset/#thread-safety","title":"Thread-safety","text":"<p>Kedro datasets should work with the {py:class}<code>~kedro.runner.SequentialRunner</code> and the {py:class}<code>~kedro.runner.ParallelRunner</code>, so they must be fully serialisable by the Python multiprocessing package. This means that your datasets should not make use of lambda functions, nested functions, closures etc. If you are using custom decorators, you need to ensure that they are using <code>functools.wraps()</code>.</p> <p>There is one dataset that is an exception: {class}<code>SparkDataset&lt;kedro-datasets:kedro_datasets.spark.SparkDataset&gt;</code>. The explanation for this exception is that Apache Spark uses its own parallelism and therefore doesn't work with Kedro {py:class}<code>~kedro.runner.ParallelRunner</code>. For parallelism within a Kedro project that uses Spark, use {py:class}<code>~kedro.runner.ThreadRunner</code> instead.</p> <p>To verify whether your dataset is serialisable by <code>multiprocessing</code>, use the console or an IPython session to try dumping it using <code>multiprocessing.reduction.ForkingPickler</code>:</p> <pre><code>dataset = context.catalog._datasets[\"pokemon\"]\nfrom multiprocessing.reduction import ForkingPickler\n\n# the following call shouldn't throw any errors\nForkingPickler.dumps(dataset)\n</code></pre>"},{"location":"pages/data/how_to_create_a_custom_dataset/#how-to-handle-credentials-and-different-filesystems","title":"How to handle credentials and different filesystems","text":"<p>If your use case requires them, Kedro allows you to pass <code>credentials</code> and filesystem-specific <code>fs_args</code> parameters to your dataset. For example, if the Pok\u00e9mon data sits in an S3 bucket, we can add the <code>credentials</code> and <code>fs_args</code> to the data catalog as follows:</p> <pre><code># in conf/base/catalog.yml\n\npikachu:\n  type: kedro_pokemon.datasets.image_dataset.ImageDataset\n  filepath: s3://data/01_raw/pokemon-images-and-types/images/images/pikachu.png\n  credentials: &lt;your_credentials&gt;\n  fs_args:\n    arg_1: &lt;value&gt;\n</code></pre> <p>These parameters are then passed to the dataset constructor so you can use them with <code>fsspec</code>:</p> <pre><code>import fsspec\n\n\nclass ImageDataset(AbstractVersionedDataset):\n    def __init__(\n        self,\n        filepath: str,\n        version: Version = None,\n        credentials: Dict[str, Any] = None,\n        fs_args: Dict[str, Any] = None,\n    ):\n        \"\"\"Creates a new instance of ImageDataset to load / save image data for given filepath.\n\n        Args:\n            filepath: The location of the image file to load / save data.\n            version: The version of the dataset being saved and loaded.\n            credentials: Credentials required to get access to the underlying filesystem.\n                E.g. for ``GCSFileSystem`` it should look like `{\"token\": None}`.\n            fs_args: Extra arguments to pass into underlying filesystem class.\n                E.g. for ``GCSFileSystem`` class: `{\"project\": \"my-project\", ...}`.\n        \"\"\"\n        protocol, path = get_protocol_and_path(filepath)\n        self._protocol = protocol\n        self._fs = fsspec.filesystem(self._protocol, **credentials, **fs_args)\n\n    ...\n</code></pre> <p>We provide additional examples of how to use parameters through the data catalog's YAML API. For an example of how to use these parameters in your dataset constructor, see the implementation of the {class}<code>SparkDataset&lt;kedro-datasets:kedro_datasets.spark.SparkDataset&gt;</code>.</p>"},{"location":"pages/data/how_to_create_a_custom_dataset/#how-to-contribute-a-custom-dataset-implementation","title":"How to contribute a custom dataset implementation","text":"<p>One of the easiest ways to contribute back to Kedro is to share a custom dataset. Kedro has a <code>kedro-datasets</code> package in <code>kedro-plugins</code> repository where you can add a new custom dataset implementation to share it with others. You can find out more in the Kedro contribution guide on GitHub.</p> <p>To contribute your custom dataset:</p> <ol> <li>Add your dataset package to <code>kedro-plugins/kedro-datasets/kedro_datasets/</code>.</li> </ol> <p>For example, in our <code>ImageDataset</code> example, the directory structure should be:</p> <pre><code>kedro-plugins/kedro-datasets/kedro_datasets/image\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 image_dataset.py\n</code></pre> <ol> <li> <p>If the dataset is complex, create a <code>README.md</code> file to explain how it works and document its API.</p> </li> <li> <p>The dataset should be accompanied by full test coverage in <code>kedro-plugins/kedro-datasets/tests/</code>.</p> </li> <li> <p>Make a pull request against the <code>main</code> branch of Kedro's plugin repository.</p> </li> </ol> <pre><code>There are two special considerations when contributing a dataset:\n\n   1. Add the dataset to `kedro_datasets.rst` so it shows up in the API documentation.\n   2. Add the dataset to `kedro-plugins/kedro-datasets/static/jsonschema/kedro-catalog-X.json` for IDE validation.\n\n</code></pre>"},{"location":"pages/data/kedro_data_catalog/","title":"Kedro Data Catalog","text":"<p><code>KedroDataCatalog</code> retains the core functionality of <code>DataCatalog</code>, with a few API enhancements. For a comprehensive understanding, we recommend reviewing the existing <code>DataCatalog</code> documentation before exploring the additional functionality of <code>KedroDataCatalog</code>.</p> <p>This page highlights the new features and provides usage examples: * How to make KedroDataCatalog the default catalog for Kedro run * How to access datasets in the catalog * How to add datasets to the catalog * How to iterate trough datasets in the catalog * How to get the number of datasets in the catalog * How to print the full catalog and individual datasets * How to access dataset patterns</p>"},{"location":"pages/data/kedro_data_catalog/#how-to-make-kedrodatacatalog-the-default-catalog-for-kedro-run","title":"How to make <code>KedroDataCatalog</code> the default catalog for Kedro <code>run</code>","text":"<p>To set <code>KedroDataCatalog</code> as the default catalog for the <code>kedro run</code> command and other CLI commands, update your <code>settings.py</code> as follows:</p> <pre><code>from kedro.io import KedroDataCatalog\n\nDATA_CATALOG_CLASS = KedroDataCatalog\n</code></pre> <p>Once this change is made, you can run your Kedro project as usual.</p> <p>For more information on <code>settings.py</code>, refer to the Project settings documentation.</p>"},{"location":"pages/data/kedro_data_catalog/#how-to-access-datasets-in-the-catalog","title":"How to access datasets in the catalog","text":"<p>You can retrieve a dataset from the catalog using either the dictionary-like syntax or the <code>get</code> method:</p> <pre><code>reviews_ds = catalog[\"reviews\"]\nreviews_ds = catalog.get(\"reviews\", default=default_ds)\n</code></pre>"},{"location":"pages/data/kedro_data_catalog/#how-to-add-datasets-to-the-catalog","title":"How to add datasets to the catalog","text":"<p>The new API allows you to add datasets as well as raw data directly to the catalog:</p> <pre><code>from kedro_datasets.pandas import CSVDataset\n\nbikes_ds = CSVDataset(filepath=\"../data/01_raw/bikes.csv\")\ncatalog[\"bikes\"] = bikes_ds  # Adding a dataset\ncatalog[\"cars\"] = [\"Ferrari\", \"Audi\"]  # Adding raw data\n</code></pre> <p>When you add raw data, it is automatically wrapped in a <code>MemoryDataset</code> under the hood.</p>"},{"location":"pages/data/kedro_data_catalog/#how-to-iterate-trough-datasets-in-the-catalog","title":"How to iterate trough datasets in the catalog","text":"<p><code>KedroDataCatalog</code> supports iteration over dataset names (keys), datasets (values), and both (items). Iteration defaults to dataset names, similar to standard Python dictionaries:</p> <pre><code>for ds_name in catalog:  # __iter__ defaults to keys\n    pass\n\nfor ds_name in catalog.keys():  # Iterate over dataset names\n    pass\n\nfor ds in catalog.values():  # Iterate over datasets\n    pass\n\nfor ds_name, ds in catalog.items():  # Iterate over (name, dataset) tuples\n    pass\n</code></pre>"},{"location":"pages/data/kedro_data_catalog/#how-to-get-the-number-of-datasets-in-the-catalog","title":"How to get the number of datasets in the catalog","text":"<p>You can get the number of datasets in the catalog using the <code>len()</code> function:</p> <pre><code>ds_count = len(catalog)\n</code></pre>"},{"location":"pages/data/kedro_data_catalog/#how-to-print-the-full-catalog-and-individual-datasets","title":"How to print the full catalog and individual datasets","text":"<p>To print the catalog or an individual dataset programmatically, use the <code>print()</code> function or in an interactive environment like IPython or JupyterLab, simply enter the variable:</p> <pre><code>In [1]: catalog\nOut[1]: {'shuttles': kedro_datasets.pandas.excel_dataset.ExcelDataset(filepath=PurePosixPath('/data/01_raw/shuttles.xlsx'), protocol='file', load_args={'engine': 'openpyxl'}, save_args={'index': False}, writer_args={'engine': 'openpyxl'}), 'preprocessed_companies': kedro_datasets.pandas.parquet_dataset.ParquetDataset(filepath=PurePosixPath('/data/02_intermediate/preprocessed_companies.pq'), protocol='file', load_args={}, save_args={}), 'params:model_options.test_size': kedro.io.memory_dataset.MemoryDataset(data='&lt;float&gt;'), 'params:model_options.features': kedro.io.memory_dataset.MemoryDataset(data='&lt;list&gt;'))}\n\nIn [2]: catalog[\"shuttles\"]\nOut[2]: kedro_datasets.pandas.excel_dataset.ExcelDataset(filepath=PurePosixPath('/data/01_raw/shuttles.xlsx'), protocol='file', load_args={'engine': 'openpyxl'}, save_args={'index': False}, writer_args={'engine': 'openpyxl'})\n</code></pre>"},{"location":"pages/data/kedro_data_catalog/#how-to-access-dataset-patterns","title":"How to access dataset patterns","text":"<p>The pattern resolution logic in <code>KedroDataCatalog</code> is handled by the <code>config_resolver</code>, which can be accessed as a property of the catalog:</p> <pre><code>config_resolver = catalog.config_resolver\nds_config = catalog.config_resolver.resolve_pattern(ds_name)  # Resolving a dataset pattern\npatterns = catalog.config_resolver.list_patterns() # Listing all available patterns\n</code></pre> <pre><code>`KedroDataCatalog` does not support all dictionary-specific methods, such as `pop()`, `popitem()`, or deletion by key (`del`).\n</code></pre> <p>For a full list of supported methods, refer to the KedroDataCatalog source code.</p>"},{"location":"pages/data/kedro_dataset_factories/","title":"Kedro dataset factories","text":"<p>You can load multiple datasets with similar configuration using dataset factories, introduced in Kedro <code>0.18.12</code>.</p> <pre><code>Datasets are not included in the core Kedro package from Kedro version **`0.19.0`**. Import them from the [`kedro-datasets`](https://github.com/kedro-org/kedro-plugins/tree/main/kedro-datasets) package instead.\nFrom version **`2.0.0`** of `kedro-datasets`, all dataset names have changed to replace the capital letter \"S\" in \"DataSet\" with a lower case \"s\". For example, `CSVDataSet` is now `CSVDataset`.\n</code></pre> <p>The dataset factories introduce a syntax that allows you to generalise your configuration and reduce the number of similar catalog entries by matching datasets used in your project's pipelines to dataset factory patterns.</p> <p>For example:</p> <pre><code>factory_data:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/factory_data.csv\n\nprocess_data:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/process_data.csv\n</code></pre> <p>With dataset factory, it can be re-written as:</p> <pre><code>\"{name}_data\":\n  type: pandas.CSVDataset\n  filepath: data/01_raw/{name}_data.csv\n</code></pre> <p>In runtime, the pattern will be matched against the name of the datasets defined in <code>inputs</code> or <code>outputs</code>.</p> <pre><code>node(\n    func=process_factory,\n    inputs=\"factory_data\",\n    outputs=\"process_data\",\n),\n\n...\n</code></pre> <pre><code>The factory pattern must always be enclosed in quotes to avoid YAML parsing errors.\n</code></pre> <p>Dataset factories is similar to regular expression and you can think of it as reversed <code>f-string</code>. In this case, the name of the input dataset <code>factory_data</code> matches the pattern <code>{name}_data</code> with the <code>_data</code> suffix, so it resolves <code>name</code> to <code>factory</code>. Similarly, it resolves <code>name</code> to <code>process</code> for the output dataset <code>process_data</code>.</p> <p>This allows you to use one dataset factory pattern to replace multiple datasets entries. It keeps your catalog concise and you can generalise datasets using similar names, type or namespaces.</p>"},{"location":"pages/data/kedro_dataset_factories/#how-to-generalise-datasets-of-the-same-type","title":"How to generalise datasets of the same type","text":"<p>You can also combine all the datasets with the same type and configuration details. For example, consider the following catalog with three datasets named <code>boats</code>, <code>cars</code> and <code>planes</code> of the type <code>pandas.CSVDataset</code>:</p> <pre><code>boats:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/shuttles.csv\n\ncars:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/reviews.csv\n\nplanes:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/companies.csv\n</code></pre> <p>These datasets can be combined into the following dataset factory:</p> <pre><code>\"{dataset_name}#csv\":\n  type: pandas.CSVDataset\n  filepath: data/01_raw/{dataset_name}.csv\n</code></pre> <p>You will then have to update the pipelines in your project located at <code>src/&lt;project_name&gt;/&lt;pipeline_name&gt;/pipeline.py</code> to refer to these datasets as <code>boats#csv</code>, <code>cars#csv</code> and <code>planes#csv</code>. Adding a suffix or a prefix to the dataset names and the dataset factory patterns, like <code>#csv</code> here, ensures that the dataset names are matched with the intended pattern.</p> <pre><code>from .nodes import create_model_input_table, preprocess_companies, preprocess_shuttles\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        [\n            node(\n                func=preprocess_boats,\n                inputs=\"boats#csv\",\n                outputs=\"preprocessed_boats\",\n                name=\"preprocess_boats_node\",\n            ),\n            node(\n                func=preprocess_cars,\n                inputs=\"cars#csv\",\n                outputs=\"preprocessed_cars\",\n                name=\"preprocess_cars_node\",\n            ),\n            node(\n                func=preprocess_planes,\n                inputs=\"planes#csv\",\n                outputs=\"preprocessed_planes\",\n                name=\"preprocess_planes_node\",\n            ),\n            node(\n                func=create_model_input_table,\n                inputs=[\n                    \"preprocessed_boats\",\n                    \"preprocessed_planes\",\n                    \"preprocessed_cars\",\n                ],\n                outputs=\"model_input_table\",\n                name=\"create_model_input_table_node\",\n            ),\n        ]\n    )\n</code></pre>"},{"location":"pages/data/kedro_dataset_factories/#how-to-generalise-datasets-using-namespaces","title":"How to generalise datasets using namespaces","text":"<p>You can also generalise the catalog entries for datasets belonging to namespaced modular pipelines. Consider the following pipeline which takes in a <code>model_input_table</code> and outputs two regressors belonging to the <code>active_modelling_pipeline</code> and the <code>candidate_modelling_pipeline</code> namespaces:</p> <pre><code>from kedro.pipeline import Pipeline, node\nfrom kedro.pipeline.modular_pipeline import pipeline\n\nfrom .nodes import evaluate_model, split_data, train_model\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    pipeline_instance = pipeline(\n        [\n            node(\n                func=split_data,\n                inputs=[\"model_input_table\", \"params:model_options\"],\n                outputs=[\"X_train\", \"y_train\"],\n                name=\"split_data_node\",\n            ),\n            node(\n                func=train_model,\n                inputs=[\"X_train\", \"y_train\"],\n                outputs=\"regressor\",\n                name=\"train_model_node\",\n            ),\n        ]\n    )\n    ds_pipeline_1 = pipeline(\n        pipe=pipeline_instance,\n        inputs=\"model_input_table\",\n        namespace=\"active_modelling_pipeline\",\n    )\n    ds_pipeline_2 = pipeline(\n        pipe=pipeline_instance,\n        inputs=\"model_input_table\",\n        namespace=\"candidate_modelling_pipeline\",\n    )\n\n    return ds_pipeline_1 + ds_pipeline_2\n</code></pre> <p>You can now have one dataset factory pattern in your catalog instead of two separate entries for <code>active_modelling_pipeline.regressor</code> and <code>candidate_modelling_pipeline.regressor</code> as below:</p> <pre><code>\"{namespace}.regressor\":\n  type: pickle.PickleDataset\n  filepath: data/06_models/regressor_{namespace}.pkl\n  versioned: true\n</code></pre>"},{"location":"pages/data/kedro_dataset_factories/#how-to-generalise-datasets-of-the-same-type-in-different-layers","title":"How to generalise datasets of the same type in different layers","text":"<p>You can use multiple placeholders in the same pattern. For example, consider the following catalog where the dataset entries share <code>type</code>, <code>file_format</code> and <code>save_args</code>:</p> <pre><code>processing.factory_data:\n  type: spark.SparkDataset\n  filepath: data/processing/factory_data.parquet\n  file_format: parquet\n  save_args:\n    mode: overwrite\n\nprocessing.process_data:\n  type: spark.SparkDataset\n  filepath: data/processing/process_data.parquet\n  file_format: parquet\n  save_args:\n    mode: overwrite\n\nmodelling.metrics:\n  type: spark.SparkDataset\n  filepath: data/modelling/factory_data.parquet\n  file_format: parquet\n  save_args:\n    mode: overwrite\n</code></pre> <p>This could be generalised to the following pattern:</p> <pre><code>\"{layer}.{dataset_name}\":\n  type: spark.SparkDataset\n  filepath: data/{layer}/{dataset_name}.parquet\n  file_format: parquet\n  save_args:\n    mode: overwrite\n</code></pre> <p>All the placeholders used in the catalog entry body must exist in the factory pattern name.</p>"},{"location":"pages/data/kedro_dataset_factories/#how-to-generalise-datasets-using-multiple-dataset-factories","title":"How to generalise datasets using multiple dataset factories","text":"<p>You can have multiple dataset factories in your catalog. For example:</p> <pre><code>\"{namespace}.{dataset_name}@spark\":\n  type: spark.SparkDataset\n  filepath: data/{namespace}/{dataset_name}.parquet\n  file_format: parquet\n\n\"{dataset_name}@csv\":\n  type: pandas.CSVDataset\n  filepath: data/01_raw/{dataset_name}.csv\n</code></pre> <p>Having multiple dataset factories in your catalog can lead to a situation where a dataset name from your pipeline might match multiple patterns. To overcome this, Kedro sorts all the potential matches for the dataset name in the pipeline and picks the best match. The matches are ranked according to the following criteria:</p> <ol> <li>Number of exact character matches between the dataset name and the factory pattern. For example, a dataset named <code>factory_data$csv</code> would match <code>{dataset}_data$csv</code> over <code>{dataset_name}$csv</code>.</li> <li>Number of placeholders. For example, the dataset <code>preprocessing.shuttles+csv</code> would match <code>{namespace}.{dataset}+csv</code> over <code>{dataset}+csv</code>.</li> <li>Alphabetical order</li> </ol>"},{"location":"pages/data/kedro_dataset_factories/#how-to-override-the-default-dataset-creation-with-dataset-factories","title":"How to override the default dataset creation with dataset factories","text":"<p>You can use dataset factories to define a catch-all pattern which will overwrite the default {py:class}<code>~kedro.io.MemoryDataset</code> creation.</p> <pre><code>\"{default_dataset}\":\n  type: pandas.CSVDataset\n  filepath: data/{default_dataset}.csv\n\n</code></pre> <p>Kedro will now treat all the datasets mentioned in your project's pipelines that do not appear as specific patterns or explicit entries in your catalog as <code>pandas.CSVDataset</code>.</p>"},{"location":"pages/data/kedro_dataset_factories/#cli-commands-for-dataset-factories","title":"CLI commands for dataset factories","text":"<p>To manage your dataset factories, two new commands have been added to the Kedro CLI: <code>kedro catalog rank</code> (0.18.12) and <code>kedro catalog resolve</code> (0.18.13).</p>"},{"location":"pages/data/kedro_dataset_factories/#how-to-use-kedro-catalog-rank","title":"How to use <code>kedro catalog rank</code>","text":"<p>This command outputs a list of all dataset factories in the catalog, ranked in the order by which pipeline datasets are matched against them. The ordering is determined by the following criteria:</p> <ol> <li>The number of non-placeholder characters in the pattern</li> <li>The number of placeholders in the pattern</li> <li>Alphabetic ordering</li> </ol> <p>Consider a catalog file with the following patterns:</p> Click to expand <pre><code>\"{layer}.{dataset_name}\":\n  type: pandas.CSVDataset\n  filepath: data/{layer}/{dataset_name}.csv\n\n\"preprocessed_{dataset_name}\":\n  type: pandas.ParquetDataset\n  filepath: data/02_intermediate/preprocessed_{dataset_name}.parquet\n\n\"processed_{dataset_name}\":\n  type: pandas.ParquetDataset\n  filepath: data/03_primary/processed_{dataset_name}.parquet\n\n\"{dataset_name}_csv\":\n  type: pandas.CSVDataset\n  filepath: data/03_primary/{dataset_name}.csv\n\n\"{namespace}.{dataset_name}_pq\":\n  type: pandas.ParquetDataset\n  filepath: data/03_primary/{dataset_name}_{namespace}.parquet\n\n\"{default_dataset}\":\n  type: pickle.PickleDataset\n  filepath: data/01_raw/{default_dataset}.pickle\n</code></pre> <p>Running <code>kedro catalog rank</code> will result in the following output:</p> <pre><code>- 'preprocessed_{dataset_name}'\n- 'processed_{dataset_name}'\n- '{namespace}.{dataset_name}_pq'\n- '{dataset_name}_csv'\n- '{layer}.{dataset_name}'\n- '{default_dataset}'\n</code></pre> <p>As we can see, the entries are ranked firstly by how many non-placeholders are in the pattern, in descending order. Where two entries have the same number of non-placeholder characters, <code>{namespace}.{dataset_name}_pq</code> and <code>{dataset_name}_csv</code> with four each, they are then ranked by the number of placeholders, also in decreasing order. <code>{default_dataset}</code> is the least specific pattern possible, and will always be matched against last.</p>"},{"location":"pages/data/kedro_dataset_factories/#how-to-use-kedro-catalog-resolve","title":"How to use <code>kedro catalog resolve</code>","text":"<p>This command resolves dataset patterns in the catalog against any explicit dataset entries in the project pipeline. The resulting output contains all explicit dataset entries in the catalog and any dataset in the default pipeline that resolves some dataset pattern. This command is runner agnostic and thus won't take into account any default dataset creation defined in the runner.</p> <p>To illustrate this, consider the following catalog file:</p> Click to expand <pre><code>companies:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/companies.csv\n\nreviews:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/reviews.csv\n\nshuttles:\n  type: pandas.ExcelDataset\n  filepath: data/01_raw/shuttles.xlsx\n  load_args:\n    engine: openpyxl # Use modern Excel engine, it is the default since Kedro 0.18.0\n\n\"preprocessed_{name}\":\n  type: pandas.ParquetDataset\n  filepath: data/02_intermediate/preprocessed_{name}.parquet\n\n\"{default}\":\n  type: pandas.ParquetDataset\n  filepath: data/03_primary/{default}.parquet\n</code></pre> <p>and the following pipeline in <code>pipeline.py</code>:</p> Click to expand <pre><code>def create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        [\n            node(\n                func=preprocess_companies,\n                inputs=\"companies\",\n                outputs=\"preprocessed_companies\",\n                name=\"preprocess_companies_node\",\n            ),\n            node(\n                func=preprocess_shuttles,\n                inputs=\"shuttles\",\n                outputs=\"preprocessed_shuttles\",\n                name=\"preprocess_shuttles_node\",\n            ),\n            node(\n                func=create_model_input_table,\n                inputs=[\"preprocessed_shuttles\", \"preprocessed_companies\", \"reviews\"],\n                outputs=\"model_input_table\",\n                name=\"create_model_input_table_node\",\n            ),\n        ]\n    )\n</code></pre> <p>The resolved catalog output by the command will be as follows:</p> Click to expand <pre><code>companies:\n  filepath: data/01_raw/companies.csv\n  type: pandas.CSVDataset\nmodel_input_table:\n  filepath: data/03_primary/model_input_table.parquet\n  type: pandas.ParquetDataset\npreprocessed_companies:\n  filepath: data/02_intermediate/preprocessed_companies.parquet\n  type: pandas.ParquetDataset\npreprocessed_shuttles:\n  filepath: data/02_intermediate/preprocessed_shuttles.parquet\n  type: pandas.ParquetDataset\nreviews:\n  filepath: data/01_raw/reviews.csv\n  type: pandas.CSVDataset\nshuttles:\n  filepath: data/01_raw/shuttles.xlsx\n  load_args:\n    engine: openpyxl\n  type: pandas.ExcelDataset\n</code></pre> <p>By default this is output to the terminal. However, if you wish to output the resolved catalog to a specific file, you can use the redirection operator <code>&gt;</code>:</p> <pre><code>kedro catalog resolve &gt; output_file.yaml\n</code></pre>"},{"location":"pages/data/partitioned_and_incremental_datasets/","title":"Advanced: Partitioned and incremental datasets","text":""},{"location":"pages/data/partitioned_and_incremental_datasets/#partitioned-datasets","title":"Partitioned datasets","text":"<p>Distributed systems play an increasingly important role in ETL data pipelines. They increase the processing throughput, enabling us to work with much larger volumes of input data. A situation may arise where your Kedro node needs to read the data from a directory full of uniform files of the same type like JSON or CSV. Tools like <code>PySpark</code> and the corresponding {class}<code>SparkDataset&lt;kedro-datasets:kedro_datasets.spark.SparkDataset&gt;</code> cater for such use cases but may not always be possible.</p> <p>This is why Kedro provides {class}<code>PartitionedDataset&lt;kedro-datasets:kedro_datasets.partitions.PartitionedDataset&gt;</code> with the following features:</p> <ul> <li><code>PartitionedDataset</code> can recursively load/save all or specific files from a given location.</li> <li>It is platform agnostic, and can work with any filesystem implementation supported by fsspec including local, S3, GCS, and many more.</li> <li>It implements a lazy loading approach, and does not attempt to load any partition data until a processing node explicitly requests it.</li> <li>It supports lazy saving by using <code>Callable</code>s.</li> </ul> <pre><code>In this section, each individual file inside a given location is called a partition.\n</code></pre>"},{"location":"pages/data/partitioned_and_incremental_datasets/#how-to-use-partitioneddataset","title":"How to use <code>PartitionedDataset</code>","text":"<p>You can use a <code>PartitionedDataset</code> in <code>catalog.yml</code> file like any other regular dataset definition:</p> <pre><code># conf/base/catalog.yml\n\nmy_partitioned_dataset:\n  type: partitions.PartitionedDataset\n  path: s3://my-bucket-name/path/to/folder  # path to the location of partitions\n  dataset: pandas.CSVDataset  # shorthand notation for the dataset which will handle individual partitions\n  credentials: my_credentials\n  load_args:\n    load_arg1: value1\n    load_arg2: value2\n</code></pre> <pre><code>Like any other dataset, `PartitionedDataset` can also be instantiated programmatically in Python:\n</code></pre> <pre><code>from kedro_datasets.pandas import CSVDataset\nfrom kedro_datasets.partitions import PartitionedDataset\n\nmy_credentials = {...}  # credentials dictionary\n\nmy_partitioned_dataset = PartitionedDataset(\n    path=\"s3://my-bucket-name/path/to/folder\",\n    dataset=CSVDataset,\n    credentials=my_credentials,\n    load_args={\"load_arg1\": \"value1\", \"load_arg2\": \"value2\"},\n)\n</code></pre> <p>Alternatively, if you need more granular configuration of the underlying dataset, its definition can be provided in full:</p> <pre><code># conf/base/catalog.yml\n\nmy_partitioned_dataset:\n  type: partitions.PartitionedDataset\n  path: s3://my-bucket-name/path/to/folder\n  dataset:  # full dataset config notation\n    type: pandas.CSVDataset\n    load_args:\n      delimiter: \",\"\n    save_args:\n      index: false\n  credentials: my_credentials\n  load_args:\n    load_arg1: value1\n    load_arg2: value2\n  filepath_arg: filepath  # the argument of the dataset to pass the filepath to\n  filename_suffix: \".csv\"\n</code></pre> <p>Here is an exhaustive list of the arguments supported by <code>PartitionedDataset</code>:</p> Argument Required Supported types Description <code>path</code> Yes <code>str</code> Path to the folder containing partitioned data. If path starts with the protocol (e.g., <code>s3://</code>) then the corresponding <code>fsspec</code> concrete filesystem implementation will be used. If protocol is not specified, local filesystem will be used <code>dataset</code> Yes <code>str</code>, <code>Type[AbstractDataset]</code>, <code>Dict[str, Any]</code> Underlying dataset definition, for more details see the section below <code>credentials</code> No <code>Dict[str, Any]</code> Protocol-specific options that will be passed to <code>fsspec.filesystemcall</code>, for more details see the section below <code>load_args</code> No <code>Dict[str, Any]</code> Keyword arguments to be passed into <code>find()</code> method of the corresponding filesystem implementation <code>filepath_arg</code> No <code>str</code> (defaults to <code>filepath</code>) Argument name of the underlying dataset initialiser that will contain a path to an individual partition <code>filename_suffix</code> No <code>str</code> (defaults to an empty string) If specified, partitions that don't end with this string will be ignored"},{"location":"pages/data/partitioned_and_incremental_datasets/#dataset-definition","title":"Dataset definition","text":"<p>The dataset definition should be passed into the <code>dataset</code> argument of the <code>PartitionedDataset</code>. The dataset definition is used to instantiate a new dataset object for each individual partition, and use that dataset object for load and save operations. Dataset definition supports shorthand and full notations.</p>"},{"location":"pages/data/partitioned_and_incremental_datasets/#shorthand-notation","title":"Shorthand notation","text":"<p>Requires you only to specify a class of the underlying dataset either as a string (e.g. <code>pandas.CSVDataset</code> or a fully qualified class path like <code>kedro_datasets.pandas.CSVDataset</code>) or as a class object that is a subclass of the {py:class}<code>~kedro.io.AbstractDataset</code>.</p>"},{"location":"pages/data/partitioned_and_incremental_datasets/#full-notation","title":"Full notation","text":"<p>Full notation allows you to specify a dictionary with the full underlying dataset definition except the following arguments: * The argument that receives the partition path (<code>filepath</code> by default) - if specified, a <code>UserWarning</code> will be emitted stating that this value will be overridden by individual partition paths * <code>credentials</code> key - specifying it will result in a <code>DatasetError</code> being raised; dataset credentials should be passed into the <code>credentials</code> argument of the <code>PartitionedDataset</code> rather than the underlying dataset definition - see the section below on partitioned dataset credentials for details * <code>versioned</code> flag - specifying it will result in a <code>DatasetError</code> being raised; versioning cannot be enabled for the underlying datasets</p>"},{"location":"pages/data/partitioned_and_incremental_datasets/#partitioned-dataset-credentials","title":"Partitioned dataset credentials","text":"<pre><code>Support for `dataset_credentials` key in the credentials for `PartitionedDataset` is now deprecated. The dataset credentials should be specified explicitly inside the dataset config.\n</code></pre> <p>Credentials management for <code>PartitionedDataset</code> is somewhat special, because it might contain credentials for both <code>PartitionedDataset</code> itself and the underlying dataset that is used for partition load and save. Top-level credentials are passed to the underlying dataset config (unless such config already has credentials configured), but not the other way around - dataset credentials are never propagated to the filesystem.</p> <p>Here is the full list of possible scenarios:</p> Top-level credentials Underlying dataset credentials Example <code>PartitionedDataset</code> definition Description Undefined Undefined <code>PartitionedDataset(path=\"s3://bucket-name/path/to/folder\", dataset=\"pandas.CSVDataset\")</code> Credentials are not passed to the underlying dataset or the filesystem Undefined Specified <code>PartitionedDataset(path=\"s3://bucket-name/path/to/folder\", dataset={\"type\": \"pandas.CSVDataset\", \"credentials\": {\"secret\": True}})</code> Underlying dataset credentials are passed to the <code>CSVDataset</code> constructor, filesystem is instantiated without credentials Specified Undefined <code>PartitionedDataset(path=\"s3://bucket-name/path/to/folder\", dataset=\"pandas.CSVDataset\", credentials={\"secret\": True})</code> Top-level credentials are passed to the underlying <code>CSVDataset</code> constructor and the filesystem Specified <code>None</code> <code>PartitionedDataset(path=\"s3://bucket-name/path/to/folder\", dataset={\"type\": \"pandas.CSVDataset\", \"credentials\": None}, credentials={\"dataset_secret\": True})</code> Top-level credentials are passed to the filesystem, <code>CSVDataset</code> is instantiated without credentials - this way you can stop the top-level credentials from propagating into the dataset config Specified Specified <code>PartitionedDataset(path=\"s3://bucket-name/path/to/folder\", dataset={\"type\": \"pandas.CSVDataset\", \"credentials\": {\"dataset_secret\": True}}, credentials={\"secret\": True})</code> Top-level credentials are passed to the filesystem, underlying dataset credentials are passed to the <code>CSVDataset</code> constructor"},{"location":"pages/data/partitioned_and_incremental_datasets/#partitioned-dataset-load","title":"Partitioned dataset load","text":"<p>Let's assume that the Kedro pipeline that you are working with contains the node, defined as follows:</p> <pre><code>from kedro.pipeline import node\n\nnode(concat_partitions, inputs=\"my_partitioned_dataset\", outputs=\"concatenated_result\")\n</code></pre> <p>The underlying node function <code>concat_partitions</code> might look like this:</p> <pre><code>from typing import Any, Callable, Dict\nimport pandas as pd\n\n\ndef concat_partitions(partitioned_input: Dict[str, Callable[[], Any]]) -&gt; pd.DataFrame:\n    \"\"\"Concatenate input partitions into one pandas DataFrame.\n\n    Args:\n        partitioned_input: A dictionary with partition ids as keys and load functions as values.\n\n    Returns:\n        Pandas DataFrame representing a concatenation of all loaded partitions.\n    \"\"\"\n    result = pd.DataFrame()\n\n    for partition_key, partition_load_func in sorted(partitioned_input.items()):\n        partition_data = partition_load_func()  # load the actual partition data\n        # concat with existing result\n        result = pd.concat([result, partition_data], ignore_index=True, sort=True)\n\n    return result\n</code></pre> <p>As you can see from the above example, on load <code>PartitionedDataset</code> does not automatically load the data from the located partitions. Instead, <code>PartitionedDataset</code> returns a dictionary with partition IDs as keys and the corresponding load functions as values. It allows the node that consumes the <code>PartitionedDataset</code> to implement the logic that defines what partitions need to be loaded, and how this data is going to be processed.</p> <p>Partition ID does not represent the whole partition path, but only a part of it that is unique for a given partition and filename suffix:</p> <ul> <li> <p>Example 1: if <code>path=s3://my-bucket-name/folder</code> and partition is stored in <code>s3://my-bucket-name/folder/2019-12-04/data.csv</code>, then its Partition ID is <code>2019-12-04/data.csv</code>.</p> </li> <li> <p>Example 2: if <code>path=s3://my-bucket-name/folder</code> and <code>filename_suffix=\".csv\"</code> and partition is stored in <code>s3://my-bucket-name/folder/2019-12-04/data.csv</code>, then its Partition ID is <code>2019-12-04/data</code>.</p> </li> </ul> <p><code>PartitionedDataset</code> implements caching on load operation, which means that if multiple nodes consume the same <code>PartitionedDataset</code>, they will all receive the same partition dictionary even if some new partitions were added to the folder after the first load has been completed. This is done deliberately to guarantee the consistency of load operations between the nodes and avoid race conditions. To reset the cache, call the <code>release()</code> method of the partitioned dataset object.</p>"},{"location":"pages/data/partitioned_and_incremental_datasets/#partitioned-dataset-save","title":"Partitioned dataset save","text":"<p><code>PartitionedDataset</code> also supports a save operation. Let's assume the following configuration:</p> <pre><code># conf/base/catalog.yml\n\nnew_partitioned_dataset:\n  type: partitions.PartitionedDataset\n  path: s3://my-bucket-name\n  dataset: pandas.CSVDataset\n  filename_suffix: \".csv\"\n  save_lazily: True\n</code></pre> <p>Here is the node definition:</p> <pre><code>from kedro.pipeline import node\n\nnode(create_partitions, inputs=None, outputs=\"new_partitioned_dataset\")\n</code></pre> <p>The underlying node function is as follows in <code>create_partitions</code>:</p> <pre><code>from typing import Any, Dict\nimport pandas as pd\n\n\ndef create_partitions() -&gt; Dict[str, Any]:\n    \"\"\"Create new partitions and save using PartitionedDataset.\n\n    Returns:\n        Dictionary with the partitions to create.\n    \"\"\"\n    return {\n        # create a file \"s3://my-bucket-name/part/foo.csv\"\n        \"part/foo\": pd.DataFrame({\"data\": [1, 2]}),\n        # create a file \"s3://my-bucket-name/part/bar.csv.csv\"\n        \"part/bar.csv\": pd.DataFrame({\"data\": [3, 4]}),\n    }\n</code></pre> <pre><code>Writing to an existing partition may result in its data being overwritten, if this case is not specifically handled by the underlying dataset implementation. You should implement your own checks to ensure that no existing data is lost when writing to a `PartitionedDataset`. The simplest safety mechanism could be to use partition IDs with a high chance of uniqueness: for example, the current timestamp.\n</code></pre>"},{"location":"pages/data/partitioned_and_incremental_datasets/#partitioned-dataset-lazy-saving","title":"Partitioned dataset lazy saving","text":"<p><code>PartitionedDataset</code> also supports lazy saving, where the partition's data is not materialised until it is time to write.</p> <p>To use this, simply return <code>Callable</code> types in the dictionary:</p> <pre><code>from typing import Any, Dict, Callable\nimport pandas as pd\n\n\ndef create_partitions() -&gt; Dict[str, Callable[[], Any]]:\n    \"\"\"Create new partitions and save using PartitionedDataset.\n\n    Returns:\n        Dictionary of the partitions to create to a function that creates them.\n    \"\"\"\n    return {\n        # create a file \"s3://my-bucket-name/part/foo.csv\"\n        \"part/foo\": lambda: pd.DataFrame({\"data\": [1, 2]}),\n        # create a file \"s3://my-bucket-name/part/bar.csv\"\n        \"part/bar\": lambda: pd.DataFrame({\"data\": [3, 4]}),\n    }\n</code></pre> <pre><code>When using lazy saving, the dataset will be written _after_ the `after_node_run` [hook](../hooks/introduction).\n</code></pre> <pre><code>Lazy saving is the default behaviour, meaning that if a `Callable` type is provided, the dataset will be written _after_ the `after_node_run` hook is executed.\n</code></pre> <p>In certain cases, it might be useful to disable lazy saving, such as when your object is already a <code>Callable</code> (e.g., a TensorFlow model) and you do not intend to save it lazily. To disable the lazy saving set <code>save_lazily</code> parameter to <code>False</code>:</p> <pre><code># conf/base/catalog.yml\n\nnew_partitioned_dataset:\n  type: partitions.PartitionedDataset\n  path: s3://my-bucket-name\n  dataset: pandas.CSVDataset\n  filename_suffix: \".csv\"\n  save_lazily: False\n</code></pre>"},{"location":"pages/data/partitioned_and_incremental_datasets/#incremental-datasets","title":"Incremental datasets","text":"<p>{class}<code>IncrementalDataset&lt;kedro-datasets:kedro_datasets.partitions.IncrementalDataset&gt;</code> is a subclass of <code>PartitionedDataset</code>, which stores the information about the last processed partition in the so-called <code>checkpoint</code>. <code>IncrementalDataset</code> addresses the use case when partitions have to be processed incrementally, that is, each subsequent pipeline run should process just the partitions which were not processed by the previous runs.</p> <p>This checkpoint, by default, is persisted to the location of the data partitions. For example, for <code>IncrementalDataset</code> instantiated with path <code>s3://my-bucket-name/path/to/folder</code>, the checkpoint will be saved to <code>s3://my-bucket-name/path/to/folder/CHECKPOINT</code>, unless the checkpoint configuration is explicitly overwritten.</p> <p>The checkpoint file is only created after the partitioned dataset is explicitly confirmed.</p>"},{"location":"pages/data/partitioned_and_incremental_datasets/#incremental-dataset-loads","title":"Incremental dataset loads","text":"<p>Loading <code>IncrementalDataset</code> works similarly to <code>PartitionedDataset</code> with several exceptions: 1. <code>IncrementalDataset</code> loads the data eagerly, so the values in the returned dictionary represent the actual data stored in the corresponding partition, rather than a pointer to the load function. <code>IncrementalDataset</code> considers a partition relevant for processing if its ID satisfies the comparison function, given the checkpoint value. 2. <code>IncrementalDataset</code> does not raise a <code>DatasetError</code> if load finds no partitions to return - an empty dictionary is returned instead. An empty list of available partitions is part of a normal workflow for <code>IncrementalDataset</code>.</p>"},{"location":"pages/data/partitioned_and_incremental_datasets/#incremental-dataset-save","title":"Incremental dataset save","text":"<p>The <code>IncrementalDataset</code> save operation is identical to the save operation of the <code>PartitionedDataset</code>.</p>"},{"location":"pages/data/partitioned_and_incremental_datasets/#incremental-dataset-confirm","title":"Incremental dataset confirm","text":"<pre><code>The checkpoint value *is not* automatically updated when a new set of partitions is successfully loaded or saved.\n</code></pre> <p>Partitioned dataset checkpoint update is triggered by an explicit <code>confirms</code> instruction in one of the nodes downstream. It can be the same node, which processes the partitioned dataset:</p> <pre><code>from kedro.pipeline import node\n\n# process and then confirm `IncrementalDataset` within the same node\nnode(\n    process_partitions,\n    inputs=\"my_partitioned_dataset\",\n    outputs=\"my_processed_dataset\",\n    confirms=\"my_partitioned_dataset\",\n)\n</code></pre> <p>Alternatively, confirmation can be deferred to one of the nodes downstream, allowing you to implement extra validations before the loaded partitions are considered successfully processed:</p> <pre><code>from kedro.pipeline import node, pipeline\n\npipeline(\n    [\n        node(\n            func=process_partitions,\n            inputs=\"my_partitioned_dataset\",\n            outputs=\"my_processed_dataset\",\n        ),\n        # do something else\n        node(\n            func=confirm_partitions,\n            # note that the node may not require 'my_partitioned_dataset' as an input\n            inputs=\"my_processed_dataset\",\n            outputs=None,\n            confirms=\"my_partitioned_dataset\",\n        ),\n        # ...\n        node(\n            func=do_something_else_with_partitions,\n            # will return the same partitions even though they were already confirmed\n            inputs=[\"my_partitioned_dataset\", \"my_processed_dataset\"],\n            outputs=None,\n        ),\n    ]\n)\n</code></pre> <p>Important notes about the confirmation operation:</p> <ul> <li>Confirming a partitioned dataset does not affect any subsequent loads within the same run. All downstream nodes that input the same partitioned dataset as input will all receive the same partitions. Partitions that are created externally during the run will also not affect the dataset loads and won't appear in the list of loaded partitions until the next run or until the <code>release()</code> method is called on the dataset object.</li> <li>A pipeline cannot contain more than one node confirming the same dataset.</li> </ul>"},{"location":"pages/data/partitioned_and_incremental_datasets/#checkpoint-configuration","title":"Checkpoint configuration","text":"<p><code>IncrementalDataset</code> does not require explicit configuration of the checkpoint unless there is a need to deviate from the defaults. To update the checkpoint configuration, add a <code>checkpoint</code> key containing the valid dataset configuration. This may be required if, say, the pipeline has read-only permissions to the location of partitions (or write operations are undesirable for any other reason). In such cases, <code>IncrementalDataset</code> can be configured to save the checkpoint elsewhere. The <code>checkpoint</code> key also supports partial config updates where only some checkpoint attributes are overwritten, while the defaults are kept for the rest:</p> <pre><code>my_partitioned_dataset:\n  type: partitions.IncrementalDataset\n  path: s3://my-bucket-name/path/to/folder\n  dataset: pandas.CSVDataset\n  checkpoint:\n    # update the filepath and load_args, but keep the dataset type unchanged\n    filepath: gcs://other-bucket/CHECKPOINT\n    load_args:\n      k1: v1\n</code></pre>"},{"location":"pages/data/partitioned_and_incremental_datasets/#special-checkpoint-config-keys","title":"Special checkpoint config keys","text":"<p>Along with the standard dataset attributes, <code>checkpoint</code> config also accepts two special optional keys: * <code>comparison_func</code> (defaults to <code>operator.gt</code>) - a fully qualified import path to the function that will be used to compare a partition ID with the checkpoint value, to determine whether a partition should be processed. Such functions must accept two positional string arguments - partition ID and checkpoint value - and return <code>True</code> if such partition is considered to be past the checkpoint. It might be useful to specify your own <code>comparison_func</code> if you need to customise the checkpoint filtration mechanism - for example, you might want to implement windowed loading, where you always want to load the partitions representing the last calendar month. See the example config specifying a custom comparison function:</p> <pre><code>my_partitioned_dataset:\n  type: partitions.IncrementalDataset\n  path: s3://my-bucket-name/path/to/folder\n  dataset: pandas.CSVDataset\n  checkpoint:\n    comparison_func: my_module.path.to.custom_comparison_function  # the path must be importable\n</code></pre> <ul> <li><code>force_checkpoint</code> - if set, the partitioned dataset will use this value as the checkpoint instead of loading the corresponding checkpoint file. This might be useful if you need to roll back the processing steps and reprocess some (or all) of the available partitions. See the example config forcing the checkpoint value:</li> </ul> <pre><code>my_partitioned_dataset:\n  type: partitions.IncrementalDataset\n  path: s3://my-bucket-name/path/to/folder\n  dataset: pandas.CSVDataset\n  checkpoint:\n    force_checkpoint: 2020-01-01/data.csv\n</code></pre> <pre><code>Specification of `force_checkpoint` is also supported via the shorthand notation, as follows:\n</code></pre> <pre><code>my_partitioned_dataset:\n  type: partitions.IncrementalDataset\n  path: s3://my-bucket-name/path/to/folder\n  dataset: pandas.CSVDataset\n  checkpoint: 2020-01-01/data.csv\n</code></pre> <pre><code>If you need to force the partitioned dataset to load all available partitions, set `checkpoint` to an empty string:\n</code></pre> <pre><code>my_partitioned_dataset:\n  type: partitions.IncrementalDataset\n  path: s3://my-bucket-name/path/to/folder\n  dataset: pandas.CSVDataset\n  checkpoint: \"\"\n</code></pre>"},{"location":"pages/deployment/","title":"Deployment","text":""},{"location":"pages/deployment/#overview","title":"Overview","text":"<p>In this section we provide guides for different deployment methods; your choice  will depend on a range of factors.</p> <p>If you decide to deploy your Kedro project onto a single machine, you should consult our guide to single-machine deployment, and decide whether to:</p> <ul> <li>use Docker for container-based deployment</li> <li>use package-based deployment</li> <li>use the CLI to clone and deploy your codebase to a server</li> </ul> <p>If your pipeline is sizeable, you may want to run it across separate machines, so will need to consult our guide to distributed deployment.</p> <p></p>"},{"location":"pages/deployment/#deployment-methods","title":"Deployment Methods","text":"<pre><code>flowchart TD\n    A{Can your Kedro pipeline run on a single machine?} -- YES --&gt; B[Consult the single-machine deployment guide]\n    B --&gt; C{Do you have Docker on your machine?}\n    C -- YES --&gt; D[Use a container-based approach]\n    C -- NO --&gt; E[Use the CLI or package mode]\n    A -- NO --&gt; F[Consult the distributed deployment guide]\n    F --&gt; G[\"What distributed platform are you using?&lt;br/&gt;&lt;br/&gt;Check out the guides for:&lt;br/&gt;&lt;br/&gt;&lt;li&gt;Airflow&lt;/li&gt;&lt;li&gt;Amazon SageMaker&lt;/li&gt;&lt;li&gt;AWS Step functions&lt;/li&gt;&lt;li&gt;Azure&lt;/li&gt;&lt;li&gt;Dask&lt;/li&gt;&lt;li&gt;Databricks&lt;/li&gt;&lt;li&gt;Kubeflow Workflows&lt;/li&gt;&lt;li&gt;Prefect&lt;/li&gt;&lt;li&gt;Vertex AI&lt;/li&gt;\"]\n</code></pre> <p>This following pages provide information for deployment to, or integration with, the following:</p> <ul> <li>Airflow</li> <li>Amazon SageMaker</li> <li>Amazon EMR Serverless</li> <li>AWS Step functions</li> <li>Azure</li> <li>Dask</li> <li>Databricks</li> <li>Kubeflow Workflows</li> <li>Prefect</li> <li>Vertex AI</li> </ul> <p>Warning We also have legacy documentation pages for the following deployment targets, but these have not been tested against recent Kedro releases and we cannot guarantee them:</p> <ul> <li>for Argo Workflows</li> <li>for AWS Batch</li> </ul>"},{"location":"pages/deployment/#effective-node-grouping-for-deployment","title":"Effective node grouping for deployment","text":"<p>Effectively grouping nodes in deployment makes pipelines easier to manage and update, improves performance by using resources more efficiently, and enables them to handle larger datasets as they scale across different deployment environments. To learn more about the best ways to group nodes using Pipelines, Tags, and Namespaces, follow our detailed guide:</p> <ul> <li>Node Grouping in Kedro</li> </ul>"},{"location":"pages/deployment/airflow/","title":"Apache Airflow","text":"<p>Apache Airflow is a popular open-source workflow management platform. It is a suitable engine to orchestrate and execute a pipeline authored with Kedro, because workflows in Airflow are modelled and organised as DAGs.</p>"},{"location":"pages/deployment/airflow/#introduction-and-strategy","title":"Introduction and strategy","text":"<p>The general strategy to deploy a Kedro pipeline on Apache Airflow is to run every Kedro node as an Airflow task while the whole pipeline is converted to an Airflow DAG. This approach mirrors the principles of running Kedro in a distributed environment.</p> <p>Each node will be executed within a new Kedro session, which implies that <code>MemoryDataset</code>s cannot serve as storage for the intermediate results of nodes. Instead, all datasets must be registered in the <code>DataCatalog</code> and stored in persistent storage. This approach enables nodes to access the results from preceding nodes.</p> <p>This guide provides instructions on running a Kedro pipeline on different Airflow platforms. You can jump to the specific sections by clicking the links below, how to run a Kedro pipeline on:</p> <ul> <li>Apache Airflow with Astronomer</li> <li>Amazon AWS Managed Workflows for Apache Airflow (MWAA)</li> <li>Apache Airflow using a Kubernetes cluster</li> </ul>"},{"location":"pages/deployment/airflow/#how-to-run-a-kedro-pipeline-on-apache-airflow-with-astronomer","title":"How to run a Kedro pipeline on Apache Airflow with Astronomer","text":"<p>The following tutorial shows how to deploy an example Spaceflights Kedro project on Apache Airflow with Astro CLI, a command-line tool created by Astronomer that streamlines the creation of local Airflow projects. You will deploy it locally first, and then transition to Astro Cloud.</p> <p>Astronomer is a managed Airflow platform which allows users to spin up and run an Airflow cluster in production. Additionally, it also provides a set of tools to help users get started with Airflow locally in the easiest way possible.</p>"},{"location":"pages/deployment/airflow/#prerequisites","title":"Prerequisites","text":"<p>To follow this tutorial, ensure you have the following:</p> <ul> <li>The Astro CLI installed</li> <li>A container service like Docker Desktop (v18.09 or higher)</li> <li><code>kedro&gt;=0.19</code> installed</li> <li><code>kedro-airflow&gt;=0.8</code> installed. We will use this plugin to convert the Kedro pipeline into an Airflow DAG.</li> </ul>"},{"location":"pages/deployment/airflow/#create-prepare-and-package-example-kedro-project","title":"Create, prepare and package example Kedro project","text":"<p>In this section, you will create a new Kedro project equipped with an example pipeline designed to solve a typical data science task: predicting spaceflights prices. You will need to customise this project to ensure compatibility with Airflow, which includes enriching the Kedro <code>DataCatalog</code> with datasets previously stored only in memory and simplifying logging through custom settings. Following these modifications, you will package the project for installation in an Airflow Docker container and generate an Airflow DAG that mirrors our Kedro pipeline.</p> <ol> <li> <p>To create a new Kedro project, select the <code>example=yes</code> option to include example code. Additionally, to implement custom logging, select <code>tools=log</code>. Proceed with the default project name, but feel free to add any other tools as desired:</p> <p><code>shell kedro new --example=yes --name=new-kedro-project --tools=log</code></p> </li> <li> <p>Navigate to your project's directory, create a new <code>conf/airflow</code> directory for Airflow-specific configurations, and copy the <code>catalog.yml</code> file from <code>conf/base</code> to <code>conf/airflow</code>. This setup allows you to customise the <code>DataCatalog</code> for use with Airflow:</p> <p><code>shell cd new-kedro-project mkdir conf/airflow cp conf/base/catalog.yml conf/airflow/catalog.yml</code></p> </li> <li> <p>Open <code>conf/airflow/catalog.yml</code> to see the list of datasets used in the project. Note that additional intermediate datasets (<code>X_train</code>, <code>X_test</code>, <code>y_train</code>, <code>y_test</code>) are stored only in memory. You can locate these in the pipeline description under <code>/src/new_kedro_project/pipelines/data_science/pipeline.py</code>. To ensure these datasets are preserved and accessible across different tasks in Airflow, we need to include them in our <code>DataCatalog</code>. Instead of repeating similar code for each dataset, you can use Dataset Factories, a special syntax that allows defining a catch-all pattern to overwrite the default <code>MemoryDataset</code> creation. Add this code to the end of the file:</p> </li> </ol> <pre><code>\"{base_dataset}\":\n  type: pandas.CSVDataset\n  filepath: data/02_intermediate/{base_dataset}.csv\n</code></pre> <p>In the example here we assume that all Airflow tasks share one disk, but for distributed environments you would need to use non-local file paths.</p> <p>Starting with kedro-airflow release version 0.9.0, you can adopt a different strategy instead of following steps 2-3: group nodes that use intermediate <code>MemoryDataset</code>s into larger tasks. This approach allows intermediate data manipulation to occur within a single task, eliminating the need to transfer data between nodes. You can implement this by running <code>kedro airflow create</code> with the <code>--group-in-memory</code> flag on Step 6.</p> <ol> <li>Open <code>conf/logging.yml</code> and modify the <code>root: handlers</code> section to <code>[console]</code> at the end of the file. By default, Kedro uses the Rich library to enhance log output with sophisticated formatting. However, some deployment systems, including Airflow, don't work well with Rich. Therefore, we're adjusting the logging to a simpler console version. For more information on logging in Kedro, you can refer to the Kedro docs.</li> </ol> <pre><code>root:\n  handlers: [console]\n</code></pre> <ol> <li>Package the Kedro pipeline as a Python package so you can install it into the Airflow container later on:</li> </ol> <pre><code>kedro package\n</code></pre> <p>This step should produce a wheel file called <code>new_kedro_project-0.1-py3-none-any.whl</code> located at <code>dist/</code>.</p> <ol> <li>Convert the Kedro pipeline into an Airflow DAG with <code>kedro airflow</code></li> </ol> <pre><code>kedro airflow create --target-dir=dags/ --env=airflow\n</code></pre> <p>This step should produce a <code>.py</code> file called <code>new_kedro_project_airflow_dag.py</code> located at <code>dags/</code>.</p>"},{"location":"pages/deployment/airflow/#deployment-process-with-astro-cli","title":"Deployment process with Astro CLI","text":"<p>In this section, you will start by setting up a new blank Airflow project using Astro and then copy the files prepared in the previous section from the Kedro project. Next, you will need to customise the Dockerfile to enhance logging capabilities and manage the installation of our Kedro package. Finally, you will be able to run and explore the Airflow cluster.</p> <ol> <li> <p>To complete this section, you have to install both the Astro CLI and Docker Desktop.</p> </li> <li> <p>Initialise an Airflow project with Astro in a new folder outside of your Kedro project. Let's call it <code>kedro-airflow-spaceflights</code></p> <p><code>shell cd .. mkdir kedro-airflow-spaceflights cd kedro-airflow-spaceflights astro dev init</code></p> </li> <li> <p>The folder <code>kedro-airflow-spaceflights</code> will be executed within the Airflow container. To run the Kedro project there, you need to copy several items from the previous section into it:</p> </li> <li>the <code>/data</code> folder from Step 1, containing sample input datasets for our pipeline. This folder will also store the output results.</li> <li>the <code>/conf</code> folder from Steps 2-4, which includes our <code>DataCatalog</code>, parameters, and customised logging files. These files will be used by Kedro during its execution in the Airflow container.</li> <li>the <code>.whl</code> file from Step 5, which you will need to install in the Airflow Docker container to execute our project node by node.</li> <li>the Airflow DAG from Step 6 for deployment in the Airflow cluster.     <code>shell     cd ..     cp -r new-kedro-project/data kedro-airflow-spaceflights/data     cp -r new-kedro-project/conf kedro-airflow-spaceflights/conf     mkdir -p kedro-airflow-spaceflights/dist/     cp new-kedro-project/dist/new_kedro_project-0.1-py3-none-any.whl kedro-airflow-spaceflights/dist/     cp new-kedro-project/dags/new_kedro_project_airflow_dag.py kedro-airflow-spaceflights/dags/</code></li> </ol> <p>Feel free to completely copy <code>new-kedro-project</code> into <code>kedro-airflow-spaceflights</code> if your project requires frequent updates, DAG recreation, and repackaging. This approach allows you to work with kedro and astro projects in a single folder, eliminating the need to copy kedro files for each development iteration. However, be aware that both projects will share common files such as <code>requirements.txt</code>, <code>README.md</code>, and <code>.gitignore</code>.</p> <ol> <li>Add a few lines to the <code>Dockerfile</code> located in the <code>kedro-airflow-spaceflights</code> folder to set the environment variable <code>KEDRO_LOGGING_CONFIG</code> to point to <code>conf/logging.yml</code> to enable custom logging in Kedro (note that from Kedro 0.19.6 onwards, this step is unnecessary because Kedro uses the <code>conf/logging.yml</code> file by default) and to install the .whl file of our prepared Kedro project into the Airflow container:</li> </ol> <pre><code>ENV KEDRO_LOGGING_CONFIG=\"conf/logging.yml\" # This line is not needed from Kedro 0.19.6\n\nRUN pip install --user dist/new_kedro_project-0.1-py3-none-any.whl\n</code></pre> <ol> <li>Navigate to <code>kedro-airflow-spaceflights</code> folder and launch the local Airflow cluster with Astronomer</li> </ol> <pre><code>cd kedro-airflow-spaceflights\nastro dev start\n</code></pre> <ol> <li>Visit the Airflow Webserver UI at its default address, http://localhost:8080, using the default login credentials: username and password both set to <code>admin</code>. There, you'll find a list of all DAGs. Navigate to the <code>new-kedro-project</code> DAG, and then press the <code>Trigger DAG</code> play button to initiate it. You can then observe the steps of your project as they run successfully:</li> </ol> <p></p> <ol> <li>The Kedro project was run inside an Airflow Docker container, and the results are stored there as well. To copy these results to your host, first identify the relevant Docker containers by listing them:</li> </ol> <pre><code>docker ps\n</code></pre> <p>Select the container acting as the scheduler and note its ID. Then, use the following command to copy the results, substituting <code>d36ef786892a</code> with the actual container ID:</p> <pre><code>docker cp  d36ef786892a:/usr/local/airflow/data/ ./data/\n</code></pre> <ol> <li>To stop the Astro Airflow environment, you can use the command:</li> </ol> <pre><code>astro dev stop\n</code></pre>"},{"location":"pages/deployment/airflow/#deployment-to-astro-cloud","title":"Deployment to Astro Cloud","text":"<p>You can easily deploy and run your project on Astro Cloud, the cloud infrastructure provided by Astronomer, by following these steps:</p> <ol> <li> <p>Log in to your account on the Astronomer portal and create a new deployment if you don't already have one: </p> </li> <li> <p>Use the Astro CLI to log in to your Astro Cloud account:</p> </li> </ol> <pre><code>astro auth\n</code></pre> <p>You will be redirected to enter your login credentials in your browser. Successful login indicates that your terminal is now linked with your Astro Cloud account:</p> <p></p> <ol> <li>To deploy your local project to the cloud, navigate to the <code>kedro-airflow-spaceflights</code> folder and run:</li> </ol> <pre><code>astro deploy\n</code></pre> <ol> <li>At the end of the deployment process, a link will be provided. Use this link to manage and monitor your project in the cloud:</li> </ol> <p></p>"},{"location":"pages/deployment/airflow/#how-to-run-a-kedro-pipeline-on-amazon-aws-managed-workflows-for-apache-airflow-mwaa","title":"How to run a Kedro pipeline on Amazon AWS Managed Workflows for Apache Airflow (MWAA)","text":""},{"location":"pages/deployment/airflow/#kedro-project-preparation","title":"Kedro project preparation","text":"<p>MWAA, or Managed Workflows for Apache Airflow, is an AWS service that makes it easier to set up, operate, and scale Apache Airflow in the cloud. Deploying a Kedro pipeline to MWAA is similar to Astronomer, but there are some key differences: you need to store your project data in an AWS S3 bucket and make necessary changes to your <code>DataCatalog</code>. Additionally, you must configure how you upload your Kedro configuration, install your Kedro package, and set up the necessary environment variables. 1. Complete steps 1-4 from the Create, prepare and package example Kedro project section. 2. Your project's data should not reside in the working directory of the Airflow container. Instead, create an S3 bucket and upload your data folder from the new-kedro-project folder to your S3 bucket. 3. Modify the <code>DataCatalog</code> to reference data in your S3 bucket by updating the filepath and add credentials line for each dataset in <code>new-kedro-project/conf/airflow/catalog.yml</code>. Add the S3 prefix to the filepath as shown below:</p> <pre><code>companies:\n  type: pandas.CSVDataset\n  filepath: s3://&lt;your_S3_bucket&gt;/data/01_raw/companies.csv\n  credentials: dev_s3\n</code></pre> <ol> <li>Set up AWS credentials to provide read and write access to your S3 bucket. Update <code>new-kedro-project/conf/local/credentials.yml</code> with your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY and copy it to the <code>new-kedro-project/conf/airflow/</code> folder:</li> </ol> <pre><code>dev_s3:\n  client_kwargs:\n    aws_access_key_id: *********************\n    aws_secret_access_key: ******************************************\n</code></pre> <ol> <li>Add <code>s3fs</code> to your project\u2019s <code>requirements.txt</code> in <code>new-kedro-project</code> to facilitate communication with AWS S3. Some libraries could cause dependency conflicts in the Airflow environment, so make sure to minimise the requirements list and avoid using <code>kedro-viz</code> and <code>pytest</code>.</li> </ol> <pre><code>s3fs\n</code></pre> <ol> <li>Follow steps 5-6 from the Create, prepare and package example Kedro project section to package your Kedro project and generate an Airflow DAG.</li> <li>Update the DAG file <code>new_kedro_project_airflow_dag.py</code> located in the <code>dags/</code> folder by adding <code>conf_source=\"plugins/conf-new_kedro_project.tar.gz\"</code> to the arguments of <code>KedroSession.create()</code> in the Kedro operator execution function. This change is necessary because your Kedro configuration archive will be stored in the <code>plugins/</code> folder, not the root directory:</li> </ol> <pre><code>    def execute(self, context):\n        configure_project(self.package_name)\n        with KedroSession.create(project_path=self.project_path,\n                                 env=self.env, conf_source=\"plugins/conf-new_kedro_project.tar.gz\") as session:\n            session.run(self.pipeline_name, node_names=[self.node_name])\n</code></pre>"},{"location":"pages/deployment/airflow/#deployment-on-awaa","title":"Deployment on AWAA","text":"<ol> <li>Archive your three files: <code>new_kedro_project-0.1-py3-none-any.whl</code> and <code>conf-new_kedro_project.tar.gz</code> located in <code>new-kedro-project/dist</code>, and <code>logging.yml</code> located in <code>new-kedro-project/conf/</code> into a file called <code>plugins.zip</code> and upload it to <code>s3://your_S3_bucket</code>.</li> </ol> <pre><code>zip -j plugins.zip dist/new_kedro_project-0.1-py3-none-any.whl dist/conf-new_kedro_project.tar.gz conf/logging.yml\n</code></pre> <p>This archive will be later unpacked to the <code>/plugins</code> folder in the working directory of the Airflow container.</p> <ol> <li>Create a new <code>requirements.txt</code> file, add the path where your Kedro project will be unpacked in the Airflow container, and upload <code>requirements.txt</code> to <code>s3://your_S3_bucket</code>:</li> </ol> <pre><code>./plugins/new_kedro_project-0.1-py3-none-any.whl\n</code></pre> <p>Libraries from <code>requirements.txt</code> will be installed during container initialisation.</p> <ol> <li>Upload <code>new_kedro_project_airflow_dag.py</code> from the <code>new-kedro-project/dags</code> to <code>s3://your_S3_bucket/dags</code>.</li> <li>Create an empty <code>startup.sh</code> file for container startup commands. Set an environment variable for custom Kedro logging:</li> </ol> <pre><code>export KEDRO_LOGGING_CONFIG=\"plugins/logging.yml\"\n</code></pre> <ol> <li>Set up a new AWS MWAA environment using the following settings:</li> </ol> <pre><code>S3 Bucket:\n  s3://your_S3_bucket\nDAGs folder\n  s3://your_S3_bucket/dags\nPlugins file - optional\n  s3://your_S3_bucket/plugins.zip\nRequirements file - optional\n  s3://your_S3_bucket/requrements.txt\nStartup script file - optional\n  s3://your_S3_bucket/startup.sh\n</code></pre> <p>On the next page, set the <code>Public network (Internet accessible)</code> option in the <code>Web server access</code> section if you want to access your Airflow UI from the internet. Continue with the default options on the subsequent pages.</p> <ol> <li>Once the environment is created, use the <code>Open Airflow UI</code> button to access the standard Airflow interface, where you can manage your DAG.</li> </ol>"},{"location":"pages/deployment/airflow/#how-to-run-a-kedro-pipeline-on-apache-airflow-using-a-kubernetes-cluster","title":"How to run a Kedro pipeline on Apache Airflow using a Kubernetes cluster","text":"<p>If you want to execute your DAG in an isolated environment on Airflow using a Kubernetes cluster, you can use a combination of the <code>kedro-airflow</code> and <code>kedro-docker</code> plugins.</p> <ol> <li> <p>Package your Kedro project as a Docker container Use the <code>kedro docker init</code> and <code>kedro docker build</code> commands to containerise your Kedro project.</p> </li> <li> <p>Push the Docker image to a container registry    Upload the built Docker image to a cloud container registry, such as AWS ECR, Google Container Registry, or Docker Hub.</p> </li> <li> <p>Generate an Airflow DAG    Run the following command to generate an Airflow DAG:    <code>sh    kedro airflow create</code>    This will create a DAG file that includes the <code>KedroOperator()</code> by default.</p> </li> <li> <p>Modify the DAG to use <code>KubernetesPodOperator</code>    To execute each Kedro node in an isolated Kubernetes pod, replace <code>KedroOperator()</code> with <code>KubernetesPodOperator()</code>, as shown in the example below:</p> </li> </ol> <p>```python    from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator</p> <p>KubernetesPodOperator(        task_id=node_name,        name=node_name,        namespace=NAMESPACE,        image=DOCKER_IMAGE,        cmds=[\"kedro\"],        arguments=[\"run\", f\"--nodes={node_name}\"],        get_logs=True,        is_delete_operator_pod=True,  # Cleanup after execution        in_cluster=False, # Set to True if Airflow runs inside the Kubernetes cluster        do_xcom_push=False,        image_pull_policy=\"Always\",        # Uncomment the following lines if Airflow is running outside Kubernetes        # cluster_context=\"k3d-your-cluster\",  # Specify the Kubernetes context from your kubeconfig        # config_file=\"~/.kube/config\",  # Path to your kubeconfig file    )    ```</p>"},{"location":"pages/deployment/airflow/#running-multiple-nodes-in-a-single-container","title":"Running multiple nodes in a single container","text":"<p>By default, this approach runs each node in an isolated Docker container. However, to reduce computational overhead, you can choose to run multiple nodes together within the same container. If you opt for this, you must modify the DAG accordingly to adjust task dependencies and execution order.</p> <p>For example, in the <code>spaceflights-pandas</code> tutorial, if you want to execute the first two nodes together, your DAG may look like this:</p> <pre><code>from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n\nwith DAG(...) as dag:\n    tasks = {\n        \"preprocess-companies-and-shuttles\": KubernetesPodOperator(\n            task_id=\"preprocess-companies-and-shuttles\",\n            name=\"preprocess-companies-and-shuttles\",\n            namespace=NAMESPACE,\n            image=DOCKER_IMAGE,\n            cmds=[\"kedro\"],\n            arguments=[\"run\", \"--nodes=preprocess-companies-node,preprocess-shuttles-node\"],\n            ...\n        ),\n        \"create-model-input-table-node\": KubernetesPodOperator(...),\n        ...\n    }\n\n    tasks[\"preprocess-companies-and-shuttles\"] &gt;&gt; tasks[\"create-model-input-table-node\"]\n    tasks[\"create-model-input-table-node\"] &gt;&gt; tasks[\"split-data-node\"]\n    ...\n</code></pre> <p>In this example, we modified the original DAG generated by the <code>kedro airflow create</code> command by replacing <code>KedroOperator()</code> with <code>KubernetesPodOperator()</code>. Additionally, we merged the first two tasks into a single task named <code>preprocess-companies-and-shuttles</code>. This task executes the Docker image running two Kedro nodes: <code>preprocess-companies-node</code> and <code>preprocess-shuttles-node</code>.</p> <p>Furthermore, we adjusted the task order at the end of the DAG. Instead of having separate dependencies for the first two tasks, we consolidated them into a single line:</p> <pre><code>tasks[\"preprocess-companies-and-shuttles\"] &gt;&gt; tasks[\"create-model-input-table-node\"]\n</code></pre> <p>This ensures that the <code>create-model-input-table-node</code> task runs only after <code>preprocess-companies-and-shuttles</code> has completed.</p>"},{"location":"pages/deployment/amazon_emr_serverless/","title":"Amazon EMR Serverless","text":"<p>Amazon EMR Serverless can be used to manage and execute distributed computing workloads using Apache Spark. Its serverless architecture eliminates the need to provision or manage clusters to execute your Spark jobs. Instead, EMR Serverless independently allocates the resources needed for each job and releases them at completion.</p> <p>EMR Serverless is typically used for pipelines that are either fully or partially dependent on PySpark. For other parts of the pipeline such as modeling, where a non-distributed computing approach may be suitable, EMR Serverless might not be needed.</p>"},{"location":"pages/deployment/amazon_emr_serverless/#context","title":"Context","text":"<p>Python applications on Amazon Elastic MapReduce (EMR) have traditionally managed dependencies using bootstrap actions. This can sometimes lead to complications due to script complexity, longer bootstrapping times, and potential inconsistencies across cluster instances.</p> <p>Some applications may need a different approach, such as:</p> <ul> <li>Custom Python version: By default, the latest EMR releases (6.10.0, 6.11.0) run Python 3.7, but an application may require Python 3.9 or later.</li> <li>Python dependencies: Some applications use a range of third-party dependencies that need to be installed on both the driver and worker nodes.</li> </ul> <p>Even though the official AWS documentation provides methods for configuring a custom Python version and for using custom dependencies, there are some limitations. The methods are prone to errors, can be difficult to debug, and do not ensure full compatibility (see the FAQ section below for more details).</p> <p>EMR Serverless offers a solution to the limitations described, starting with Amazon EMR 6.9.0, which supports custom images. With a custom Docker image you can package a specific Python version along with all required dependencies into a single immutable container. This ensures a consistent runtime environment across the entire setup and also enables control over the runtime environment.</p> <p>This approach helps to avoid job failures due to misconfigurations or potential conflicts with system-level packages, and ensures the package is portable so it can be debugged and tested locally (see more details on validation). Using the approach can provide a repeatable, reliable environment and improve operational flexibility.</p> <p>With this context established, the rest of this page describes how to deploy a Kedro project to EMR Serverless.</p>"},{"location":"pages/deployment/amazon_emr_serverless/#overview-of-approach","title":"Overview of approach","text":"<p>This approach creates a custom Docker image for EMR Serverless to package dependencies and manage the runtime environment, and follows these steps:</p> <ul> <li>Packaging the Kedro project to install it on all EMR Serverless worker nodes. <code>kedro package</code> can be used for this: the resultant <code>.whl</code> file is used for installation, while <code>conf.tar.gz</code> contains the project configuration files.</li> <li>Using a custom Python version instead of the default Python installed on EMR Serverless. In the example, pyenv is used for installing the custom Python version.</li> <li>Running a job on EMR Serverless specifying Spark properties. This is needed to use the custom Python and provide an entrypoint script that accepts command line arguments for running Kedro.</li> </ul> <p>Here is an example Dockerfile that can be used:</p> <pre><code>FROM public.ecr.aws/emr-serverless/spark/emr-6.10.0:latest AS base\n\nUSER root\n\n# Install Python build dependencies for pyenv\nRUN yum install -y gcc make patch zlib-devel bzip2 bzip2-devel readline-devel  \\\n        sqlite sqlite-devel openssl11-devel tk-devel libffi-devel xz-devel tar\n\n# Install git for pyenv installation\nRUN yum install -y git\n\n# Add pyenv to PATH and set up environment variables\nENV PYENV_ROOT /usr/.pyenv\nENV PATH $PYENV_ROOT/shims:$PYENV_ROOT/bin:$PATH\nENV PYTHON_VERSION=3.9.16\n\n# Install pyenv, initialize it, install desired Python version and set as global\nRUN curl https://pyenv.run | bash\nRUN eval \"$(pyenv init -)\"\nRUN pyenv install ${PYTHON_VERSION} &amp;&amp; pyenv global ${PYTHON_VERSION}\n\n# Copy and install packaged Kedro project\nENV KEDRO_PACKAGE &lt;PACKAGE_WHEEL_NAME&gt;\nCOPY dist/$KEDRO_PACKAGE /tmp/dist/$KEDRO_PACKAGE\nRUN pip install --upgrade pip &amp;&amp; pip install /tmp/dist/$KEDRO_PACKAGE  \\\n    &amp;&amp; rm -f /tmp/dist/$KEDRO_PACKAGE\n\n# Copy and extract conf folder\nADD dist/conf.tar.gz /home/hadoop/\n\n# EMRS will run the image as hadoop\nUSER hadoop:hadoop\n</code></pre> <p>Make sure to replace <code>&lt;PACKAGE_WHEEL_NAME&gt;</code> with your own <code>.whl</code> file name. Here is the <code>entrypoint.py</code> entrypoint script:</p> <pre><code>import sys\nfrom &lt;PACKAGE_NAME&gt;.__main__ import main\n\nmain(sys.argv[1:])\n\n</code></pre> <p>Replace <code>&lt;PACKAGE_NAME&gt;</code> with your package name.</p>"},{"location":"pages/deployment/amazon_emr_serverless/#resources","title":"Resources","text":"<p>For more details, see the following resources:</p> <ul> <li>Package a Kedro project</li> <li>Run a packaged project</li> <li>Customizing an EMR Serverless image</li> <li>Using custom images with EMR Serverless</li> </ul>"},{"location":"pages/deployment/amazon_emr_serverless/#setup","title":"Setup","text":""},{"location":"pages/deployment/amazon_emr_serverless/#prerequisites","title":"Prerequisites","text":"<p>You must create an S3 bucket to store data for EMR Serverless.</p>"},{"location":"pages/deployment/amazon_emr_serverless/#infrastructure","title":"Infrastructure","text":"<ol> <li> <p>Create a private repository in ECR. See details on how to do so in the AWS documentation. You can use the default settings. This repository will be used to upload the EMR Serverless custom image.</p> </li> <li> <p>Create an EMR Studio.</p> </li> <li> <p>Go to the AWS Console &gt; EMR &gt; EMR Serverless.</p> </li> <li>Make sure you are in the correct region where you want to create your resource.</li> <li> <p>Then, click \"Get started\" &gt; \"Create and launch EMR Studio\".</p> </li> <li> <p>Create an application.</p> </li> <li> <p>Type: Spark</p> </li> <li>Release version: <code>emr-6.10.0</code></li> <li>Architecture: <code>x86_64</code></li> <li>Application setup options &gt; \"Choose custom settings\"</li> <li>Custom image settings &gt; \"Use the custom image with this application\"</li> <li>For simplicity, use the same custom image for both Spark drivers and executors</li> <li>Provide the ECR image URI; it must be located in the same region as the EMR Studio</li> </ol> <p>You may customise other settings as required, or even change the release version and architecture options (along with making the changes in the custom image).</p> <pre><code>The documented approach has only been tested with the above options.\n</code></pre> <p>See the AWS EMR Serverless documentation for more details on creating an application.</p>"},{"location":"pages/deployment/amazon_emr_serverless/#iam","title":"IAM","text":"<ol> <li> <p>Create a job runtime role. Follow the instructions under \"Create a job runtime role\". This role will be used when submitting a job to EMR Serverless. The permissions defined in the policy also grant access to an S3 bucket (specify the S3 bucket you have created).</p> </li> <li> <p>Grant access to ECR repository.</p> </li> <li> <p>Go to ECR in the AWS console.</p> </li> <li>Click into the repository you created earlier, then on the left under Repositories &gt; Permissions.</li> <li>Click \"Edit policy JSON\" and paste this policy under \"Allow EMR Serverless to access the custom image repository\".</li> <li>Make sure to enter the ARN for your EMR Serverless application in the <code>aws:SourceArn</code> value.</li> </ol>"},{"location":"pages/deployment/amazon_emr_serverless/#optional-validate-the-custom-image","title":"(Optional) Validate the custom image","text":"<p>EMR Serverless provides a utility to validate your custom images locally, to make sure your modifications are still compatible to EMR Serverless and to prevent job failures. See the Amazon EMR Serverless Image CLI documentation for more details.</p>"},{"location":"pages/deployment/amazon_emr_serverless/#run-a-job","title":"Run a job","text":"<pre><code>On making changes to the custom image, and rebuilding and pushing to ECR, be sure to restart the EMR Serverless\napplication before submitting a job if your application is **already started**. Otherwise, new changes may not be reflected in the job run.\n\nThis may be due to the fact that when the application has started, EMR Serverless keeps a pool of warm resources (also referred to as\n[pre-initialized capacity](https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/pre-init-capacity.html))\nready to run a job, and the nodes may have already used the previous version of the ECR image.\n</code></pre> <p>See details on how to run a Spark job on EMR Serverless.</p> <p>The following parameters need to be provided: - \"entryPoint\" - the path to S3 where your <code>entrypoint.py</code> entrypoint script is uploaded. - \"entryPointArguments\" - a list of arguments for running Kedro - \"sparkSubmitParameters\" - set your properties to use the custom Python version</p> <pre><code>The use of an \"entrypoint script\" is needed because [EMR Serverless disregards `[CMD]` or `[ENTRYPOINT]` instructions\nin the Dockerfile](https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/application-custom-image.html#considerations).\nWe recommend to run Kedro programmatically through an \"entrypoint script\" to eliminate the need to use, for example,\na [subprocess](https://docs.python.org/3/library/subprocess.html) to invoke `kedro run`. See [FAQ](#faq) for more details.\n</code></pre> <p>Example using AWS CLI:</p> <pre><code>aws emr-serverless start-job-run \\\n    --application-id &lt;application-id&gt; \\\n    --execution-role-arn &lt;execution-role-arn&gt; \\\n    --job-driver '{\n        \"sparkSubmit\": {\n            \"entryPoint\": \"&lt;s3-path-to-entrypoint-script&gt;\",\n            \"entryPointArguments\": [\"--env\", \"&lt;emr-conf&gt;\", \"--runner\", \"ThreadRunner\", \"--pipeline\", \"&lt;kedro-pipeline-name&gt;\"],\n            \"sparkSubmitParameters\": \"--conf spark.emr-serverless.driverEnv.PYSPARK_DRIVER_PYTHON=/usr/.pyenv/versions/3.9.16/bin/python --conf spark.emr-serverless.driverEnv.PYSPARK_PYTHON=/usr/.pyenv/versions/3.9.16/bin/python --conf spark.executorEnv.PYSPARK_PYTHON=/usr/.pyenv/versions/3.9.16/bin/python\"\n        }\n    }'\n</code></pre> <p>Enter the respective values in the placeholders above. For example, use the ARN from the job runtime role created earlier for <code>&lt;execution-role-arn&gt;</code>.</p>"},{"location":"pages/deployment/amazon_emr_serverless/#faq","title":"FAQ","text":""},{"location":"pages/deployment/amazon_emr_serverless/#how-is-the-approach-defined-here-different-from-the-approach-in-seven-steps-to-deploy-kedro-pipelines-on-amazon-emr-on-the-kedro-blog","title":"How is the approach defined here different from the approach in \"Seven steps to deploy Kedro pipelines on Amazon EMR\" on the Kedro blog?","text":"<p>There are similarities in steps in both approaches. The key difference is that this page explains how to provide a custom Python version through the custom image. The blog post provides Python dependencies in a virtual environment for EMR.</p> <p>The approach of providing a custom image applies to EMR Serverless. On EMR it would be worth considering using a custom AMI, as an alternative to using bootstrap actions. This has not been explored nor tested as yet.</p>"},{"location":"pages/deployment/amazon_emr_serverless/#emr-serverless-already-has-python-installed-why-do-we-need-a-custom-python-version","title":"EMR Serverless already has Python installed. Why do we need a custom Python version?","text":"<p>Some applications may require a different Python version than the default version installed. For example, your code base may require Python 3.9 while the default Python installation on the latest EMR releases (6.10.0, 6.11.0) is based on Python 3.7.</p>"},{"location":"pages/deployment/amazon_emr_serverless/#why-do-we-need-to-create-a-custom-image-to-provide-the-custom-python-version","title":"Why do we need to create a custom image to provide the custom Python version?","text":"<p>You may encounter difficulties with the virtual environment approach when modifying it to use pyenv. While using venv-pack we found a limitation that returned the error <code>Too many levels of symbolic links</code>, which is documented as follows:</p> <p>Python is not packaged with the environment, but rather symlinked in the environment. This is useful for deployment situations where Python is already installed on the machine, but the required library dependencies may not be.</p> <p>The custom image approach includes both installing the Kedro project and provides the custom Python version in one go. Otherwise, you would need to separately provide the files, and specify them as extra Spark configuration each time you submit a job.</p>"},{"location":"pages/deployment/amazon_emr_serverless/#why-do-we-need-to-package-the-kedro-project-and-invoke-using-an-entrypoint-script-why-cant-we-use-cmd-or-entrypoint-with-kedro-run-in-the-custom-image","title":"Why do we need to package the Kedro project and invoke using an entrypoint script? Why can't we use [CMD] or [ENTRYPOINT] with <code>kedro run</code> in the custom image?","text":"<p>As mentioned in the AWS documentation, EMR Serverless ignores <code>[CMD]</code> or <code>[ENTRYPOINT]</code> instructions in the Dockerfile.</p> <p>We recommend using the <code>entrypoint.py</code> entrypoint script approach to run Kedro programmatically, instead of using subprocess to invoke <code>kedro run</code>.</p>"},{"location":"pages/deployment/amazon_emr_serverless/#how-about-using-the-method-described-in-lifecycle-management-with-kedrosession-to-run-kedro-programmatically","title":"How about using the method described in Lifecycle management with KedroSession to run Kedro programmatically?","text":"<p>This is a valid alternative to run Kedro programmatically, without needing to package the Kedro project, but the arguments are not a direct mapping to Kedro command line arguments: - Different names are used. For example, \"pipeline_name\" and \"node_names\" are used instead of \"pipeline\" and \"nodes\". - Different types are used. For example, to specify the \"runner\" you need to pass an <code>AbstractRunner</code> object, instead of a string value like \"ThreadRunner\".</p> <p>Arguments need to be passed separately: \"env\" is passed directly to <code>KedroSession.create()</code> while other arguments such as \"pipeline_name\" and \"node_names\" need to be passed to <code>session.run()</code>.</p> <p>It is most suited to scenarios such as invoking <code>kedro run</code>, or where you do not provide many command line arguments to Kedro.</p>"},{"location":"pages/deployment/amazon_sagemaker/","title":"Amazon SageMaker","text":"<p>Amazon SageMaker provides the components used for machine learning in a single toolset that supports both classical machine learning libraries like <code>scikit-learn</code> or <code>XGBoost</code>, and Deep Learning frameworks such as <code>TensorFlow</code> or <code>PyTorch</code>.</p> <p>Amazon SageMaker is a fully-managed service and its features are covered by the official service documentation.</p>"},{"location":"pages/deployment/amazon_sagemaker/#the-kedro-sagemaker-plugin","title":"The <code>kedro-sagemaker</code> plugin","text":"<p>The <code>kedro-sagemaker</code> plugin from GetInData | Part of Xebia enables you to run a Kedro pipeline on Amazon Sagemaker. Consult the GitHub repository for <code>kedro-sagemaker</code> for further details, or take a look at the documentation.</p>"},{"location":"pages/deployment/argo/","title":"Argo Workflows (outdated documentation that needs review)","text":"<p>Important This page contains outdated documentation that has not been tested against recent Kedro releases. If you successfully use Argo Workflows with a recent version of Kedro, consider telling us the steps you took on Slack or GitHub.</p> <p>This page explains how to convert your Kedro pipeline to use Argo Workflows, an open-source container-native workflow engine for orchestrating parallel jobs on Kubernetes.</p>"},{"location":"pages/deployment/argo/#why-would-you-use-argo-workflows","title":"Why would you use Argo Workflows?","text":"<p>We are interested in Argo Workflows, one of the 4 components of the Argo project. Argo Workflows is an open-source container-native workflow engine for orchestrating parallel jobs on Kubernetes.</p> <p>Here are the main reasons to use Argo Workflows:</p> <ul> <li>It is cloud-agnostic and can run on any Kubernetes cluster</li> <li>It allows you to easily run and orchestrate compute intensive jobs in parallel on Kubernetes</li> <li>It manages the dependencies between tasks using a directed acyclic graph (DAG)</li> </ul>"},{"location":"pages/deployment/argo/#prerequisites","title":"Prerequisites","text":"<p>To use Argo Workflows, ensure you have the following prerequisites in place:</p> <ul> <li>Argo Workflows is installed on your Kubernetes cluster</li> <li>Argo CLI is installed on your machine</li> <li>A <code>name</code> attribute is set for each Kedro {py:mod}<code>~kedro.pipeline.node</code> since it is used to build a DAG</li> <li>All node input/output datasets must be configured in <code>catalog.yml</code> and refer to an external location (e.g. AWS S3); you cannot use the <code>MemoryDataset</code> in your workflow</li> </ul> <p>Note Each node will run in its own container.</p>"},{"location":"pages/deployment/argo/#how-to-run-your-kedro-pipeline-using-argo-workflows","title":"How to run your Kedro pipeline using Argo Workflows","text":""},{"location":"pages/deployment/argo/#containerise-your-kedro-project","title":"Containerise your Kedro project","text":"<p>First, you need to containerise your Kedro project, using any preferred container solution (e.g. <code>Docker</code>), to build an image to use in Argo Workflows.</p> <p>For the purpose of this walk-through, we are going to assume a <code>Docker</code> workflow. We recommend the <code>Kedro-Docker</code> plugin to streamline the process. Instructions for Kedro-Docker are in the plugin's README.md.</p> <p>After you\u2019ve built the Docker image for your project locally, transfer the image to a container registry.</p>"},{"location":"pages/deployment/argo/#create-argo-workflows-spec","title":"Create Argo Workflows spec","text":"<p>In order to build an Argo Workflows spec for your Kedro pipeline programmatically you can use the following Python script that should be stored in your project\u2019s root directory:</p> <pre><code># &lt;project_root&gt;/build_argo_spec.py\nimport re\nfrom pathlib import Path\n\nimport click\nfrom jinja2 import Environment, FileSystemLoader\n\nfrom kedro.framework.project import pipelines\nfrom kedro.framework.startup import bootstrap_project\n\nTEMPLATE_FILE = \"argo_spec.tmpl\"\nSEARCH_PATH = Path(\"templates\")\n\n\n@click.command()\n@click.argument(\"image\", required=True)\n@click.option(\"-p\", \"--pipeline\", \"pipeline_name\", default=None)\n@click.option(\"--env\", \"-e\", type=str, default=None)\ndef generate_argo_config(image, pipeline_name, env):\n    loader = FileSystemLoader(searchpath=SEARCH_PATH)\n    template_env = Environment(loader=loader, trim_blocks=True, lstrip_blocks=True)\n    template = template_env.get_template(TEMPLATE_FILE)\n\n    project_path = Path.cwd()\n    metadata = bootstrap_project(project_path)\n    package_name = metadata.package_name\n\n    pipeline_name = pipeline_name or \"__default__\"\n    pipeline = pipelines.get(pipeline_name)\n\n    tasks = get_dependencies(pipeline.node_dependencies)\n\n    output = template.render(image=image, package_name=package_name, tasks=tasks)\n\n    (SEARCH_PATH / f\"argo-{package_name}.yml\").write_text(output)\n\n\ndef get_dependencies(dependencies):\n    deps_dict = [\n        {\n            \"node\": node.name,\n            \"name\": clean_name(node.name),\n            \"deps\": [clean_name(val.name) for val in parent_nodes],\n        }\n        for node, parent_nodes in dependencies.items()\n    ]\n    return deps_dict\n\n\ndef clean_name(name):\n    return re.sub(r\"[\\W_]+\", \"-\", name).strip(\"-\")\n\n\nif __name__ == \"__main__\":\n    generate_argo_config()\n</code></pre> <p>The script accepts one required argument:</p> <ul> <li><code>image</code>: image transferred to the container registry</li> </ul> <p>You can also specify two optional arguments:</p> <ul> <li><code>--pipeline</code>: pipeline name for which you want to build an Argo Workflows spec</li> <li><code>--env</code>: Kedro configuration environment name, defaults to <code>local</code></li> </ul> <p>Add the following Argo Workflows spec template to <code>&lt;project_root&gt;/templates/argo_spec.tmpl</code>:</p> <pre><code>{# &lt;project_root&gt;/templates/argo_spec.tmpl #}\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: {{ package_name }}-\nspec:\n  entrypoint: dag\n  templates:\n  - name: kedro\n    metadata:\n      labels:\n        {# Add label to have an ability to remove Kedro Pods easily #}\n        app: kedro-argo\n    retryStrategy:\n      limit: 1\n    inputs:\n      parameters:\n      - name: kedro_node\n    container:\n      imagePullPolicy: Always\n      image: {{ image }}\n      env:\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              {# Secrets name #}\n              name: aws-secrets\n              key: access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secrets\n              key: secret_access_key\n      command: [kedro]{% raw %}\n      args: [\"run\", \"-n\",  \"{{inputs.parameters.kedro_node}}\"]\n      {% endraw %}\n  - name: dag\n    dag:\n      tasks:\n      {% for task in tasks %}\n      - name: {{ task.name }}\n        template: kedro\n        {% if task.deps %}\n        dependencies:\n        {% for dep in task.deps %}\n          - {{ dep }}\n        {% endfor %}\n        {% endif %}\n        arguments:\n          parameters:\n          - name: kedro_node\n            value: {{ task.node }}\n      {% endfor %}\n</code></pre> <pre><code>The Argo Workflows is defined as the dependencies between tasks using a directed-acyclic graph (DAG).\n</code></pre> <p>For the purpose of this walk-through, we will use an AWS S3 bucket for datasets; therefore <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables must be set to have an ability to communicate with S3. The <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> values should be stored in Kubernetes Secrets (an example Kubernetes Secrets spec is given below).</p> <p>The spec template is written with the Jinja templating language, so you must install the Jinja Python package:</p> <pre><code>$ pip install Jinja2\n</code></pre> <p>Finally, run the helper script from project's directory to build the Argo Workflows spec (the spec will be saved to <code>&lt;project_root&gt;/templates/argo-&lt;package_name&gt;.yml</code> file).</p> <pre><code>$ cd &lt;project_root&gt;\n$ python build_argo_spec.py &lt;project_image&gt;\n</code></pre>"},{"location":"pages/deployment/argo/#submit-argo-workflows-spec-to-kubernetes","title":"Submit Argo Workflows spec to Kubernetes","text":"<p>Before submitting the Argo Workflows spec you need to deploy a Kubernetes Secrets.</p> <p>Here's an example Secrets spec:</p> <pre><code># secret.yml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-secrets\ndata:\n  access_key_id: &lt;AWS_ACCESS_KEY_ID value encoded with base64&gt;\n  secret_access_key: &lt;AWS_SECRET_ACCESS_KEY value encoded with base64&gt;\ntype: Opaque\n</code></pre> <p>You can use the following command to encode AWS keys to base64:</p> <pre><code>$ echo -n &lt;original_key&gt; | base64\n</code></pre> <p>Run the following commands to deploy the Kubernetes Secrets to the <code>default</code> namespace and check that it was created:</p> <pre><code>$ kubectl create -f secret.yml\n$ kubectl get secrets aws-secrets\n</code></pre> <p>Now, you are ready to submit the Argo Workflows spec as follows:</p> <pre><code>$ cd &lt;project_root&gt;\n$ argo submit --watch templates/argo-&lt;package_name&gt;.yml\n</code></pre> <pre><code>The Argo Workflows should be submitted to the same namespace as the Kubernetes Secrets. Please refer to the Argo CLI help to get more details about the usage.\n</code></pre> <p>In order to clean up your Kubernetes cluster you can use the following commands:</p> <pre><code>$ kubectl delete pods --selector app=kedro-argo\n$ kubectl delete -f secret.yml\n</code></pre>"},{"location":"pages/deployment/argo/#kedro-argo-plugin","title":"Kedro-Argo plugin","text":"<p>As an alternative, you can use Kedro-Argo plugin to convert a Kedro project to Argo Workflows.</p> <pre><code>The plugin is not supported by the Kedro team and we can't guarantee its workability.\n</code></pre>"},{"location":"pages/deployment/aws_batch/","title":"AWS Batch (outdated documentation that needs review)","text":"<p>Important This page contains outdated documentation that has not been tested against recent Kedro releases. If you successfully use AWS Batch with a recent version of Kedro, consider telling us the steps you took on Slack or GitHub.</p>"},{"location":"pages/deployment/aws_batch/#why-would-you-use-aws-batch","title":"Why would you use AWS Batch?","text":"<p>AWS Batch is optimised for batch computing and applications that scale with the number of jobs running in parallel. It manages job execution and compute resources, and dynamically provisions the optimal quantity and type. AWS Batch can assist with planning, scheduling, and executing your batch computing workloads, using Amazon EC2 On-Demand and Spot Instances, and it has native integration with CloudWatch for log collection.</p> <p>AWS Batch helps you run massively parallel Kedro pipelines in a cost-effective way, and allows you to parallelise the pipeline execution across multiple compute instances. Each Batch job is run in an isolated Docker container environment.</p> <p>The following sections are a guide on how to deploy a Kedro project to AWS Batch, and uses the spaceflights tutorial as primary example. The guide assumes that you have already completed the tutorial, and that the project was created with the project name Kedro Tutorial.</p>"},{"location":"pages/deployment/aws_batch/#prerequisites","title":"Prerequisites","text":"<p>To use AWS Batch, ensure you have the following prerequisites in place:</p> <ul> <li>An AWS account set up.</li> <li>A <code>name</code> attribute is set for each Kedro {py:mod}<code>~kedro.pipeline.node</code>. Each node will run in its own Batch job, so having sensible node names will make it easier to <code>kedro run --nodes=&lt;node_name&gt;</code>.</li> <li>All node input/output datasets must be configured in <code>catalog.yml</code> and refer to an external location (e.g. AWS S3). A clean way to do this is to create a new configuration environment <code>conf/aws_batch</code> containing a <code>catalog.yml</code> file with the appropriate configuration, as illustrated below.</li> </ul> Click to expand <pre><code>companies:\n  type: pandas.CSVDataset\n  filepath: s3://&lt;your-bucket&gt;/companies.csv\n\nreviews:\n  type: pandas.CSVDataset\n  filepath: s3://&lt;your-bucket&gt;/reviews.csv\n\nshuttles:\n  type: pandas.ExcelDataset\n  filepath: s3://&lt;your-bucket&gt;/shuttles.xlsx\n\npreprocessed_companies:\n  type: pandas.CSVDataset\n  filepath: s3://&lt;your-bucket&gt;/preprocessed_companies.csv\n\npreprocessed_shuttles:\n  type: pandas.CSVDataset\n  filepath: s3://&lt;your-bucket&gt;/preprocessed_shuttles.csv\n\nmodel_input_table:\n  type: pandas.CSVDataset\n  filepath: s3://&lt;your-bucket&gt;/model_input_table.csv\n\nregressor:\n  type: pickle.PickleDataset\n  filepath: s3://&lt;your-bucket&gt;/regressor.pickle\n  versioned: true\n\nX_train:\n  type: pickle.PickleDataset\n  filepath: s3://&lt;your-bucket&gt;/X_train.pickle\n\nX_test:\n  type: pickle.PickleDataset\n  filepath: s3://&lt;your-bucket&gt;/X_test.pickle\n\ny_train:\n  type: pickle.PickleDataset\n  filepath: s3://&lt;your-bucket&gt;/y_train.pickle\n\ny_test:\n  type: pickle.PickleDataset\n  filepath: s3://&lt;your-bucket&gt;/y_test.pickle\n</code></pre>"},{"location":"pages/deployment/aws_batch/#how-to-run-a-kedro-pipeline-using-aws-batch","title":"How to run a Kedro pipeline using AWS Batch","text":""},{"location":"pages/deployment/aws_batch/#containerise-your-kedro-project","title":"Containerise your Kedro project","text":"<p>First, you need to containerise your Kedro project, using any preferred container solution (e.g. Docker), to build an image to use in AWS Batch.</p> <p>For the purpose of this walk-through, we are going to assume a Docker workflow. We recommend using the Kedro-Docker plugin to streamline the process.  Instructions for using this are in the plugin's README.md.</p> <p>After you\u2019ve built the Docker image for your project locally, transfer the image to a container registry, for instance AWS ECR. You can find instructions on how to push your Docker image to ECR in Amazon's ECR documentation.</p> <p>Alternatively, once you've created a container repository, click the <code>View Push Commands</code> button in the top-right corner of the ECR dashboard.</p>"},{"location":"pages/deployment/aws_batch/#provision-resources","title":"Provision resources","text":"<p>In order to be able to deploy your pipeline to Batch, you need to provision the following four resources in advance:</p>"},{"location":"pages/deployment/aws_batch/#create-iam-role","title":"Create IAM Role","text":"<p>If you are storing your datasets to S3, you first need to create an IAM role to be able to grant Batch access to read and write to the respective locations. Follow the instructions from the AWS tutorial on how to do so, but note that the policy (step 3) should be <code>AmazonS3FullAccess</code>. Name the newly-created IAM role <code>batchJobRole</code>.</p>"},{"location":"pages/deployment/aws_batch/#create-aws-batch-job-definition","title":"Create AWS Batch job definition","text":"<p>Job definitions provide the template for resources needed for running a job. Create a job definition named <code>kedro_run</code>, assign it the newly created <code>batchJobRole</code> IAM role, the container image you've packaged above, execution timeout of 300s and 2000MB of memory. You can leave the <code>Command</code> field empty and all the defaults in place.</p>"},{"location":"pages/deployment/aws_batch/#create-aws-batch-compute-environment","title":"Create AWS Batch compute environment","text":"<p>Next you need a compute environment where the work will be executed. Create a managed, on-demand one named <code>spaceflights_env</code> and let it choose to create new service and instance roles if you don't have any yet. Having a managed environment means that AWS will automatically handle the scaling of your instances.</p> <pre><code>This compute environment won't contain any instances until you trigger the pipeline run. Therefore, creating it does not incur any immediate costs.\n</code></pre>"},{"location":"pages/deployment/aws_batch/#create-aws-batch-job-queue","title":"Create AWS Batch job queue","text":"<p>A job queue is the bridge between the submitted jobs and the compute environment they should run on. Create a queue named <code>spaceflights_queue</code>, connected to your newly created compute environment <code>spaceflights_env</code>, and give it <code>Priority</code> 1.</p>"},{"location":"pages/deployment/aws_batch/#configure-the-credentials","title":"Configure the credentials","text":"<p>Ensure you have the necessary AWS credentials in place before moving on, so that your pipeline can access and interact with the AWS services. Check out the AWS CLI documentation for instructions on how to set this up.</p> <pre><code>You should configure the default region to match the region where you've created the Batch resources.\n</code></pre>"},{"location":"pages/deployment/aws_batch/#submit-aws-batch-jobs","title":"Submit AWS Batch jobs","text":"<p>Now that all the resources are in place, it's time to submit jobs to Batch programmatically, using the newly-created job definition and job queue. Each job will correspond to running one node, and in order to maintain the correct order of execution, you'll need to specify the dependencies (job IDs) of each submitted job. You can leverage a custom runner for this step.</p>"},{"location":"pages/deployment/aws_batch/#create-a-custom-runner","title":"Create a custom runner","text":"<p>Create a new Python package <code>runner</code> in your <code>src</code> folder, i.e. <code>kedro_tutorial/src/kedro_tutorial/runner/</code>. Make sure there is an <code>__init__.py</code> file at this location, and add another file named <code>batch_runner.py</code>, which will contain the implementation of your custom runner, <code>AWSBatchRunner</code>. The <code>AWSBatchRunner</code> will submit and monitor jobs asynchronously, surfacing any errors that occur on Batch.</p> <p>Make sure the <code>__init__.py</code> file in the <code>runner</code> folder includes the following import and declaration:</p> <pre><code>from .batch_runner import AWSBatchRunner\n\n__all__ = [\"AWSBatchRunner\"]\n</code></pre> <p>Copy the contents of the script below into <code>batch_runner.py</code>:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom time import sleep\nfrom typing import Any, Dict, Set\n\nimport boto3\n\nfrom kedro.io import DataCatalog\nfrom kedro.pipeline.pipeline import Pipeline, Node\nfrom kedro.runner import ThreadRunner\n\n\nclass AWSBatchRunner(ThreadRunner):\n    def __init__(\n        self,\n        max_workers: int = None,\n        job_queue: str = None,\n        job_definition: str = None,\n        is_async: bool = False,\n    ):\n        super().__init__(max_workers, is_async=is_async)\n        self._job_queue = job_queue\n        self._job_definition = job_definition\n        self._client = boto3.client(\"batch\")\n\n    def create_default_dataset(self, ds_name: str):\n        raise NotImplementedError(\"All datasets must be defined in the catalog\")\n\n    def _get_required_workers_count(self, pipeline: Pipeline):\n        if self._max_workers is not None:\n            return self._max_workers\n\n        return super()._get_required_workers_count(pipeline)\n\n    def _run(\n        self,\n        pipeline: Pipeline,\n        catalog: DataCatalog,\n        hook_manager: PluginManager,\n        session_id: str = None,\n    ) -&gt; None:\n        nodes = pipeline.nodes\n        node_dependencies = pipeline.node_dependencies\n        todo_nodes = set(node_dependencies.keys())\n        node_to_job = dict()\n        done_nodes = set()  # type: Set[Node]\n        futures = set()\n        max_workers = self._get_required_workers_count(pipeline)\n\n        self._logger.info(\"Max workers: %d\", max_workers)\n        with ThreadPoolExecutor(max_workers=max_workers) as pool:\n            while True:\n                # Process the nodes that have completed, i.e. jobs that reached\n                # FAILED or SUCCEEDED state\n                done = {fut for fut in futures if fut.done()}\n                futures -= done\n                for future in done:\n                    try:\n                        node = future.result()\n                    except Exception:\n                        self._suggest_resume_scenario(pipeline, done_nodes)\n                        raise\n                    done_nodes.add(node)\n                    self._logger.info(\n                        \"Completed %d out of %d jobs\", len(done_nodes), len(nodes)\n                    )\n\n                # A node is ready to be run if all its upstream dependencies have been\n                # submitted to Batch, i.e. all node dependencies were assigned a job ID\n                ready = {\n                    n for n in todo_nodes if node_dependencies[n] &lt;= node_to_job.keys()\n                }\n                todo_nodes -= ready\n                # Asynchronously submit Batch jobs\n                for node in ready:\n                    future = pool.submit(\n                        self._submit_job,\n                        node,\n                        node_to_job,\n                        node_dependencies[node],\n                        session_id,\n                    )\n                    futures.add(future)\n\n                # If no more nodes left to run, ensure the entire pipeline was run\n                if not futures:\n                    assert not todo_nodes, (todo_nodes, done_nodes, ready, done)\n                    break\n</code></pre> <p>Next you will want to add the implementation of the <code>_submit_job()</code> method referenced in <code>_run()</code>. This method will create and submit jobs to AWS Batch with the following:</p> <ul> <li>Correctly specified upstream dependencies</li> <li>A unique job name</li> <li>The corresponding command to run, namely <code>kedro run --nodes=&lt;node_name&gt;</code>.</li> </ul> <p>Once submitted, the method tracks progress and surfaces any errors if the jobs end in <code>FAILED</code> state.</p> <p>Make sure the contents below are placed inside the <code>AWSBatchRunner</code> class:</p> <pre><code>def _submit_job(\n    self,\n    node: Node,\n    node_to_job: Dict[Node, str],\n    node_dependencies: Set[Node],\n    session_id: str,\n) -&gt; Node:\n    self._logger.info(\"Submitting the job for node: %s\", str(node))\n\n    job_name = f\"kedro_{session_id}_{node.name}\".replace(\".\", \"-\")\n    depends_on = [{\"jobId\": node_to_job[dep]} for dep in node_dependencies]\n    command = [\"kedro\", \"run\", \"--nodes\", node.name]\n\n    response = self._client.submit_job(\n        jobName=job_name,\n        jobQueue=self._job_queue,\n        jobDefinition=self._job_definition,\n        dependsOn=depends_on,\n        containerOverrides={\"command\": command},\n    )\n\n    job_id = response[\"jobId\"]\n    node_to_job[node] = job_id\n\n    _track_batch_job(job_id, self._client)  # make sure the job finishes\n\n    return node\n</code></pre> <p>The last part of the implementation is the helper function <code>_track_batch_job()</code>, called from <code>_submit_job()</code>, which looks like this:</p> <pre><code>def _track_batch_job(job_id: str, client: Any) -&gt; None:\n    \"\"\"Continuously poll the Batch client for a job's status,\n    given the job ID. If it ends in FAILED state, raise an exception\n    and log the reason. Return if successful.\n    \"\"\"\n    while True:\n        # we don't want to bombard AWS with the requests\n        # to not get throttled\n        sleep(1.0)\n\n        jobs = client.describe_jobs(jobs=[job_id])[\"jobs\"]\n        if not jobs:\n            raise ValueError(f\"Job ID {job_id} not found.\")\n\n        job = jobs[0]\n        status = job[\"status\"]\n\n        if status == \"FAILED\":\n            reason = job[\"statusReason\"]\n            raise Exception(\n                f\"Job {job_id} has failed with the following reason: {reason}\"\n            )\n\n        if status == \"SUCCEEDED\":\n            return\n</code></pre>"},{"location":"pages/deployment/aws_batch/#set-up-batch-related-configuration","title":"Set up Batch-related configuration","text":"<p>You'll need to set the Batch-related configuration that the runner will use. Add a <code>parameters.yml</code> file inside the <code>conf/aws_batch/</code> directory created as part of the prerequisites with the following keys:</p> <pre><code>aws_batch:\n  job_queue: \"spaceflights_queue\"\n  job_definition: \"kedro_run\"\n  max_workers: 2\n</code></pre>"},{"location":"pages/deployment/aws_batch/#update-cli-implementation","title":"Update CLI implementation","text":"<p>You're nearly there! Before you can use the new runner, you need to add a <code>cli.py</code> file at the same level as <code>settings.py</code>, using the template we provide. Update the <code>run()</code> function in the newly-created <code>cli.py</code> file to make sure the runner class is instantiated correctly:</p> <pre><code>def run(tag, env, ...):\n    \"\"\"Run the pipeline.\"\"\"\n    runner = runner or \"SequentialRunner\"\n\n    tag = _get_values_as_tuple(tag) if tag else tag\n    node_names = _get_values_as_tuple(node_names) if node_names else node_names\n\n    with KedroSession.create(env=env, extra_params=params) as session:\n        context = session.load_context()\n        runner_instance = _instantiate_runner(runner, is_async, context)\n        session.run(\n            tags=tag,\n            runner=runner_instance,\n            node_names=node_names,\n            from_nodes=from_nodes,\n            to_nodes=to_nodes,\n            from_inputs=from_inputs,\n            to_outputs=to_outputs,\n            load_versions=load_version,\n            pipeline_name=pipeline,\n        )\n</code></pre> <p>where the helper function <code>_instantiate_runner()</code> looks like this:</p> <pre><code>def _instantiate_runner(runner, is_async, project_context):\n    runner_class = load_obj(runner, \"kedro.runner\")\n    runner_kwargs = dict(is_async=is_async)\n\n    if runner.endswith(\"AWSBatchRunner\"):\n        batch_kwargs = project_context.params.get(\"aws_batch\") or {}\n        runner_kwargs.update(batch_kwargs)\n\n    return runner_class(**runner_kwargs)\n</code></pre>"},{"location":"pages/deployment/aws_batch/#deploy","title":"Deploy","text":"<p>You're now ready to trigger the run. Execute the following command:</p> <pre><code>kedro run --env=aws_batch --runner=kedro_tutorial.runner.AWSBatchRunner\n</code></pre> <p>You should start seeing jobs appearing on your Jobs dashboard, under the <code>Runnable</code> tab - meaning they're ready to start as soon as the resources are provisioned in the compute environment.</p> <p>AWS Batch has native integration with CloudWatch, where you can check the logs for a particular job. You can either click on the Batch job in the Jobs tab and click <code>View logs</code> in the pop-up panel, or go to CloudWatch dashboard, click <code>Log groups</code> in the side bar and find <code>/aws/batch/job</code>.</p>"},{"location":"pages/deployment/aws_step_functions/","title":"AWS Step Functions","text":"<p>This tutorial explains how to deploy a Kedro project with AWS Step Functions in order to run a Kedro pipeline in production on AWS Serverless Computing platform.</p>"},{"location":"pages/deployment/aws_step_functions/#why-would-you-run-a-kedro-pipeline-with-aws-step-functions","title":"Why would you run a Kedro pipeline with AWS Step Functions?","text":"<p>A major problem when data pipelines move to production is to build and maintain the underlying compute infrastructure, or servers. Serverless computing hands the provisioning and management of distributed computing resources to cloud providers, enabling data engineers and data scientists to focus on their business problems.</p> <p>Azure Functions and AWS Lambda are good examples of this solution, but others are available. Services like AWS Step Functions offer a managed orchestration capability that makes it easy to sequence serverless functions and multiple cloud-native services into business-critical applications.</p> <p>From a Kedro perspective, this means the ability to run each node and retain the pipeline's correctness and reliability through a managed orchestrator without the concerns of managing underlying infrastructure. Another benefit of running a Kedro pipeline in a serverless computing platform is the ability to take advantage of other services from the same provider, such as the use of the feature store for Amazon SageMaker to store features data.</p> <p>The following discusses how to run the Kedro pipeline from the spaceflights tutorial on AWS Step Functions.</p>"},{"location":"pages/deployment/aws_step_functions/#strategy","title":"Strategy","text":"<p>The general strategy to deploy a Kedro pipeline on AWS Step Functions is to run every Kedro node as an AWS Lambda function. The whole pipeline is converted into an AWS Step Functions State Machine for orchestration. This approach mirrors the principles of running Kedro in a distributed environment.</p>"},{"location":"pages/deployment/aws_step_functions/#prerequisites","title":"Prerequisites","text":"<p>To use AWS Step Functions, ensure you have the following:</p> <ul> <li>An AWS account set up</li> <li>Configured AWS credentials on your local machine</li> <li>Generated Kedro project called Spaceflights Step Functions using Kedro Spaceflights starter.</li> <li>The final project directory's name should be <code>spaceflights-step-functions</code>.</li> <li> <p>You should complete the spaceflights tutorial to understand the project's structure.</p> </li> <li> <p>In this tutorial, we will also be using AWS Cloud Development Kit (CDK) to write our deployment script. To install the <code>cdk</code> command, please consult AWS guide. The official method of installation is using npm:</p> </li> </ul> <pre><code>$ npm install -g aws-cdk\n# to verify that the cdk command has been installed\n$ cdk -h\n</code></pre>"},{"location":"pages/deployment/aws_step_functions/#deployment-process","title":"Deployment process","text":"<p>The deployment process for a Kedro pipeline on AWS Step Functions consists of the following steps:</p> <ul> <li>Develop the Kedro pipeline locally as normal</li> <li>Create a new configuration environment in which we ensure all nodes' inputs and outputs have a persistent location on S3, since <code>MemoryDataset</code> can't be shared between AWS Lambda functions</li> <li>Package the Kedro pipeline as an AWS Lambda-compliant Docker image</li> <li>Write a script to convert and deploy each Kedro node as an AWS Lambda function. Each function will use the same pipeline Docker image created in the previous step and run a single Kedro node associated with it. This follows the principles laid out in our distributed deployment guide.</li> <li>The script above will also convert and deploy the entire Kedro pipeline as an AWS Step Functions State Machine.</li> </ul> <p>The final deployed AWS Step Functions State Machine will have the following visualisation in AWS Management Console:</p> <p></p> <p>The rest of the tutorial will explain each step in the deployment process above in details.</p>"},{"location":"pages/deployment/aws_step_functions/#step-1-create-new-configuration-environment-to-prepare-a-compatible-datacatalog","title":"Step 1. Create new configuration environment to prepare a compatible <code>DataCatalog</code>","text":"<ul> <li>Create a <code>conf/aws</code> directory in your Kedro project</li> <li>Put a <code>catalog.yml</code> file in this directory with the following content</li> <li>Ensure that you have <code>s3fs&gt;=0.3.0,&lt;0.5</code> defined in your <code>requirements.txt</code> so the data can be read from S3.</li> </ul> Click to expand <pre><code>companies:\n  type: pandas.CSVDataset\n  filepath: s3://&lt;your-bucket&gt;/companies.csv\n\nreviews:\n  type: pandas.CSVDataset\n  filepath: s3://&lt;your-bucket&gt;/reviews.csv\n\nshuttles:\n  type: pandas.ExcelDataset\n  filepath: s3://&lt;your-bucket&gt;/shuttles.xlsx\n\npreprocessed_companies:\n  type: pandas.CSVDataset\n  filepath: s3://&lt;your-bucket&gt;/preprocessed_companies.csv\n\npreprocessed_shuttles:\n  type: pandas.CSVDataset\n  filepath: s3://&lt;your-bucket&gt;/preprocessed_shuttles.csv\n\nmodel_input_table:\n  type: pandas.CSVDataset\n  filepath: s3://&lt;your-bucket&gt;/model_input_table.csv\n\nregressor:\n  type: pickle.PickleDataset\n  filepath: s3://&lt;your-bucket&gt;/regressor.pickle\n  versioned: true\n\nX_train:\n  type: pickle.PickleDataset\n  filepath: s3://&lt;your-bucket&gt;/X_train.pickle\n\nX_test:\n  type: pickle.PickleDataset\n  filepath: s3://&lt;your-bucket&gt;/X_test.pickle\n\ny_train:\n  type: pickle.PickleDataset\n  filepath: s3://&lt;your-bucket&gt;/y_train.pickle\n\ny_test:\n  type: pickle.PickleDataset\n  filepath: s3://&lt;your-bucket&gt;/y_test.pickle\n</code></pre>"},{"location":"pages/deployment/aws_step_functions/#step-2-package-the-kedro-pipeline-as-an-aws-lambda-compliant-docker-image","title":"Step 2. Package the Kedro pipeline as an AWS Lambda-compliant Docker image","text":"<p>In December 2020, AWS announced that an AWS Lambda function can now use a container image up to 10 GB in size as its deployment package, besides the original zip method. As it has a few requirements for the container image to work properly, you must build your own custom Docker container image, both to contain the Kedro pipeline and to comply with Lambda's requirements.</p> <pre><code>All the following steps should be done in the Kedro project's root directory.\n</code></pre> <ul> <li>Step 2.1: Package the Kedro pipeline as a Python package so you can install it into the container later on:</li> </ul> <pre><code>$ kedro package\n</code></pre> <p>For more information, please visit the guide on packaging Kedro as a Python package.</p> <ul> <li>Step 2.2: Create a <code>lambda_handler.py</code> file:</li> </ul> <pre><code>from unittest.mock import patch\n\n\ndef handler(event, context):\n    from kedro.framework.project import configure_project\n\n    configure_project(\"spaceflights_step_functions\")\n    node_to_run = event[\"node_name\"]\n\n    # Since _multiprocessing.SemLock is not implemented on lambda yet,\n    # we mock it out so we could import the session. This has no impact on the correctness\n    # of the pipeline, as each Lambda function runs a single Kedro node, hence no need for Lock\n    # during import. For more information, please see this StackOverflow discussion:\n    # https://stackoverflow.com/questions/34005930/multiprocessing-semlock-is-not-implemented-when-running-on-aws-lambda\n    with patch(\"multiprocessing.Lock\"):\n        from kedro.framework.session import KedroSession\n\n        with KedroSession.create(env=\"aws\") as session:\n            session.run(node_names=[node_to_run])\n</code></pre> <p>This file acts as the handler for each Lambda function in our pipeline, receives the name of a Kedro node from a triggering event and executes it accordingly.</p> <ul> <li>Step 2.3: Create a <code>Dockerfile</code> to define the custom Docker image to act as the base for our Lambda functions:</li> </ul> <pre><code># Define global args\nARG FUNCTION_DIR=\"/home/app/\"\nARG RUNTIME_VERSION=\"3.9\"\n\n# Stage 1 - bundle base image + runtime\n# Grab a fresh copy of the image and install GCC\nFROM python:${RUNTIME_VERSION}-buster as build-image\n\n# Install aws-lambda-cpp build dependencies\nRUN apt-get update &amp;&amp; \\\n  apt-get install -y \\\n  g++ \\\n  make \\\n  cmake \\\n  unzip \\\n  libcurl4-openssl-dev\n\n# Include global args in this stage of the build\nARG FUNCTION_DIR\nARG RUNTIME_VERSION\n# Create the function directory\nRUN mkdir -p ${FUNCTION_DIR}\nRUN mkdir -p ${FUNCTION_DIR}/{conf}\n# Add handler function\nCOPY lambda_handler.py ${FUNCTION_DIR}\n# Add conf/ directory\nCOPY conf ${FUNCTION_DIR}/conf\n# Install Kedro pipeline\nCOPY dist/spaceflights_step_functions-0.1-py3-none-any.whl .\nRUN python${RUNTIME_VERSION} -m pip install --no-cache-dir spaceflights_step_functions-0.1-py3-none-any.whl --target ${FUNCTION_DIR}\n# Install Lambda Runtime Interface Client for Python\nRUN python${RUNTIME_VERSION} -m pip install --no-cache-dir awslambdaric --target ${FUNCTION_DIR}\n\n# Stage 3 - final runtime image\n# Grab a fresh copy of the Python image\nFROM python:${RUNTIME_VERSION}-buster\n# Include global arg in this stage of the build\nARG FUNCTION_DIR\n# Set working directory to function root directory\nWORKDIR ${FUNCTION_DIR}\n# Copy in the built dependencies\nCOPY --from=build-image ${FUNCTION_DIR} ${FUNCTION_DIR}\nENTRYPOINT [ \"/usr/local/bin/python\", \"-m\", \"awslambdaric\" ]\nCMD [ \"lambda_handler.handler\" ]\n</code></pre> <p>This <code>Dockerfile</code> is adapted from the official guide on how to create a custom image for Lambda to include Kedro-specific steps.</p> <ul> <li>Step 2.4: Build the Docker image and push it to AWS Elastic Container Registry (ECR):</li> </ul> <pre><code># build and tag the image\n$ docker build -t spaceflights-step-functions .\n$ docker tag spaceflights-step-functions:latest &lt;your-aws-account-id&gt;.dkr.ecr.&lt;your-aws-region&gt;.amazonaws.com/spaceflights-step-functions:latest\n#\u00a0login to ECR\n$ aws ecr get-login-password | docker login --username AWS --password-stdin &lt;your-aws-account-id&gt;.dkr.ecr.&lt;your-aws-region&gt;.amazonaws.com\n# push the image to ECR\n$ docker push &lt;your-aws-account-id&gt;.dkr.ecr.&lt;your-aws-region&gt;.amazonaws.com/spaceflights-step-functions:latest\n</code></pre>"},{"location":"pages/deployment/aws_step_functions/#step-3-write-the-deployment-script","title":"Step 3. Write the deployment script","text":"<p>As you will write our deployment script using AWS CDK in Python, you will have to install some required dependencies from CDK.</p> <ul> <li>Step 3.1: Create a <code>deploy_requirements.txt</code> with the following content:</li> </ul> <pre><code>aws_cdk.aws_s3\naws_cdk.core\naws-cdk.aws_ecr\naws-cdk.aws_lambda\naws-cdk.aws-stepfunctions\naws-cdk.aws-stepfunctions-tasks\n</code></pre> <p>Then install these dependencies with <code>pip</code>:</p> <pre><code>$ pip install -r deploy_requirements.txt\n</code></pre> <ul> <li>Step 3.2: Create a <code>deploy.py</code> file:</li> </ul> <pre><code>import re\nfrom pathlib import Path\n\nfrom aws_cdk import aws_stepfunctions as sfn\nfrom aws_cdk import aws_s3 as s3\nfrom aws_cdk import core, aws_lambda, aws_ecr\nfrom aws_cdk.aws_lambda import IFunction\nfrom aws_cdk.aws_stepfunctions_tasks import LambdaInvoke\nfrom kedro.framework.project import pipelines\nfrom kedro.framework.session import KedroSession\nfrom kedro.framework.startup import bootstrap_project\nfrom kedro.pipeline.node import Node\n\n\ndef _clean_name(name: str) -&gt; str:\n    \"\"\"Reformat a name to be compliant with AWS requirements for their resources.\n\n    Returns:\n        name: formatted name.\n    \"\"\"\n    return re.sub(r\"[\\W_]+\", \"-\", name).strip(\"-\")[:63]\n\n\nclass KedroStepFunctionsStack(core.Stack):\n    \"\"\"A CDK Stack to deploy a Kedro pipeline to AWS Step Functions.\"\"\"\n\n    env = \"aws\"\n    project_path = Path.cwd()\n    erc_repository_name = project_path.name\n    s3_data_bucket_name = (\n        \"spaceflights-step-functions\"  # this is where the raw data is located\n    )\n\n    def __init__(self, scope: core.Construct, id: str, **kwargs) -&gt; None:\n        super().__init__(scope, id, **kwargs)\n\n        self._parse_kedro_pipeline()\n        self._set_ecr_repository()\n        self._set_ecr_image()\n        self._set_s3_data_bucket()\n        self._convert_kedro_pipeline_to_step_functions_state_machine()\n\n    def _parse_kedro_pipeline(self) -&gt; None:\n        \"\"\"Extract the Kedro pipeline from the project\"\"\"\n        metadata = bootstrap_project(self.project_path)\n\n        self.project_name = metadata.project_name\n        self.pipeline = pipelines.get(\"__default__\")\n\n    def _set_ecr_repository(self) -&gt; None:\n        \"\"\"Set the ECR repository for the Lambda base image\"\"\"\n        self.ecr_repository = aws_ecr.Repository.from_repository_name(\n            self, id=\"ECR\", repository_name=self.erc_repository_name\n        )\n\n    def _set_ecr_image(self) -&gt; None:\n        \"\"\"Set the Lambda base image\"\"\"\n        self.ecr_image = aws_lambda.EcrImageCode(repository=self.ecr_repository)\n\n    def _set_s3_data_bucket(self) -&gt; None:\n        \"\"\"Set the S3 bucket containing the raw data\"\"\"\n        self.s3_bucket = s3.Bucket(\n            self, \"RawDataBucket\", bucket_name=self.s3_data_bucket_name\n        )\n\n    def _convert_kedro_node_to_lambda_function(self, node: Node) -&gt; IFunction:\n        \"\"\"Convert a Kedro node into an AWS Lambda function\"\"\"\n        func = aws_lambda.Function(\n            self,\n            id=_clean_name(f\"{node.name}_fn\"),\n            description=str(node),\n            code=self.ecr_image,\n            handler=aws_lambda.Handler.FROM_IMAGE,\n            runtime=aws_lambda.Runtime.FROM_IMAGE,\n            environment={},\n            function_name=_clean_name(node.name),\n            memory_size=256,\n            reserved_concurrent_executions=10,\n            timeout=core.Duration.seconds(15 * 60),\n        )\n        self.s3_bucket.grant_read_write(func)\n        return func\n\n    def _convert_kedro_node_to_sfn_task(self, node: Node) -&gt; LambdaInvoke:\n        \"\"\"Convert a Kedro node into an AWS Step Functions Task\"\"\"\n        return LambdaInvoke(\n            self,\n            _clean_name(node.name),\n            lambda_function=self._convert_kedro_node_to_lambda_function(node),\n            payload=sfn.TaskInput.from_object({\"node_name\": node.name}),\n        )\n\n    def _convert_kedro_pipeline_to_step_functions_state_machine(self) -&gt; None:\n        \"\"\"Convert Kedro pipeline into an AWS Step Functions State Machine\"\"\"\n        definition = sfn.Pass(self, \"Start\")\n\n        for i, group in enumerate(self.pipeline.grouped_nodes, 1):\n            group_name = f\"Group {i}\"\n            sfn_state = sfn.Parallel(self, group_name)\n            for node in group:\n                sfn_task = self._convert_kedro_node_to_sfn_task(node)\n                sfn_state.branch(sfn_task)\n\n            definition = definition.next(sfn_state)\n\n        sfn.StateMachine(\n            self,\n            self.project_name,\n            definition=definition,\n            timeout=core.Duration.seconds(5 * 60),\n        )\n\n\napp = core.App()\nKedroStepFunctionsStack(app, \"KedroStepFunctionsStack\")\napp.synth()\n</code></pre> <p>This script contains the logic to convert a Kedro pipeline into an AWS Step Functions State Machine with each Kedro node defined as a Lambda function using the Docker image in Step 2. You will then need to register it with CDK by creating a <code>cdk.json</code> with the following content:</p> <pre><code>{\n  \"app\": \"python3 deploy.py\"\n}\n</code></pre> <p>And that's it! You are now ready to deploy and run the Kedro pipeline on AWS Step Functions.</p>"},{"location":"pages/deployment/aws_step_functions/#step-4-deploy-the-pipeline","title":"Step 4. Deploy the pipeline","text":"<p>Deploying with CDK is very straightforward. You just need to run:</p> <pre><code>$ cdk deploy\n</code></pre> <p>After the deployment finishes, when you log into AWS Management Console, you should be able to see an AWS Step Functions State Machine created for your pipeline:</p> <p></p> <p>As well as the corresponding Lambda functions for each Kedro node:</p> <p></p> <p>If you go into the state machine and click on <code>Start Execution</code>, you will be able to see a full end-to-end (E2E) run of the Kedro pipeline on AWS Step Functions.</p>"},{"location":"pages/deployment/aws_step_functions/#limitations","title":"Limitations","text":"<p>Generally speaking, the limitations on AWS Lambda have improved dramatically in recent years. However, it's still worth noting that each Lambda function has a 15-minute timeout, 10GB maximum memory limit and 10GB container image code package size limit. This means, for example, if you have a node that takes longer than 15 minutes to run, you should switch to some other AWS services, such as AWS Batch or AWS ECS, to execute that node.</p>"},{"location":"pages/deployment/azure/","title":"Azure ML pipelines","text":""},{"location":"pages/deployment/azure/#kedro-azureml-plugin","title":"<code>kedro-azureml</code> plugin","text":"<p>For deployment to Azure ML pipelines, you should consult the documentation for the <code>kedro-azureml</code> plugin from GetInData | Part of Xebia that enables you to run your code on Azure ML Pipelines in a fully managed fashion.</p> <p>The plugin supports both: docker-based workflows and code-upload workflows. Besides that, kedro-azureml also supports distributed training in PyTorch/TensorFlow/MPI and works well with Azure ML native MLflow integration.</p>"},{"location":"pages/deployment/dask/","title":"Dask","text":"<p>This page explains how to distribute execution of the nodes composing your Kedro pipeline using Dask, a flexible, open-source library for parallel computing in Python.</p> <p>Dask offers both a default, single-machine scheduler and a more sophisticated, distributed scheduler. The newer <code>dask.distributed</code> scheduler is often preferable, even on single workstations, and is the focus of our deployment guide. For more information on the various ways to set up Dask on varied hardware, see the official Dask how-to guide.</p>"},{"location":"pages/deployment/dask/#why-would-you-use-dask","title":"Why would you use Dask?","text":"<p><code>Dask.distributed</code> is a lightweight library for distributed computing in Python. It complements the existing PyData analysis stack, which forms the basis of many Kedro pipelines. It's also pure Python, which eases installation and simplifies debugging. For further motivation on why people choose to adopt Dask, and, more specifically, <code>dask.distributed</code>, see Why Dask? and the <code>dask.distributed</code> documentation, respectively.</p>"},{"location":"pages/deployment/dask/#prerequisites","title":"Prerequisites","text":"<p>The only additional requirement, beyond what was already required by your Kedro pipeline, is to install <code>dask.distributed</code>. To review the full installation instructions, including how to set up Python virtual environments, see our Get Started guide.</p>"},{"location":"pages/deployment/dask/#how-to-distribute-your-kedro-pipeline-using-dask","title":"How to distribute your Kedro pipeline using Dask","text":""},{"location":"pages/deployment/dask/#create-a-custom-runner","title":"Create a custom runner","text":"<p>Create a new Python package <code>runner</code> in your <code>src</code> folder, i.e. <code>kedro_tutorial/src/kedro_tutorial/runner/</code>. Make sure there is an <code>__init__.py</code> file at this location, and add another file named <code>dask_runner.py</code>, which will contain the implementation of your custom runner, <code>DaskRunner</code>. The <code>DaskRunner</code> will submit and monitor tasks asynchronously, surfacing any errors that occur during execution.</p> <p>Make sure the <code>__init__.py</code> file in the <code>runner</code> folder includes the following import and declaration:</p> <pre><code>from .dask_runner import DaskRunner\n\n__all__ = [\"DaskRunner\"]\n</code></pre> <p>Copy the contents of the script below into <code>dask_runner.py</code>:</p> <pre><code>\"\"\"``DaskRunner`` is an ``AbstractRunner`` implementation. It can be\nused to distribute execution of ``Node``s in the ``Pipeline`` across\na Dask cluster, taking into account the inter-``Node`` dependencies.\n\"\"\"\nfrom collections import Counter\nfrom itertools import chain\nfrom typing import Any, Dict\n\nfrom distributed import Client, as_completed, worker_client\nfrom kedro.framework.hooks.manager import (\n    _create_hook_manager,\n    _register_hooks,\n    _register_hooks_entry_points,\n)\nfrom kedro.framework.project import settings\nfrom kedro.io import AbstractDataset, DataCatalog\nfrom kedro.pipeline import Pipeline\nfrom kedro.pipeline.node import Node\nfrom kedro.runner import AbstractRunner, run_node\nfrom pluggy import PluginManager\n\n\nclass _DaskDataset(AbstractDataset):\n    \"\"\"``_DaskDataset`` publishes/gets named datasets to/from the Dask\n    scheduler.\"\"\"\n\n    def __init__(self, name: str):\n        self._name = name\n\n    def _load(self) -&gt; Any:\n        try:\n            with worker_client() as client:\n                return client.get_dataset(self._name)\n        except ValueError:\n            # Upon successfully executing the pipeline, the runner loads\n            # free outputs on the scheduler (as opposed to on a worker).\n            Client.current().get_dataset(self._name)\n\n    def _save(self, data: Any) -&gt; None:\n        with worker_client() as client:\n            client.publish_dataset(data, name=self._name, override=True)\n\n    def _exists(self) -&gt; bool:\n        return self._name in Client.current().list_datasets()\n\n    def _release(self) -&gt; None:\n        Client.current().unpublish_dataset(self._name)\n\n    def _describe(self) -&gt; Dict[str, Any]:\n        return dict(name=self._name)\n\n\nclass DaskRunner(AbstractRunner):\n    \"\"\"``DaskRunner`` is an ``AbstractRunner`` implementation. It can be\n    used to distribute execution of ``Node``s in the ``Pipeline`` across\n    a Dask cluster, taking into account the inter-``Node`` dependencies.\n    \"\"\"\n\n    def __init__(self, client_args: Dict[str, Any] = {}, is_async: bool = False):\n        \"\"\"Instantiates the runner by creating a ``distributed.Client``.\n\n        Args:\n            client_args: Arguments to pass to the ``distributed.Client``\n                constructor.\n            is_async: If True, the node inputs and outputs are loaded and saved\n                asynchronously with threads. Defaults to False.\n        \"\"\"\n        super().__init__(is_async=is_async)\n        Client(**client_args)\n\n    def __del__(self):\n        Client.current().close()\n\n    def create_default_dataset(self, ds_name: str) -&gt; _DaskDataset:\n        \"\"\"Factory method for creating the default dataset for the runner.\n\n        Args:\n            ds_name: Name of the missing dataset.\n\n        Returns:\n            An instance of ``_DaskDataset`` to be used for all\n            unregistered datasets.\n        \"\"\"\n        return _DaskDataset(ds_name)\n\n    @staticmethod\n    def _run_node(\n        node: Node,\n        catalog: DataCatalog,\n        is_async: bool = False,\n        session_id: str = None,\n        *dependencies: Node,\n    ) -&gt; Node:\n        \"\"\"Run a single `Node` with inputs from and outputs to the `catalog`.\n\n        Wraps ``run_node`` to accept the set of ``Node``s that this node\n        depends on. When ``dependencies`` are futures, Dask ensures that\n        the upstream node futures are completed before running ``node``.\n\n        A ``PluginManager`` instance is created on each worker because the\n        ``PluginManager`` can't be serialised.\n\n        Args:\n            node: The ``Node`` to run.\n            catalog: A ``DataCatalog`` containing the node's inputs and outputs.\n            is_async: If True, the node inputs and outputs are loaded and saved\n                asynchronously with threads. Defaults to False.\n            session_id: The session id of the pipeline run.\n            dependencies: The upstream ``Node``s to allow Dask to handle\n                dependency tracking. Their values are not actually used.\n\n        Returns:\n            The node argument.\n        \"\"\"\n        hook_manager = _create_hook_manager()\n        _register_hooks(hook_manager, settings.HOOKS)\n        _register_hooks_entry_points(hook_manager, settings.DISABLE_HOOKS_FOR_PLUGINS)\n\n        return run_node(node, catalog, hook_manager, is_async, session_id)\n\n    def _run(\n        self,\n        pipeline: Pipeline,\n        catalog: DataCatalog,\n        hook_manager: PluginManager,\n        session_id: str = None,\n    ) -&gt; None:\n        nodes = pipeline.nodes\n        load_counts = Counter(chain.from_iterable(n.inputs for n in nodes))\n        node_dependencies = pipeline.node_dependencies\n        node_futures = {}\n\n        client = Client.current()\n        for node in nodes:\n            dependencies = (\n                node_futures[dependency] for dependency in node_dependencies[node]\n            )\n            node_futures[node] = client.submit(\n                DaskRunner._run_node,\n                node,\n                catalog,\n                self._is_async,\n                session_id,\n                *dependencies,\n            )\n\n        for i, (_, node) in enumerate(\n            as_completed(node_futures.values(), with_results=True)\n        ):\n            self._logger.info(\"Completed node: %s\", node.name)\n            self._logger.info(\"Completed %d out of %d tasks\", i + 1, len(nodes))\n\n            # Decrement load counts, and release any datasets we\n            # have finished with. This is particularly important\n            # for the shared, default datasets we created above.\n            for dataset in node.inputs:\n                load_counts[dataset] -= 1\n                if load_counts[dataset] &lt; 1 and dataset not in pipeline.inputs():\n                    catalog.release(dataset)\n            for dataset in node.outputs:\n                if load_counts[dataset] &lt; 1 and dataset not in pipeline.outputs():\n                    catalog.release(dataset)\n\n    def run_only_missing(\n        self, pipeline: Pipeline, catalog: DataCatalog\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Run only the missing outputs from the ``Pipeline`` using the\n        datasets provided by ``catalog``, and save results back to the\n        same objects.\n\n        Args:\n            pipeline: The ``Pipeline`` to run.\n            catalog: The ``DataCatalog`` from which to fetch data.\n        Raises:\n            ValueError: Raised when ``Pipeline`` inputs cannot be\n                satisfied.\n\n        Returns:\n            Any node outputs that cannot be processed by the\n            ``DataCatalog``. These are returned in a dictionary, where\n            the keys are defined by the node outputs.\n        \"\"\"\n        free_outputs = pipeline.outputs() - set(catalog.list())\n        missing = {ds for ds in catalog.list() if not catalog.exists(ds)}\n        to_build = free_outputs | missing\n        to_rerun = pipeline.only_nodes_with_outputs(*to_build) + pipeline.from_inputs(\n            *to_build\n        )\n\n        # We also need any missing datasets that are required to run the\n        # `to_rerun` pipeline, including any chains of missing datasets.\n        unregistered_ds = pipeline.datasets() - set(catalog.list())\n        # Some of the unregistered datasets could have been published to\n        # the scheduler in a previous run, so we need not recreate them.\n        missing_unregistered_ds = {\n            ds_name\n            for ds_name in unregistered_ds\n            if not self.create_default_dataset(ds_name).exists()\n        }\n        output_to_unregistered = pipeline.only_nodes_with_outputs(\n            *missing_unregistered_ds\n        )\n        input_from_unregistered = to_rerun.inputs() &amp; missing_unregistered_ds\n        to_rerun += output_to_unregistered.to_outputs(*input_from_unregistered)\n\n        # We need to add any previously-published, unregistered datasets\n        # to the catalog passed to the `run` method, so that it does not\n        # think that the `to_rerun` pipeline's inputs are not satisfied.\n        catalog = catalog.shallow_copy()\n        for ds_name in unregistered_ds - missing_unregistered_ds:\n            catalog.add(ds_name, self.create_default_dataset(ds_name))\n\n        return self.run(to_rerun, catalog)\n</code></pre>"},{"location":"pages/deployment/dask/#update-cli-implementation","title":"Update CLI implementation","text":"<p>You're nearly there! Before you can use the new runner, you need to add a <code>cli.py</code> file at the same level as <code>settings.py</code>, using the template we provide. Update the <code>run()</code> function in the newly-created <code>cli.py</code> file to make sure the runner class is instantiated correctly:</p> <pre><code>def run(tag, env, ...):\n    \"\"\"Run the pipeline.\"\"\"\n    runner = runner or \"SequentialRunner\"\n\n    tag = _get_values_as_tuple(tag) if tag else tag\n    node_names = _get_values_as_tuple(node_names) if node_names else node_names\n\n    with KedroSession.create(env=env, extra_params=params) as session:\n        context = session.load_context()\n        runner_instance = _instantiate_runner(runner, is_async, context)\n        session.run(\n            tags=tag,\n            runner=runner_instance,\n            node_names=node_names,\n            from_nodes=from_nodes,\n            to_nodes=to_nodes,\n            from_inputs=from_inputs,\n            to_outputs=to_outputs,\n            load_versions=load_version,\n            pipeline_name=pipeline,\n        )\n</code></pre> <p>where the helper function <code>_instantiate_runner()</code> looks like this:</p> <pre><code>def _instantiate_runner(runner, is_async, project_context):\n    runner_class = load_obj(runner, \"kedro.runner\")\n    runner_kwargs = dict(is_async=is_async)\n\n    if runner.endswith(\"DaskRunner\"):\n        client_args = project_context.params.get(\"dask_client\") or {}\n        runner_kwargs.update(client_args=client_args)\n\n    return runner_class(**runner_kwargs)\n</code></pre>"},{"location":"pages/deployment/dask/#deploy","title":"Deploy","text":"<p>You're now ready to trigger the run. Without any further configuration, the underlying Dask <code>Client</code> creates a <code>LocalCluster</code> in the background and connects to that:</p> <pre><code>kedro run --runner=kedro_tutorial.runner.DaskRunner\n</code></pre>"},{"location":"pages/deployment/dask/#set-up-dask-and-related-configuration","title":"Set up Dask and related configuration","text":"<p>To connect to an existing Dask cluster, you'll need to set the Dask-related configuration that the runner will use. Create the <code>conf/dask/</code> directory and add a <code>parameters.yml</code> file inside of it with the following keys:</p> <pre><code>dask_client:\n  address: 127.0.0.1:8786\n</code></pre> <p>Next, set up scheduler and worker processes on your local computer:</p> <pre><code>$ dask-scheduler\nScheduler started at 127.0.0.1:8786\n\n$ PYTHONPATH=$PWD/src dask-worker 127.0.0.1:8786\n$ PYTHONPATH=$PWD/src dask-worker 127.0.0.1:8786\n$ PYTHONPATH=$PWD/src dask-worker 127.0.0.1:8786\n</code></pre> <pre><code>The above code snippet assumes each worker is started from the root directory of the Kedro project in a Python environment where all required dependencies are installed.\n</code></pre> <p>You're once again ready to trigger the run. Execute the following command:</p> <pre><code>kedro run --env=dask --runner=kedro_tutorial.runner.DaskRunner\n</code></pre> <p>You should start seeing tasks appearing on Dask's diagnostics dashboard:</p> <p></p>"},{"location":"pages/deployment/distributed/","title":"Distributed deployment","text":"<p>This topic explains how to deploy Kedro in a distributed system.</p> <p>Distributed applications refer to software that runs on multiple computers within a network at the same time and can be stored on servers or with cloud computing. Unlike traditional applications that run on a single machine, distributed applications run on multiple systems simultaneously for a single task or job.</p> <p>You may select to use a distributed system if your Kedro pipelines are very compute-intensive to benefit from the cloud's elasticity and scalability to manage compute resources.</p> <p>As a distributed deployment strategy, we recommend the following series of steps:</p>"},{"location":"pages/deployment/distributed/#1-containerise-the-pipeline","title":"1. Containerise the pipeline","text":"<p>For better dependency management, we encourage you to containerise the entire pipeline/project. We recommend using Docker, but you're free to use any preferred container solution available to you. For the purpose of this walk-through, we are going to assume a <code>Docker</code> workflow.</p> <p>Firstly make sure your project requirements are up-to-date by running:</p> <pre><code>pip-compile --output-file=&lt;project_root&gt;/requirements.txt --input-file=&lt;project_root&gt;/requirements.txt\n</code></pre> <p>We then recommend the <code>Kedro-Docker</code> plugin to streamline the process of building the image. Instructions for using this are in the plugin's README.md.</p> <p>After you\u2019ve built the Docker image for your project locally, you would typically have to transfer the image to a container registry, such as DockerHub or AWS Elastic Container Registry, to be able to pull it on your remote servers. You can find instructions on how to do so in our guide for single-machine deployment.</p>"},{"location":"pages/deployment/distributed/#2-convert-your-kedro-pipeline-into-targeted-platform-primitives","title":"2. Convert your Kedro pipeline into targeted platform primitives","text":"<p>A Kedro pipeline benefits from a structure that's normally easy to translate (at least semantically) into the language that different platforms would understand. A DAG of <code>nodes</code> can be converted into a series of tasks where each node maps to an individual task, whether it being a Kubeflow operator, an AWS Batch job, etc, and the dependencies are the same as those mapped in <code>Pipeline.node_dependencies</code>.</p> <p>To perform the conversion programmatically, you will need to develop a script. Make sure you save all your catalog entries to a remote location and that you make best use of node <code>names</code> as <code>tags</code> in your pipeline to simplify the process. For example, you should name all nodes, and use a programmer-friendly naming convention.</p>"},{"location":"pages/deployment/distributed/#3-parameterise-the-runs","title":"3. Parameterise the runs","text":"<p>A <code>node</code> typically corresponds to a unit of compute, which can be run by parameterising the basic <code>kedro run</code>:</p> <pre><code>kedro run --nodes=&lt;node_name&gt;\n</code></pre> <p>We encourage you to play with different ways of parameterising your runs as you see fit. Use names, tags, custom flags, in preference to making a code change to execute different behaviour. All your jobs/tasks/operators/etc. should have the same version of the code, i.e. same Docker image, to run on.</p>"},{"location":"pages/deployment/distributed/#4-optional-create-starters","title":"4. (Optional) Create starters","text":"<p>You may opt to build your own Kedro starter if you regularly have to deploy in a similar environment or to a similar platform. The starter enables you to re-use any deployment scripts written as part of step 2.</p>"},{"location":"pages/deployment/kubeflow/","title":"Kubeflow Pipelines","text":""},{"location":"pages/deployment/kubeflow/#why-would-you-use-kubeflow-pipelines","title":"Why would you use Kubeflow Pipelines?","text":"<p>Kubeflow Pipelines is an end-to-end (E2E) orchestration tool to deploy, scale and manage your machine learning systems within Docker containers. You can schedule and compare runs, and examine detailed reports on each run.</p> <p>Here are the main reasons to use Kubeflow Pipelines:</p> <ul> <li>It is cloud-agnostic and can run on any Kubernetes cluster</li> <li>Kubeflow is tailored towards machine learning workflows for model deployment, experiment tracking, and hyperparameter tuning</li> <li>You can re-use components and pipelines to create E2E solutions</li> </ul>"},{"location":"pages/deployment/kubeflow/#the-kedro-kubeflow-plugin","title":"The <code>kedro-kubeflow</code> plugin","text":"<p>The <code>kedro-kubeflow</code> plugin from GetInData | Part of Xebia enables you to run a Kedro pipeline on Kubeflow Pipelines. Consult the GitHub repository for <code>kedro-kubeflow</code> for further details, or take a look at the documentation.</p>"},{"location":"pages/deployment/nodes_grouping/","title":"Nodes grouping in Kedro: pipelines, tags, and namespaces","text":"<p>Effectively grouping nodes in deployment is crucial for maintainability, debugging, and execution control. This document provides an overview of three key grouping methods: pipelines, tags, and namespaces, along with their strengths, limitations, best uses, and relevant documentation links.</p>"},{"location":"pages/deployment/nodes_grouping/#grouping-by-pipelines","title":"Grouping by pipelines","text":"<p>If your project contains different pipelines, you can use them as predefined node groupings for deployment. Pipelines can be executed separately in the deployment environment. With the visualisation in Kedro Viz, you can switch to see different pipelines in an isolated view.  </p> <p>If you want to group nodes differently from the existing pipeline structure, you can use tags or namespaces instead of creating a new pipeline. The <code>kedro run --pipeline</code> command allows running one pipeline at a time, so multiple pipelines cannot be executed in a single step. While you can switch between pipelines in Kedro Viz, the flowchart view does not support collapsing or expanding them.</p> <p>Best used when - You have already separated your logic into different pipelines, and your project is structured to execute them independently in the deployment environment.</p> <p>Not to use when - You need to run more than one pipeline at a time. - You want to use the expand and collapse functionality in Kedro Viz.</p> <p>How to use</p> <pre><code>  kedro run --pipeline=&lt;your_pipeline_name&gt;\n</code></pre> <p>More information: Run a pipeline by name</p>"},{"location":"pages/deployment/nodes_grouping/#grouping-by-tags","title":"Grouping by tags","text":"<p>You can tag individual nodes or the entire pipeline, allowing flexible execution of specific sections without modifying the pipeline structure. Kedro-Viz provides a clear visualisation of tagged nodes, making it easier to understand.  </p> <p>Please note that nodes with the same tag can exist in different pipelines, making debugging and maintaining the codebase more challenging, and tags do not enforce structure like pipelines or namespaces.</p> <p>Best used when - You need to run specific nodes that don\u2019t belong to the same pipeline. - You want to rerun a subset of nodes in a large pipeline.</p> <p>Not to use when - The tagged nodes have strong dependencies, which might cause execution failures. - Tags are not hierarchical, so tracking groups of nodes can become difficult.</p> <p>How to use</p> <pre><code>  kedro run --tags=&lt;your_tag_name&gt;\n</code></pre> <p>More information: How to tag a node</p>"},{"location":"pages/deployment/nodes_grouping/#grouping-by-namespaces","title":"Grouping by namespaces","text":"<p>Namespaces allow you to group nodes, ensuring clear dependencies and separation within a pipeline while maintaining a consistent structure. Like with pipelines or tags, you can enable selective execution using namespaces, and you cannot run more than one namespace simultaneously\u2014Kedro allows executing one namespace at a time. Kedro Viz allows expanding and collapsing namespace pipelines in the visualisation.  </p> <p>Using namespaces comes with a few challenges: - Defining namespace at Pipeline-level: When applying a namespace at the pipeline level, Kedro automatically renames all inputs, outputs, and parameters within that pipeline. You will need to update your catalog accordingly. If you don't want to change the names of your inputs, outputs, or parameters with the <code>namespace_name.</code> prefix while using a namespace, you should list these objects inside the corresponding parameters of the <code>pipeline()</code> creation function. For example:</p> <pre><code>return pipeline(\n    base_pipeline,\n    namespace = \"new_namespaced_pipeline\", # With that namespace, \"new_namespaced_pipeline\" prefix will be added to inputs, outputs, params, and node names\n    inputs={\"the_original_input_name\"}, # Inputs remain the same, without namespace prefix\n)\n</code></pre> <ul> <li>Defining namespace at Node-level: Defining namespaces at node level is not recommended for grouping your nodes. The node level definition of namespaces should be used for creating collapsible views on Kedro-Viz for high level representation of your nodes. If you define namespaces at the node level, they behave similarly to tags and do not guarantee execution consistency.</li> </ul> <p>Best used when - You want to organise nodes logically within a pipeline while keeping a structured execution flow. You can also nest namespaced pipelines within each other for visualisation. - Your pipeline structure is well-defined, and using namespaces improves visualisation in Kedro-Viz.</p> <p>Not to use when - In small and simple projects, using namespaces can introduce unnecessary complexity, making pipeline grouping a more suitable choice. - Namespaces require additional effort, such as updating catalog names, since namespace prefixes are automatically applied to all the elements unless explicitly overridden in the namespaced pipeline parameters.</p> <p>How to use</p> <pre><code>  kedro run --namespace=&lt;your_namespace_name&gt;\n</code></pre> <p>More information: Namespaces</p> <p>Summary table</p> Aspect Pipelines Tags Namespaces What Works If you're happy with how the nodes are structured in your existing pipeline, or your pipeline is low complexity and a new grouping view is not required then you don't have to use any alternatives Tagging individual nodes or the entire pipeline allows flexible execution of specific sections without altering the pipeline structure, and Kedro-Viz offers clear visualisation of these tagged nodes for better understanding. Namespaces group nodes to ensure clear dependencies and separation within a pipeline, allow selective execution, and can be visualised using Kedro-Viz. What Doesn't Work If you want to group nodes differently from the current pipeline structure, instead of creating a new pipeline, you can use alternative grouping methods such as tags or namespaces. Lack of hierarchical structure, using tags makes debugging and maintaining the codebase more challenging Defining namespaces at the node level behaves like tags without ensuring execution consistency, while defining them at the pipeline level helps with modularisation by renaming inputs, outputs, and parameters but can introduce naming conflicts if the pipeline is connected elsewhere or parameters are referenced outside the pipeline. Syntax <code>kedro run --pipeline=&lt;your_pipeline_name&gt;</code> <code>kedro run --tags=&lt;your_tag_name&gt;</code> <code>kedro run --namespace=&lt;your_namespace_name&gt;</code>"},{"location":"pages/deployment/prefect/","title":"Prefect","text":"<p>This page explains how to run your Kedro pipeline using Prefect 2.0, an open-source workflow management system.</p> <p>The scope of this documentation is the deployment to a self hosted Prefect Server, which is an open-source backend that makes it easy to monitor and execute your Prefect flows and automatically extends Prefect 2.0. We will use an Agent that dequeues submitted flow runs from a Work Queue.</p> <p>Note This deployment has been tested using Kedro 0.18.10 with Prefect version 2.10.17. If you want to deploy with Prefect 1.0, we recommend you review earlier versions of Kedro's Prefect deployment documentation.</p>"},{"location":"pages/deployment/prefect/#prerequisites","title":"Prerequisites","text":"<p>To use Prefect 2.0 and Prefect Server, ensure you have the following prerequisites in place:</p> <ul> <li>Prefect 2.0 is installed on your machine</li> </ul>"},{"location":"pages/deployment/prefect/#setup","title":"Setup","text":"<p>Configure your <code>PREFECT_API_URL</code> to point to your local Prefect instance:</p> <pre><code>prefect config set PREFECT_API_URL=\"http://127.0.0.1:4200/api\"\n</code></pre> <p>For each new Kedro project you create, you need to decide whether to opt into usage analytics. Your decision is recorded in the <code>.telemetry</code> file stored in the project root.</p> <p>Important When you run a Kedro project locally, you are asked on the first <code>kedro</code> command for the project, but in this use case, the project will hang unless you follow these instructions.</p> <p>Create a <code>.telemetry</code> file manually and put it in the root of your Kedro project and add your preference to give or decline consent. To do this, specify either <code>true</code> (to give consent) or <code>false</code>. The example given below accepts Kedro's usage analytics.</p> <pre><code>consent: true\n</code></pre> <p>Run a Prefect Server instance:</p> <pre><code>prefect server start\n</code></pre> <p>In a separate terminal, create a work pool to organise the work and create a work queue for your agent to pull from:</p> <pre><code>prefect work-pool create --type prefect-agent &lt;work_pool_name&gt;\nprefect work-queue create --pool &lt;work_pool_name&gt; &lt;work_queue_name&gt;\n</code></pre> <p>Now run a Prefect Agent that subscribes to a work queue inside the work pool you created:</p> <pre><code>prefect agent start --pool &lt;work_pool_name&gt; --work-queue &lt;work_queue_name&gt;\n</code></pre>"},{"location":"pages/deployment/prefect/#how-to-run-your-kedro-pipeline-using-prefect-20","title":"How to run your Kedro pipeline using Prefect 2.0","text":""},{"location":"pages/deployment/prefect/#convert-your-kedro-pipeline-to-prefect-20-flow","title":"Convert your Kedro pipeline to Prefect 2.0 flow","text":"<p>To build a Prefect flow for your Kedro pipeline programmatically and register it with the Prefect API, use the following Python script, which should be stored in your project\u2019s root directory:</p> <pre><code># &lt;project_root&gt;/register_prefect_flow.py\nimport click\nfrom pathlib import Path\nfrom typing import Dict, List, Union, Callable\n\nfrom kedro.framework.hooks.manager import _create_hook_manager\nfrom kedro.framework.project import pipelines\nfrom kedro.framework.session import KedroSession\nfrom kedro.framework.startup import bootstrap_project\nfrom kedro.io import DataCatalog, MemoryDataset\nfrom kedro.pipeline.node import Node\nfrom kedro.runner import run_node\n\nfrom prefect import flow, task, get_run_logger\nfrom prefect.deployments import Deployment\n\n\n@click.command()\n@click.option(\"-p\", \"--pipeline\", \"pipeline_name\", default=\"__default__\")\n@click.option(\"--env\", \"-e\", type=str, default=\"base\")\n@click.option(\"--deployment_name\", \"deployment_name\", default=\"example\")\n@click.option(\"--work_pool_name\", \"work_pool_name\", default=\"default\")\n@click.option(\"--work_queue_name\", \"work_queue_name\", default=\"default\")\n@click.option(\"--version\", \"version\", default=\"1.0\")\ndef prefect_deploy(\n    pipeline_name, env, deployment_name, work_pool_name, work_queue_name, version\n):\n    \"\"\"Register a Kedro pipeline as a Prefect flow.\"\"\"\n\n    # Pipeline name to execute\n    pipeline_name = pipeline_name or \"__default__\"\n\n    # Use standard deployment configuration for local execution. If you require a different\n    # infrastructure, check the API docs for Deployments at: https://docs.prefect.io/latest/api-ref/prefect/deployments/\n    deployment = Deployment.build_from_flow(\n        flow=my_flow,\n        name=deployment_name,\n        path=str(Path.cwd()),\n        version=version,\n        parameters={\n            \"pipeline_name\": pipeline_name,\n            \"env\": env,\n        },\n        infra_overrides={\"env\": {\"PREFECT_LOGGING_LEVEL\": \"DEBUG\"}},\n        work_pool_name=work_pool_name,\n        work_queue_name=work_queue_name,\n    )\n\n    deployment.apply()\n\n\n@flow(name=\"my_flow\")\ndef my_flow(pipeline_name: str, env: str):\n    logger = get_run_logger()\n    project_path = Path.cwd()\n\n    metadata = bootstrap_project(project_path)\n    logger.info(\"Project name: %s\", metadata.project_name)\n\n    logger.info(\"Initializing Kedro...\")\n    execution_config = kedro_init(\n        pipeline_name=pipeline_name, project_path=project_path, env=env\n    )\n\n    logger.info(\"Building execution layers...\")\n    execution_layers = init_kedro_tasks_by_execution_layer(\n        pipeline_name, execution_config\n    )\n\n    for layer in execution_layers:\n        logger.info(\"Running layer...\")\n        for node_task in layer:\n            logger.info(\"Running node...\")\n            node_task()\n\n\n@task()\ndef kedro_init(\n    pipeline_name: str,\n    project_path: Path,\n    env: str,\n):\n    \"\"\"\n    Initializes a Kedro session and returns the DataCatalog and\n    KedroSession\n    \"\"\"\n    # bootstrap project within task / flow scope\n\n    logger = get_run_logger()\n    logger.info(\"Bootstrapping project\")\n    bootstrap_project(project_path)\n\n    session = KedroSession.create(\n        project_path=project_path,\n        env=env,\n    )\n    # Note that for logging inside a Prefect task logger is used.\n    logger.info(\"Session created with ID %s\", session.session_id)\n    pipeline = pipelines.get(pipeline_name)\n    logger.info(\"Loading context...\")\n    context = session.load_context()\n    catalog = context.catalog\n    logger.info(\"Registering datasets...\")\n    unregistered_ds = pipeline.datasets() - set(catalog.list())\n    for ds_name in unregistered_ds:\n        catalog.add(ds_name, MemoryDataset())\n    return {\"catalog\": catalog, \"sess_id\": session.session_id}\n\n\ndef init_kedro_tasks_by_execution_layer(\n    pipeline_name: str,\n    execution_config: Union[None, Dict[str, Union[DataCatalog, str]]] = None,\n) -&gt; List[List[Callable]]:\n    \"\"\"\n    Inits the Kedro tasks ordered topologically in groups, which implies that an earlier group\n    is the dependency of later one.\n\n    Args:\n        pipeline_name (str): The pipeline name to execute\n        execution_config (Union[None, Dict[str, Union[DataCatalog, str]]], optional):\n        The required execution config for each node. Defaults to None.\n\n    Returns:\n        List[List[Callable]]: A list of topologically ordered task groups\n    \"\"\"\n\n    pipeline = pipelines.get(pipeline_name)\n\n    execution_layers = []\n\n    # Return a list of the pipeline nodes in topologically ordered groups,\n    #  i.e. if node A needs to be run before node B, it will appear in an\n    #  earlier group.\n    for layer in pipeline.grouped_nodes:\n        execution_layer = []\n        for node in layer:\n            # Use a function for task instantiation which avoids duplication of\n            # tasks\n            task = instantiate_task(node, execution_config)\n            execution_layer.append(task)\n        execution_layers.append(execution_layer)\n\n    return execution_layers\n\n\ndef kedro_task(\n    node: Node, task_dict: Union[None, Dict[str, Union[DataCatalog, str]]] = None\n):\n    run_node(\n        node,\n        task_dict[\"catalog\"],\n        _create_hook_manager(),\n        task_dict[\"sess_id\"],\n    )\n\n\ndef instantiate_task(\n    node: Node,\n    execution_config: Union[None, Dict[str, Union[DataCatalog, str]]] = None,\n) -&gt; Callable:\n    \"\"\"\n    Function that wraps a Node inside a task for future execution\n\n    Args:\n        node: Kedro node for which a Prefect task is being created.\n        execution_config: The configurations required for the node to execute\n        that includes catalogs and session id\n\n    Returns: Prefect task for the passed node\n\n    \"\"\"\n    return task(lambda: kedro_task(node, execution_config)).with_options(name=node.name)\n\n\nif __name__ == \"__main__\":\n    prefect_deploy()\n</code></pre> <p>Then, run the deployment script in other terminal:</p> <pre><code>python register_prefect_flow.py --work_pool_name &lt;work_pool_name&gt; --work_queue_name &lt;work_queue_name&gt;\n</code></pre> <pre><code>Be sure that your Prefect Server is up and running. Verify that the deployment script arguments match the work pool and work queue names.\n</code></pre>"},{"location":"pages/deployment/prefect/#run-prefect-flow","title":"Run Prefect flow","text":"<p>Now, having the flow registered, you can use Prefect Server UI to orchestrate and monitor it.</p> <p>Navigate to http://localhost:4200/deployments to see your registered flow.</p> <p></p> <p>Click on the flow to open it and then trigger your flow using the \"RUN\" &gt; \"QUICK RUN\" button and leave the parameters by default. If you want to run a specific pipeline you can replace the <code>__default__</code> value.</p> <pre><code>Be sure that both your Prefect Server and Agent are up and running.\n</code></pre> <p></p>"},{"location":"pages/deployment/single_machine/","title":"Single-machine deployment","text":"<p>This topic explains how to deploy Kedro on a production server. You can use three alternative methods to deploy your Kedro pipelines:</p> <ul> <li>Container-based deployment</li> <li>Package-based deployment</li> <li>CLI-based deployment</li> </ul>"},{"location":"pages/deployment/single_machine/#container-based","title":"Container-based","text":"<p>This approach uses containers, such as <code>Docker</code> or any other container solution, to build an image and run the entire Kedro project in your preferred environment.</p> <p>For the purpose of this walk-through, we are going to assume a Docker workflow. We recommend the Kedro-Docker plugin to streamline the process, and usage instructions are in the plugin's README.md. After you\u2019ve built the Docker image for your project locally, transfer the image to the production server. You can do this as follows:</p>"},{"location":"pages/deployment/single_machine/#how-to-use-container-registry","title":"How to use container registry","text":"<p>A container registry allows you to store and share container images. Docker Hub is one example of a container registry you can use for deploying your Kedro project. If you have a Docker ID you can use it to push and pull your images from the Docker server using the following steps.</p> <p>Tag your image on your local machine:</p> <pre><code>docker tag &lt;image-name&gt; &lt;DockerID&gt;/&lt;image-name&gt;\n</code></pre> <p>Push the image to Docker hub:</p> <pre><code>docker push &lt;DockerID&gt;/&lt;image-name&gt;\n</code></pre> <p>Pull the image from Docker hub onto your production server:</p> <pre><code>docker pull &lt;DockerID&gt;/&lt;image-name&gt;\n</code></pre> <pre><code>Repositories on Docker Hub are set to public visibility by default. You can change your project to private on the Docker Hub website.\n</code></pre> <p>The procedure for using other container registries, like AWS ECR or GitLab Container Registry, will be almost identical to the steps described above. However, authentication will be different for each solution.</p>"},{"location":"pages/deployment/single_machine/#package-based","title":"Package-based","text":"<p>If you prefer not to use containerisation, you can instead package your Kedro project using <code>kedro package</code>.</p> <p>Run the following in your project\u2019s root directory:</p> <pre><code>kedro package\n</code></pre> <p>Kedro builds the package into the <code>dist/</code> folder of your project, and creates a <code>.whl</code> file, which is a Python packaging format for binary distribution.</p> <p>The resulting <code>.whl</code> package only contains the Python source code of your Kedro pipeline, not any of the <code>conf/</code> and <code>data/</code> subfolders nor the <code>pyproject.toml</code> file. The project configuration is packaged separately in a <code>tar.gz</code> file. This compressed version of the config files excludes any files inside your <code>local</code> directory. This means that you can distribute the project to run elsewhere, such as on a separate computer with different configuration, data and logging. When distributed, the packaged project must be run from within a directory that contains the <code>pyproject.toml</code> file and <code>conf/</code> subfolder (and <code>data/</code> if your pipeline loads/saves local data). This means that you will have to create these directories on the remote servers manually.</p> <p>Recipients of the <code>.whl</code> file need to have Python and <code>pip</code> set up on their machines, but do not need to have Kedro installed. The project is installed to the root of a folder with the relevant <code>conf/</code> and <code>data/</code> subfolders, by navigating to the root and calling:</p> <pre><code>pip install &lt;path-to-wheel-file&gt;\n</code></pre> <p>After having installed your project on the remote server, run the Kedro project as follows from the root of the project:</p> <pre><code>python -m project_name\n</code></pre>"},{"location":"pages/deployment/single_machine/#cli-based","title":"CLI-based","text":"<p>If neither containers nor packages are viable options for your project, you can also run it on a production server by cloning your project codebase to the server using the Kedro CLI.</p> <p>You will need to follow these steps to get your project running:</p>"},{"location":"pages/deployment/single_machine/#use-github-workflow-to-copy-your-project","title":"Use GitHub workflow to copy your project","text":"<p>This workflow assumes that development of the Kedro project is done on a local environment under version control by Git. Commits are pushed to a remote server (e.g. GitHub, GitLab, Bitbucket, etc.).</p> <p>Deployment of the (latest) code on a production server is accomplished through cloning and the periodic pulling of changes from the Git remote. The pipeline is then executed on the server.</p> <p>Install Git on the server, how to do this depends on the type of server you're using. You can verify if the installation was successful by running:</p> <pre><code>git --version\n</code></pre> <p>Setup git (optionally)</p> <pre><code>git config --global user.name \"Server\"\ngit config --global user.email \"server@server.com\"\n</code></pre> <p>Generate a new SSH key for the server and add this new key to your GitHub account.</p> <p>Finally clone the project to the server:</p> <pre><code>git clone &lt;repository&gt;\n</code></pre>"},{"location":"pages/deployment/single_machine/#install-and-run-the-kedro-project","title":"Install and run the Kedro project","text":"<p>Once you have copied your Kedro project to the server, you need to follow these steps to install all project requirements and run the project.</p> <p>Install Kedro on the server using pip:</p> <pre><code>pip install kedro\n</code></pre> <p>or using conda:</p> <pre><code>conda install -c conda-forge kedro\n</code></pre> <p>Install the project\u2019s dependencies, by running the following in the project's root directory:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>After having installed your project on the remote server you can run the Kedro project as follows from the root of the project:</p> <pre><code>kedro run\n</code></pre> <p>You can also integrate the above steps in a bash script and run it in the relevant directory.</p>"},{"location":"pages/deployment/vertexai/","title":"VertexAI","text":"<p>Vertex AI pipelines is a Google Cloud Platform service that aims to deliver Kubeflow Pipelines functionality in a fully managed fashion.</p>"},{"location":"pages/deployment/vertexai/#the-kedro-vertexai-plugin","title":"The <code>kedro-vertexai</code> plugin","text":"<p>The <code>kedro-vertexai</code> plugin from GetInData | Part of Xebia enables you to run a Kedro pipeline on Vertex AI Pipelines. Consult the GitHub repository for <code>kedro-vertexai</code> for further details, or take a look at the documentation.</p>"},{"location":"pages/deployment/databricks/","title":"Databricks","text":"<p>Databricks offers integration with Kedro through three principal workflows, which range across a spectrum and combine local development with Databricks.</p> <p>Let's break down the advantages and use cases of each workflow to help you make an informed decision and choose the workflow that best fits your project's needs.</p> <p>I want to work within a Databricks workspace</p> <p>The workflow documented in \"Use a Databricks workspace to develop a Kedro project\" is for those who prefer to develop and test their projects directly within Databricks notebooks.</p> <p>To avoid the overhead of setting up and syncing a local development environment with Databricks, choose this as your workflow. You gain the flexibility for quick iteration, although switching to a job-based deployment workflow might be necessary when you transition into a production deployment.</p> <p>I want a hybrid workflow model combining local IDE with Databricks</p> <p>The workflow documented in \"Use Databricks Asset Bundles to deploy a Kedro project\" is for those that prefer to work in a local IDE.</p> <p>If you're in the early stages of learning Kedro, or your project requires constant testing and adjustments, choose this workflow. You can use your IDE's capabilities for faster, error-free development, while testing on Databricks. Later you can make the transition into a production deployment with this approach, although you may prefer to switch to use job-based deployment and fully optimise your workflow for production.</p> <p>I want to deploy a packaged Kedro project to Databricks</p> <p>The workflow documented in \"Use a Databricks job to deploy a Kedro project\" is the go-to choice when dealing with complex project requirements that need a high degree of structure and reproducibility. It's your best bet for a production setup, given its support for CI/CD, automated/scheduled runs and other advanced use cases. It might not be the ideal choice for projects requiring quick iterations due to its relatively rigid nature.</p> <p>Here's a flowchart to guide your choice of workflow:</p> <p></p> <p>% Mermaid code, see https://github.com/kedro-org/kedro/wiki/Render-Mermaid-diagrams % flowchart TD %   A[Start] --&gt; B{Do you prefer developing your projects in notebooks?} %   B --&gt;|Yes| C[Use a Databricks workspace to develop a Kedro project] %   B --&gt;|No| D{Are you a beginner with Kedro?} %   D --&gt;|Yes| E[Use an IDE, dbx and Databricks Repos to develop a Kedro project] %   D --&gt;|No| F{Do you have advanced project requirementse.g. CI/CD, scheduling, production-ready, complex pipelines, etc.?} %   F --&gt;|Yes| G{Is rapid development needed for your project needs?} %   F --&gt;|No| H[Use an IDE, dbx and Databricks Repos to develop a Kedro project] %   G --&gt;|Yes| I[Use an IDE, dbx and Databricks Repos to develop a Kedro project] %   G --&gt;|No| J[Use a Databricks job to deploy a Kedro project]</p> <p>Remember, the best choice of workflow is the one that aligns best with your project's requirements, whether that's quick development, notebook-based coding, or a production-ready setup. Make sure to consider these factors alongside your comfort level with Kedro when making your decision.</p> <pre><code>:maxdepth: 1\n\ndatabricks_notebooks_development_workflow.md\ndatabricks_ide_databricks_asset_bundles_workflow.md\ndatabricks_deployment_workflow\ndatabricks_visualisation\ndatabricks_dbx_workflow.md\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/","title":"Use an IDE, dbx and Databricks Repos to develop a Kedro project","text":"<pre><code>`dbx` is deprecated in 2023, the recommended workflow now is to use [Databricks Asset Bundles](./databricks_ide_databricks_asset_bundles_workflow.md)\n</code></pre> <p>This guide demonstrates a workflow for developing Kedro projects on Databricks using your local environment for development, then using dbx and Databricks Repos to sync code for testing on Databricks.</p> <p>By working in your local environment, you can take advantage of features within an IDE that are not available on Databricks notebooks:</p> <ul> <li>Auto-completion and suggestions for code, improving your development speed and accuracy.</li> <li>Linters like Ruff can be integrated to catch potential issues in your code.</li> <li>Static type checkers like Mypy can check types in your code, helping to identify potential type-related issues early in the development process.</li> </ul> <p>To set up these features, look for instructions specific to your IDE (for instance, VS Code).</p> <p>If you prefer to develop a projects in notebooks rather than an in an IDE, you should follow our guide on how to develop a Kedro project within a Databricks workspace instead.</p> <pre><code>[Databricks now recommends](https://docs.databricks.com/en/archive/dev-tools/dbx/index.html) that you use now use Databricks asset bundles instead of dbx. This Kedro deployment documentation has not yet been updated but you may wish to consult [What are Databricks Asset Bundles?](https://docs.databricks.com/en/dev-tools/bundles/index.html) and [Migrate from dbx to bundles](https://docs.databricks.com/en/archive/dev-tools/dbx/dbx-migrate.html) for further information.\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#what-this-page-covers","title":"What this page covers","text":"<p>The main steps in this tutorial are as follows:</p> <ul> <li>Create a virtual environment and install and configure dbx.</li> <li>Create a new Kedro project using the <code>databricks-iris</code> starter.</li> <li>Create a Repo on Databricks and sync your project using dbx.</li> <li>Upload project data to a location accessible by Kedro when run on Databricks (such as DBFS).</li> <li>Create a Databricks notebook to run your project.</li> <li>Modify your project in your local environment and test the changes on Databricks in an iterative loop.</li> </ul>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>An active Databricks deployment.</li> <li>A Databricks cluster configured with a recent version (&gt;= 11.3 is recommended) of the Databricks runtime.</li> <li>Conda installed on your local machine in order to create a virtual environment with a specific version of Python (&gt;= 3.9 is required). If you have Python &gt;= 3.9 installed, you can use other software to create a virtual environment.</li> </ul>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#set-up-your-project","title":"Set up your project","text":""},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#note-your-databricks-username-and-host","title":"Note your Databricks username and host","text":"<p>Note your Databricks username and host as you will need it for the remainder of this guide.</p> <p>Find your Databricks username in the top right of the workspace UI and the host in the browser's URL bar, up to the first slash (e.g., <code>https://adb-123456789123456.1.azuredatabricks.net/</code>):</p> <p></p> <pre><code>Your databricks host must include the protocol (`https://`).\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#install-kedro-and-dbx-in-a-new-virtual-environment","title":"Install Kedro and dbx in a new virtual environment","text":"<p>In your local development environment, create a virtual environment for this tutorial using Conda:</p> <pre><code>conda create --name iris-databricks python=3.10\n</code></pre> <p>Once it is created, activate it:</p> <pre><code>conda activate iris-databricks\n</code></pre> <p>With your Conda environment activated, install Kedro and dbx:</p> <pre><code>pip install kedro dbx --upgrade\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#authenticate-the-databricks-cli","title":"Authenticate the Databricks CLI","text":"<p>Now, you must authenticate the Databricks CLI with your Databricks instance.</p> <p>Refer to the Databricks documentation for a complete guide on how to authenticate your CLI. The key steps are:</p> <ol> <li>Create a personal access token for your user on your Databricks instance.</li> <li>Run <code>databricks configure --token</code>.</li> <li>Enter your token and Databricks host when prompted.</li> <li>Run <code>databricks fs ls dbfs:/</code> at the command line to verify your authentication.</li> </ol> <pre><code>dbx is an extension of the Databricks CLI, a command-line program for interacting with Databricks without using its UI. You will use dbx to sync your project's code with Databricks. While Git can sync code to Databricks Repos, dbx is preferred for development as it avoids creating new commits for every change, even if those changes do not work.\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#create-a-new-kedro-project","title":"Create a new Kedro project","text":"<p>Create a Kedro project with the <code>databricks-iris</code> starter using the following command in your local environment:</p> <pre><code>kedro new --starter=databricks-iris\n</code></pre> <p>Name your new project <code>iris-databricks</code> for consistency with the rest of this guide. This command creates a new Kedro project using the <code>databricks-iris</code> starter template.</p> <p><code>{note} If you are not using the `databricks-iris` starter to create a Kedro project, **and** you are working with a version of Kedro **earlier than 0.19.0**, then you should [disable file-based logging](https://docs.kedro.org/en/0.18.14/logging/logging.html#disable-file-based-logging) to prevent Kedro from attempting to write to the read-only file system.</code></p>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#create-a-repo-on-databricks","title":"Create a Repo on Databricks","text":"<p>Create a new Repo on Databricks by navigating to <code>New</code> tab in the Databricks workspace UI side bar and clicking <code>Repo</code> in the drop-down menu that appears.</p> <p>In this guide, you will not sync your project with a remote Git provider, so uncheck <code>Create repo by cloning a Git repository</code> and enter <code>iris-databricks</code> as the name of your new repository:</p> <p></p>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#sync-code-with-your-databricks-repo-using-dbx","title":"Sync code with your Databricks Repo using dbx","text":"<p>The next step is to use dbx to sync your project to your Repo.</p> <p>Open a new terminal instance, activate your conda environment, and navigate to your project directory and start <code>dbx sync</code>:</p> <pre><code>conda activate iris-databricks\ncd &lt;project_root&gt;\ndbx sync repo --dest-repo iris-databricks --source .\n</code></pre> <p>This command will sync your local directory (<code>--source .</code>) with your Repo (<code>--dest-repo iris-databricks</code>) on Databricks. When started for the first time, <code>dbx sync</code> will write output similar to the following to your terminal:</p> <pre><code>...\n[dbx][2023-04-13 21:59:48.148] Putting /Repos/&lt;databricks_username&gt;/iris-databricks/src/tests/__init__.py\n[dbx][2023-04-13 21:59:48.168] Putting /Repos/&lt;databricks_username&gt;/iris-databricks/src/tests/test_pipeline.py\n[dbx][2023-04-13 21:59:48.189] Putting /Repos/&lt;databricks_username&gt;/iris-databricks/src/tests/test_run.py\n[dbx][2023-04-13 21:59:48.928] Done. Watching for changes...\n</code></pre> <p>Keep the second terminal (running dbx sync) alive during development; closing it stops syncing new changes.</p> <p><code>dbx sync</code> will automatically sync any further changes made in your local project directory with your Databricks Repo while it runs.</p> <pre><code>Syncing with dbx is one-way only, meaning changes you make using the Databricks Repos code editor will not be reflected in your local environment. Only make changes to your project in your local environment while syncing, not in the editor that Databricks Repos provides.\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#create-a-conflocal-directory-in-your-databricks-repo","title":"Create a <code>conf/local</code> directory in your Databricks Repo","text":"<p>Kedro requires your project to have a <code>conf/local</code> directory to exist to successfully run, even if it is empty. <code>dbx sync</code> does not copy the contents of your local <code>conf/local</code> directory to your Databricks Repo, so you must create it manually.</p> <p>Open the Databricks workspace UI and using the panel on the left, navigate to <code>Repos -&gt; &lt;databricks_username&gt; -&gt; iris-databricks -&gt; conf</code>, right click and select <code>Create -&gt; Folder</code> as in the image below:</p> <p></p> <p>Name the new folder <code>local</code>. In this guide, we have no local credentials to store and so we will leave the newly created folder empty. Your <code>conf/local</code> and <code>local</code> directories should now look like the following:</p> <p></p>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#upload-project-data-to-dbfs","title":"Upload project data to DBFS","text":"<p>When run on Databricks, Kedro cannot access data stored in your project's directory. Therefore, you will need to upload your project's data to an accessible location. In this guide, we will store the data on the Databricks File System (DBFS).</p> <p>The <code>databricks-iris</code> starter contains a catalog that is set up to access data stored in DBFS (<code>&lt;project_root&gt;/conf/</code>). You will point your project to use configuration stored on DBFS using the <code>--conf-source</code> option when you create your job on Databricks.</p> <p>There are several ways to upload data to DBFS. In this guide, it is recommended to use Databricks CLI because of the convenience it offers. At the command line in your local environment, use the following Databricks CLI command to upload your locally stored data to DBFS:</p> <pre><code>databricks fs cp --recursive &lt;project_root&gt;/data/ dbfs:/FileStore/iris-databricks/data\n</code></pre> <p>The <code>--recursive</code> flag ensures that the entire folder and its contents are uploaded. You can list the contents of the destination folder in DBFS using the following command:</p> <pre><code>databricks fs ls dbfs:/FileStore/iris-databricks/data\n</code></pre> <p>You should see the contents of the project's <code>data/</code> directory printed to your terminal:</p> <pre><code>01_raw\n02_intermediate\n03_primary\n04_feature\n05_model_input\n06_models\n07_model_output\n08_reporting\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#create-a-new-databricks-notebook","title":"Create a new Databricks notebook","text":"<p>Now that your project is available on Databricks, you can run it on a cluster using a notebook.</p> <p>To run the Python code from your Databricks Repo, create a new Python notebook in your workspace. Name it <code>iris-databricks</code> for traceability and attach it to your cluster:</p> <p></p>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#run-your-project","title":"Run your project","text":"<p>Open your newly-created notebook and create four new cells inside it. You will fill these cells with code that runs your project. When copying the following code snippets, remember to replace <code>&lt;databricks_username&gt;</code> with your username on Databricks such that <code>project_root</code> correctly points to your project's location.</p> <ol> <li>Before you import and run your Python code, you'll need to install your project's dependencies on the cluster attached to your notebook. Your project has a <code>requirements.txt</code> file for this purpose. Add the following code to the first new cell to install the dependencies:</li> </ol> <pre><code>%pip install -r \"/Workspace/Repos/&lt;databricks_username&gt;/iris-databricks/requirements.txt\"\n</code></pre> <ol> <li>To run your project in your notebook, you must load the Kedro IPython extension. Add the following code to the second new cell to load the IPython extension:</li> </ol> <pre><code>%load_ext kedro.ipython\n</code></pre> <ol> <li>Loading the extension allows you to use the <code>%reload_kedro</code> line magic to load your Kedro project. Add the following code to the third new cell to load your Kedro project:</li> </ol> <pre><code>%reload_kedro /Workspace/Repos/&lt;databricks_username&gt;/iris-databricks\n</code></pre> <ol> <li>Loading your Kedro project with the <code>%reload_kedro</code> line magic will define four global variables in your notebook: <code>context</code>, <code>session</code>, <code>catalog</code> and <code>pipelines</code>. You will use the <code>session</code> variable to run your project. Add the following code to the fourth new cell to run your Kedro project:</li> </ol> <pre><code>session.run()\n</code></pre> <p>After completing these steps, your notebook should match the following image:</p> <p></p> <p>Run the completed notebook using the <code>Run All</code> bottom in the top right of the UI:</p> <p></p> <p>On your first run, you will be prompted to consent to analytics, type <code>y</code> or <code>N</code> in the field that appears and press <code>Enter</code>:</p> <p></p> <p>You should see logging output while the cell is running. After execution finishes, you should see output similar to the following:</p> <pre><code>...\n2023-06-06 17:21:53,221 - iris_databricks.nodes - INFO - Model has an accuracy of 0.960 on test data.\n2023-06-06 17:21:53,222 - kedro.runner.sequential_runner - INFO - Completed 3 out of 3 tasks\n2023-06-06 17:21:53,224 - kedro.runner.sequential_runner - INFO - Pipeline execution completed successfully.\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#modify-your-project-and-test-the-changes","title":"Modify your project and test the changes","text":"<p>Now that your project has run successfully once, you can make changes using the convenience and power of your local development environment. In this section, you will modify the project to use a different ratio of training data to test data and check the effect of this change on Databricks.</p>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#modify-the-training-test-split-ratio","title":"Modify the training / test split ratio","text":"<p>The <code>databricks-iris</code> starter uses a default 80-20 ratio of training data to test data when training the classifier. In this section, you will change this ratio to 70-30 by editing your project in your local environment, then sync it with the Databricks Repo using <code>dbx</code>, and then run the modified project on Databricks to observe the different result.</p> <p>Open the file <code>&lt;project_root&gt;/conf/base/parameters.yml</code> in your local environment. Edit the line <code>train_fraction: 0.8</code> to <code>train_fraction: 0.7</code> and save your changes. Look in the terminal where <code>dbx sync</code> is running, you should see it automatically sync your changes with your Databricks Repo:</p> <pre><code>...\n[dbx][2023-04-14 18:29:39.235] Putting /Repos/&lt;databricks_username&gt;/iris-databricks/conf/base/parameters.yml\n[dbx][2023-04-14 18:29:40.820] Done\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#re-run-your-project","title":"Re-run your project","text":"<p>Return to your Databricks notebook. Re-run the third and fourth cells in your notebook (containing the code <code>%reload_kedro ...</code> and <code>session.run()</code>). The project will now run again, producing output similar to the following:</p> <pre><code>...\n2023-06-06 17:23:19,561 - iris_databricks.nodes - INFO - Model has an accuracy of 0.972 on test data.\n2023-06-06 17:23:19,562 - kedro.runner.sequential_runner - INFO - Completed 3 out of 3 tasks\n2023-06-06 17:23:19,564 - kedro.runner.sequential_runner - INFO - Pipeline execution completed successfully.\n</code></pre> <p>You can see that your model's accuracy has changed now that you are using a different classifier to produce the result.</p> <pre><code>If your cluster terminates, you must re-run your entire notebook, as libraries installed using `%pip install ...` are ephemeral. If not, repeating this step is only necessary if your project's requirements change.\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_dbx_workflow/#summary","title":"Summary","text":"<p>This guide demonstrated a development workflow on Databricks, using your local development environment, dbx, and Databricks Repos to sync code. This approach improves development efficiency and provides access to powerful development features, such as auto-completion, linting, and static type checking, that are not available when working exclusively with Databricks notebooks.</p>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/","title":"Use a Databricks job to deploy a Kedro project","text":"<p>Databricks jobs are a way to execute code on Databricks clusters, allowing you to run data processing tasks, ETL jobs, or machine learning workflows. In this guide, we explain how to package and run a Kedro project as a job on Databricks.</p>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#what-are-the-advantages-of-packaging-a-kedro-project-to-run-on-databricks","title":"What are the advantages of packaging a Kedro project to run on Databricks?","text":"<p>Packaging your Kedro project and running it on Databricks enables you to execute your pipeline without a notebook. This approach is particularly well-suited for production, as it provides a structured and reproducible way to run your code.</p> <p>Here are some typical use cases for running a packaged Kedro project as a Databricks job:</p> <ul> <li>Data engineering pipeline: the output of your Kedro project is a file or set of files containing cleaned and processed data.</li> <li>Machine learning with MLflow: your Kedro project runs an ML model; metrics about your experiments are tracked in MLflow.</li> <li>Automated and scheduled runs: your Kedro project should be run on Databricks automatically.</li> <li>CI/CD integration: you have a CI/CD pipeline that produces a packaged Kedro project.</li> </ul> <p>Running your packaged project as a Databricks job is very different from running it from a Databricks notebook. The Databricks job cluster has to be provisioned and started for each run, which is significantly slower than running it as a notebook on a cluster that has already been started. In addition, there is no way to change your project's code once it has been packaged. Instead, you must change your code, create a new package, and then upload it to Databricks again.</p> <p>For those reasons, the packaging approach is unsuitable for development projects where rapid iteration is necessary. For guidance on developing a Kedro project for Databricks in a rapid build-test loop, see the development workflow guide.</p>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#what-this-page-covers","title":"What this page covers","text":"<ul> <li>Set up your Kedro project for deployment on Databricks.</li> <li>Run your project as a job using the Databricks workspace UI.</li> <li>Resources for automating your Kedro deployments to Databricks.</li> </ul>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>An active Databricks deployment.</li> <li><code>conda</code> installed on your local machine in order to create a virtual environment with a specific version of Python (&gt;= 3.7 is required). If you have Python &gt;= 3.7 installed, you can use other software to create a virtual environment.</li> </ul>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#set-up-your-project-for-deployment-to-databricks","title":"Set up your project for deployment to Databricks","text":"<p>The sequence of steps described in this section is as follows:</p> <ol> <li>Note your Databricks username and host</li> <li>Install Kedro and the Databricks CLI in a new virtual environment</li> <li>Authenticate the Databricks CLI</li> <li>Create a new Kedro project</li> <li>Package your project</li> <li>Upload project data and configuration to DBFS</li> </ol>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#note-your-databricks-username-and-host","title":"Note your Databricks username and host","text":"<p>Note your Databricks username and host as you will need it for the remainder of this guide.</p> <p>Find your Databricks username in the top right of the workspace UI and the host in the browser's URL bar, up to the first slash (e.g., <code>https://adb-123456789123456.1.azuredatabricks.net/</code>):</p> <p></p> <pre><code>Your Databricks host must include the protocol (`https://`).\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#install-kedro-and-the-databricks-cli-in-a-new-virtual-environment","title":"Install Kedro and the Databricks CLI in a new virtual environment","text":"<p>The following commands will create a new <code>conda</code> environment, activate it, and then install Kedro and the Databricks CLI.</p> <p>In your local development environment, create a virtual environment for this tutorial using <code>conda</code>:</p> <pre><code>conda create --name iris-databricks python=3.10\n</code></pre> <p>Once it is created, activate it:</p> <pre><code>conda activate iris-databricks\n</code></pre> <p>With your <code>conda</code> environment activated, install Kedro and the Databricks CLI:</p> <pre><code>pip install kedro databricks-cli\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#authenticate-the-databricks-cli","title":"Authenticate the Databricks CLI","text":"<p>Now, you must authenticate the Databricks CLI with your Databricks instance.</p> <p>Refer to the Databricks documentation for a complete guide on how to authenticate your CLI. The key steps are:</p> <ol> <li>Create a personal access token for your user on your Databricks instance.</li> <li>Run <code>databricks configure --token</code>.</li> <li>Enter your token and Databricks host when prompted.</li> <li>Run <code>databricks fs ls dbfs:/</code> at the command line to verify your authentication.</li> </ol>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#create-a-new-kedro-project","title":"Create a new Kedro project","text":"<p>Create a Kedro project by using the following command in your local environment:</p> <pre><code>kedro new --starter=databricks-iris\n</code></pre> <p>This command creates a new Kedro project using the <code>databricks-iris</code> starter template. Name your new project <code>iris-databricks</code> for consistency with the rest of this guide.</p> <p><code>{note}  If you are not using the `databricks-iris` starter to create a Kedro project, **and** you are working with a version of Kedro **earlier than 0.19.0**, then you should [disable file-based logging](https://docs.kedro.org/en/0.18.14/logging/logging.html#disable-file-based-logging) to prevent Kedro from attempting to write to the read-only file system.</code></p>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#package-your-project","title":"Package your project","text":"<p>To package your Kedro project for deployment on Databricks, you must create a Wheel (<code>.whl</code>) file, which is a binary distribution of your project. In the root directory of your Kedro project, run the following command:</p> <pre><code>kedro package\n</code></pre> <p>This command generates a <code>.whl</code> file in the <code>dist</code> directory within your project's root directory.</p>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#upload-project-data-and-configuration-to-dbfs","title":"Upload project data and configuration to DBFS","text":"<pre><code>A Kedro project's configuration and data do not get included when it is packaged. They must be stored somewhere accessible to allow your packaged project to run.\n</code></pre> <p>Your packaged Kedro project needs access to data and configuration in order to run. Therefore, you will need to upload your project's data and configuration to a location accessible to Databricks. In this guide, we will store the data on the Databricks File System (DBFS).</p> <p>The <code>databricks-iris</code> starter contains a catalog that is set up to access data stored in DBFS (<code>&lt;project_root&gt;/conf/</code>). You will point your project to use configuration stored on DBFS using the <code>--conf-source</code> option when you create your job on Databricks.</p> <p>There are several ways to upload data to DBFS: you can use the DBFS API, the <code>dbutils</code> module in a Databricks notebook or the Databricks CLI. In this guide, it is recommended to use the Databricks CLI because of the convenience it offers.</p> <ul> <li>Upload your project's data and config: at the command line in your local environment, use the following Databricks CLI commands to upload your project's locally stored data and configuration to DBFS:</li> </ul> <pre><code>databricks fs cp --recursive &lt;project_root&gt;/data/ dbfs:/FileStore/iris_databricks/data\ndatabricks fs cp --recursive &lt;project_root&gt;/conf/ dbfs:/FileStore/iris_databricks/conf\n</code></pre> <p>The <code>--recursive</code> flag ensures that the entire folder and its contents are uploaded. You can list the contents of the destination folder in DBFS using the following command:</p> <pre><code>databricks fs ls dbfs:/FileStore/iris_databricks/data\n</code></pre> <p>You should see the contents of the project's <code>data/</code> directory printed to your terminal:</p> <pre><code>01_raw\n02_intermediate\n03_primary\n04_feature\n05_model_input\n06_models\n07_model_output\n08_reporting\n</code></pre> <pre><code> If you are not using the `databricks-iris` starter to create a Kedro project, then you should make sure your catalog entries point to the DBFS storage.\n ```\n\n## Deploy and run your Kedro project using the workspace UI\n\nTo run your packaged project on Databricks, login to your Databricks account and perform the following steps in the workspace:\n\n1. [Create a new job](#create-a-new-job)\n2. [Create a new job cluster specific to your job](#create-a-new-job-cluster-specific-to-your-job)\n3. [Configure the job](#configure-the-job)\n4. [Run the job](#run-the-job)\n\n### Create a new job\n\nIn the Databricks workspace, navigate to the `Workflows` tab and click `Create Job` **or** click the `New` button, then `Job`:\n\n![Create Databricks job](../../meta/images/databricks_create_new_job.png)\n\n### Create a new job cluster specific to your job\n\nCreate a dedicated [job cluster](https://docs.databricks.com/clusters/index.html) to run your job by clicking on the drop-down menu in the `Cluster` field and then clicking `Add new job cluster`:\n\n**Do not use the default `Job_cluster`, it has not been configured to run this job.**\n\n![Create Databricks job cluster](../../meta/images/databricks_create_job_cluster.png)\n\nOnce you click `Add new job cluster`, the configuration page for this cluster appears.\n\nConfigure the job cluster with the following settings:\n\n- In the `name` field enter `kedro_deployment_demo`.\n- Select the radio button for `Single node`.\n- Select the runtime `13.3 LTS` in the `Databricks runtime version` field.\n- Leave all other settings with their default values in place.\n\nThe final configuration for the job cluster should look the same as the following:\n\n![Configure Databricks job cluster](../../meta/images/databricks_configure_job_cluster.png)\n\n### Configure the job\n\nConfigure the job with the following settings:\n\n- Enter `iris-databricks` in the `Name` field.\n- In the dropdown menu for the `Type` field, select `Python wheel`.\n- In the `Package name` field, enter `iris_databricks`. This is the name of your package as defined in your project's `pyproject.toml` file.\n- In the `Entry Point` field, enter `iris-databricks`. This is the name of the entry point to run your package from as defined in your project's `pyproject.toml` file.\n- Ensure the job cluster you created in step two is selected in the dropdown menu for the `Cluster` field.\n- In the `Dependent libraries` field, click `Add` and upload [your project's `.whl` file](#package-your-project), making sure that the radio buttons for `Upload` and `Python Whl` are selected for the `Library Source` and `Library Type` fields.\n- In the `Parameters` field, enter the following runtime option:\n\n```bash\n[\"--conf-source\", \"/dbfs/FileStore/iris_databricks/conf\"]\n</code></pre> <p>The final configuration for your job should look the same as the following:</p> <p></p> <p>Click <code>Create</code> and then <code>Confirm and create</code> in the following pop-up asking you to name the job.</p>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#run-the-job","title":"Run the job","text":"<p>Click <code>Run now</code> in the top-right corner of your new job's page to start a run of the job. The status of your run can be viewed in the <code>Runs</code> tab of your job's page. Navigate to the <code>Runs</code> tab and track the progress of your run:</p> <p></p> <p>This page also shows an overview of all past runs of your job. As you only just started your job run, it's status will be <code>Pending</code>. A status of <code>Pending</code> indicates that the cluster is being started and your code is waiting to run.</p> <p>The following things happen when you run your job:</p> <ul> <li>The job cluster is provisioned and started (job status: <code>Pending</code>).</li> <li>The packaged Kedro project and all its dependencies are installed (job status: <code>Pending</code>)</li> <li>The packaged Kedro project is run from the specified <code>iris-databricks</code> entry point (job status: <code>In Progress</code>).</li> <li>The packaged code finishes executing and the job cluster is stopped (job status: <code>Succeeded</code>).</li> </ul> <p>A run will take roughly six to seven minutes.</p> <p>When the status of your run is <code>Succeeded</code>, your job has successfully finished executing. You can view the logging output created by the run by clicking on the link with the text <code>Go to the latest successful run</code> to take you to the <code>main run</code> view. You should see logs similar to the following:</p> <pre><code>...\n2023-06-06 12:56:14,399 - iris_databricks.nodes - INFO - Model has an accuracy of 0.972 on test data.\n2023-06-06 12:56:14,403 - kedro.runner.sequential_runner - INFO - Completed 3 out of 3 tasks\n2023-06-06 12:56:14,404 - kedro.runner.sequential_runner - INFO - Pipeline execution completed successfully.\n</code></pre> <p>By following these steps, you packaged your Kedro project and manually ran it as a job on Databricks using the workspace UI.</p>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#resources-for-automatically-deploying-to-databricks","title":"Resources for automatically deploying to Databricks","text":"<p>Up to this point, this page has described a manual workflow for deploying and running a project on Databricks. The process can be automated in two ways:</p> <ul> <li>Use the Databricks API.</li> <li>Use the Databricks CLI.</li> </ul> <p>Both of these methods enable you to store information about your job declaratively in the same version control system as the rest of your project. For each method, the information stored declaratively is the same as what is entered manually in the above section on creating and running a job in Databricks.</p> <p>These methods can be integrated into a CI/CD pipeline to automatically deploy a packaged Kedro project to Databricks as a job.</p>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#how-to-use-the-databricks-api-to-automatically-deploy-a-kedro-project","title":"How to use the Databricks API to automatically deploy a Kedro project","text":"<p>The Databricks API enables you to programmatically interact with Databricks services, including job creation and execution. You can use the Jobs API to automate the deployment of your Kedro project to Databricks. The following steps outline how to use the Databricks API to do this:</p> <ol> <li>Set up your Kedro project for deployment on Databricks</li> <li>Create a JSON file containing your job's configuration.</li> <li>Use the Jobs API's <code>/create</code> endpoint to create a new job.</li> <li>Use the Jobs API's <code>/runs/submit</code> endpoint to run your newly created job.</li> </ol>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#how-to-use-the-databricks-cli-to-automatically-deploy-a-kedro-project","title":"How to use the Databricks CLI to automatically deploy a Kedro project","text":"<p>The Databricks Command Line Interface (CLI) is another way to automate deployment of your Kedro project. The following steps outline how to use the Databricks CLI to automate the deployment of a Kedro project:</p> <ol> <li>Set up your Kedro project for deployment on Databricks.</li> <li>Install the Databricks CLI and authenticate it with your workspace.</li> <li>Create a JSON file containing your job's configuration.</li> <li>Use the <code>jobs create</code> command to create a new job.</li> <li>Use the <code>jobs run-now</code> command to run your newly created job.</li> </ol>"},{"location":"pages/deployment/databricks/databricks_deployment_workflow/#summary","title":"Summary","text":"<p>This guide demonstrated how to deploy a packaged Kedro project on Databricks. This is a structured and reproducible way to run your Kedro projects on Databricks that can be automated and integrated into CI/CD pipelines.</p>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/","title":"Use an IDE and Databricks Asset Bundles to deploy a Kedro project","text":"<pre><code>The `dbx` package was deprecated by Databricks, and dbx workflow documentation is moved to a [new page](./databricks_dbx_workflow.md).\n</code></pre> <p>This guide demonstrates a workflow for developing a Kedro Project on Databricks using Databricks Asset Bundles. You will learn how to develop your project using a local environment, then use <code>kedro-databricks</code> and Databricks Asset Bundle to package your code for running pipelines on Databricks. To learn more about Databricks Asset Bundles and customisation, read What are Databricks Asset Bundles.</p>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#benefits-of-local-development","title":"Benefits of local development","text":"<p>By working in your local environment, you can take advantage of features within an IDE that are not available on Databricks notebooks:</p> <ul> <li>Auto-completion and suggestions for code, improving your development speed and accuracy.</li> <li>Linters like Ruff can be integrated to catch potential issues in your code.</li> <li>Static type checkers like Mypy can check types in your code, helping to identify potential type-related issues early in the development process.</li> </ul> <p>To set up these features, look for instructions specific to your IDE (for instance, VS Code).</p> <pre><code>If you prefer to develop projects in notebooks rather than in an IDE, you should follow our guide on [how to develop a Kedro project within a Databricks workspace](./databricks_notebooks_development_workflow.md) instead.\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#what-this-page-covers","title":"What this page covers","text":"<p>The main steps in this tutorial are as follows:</p> <ul> <li>Prerequisites</li> <li>Set up your project</li> <li>Create the Databricks Asset Bundles</li> <li>Deploy Databricks Job</li> <li>Run Databricks Job</li> </ul>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>An active Databricks deployment.</li> <li>A Databricks cluster configured with a recent version (&gt;= 11.3 is recommended) of the Databricks runtime.</li> <li>Conda installed on your local machine to create a virtual environment with Python &gt;= 3.9.</li> </ul>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#set-up-your-project","title":"Set up your project","text":""},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#note-your-databricks-username-and-host","title":"Note your Databricks username and host","text":"<p>Note your Databricks username and host as you will need it for the remainder of this guide.</p> <p>Find your Databricks username in the top right of the workspace UI and the host in the browser's URL bar, up to the first slash (e.g., <code>https://adb-123456789123456.1.azuredatabricks.net/</code>):</p> <p></p> <pre><code>Your databricks host must include the protocol (`https://`).\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#install-kedro-and-databricks-cli-in-a-new-virtual-environment","title":"Install Kedro and Databricks CLI in a new virtual environment","text":"<p>In your local development environment, create a virtual environment for this tutorial using Conda:</p> <pre><code>conda create --name databricks-iris python=3.10\n</code></pre> <p>Once it is created, activate it:</p> <pre><code>conda activate databricks-iris\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#authenticate-the-databricks-cli","title":"Authenticate the Databricks CLI","text":"<p>Now, you must authenticate the Databricks CLI with your Databricks instance.</p> <p>Refer to the Databricks documentation for a complete guide on how to authenticate your CLI. The key steps are:</p> <ol> <li>Create a personal access token for your user on your Databricks instance.</li> <li>Run <code>databricks configure --token</code>.</li> <li>Enter your token and Databricks host when prompted.</li> <li>Run <code>databricks fs ls dbfs:/</code> at the command line to verify your authentication.</li> </ol>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#create-a-new-kedro-project","title":"Create a new Kedro Project","text":"<p>Create a Kedro project with the <code>databricks-iris</code> starter using the following command in your local environment:</p> <pre><code>kedro new --starter=databricks-iris\n</code></pre> <p>Name your new project <code>iris-databricks</code> for consistency with the rest of this guide. This command creates a new Kedro project using the <code>databricks-iris</code> starter template.</p> <p><code>{note} If you are not using the `databricks-iris` starter to create a Kedro project, **and** you are working with a version of Kedro **earlier than 0.19.0**, then you should [disable file-based logging](https://docs.kedro.org/en/0.18.14/logging/logging.html#disable-file-based-logging) to prevent Kedro from attempting to write to the read-only file system.</code></p>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#create-the-databricks-asset-bundles-using-kedro-databricks","title":"Create the Databricks Asset Bundles using <code>kedro-databricks</code>","text":"<p><code>kedro-databricks</code> is a wrapper around the <code>databricks</code> CLI. It's the simplest way to get started without getting stuck with configuration. 1. Install <code>kedro-databricks</code>:</p> <pre><code>pip install kedro-databricks\n</code></pre> <ol> <li>Initialise the Databricks configuration:</li> </ol> <pre><code>kedro databricks init\n</code></pre> <p>This generates a <code>databricks.yml</code> file in the <code>conf</code> folder, which sets the default cluster type. You can override these configurations if needed.</p> <ol> <li>Create Databricks Asset Bundles:</li> </ol> <pre><code>kedro databricks bundle\n</code></pre> <p>This command reads the configuration from <code>conf/databricks.yml</code> (if it exists) and generates the Databricks job configuration inside a <code>resource</code> folder.</p>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#running-a-databricks-job-using-an-existing-cluster","title":"Running a Databricks Job Using an Existing Cluster","text":"<p>By default, Databricks creates a new job cluster for each job. However, there are instances where you might prefer to use an existing cluster, such as:</p> <ol> <li>Lack of permissions to create a new cluster.</li> <li>The need for a quick start with an all-purpose cluster.</li> </ol> <p>While it is generally not recommended to utilise all-purpose compute for running jobs, it is feasible to configure a Databricks job for testing purposes.</p> <p>To begin, you need to determine the <code>cluster_id</code>. Navigate to the <code>Compute</code> tab and select the <code>View JSON</code> option.</p> <p></p> <p>You will see the cluster configuration in JSON format, copy the <code>cluster_id</code> </p> <p>Next, update <code>conf/databricks.yml</code></p> <pre><code>    tasks:\n        - task_key: default\n-          job_cluster_key: default\n+          existing_cluster_id: 0502-***********\n</code></pre> <p>Then generate the bundle definition again with the <code>overwrite</code> option.</p> <pre><code>kedro databricks bundle --overwrite\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#deploy-databricks-job-using-databricks-asset-bundles","title":"Deploy Databricks Job using Databricks Asset Bundles","text":"<p>Once you have all the resources generated, deploy the Databricks Asset Bundles to Databricks:</p> <pre><code>kedro databricks deploy\n</code></pre> <p>You should see output similar to:</p> <pre><code>Uploading databrick_iris-0.1-py3-none-any.whl...\nUploading bundle files to /Workspace/Users/xxxxxxx.com/.bundle/databrick_iris/local/files...\nDeploying resources...\nUpdating deployment state...\nDeployment complete!\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#how-to-run-the-deployed-job","title":"How to run the Deployed job?","text":"<p>There are two options to run Databricks Jobs:</p>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#run-databricks-job-with-databricks-cli","title":"Run Databricks Job with <code>databricks</code> CLI","text":"<pre><code>databricks bundle run\n</code></pre> <p>This will shows all the job that you have created. Select the job and run it.</p> <pre><code>? Resource to run:\n  Job: [dev] databricks-iris (databricks-iris)\n</code></pre> <p>You should see similar output like this:</p> <pre><code>databricks bundle run\nRun URL: https://&lt;host&gt;/?*********#job/**************/run/**********\n</code></pre> <p>Copy that URL into your browser or go to the <code>Jobs Run</code> UI to see the run status.</p>"},{"location":"pages/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow/#run-databricks-job-with-databricks-ui","title":"Run Databricks Job with Databricks UI","text":"<p>Alternatively, you can go to the <code>Workflow</code> tab and select the desired job to run directly: </p>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/","title":"Use a Databricks workspace to develop a Kedro project","text":"<p>This guide demonstrates a workflow for developing Kedro projects on Databricks using only a Databricks Repo and a Databricks notebook. You will learn how to develop and test your Kedro projects entirely within the Databricks workspace.</p> <p>This method of developing a Kedro project for use on Databricks is ideal for developers who prefer developing their projects in notebooks rather than an in an IDE. It also avoids the overhead of setting up and syncing a local environment with Databricks. If you want to take advantage of the powerful features of an IDE to develop your project, consider following the guide for developing a Kedro project for Databricks using your local environment.</p> <p>In this guide, you will store your project's code in a repository on GitHub. Databricks integrates with many Git providers, including GitLab and Azure DevOps. The steps  to create a Git repository and sync it with Databricks also generally apply to these Git providers, though the exact details may vary.</p>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#what-this-page-covers","title":"What this page covers","text":"<p>This tutorial introduces a Kedro project development workflow using only the Databricks workspace. The main steps in this workflow are:</p> <ul> <li>Create a new Kedro project using the <code>databricks-iris</code> starter.</li> <li>Create a Databricks notebook to run your project.</li> <li>Copy project data to DBFS.</li> <li>Modify your project in the Databricks workspace</li> </ul>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>An active Databricks deployment.</li> <li>A Databricks cluster configured with a recent version (&gt;= 11.3 is recommended) of the Databricks runtime.</li> <li>Python &gt;= 3.9 installed.</li> <li>Git installed.</li> <li>A GitHub account.</li> <li>A Python environment management system installed, venv, virtualenv or Conda are popular choices.</li> </ul>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#set-up-your-project","title":"Set up your project","text":""},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#install-kedro-in-a-new-virtual-environment","title":"Install Kedro in a new virtual environment","text":"<p>In your local development environment, create a virtual environment for this tutorial. Any environment management system can be used, though the following commands use Conda:</p> <pre><code>conda create --name iris-databricks python=3.10\n</code></pre> <p>Once it is created, activate it:</p> <pre><code>conda activate iris-databricks\n</code></pre> <p>With your Conda environment activated, install Kedro:</p> <pre><code>pip install kedro\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#create-a-new-kedro-project","title":"Create a new Kedro project","text":"<p>Create a Kedro project with the <code>databricks-iris</code> starter using the following command in your local environment:</p> <pre><code>kedro new --starter=databricks-iris\n</code></pre> <p>Name your new project <code>iris-databricks</code> for consistency with the rest of this guide. This command creates a new Kedro project using the <code>databricks-iris</code> starter template.</p> <p><code>{note}  If you are not using the `databricks-iris` starter to create a Kedro project, **and** you are working with a version of Kedro **earlier than 0.19.0**, then you should [disable file-based logging](https://docs.kedro.org/en/0.18.14/logging/logging.html#disable-file-based-logging) to prevent Kedro from attempting to write to the read-only file system.</code></p>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#create-a-github-repository","title":"Create a GitHub repository","text":"<p>Now you should create a new repository in GitHub using the official guide. Keep the repository private and don't commit to it yet. For consistency with the rest of this guide, name your GitHub repository <code>iris-databricks</code>.</p>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#create-a-github-personal-access-token","title":"Create a GitHub personal access token","text":"<p>To synchronise your project between your local development environment and Databricks, you will use a private GitHub repository, which you will create in the next step. For authentication, you will need to create a GitHub personal access token. Create this token in your GitHub developer settings.</p> <p>The main steps are:</p> <ul> <li>Verify your email and navigate to \"Settings\" under your profile photo.</li> <li>Select \"Developer settings\" then \"Fine-grained tokens\" and click on \"Generate new token\".</li> <li>Select a name and expiration time for your token, choose an expiration time.</li> <li>Select which repositories your token will allow access to and define the token permissions.</li> </ul>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#push-your-kedro-project-to-the-github-repository","title":"Push your Kedro project to the GitHub repository","text":"<p>At the command line, initialise Git in your project root directory:</p> <pre><code># change the directory to the project root\ncd iris-databricks/\n# initialise git\ngit init\n</code></pre> <p>Then, create the first commit:</p> <pre><code># add all files to git staging area\ngit add .\n# create the first commit\ngit commit -m \"first commit\"\n</code></pre> <p>To connect to your GitHub repository from your local environment, use one of two options:</p> <ul> <li>SSH: If you choose to connect with SSH, you will also need to configure the SSH connection to GitHub, unless you already have an existing SSH key configured for GitHub</li> <li>HTTPS: If using HTTPS, you will be asked for your GitHub username and password when you push your first commit. Use your GitHub username and your personal access token generated in the previous step as the password, do not use your original GitHub password.</li> </ul> <p>With one of these two options chosen, run the following commands:</p> <pre><code># configure a new remote\n# for HTTPS run:\ngit remote add origin https://github.com/&lt;username&gt;/iris-databricks.git\n# or for SSH run:\ngit remote add origin git@github.com:&lt;username&gt;/iris-databricks.git\n\n# verify the new remote URL\ngit remote -v\n\n# push the first commit\ngit push --set-upstream origin main\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#create-a-repo-on-databricks","title":"Create a repo on Databricks","text":"<p>You will now create a repo on Databricks using the following steps:</p> <ol> <li> <p>Create a new repo:</p> </li> <li> <p>Navigate to the <code>Repos</code> tab in the Databricks workspace UI and click <code>Add Repo</code>.</p> </li> <li>Keep the <code>Add Repo</code> popup open for the following steps.</li> </ol> <p></p> <ol> <li> <p>Specify your GitHub repo:</p> </li> <li> <p>In the <code>Git repository URL</code> field, enter your GitHub repository's URL. This will automatically populate the <code>Git provider</code> and <code>Repository name</code> fields also.</p> </li> </ol> <p></p> <ol> <li> <p>Authenticate Databricks with GitHub:</p> </li> <li> <p>Click on the <code>Git credential</code> field.</p> </li> <li>In the <code>Git provider</code> field, select <code>GitHub</code> in the dropdown menu.</li> <li>In the <code>Git provider username or email</code> field, enter the username or email address of your GitHub account.</li> <li>In the <code>Token</code> field, enter your GitHub personal access token.</li> <li>Click the <code>Save</code> button to save your new Git credential.</li> </ol> <p></p> <ol> <li> <p>Finish the Repo creation process:</p> </li> <li> <p>Click <code>Create Repo</code>. Your GitHub repository is cloned to Databricks and the popup window closes.</p> </li> </ol>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#create-a-new-databricks-notebook","title":"Create a new Databricks notebook","text":"<p>Now that your project is available in a Databricks Repo, you can run it on a cluster using a notebook.</p> <p>To run the Python code from your Databricks repo, create a new Python notebook in your workspace. Name it <code>iris-databricks</code> for traceability and attach it to your cluster:</p> <p></p>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#copy-project-data-to-dbfs-using-dbutils","title":"Copy project data to DBFS using dbutils","text":"<p>On Databricks, Kedro cannot access data stored directly in your project's directory. As a result, you'll need to move your project's data to a location accessible by Databricks. You can store your project's data in the Databricks File System (DBFS), where it is accessible.</p> <p>A number of methods exist for moving data to DBFS. However, in this guide, you will use your new notebook and <code>dbutils</code>.</p> <p>To move your locally stored data to DBFS, open your <code>iris-databricks</code> notebook and in the first cell enter the following python code:</p> <pre><code>dbutils.fs.cp(\n    \"file:///Workspace/Repos/&lt;databricks_username&gt;/iris-databricks/data/\",\n    \"dbfs:/FileStore/iris-databricks/data\",\n    recurse=True,\n)\n</code></pre> <p>Run this cell to copy the complete directory and its contents from your Repo to DBFS.</p> <p>To ensure that your data was copied correctly, you can list the contents of the destination directory in DBFS. Create a new cell underneath the first cell and enter the following code:</p> <pre><code>dbutils.fs.ls(\"dbfs:/FileStore/iris-databricks/data\")\n</code></pre> <p>Run this command to displays the contents of your project's <code>data/</code> directory. You can expect to see the following structure:</p> <pre><code>[FileInfo(path='dbfs:/FileStore/iris-databricks/data/01_raw', name='01_raw', size=...),\n FileInfo(path='dbfs:/FileStore/iris-databricks/data/02_intermediate', name='02_intermediate', size=...),\n FileInfo(path='dbfs:/FileStore/iris-databricks/data/03_primary', name='03_primary', size=...),\n FileInfo(path='dbfs:/FileStore/iris-databricks/data/04_feature', name='04_feature', size=...),\n FileInfo(path='dbfs:/FileStore/iris-databricks/data/05_model_input', name='05_model_input', size=...),\n FileInfo(path='dbfs:/FileStore/iris-databricks/data/06_models', name='06_models', size=...),\n FileInfo(path='dbfs:/FileStore/iris-databricks/data/07_model_output', name='07_model_output', size=...),\n FileInfo(path='dbfs:/FileStore/iris-databricks/data/08_reporting', name='08_reporting', size=...)]\n</code></pre> <p>After these cells have successfully run, you should comment the code inside them so their operations are not unnecessarily performed during notebook runs. The cells should appear as below:</p> <p>Cell 1:</p> <pre><code>#dbutils.fs.cp(\n#    \"file:///Workspace/Repos/&lt;databricks_username&gt;/iris-databricks/data\",\n#    \"dbfs:/FileStore/iris-databricks/data\",\n#    recurse=True,\n#)\n</code></pre> <p>Cell 2:</p> <pre><code>#dbutils.fs.ls(\"dbfs:/FileStore/iris-databricks/data\")\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#run-your-project","title":"Run your project","text":"<p>Create four new cells inside your notebook. You will fill these cells with code that runs your project. When copying the following code snippets, remember to replace <code>&lt;databricks_username&gt;</code> with your username on Databricks such that <code>project_root</code> correctly points to your project's location.</p> <ol> <li>Before you import and run your Python code, you'll need to install your project's dependencies on the cluster attached to your notebook. Your project has a <code>requirements.txt</code> file for this purpose. Add the following code to the first new cell to install the dependencies:</li> </ol> <pre><code>%pip install -r \"/Workspace/Repos/&lt;databricks_username&gt;/iris-databricks/requirements.txt\"\n</code></pre> <ol> <li>To run your project in your notebook, you must load the Kedro IPython extension. Add the following code to the second new cell to load the IPython extension:</li> </ol> <pre><code>%load_ext kedro.ipython\n</code></pre> <ol> <li>Loading the extension allows you to use the <code>%reload_kedro</code> line magic to load your Kedro project. Add the following code to the third new cell to load your Kedro project:</li> </ol> <pre><code>%reload_kedro /Workspace/Repos/&lt;databricks_username&gt;/iris-databricks\n</code></pre> <ol> <li>Loading your Kedro project with the <code>%reload_kedro</code> line magic will define four global variables in your notebook: <code>context</code>, <code>session</code>, <code>catalog</code> and <code>pipelines</code>. You will use the <code>session</code> variable to run your project. Add the following code to the fourth new cell to run your Kedro project:</li> </ol> <pre><code>session.run()\n</code></pre> <p>After completing these steps, your notebook should match the following image:</p> <p></p> <p>Run the completed notebook using the <code>Run All</code> button in the top right of the UI:</p> <p></p> <p>On the first run of your Kedro project, you will be prompted to consent to analytics, type <code>y</code> or <code>N</code> in the field that appears and press <code>Enter</code>:</p> <p></p> <p>You should see logging output while the cell is running. After execution finishes, you should see output similar to the following:</p> <pre><code>...\n2023-06-06 12:55:22,705 - iris_databricks.nodes - INFO - Model has an accuracy of 0.953 on test data.\n2023-06-06 12:55:22,709 - kedro.runner.sequential_runner - INFO - Completed 3 out of 3 tasks\n2023-06-06 12:55:22,709 - kedro.runner.sequential_runner - INFO - Pipeline execution completed successfully.\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#modify-your-project-and-test-the-changes","title":"Modify your project and test the changes","text":"<p>Now that your project has run successfully once, you can make changes using the Databricks UI. In this section, you will modify the project to use a different ratio of training data to test data and check the effect of this change.</p>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#modify-the-training-test-split-ratio","title":"Modify the training / test split ratio","text":"<p>The <code>databricks-iris</code> starter uses a default 80-20 ratio of training data to test data when training the classifier. You will edit this ratio to 70-30 and re-run your project to view the different result.</p> <p>In the Databricks workspace, click on the <code>Repos</code> tab in the side bar and navigate to <code>&lt;databricks_username&gt;/iris-databricks/conf/base/</code>. Open the file <code>parameters.yml</code> by double-clicking it. This will take you to a built-in file editor. Edit the line <code>train_fraction: 0.8</code> to <code>train_fraction: 0.7</code>, your changes will automatically be saved.</p> <p></p>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#re-run-your-project","title":"Re-run your project","text":"<p>Return to your Databricks notebook. Re-run the third and fourth cells in your notebook (containing the code <code>%reload_kedro ...</code> and <code>session.run()</code>). The project will now run again, producing output similar to the following:</p> <pre><code>...\n2023-06-06 12:56:14,399 - iris_databricks.nodes - INFO - Model has an accuracy of 0.972 on test data.\n2023-06-06 12:56:14,403 - kedro.runner.sequential_runner - INFO - Completed 3 out of 3 tasks\n2023-06-06 12:56:14,404 - kedro.runner.sequential_runner - INFO - Pipeline execution completed successfully.\n</code></pre> <p>You can see that your model's accuracy has changed now that you are using a different classifier to produce the result.</p> <pre><code>If your cluster terminates, you must re-run your entire notebook, as libraries installed using `%pip install ...` are ephemeral. If not, repeating this step is only necessary if your project's dependencies change.\n</code></pre>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#managing-your-databricks-repo","title":"Managing your Databricks Repo","text":"<p>Your Databricks Repo now has untracked changes that are not synced with your GitHub repository. To track your changes and sync your Repo, you can use the corresponding Git operations in Databricks Repos. A basic overview of the steps to achieve this is:</p> <ul> <li>Commit your changes in your Databricks Repo.</li> <li>Push the changes to the GitHub repository linked to your Databricks Repo.</li> <li>Check that the latest commits are visible in your GitHub repository.</li> </ul>"},{"location":"pages/deployment/databricks/databricks_notebooks_development_workflow/#summary","title":"Summary","text":"<p>This guide demonstrated a development workflow on Databricks using only the Databricks workspace. This approach is ideal for users who prefer to develop using notebooks and avoids having to set up and sync a local environment with Databricks.</p>"},{"location":"pages/deployment/databricks/databricks_visualisation/","title":"Visualise a Kedro project in Databricks notebooks","text":"<p>{doc}<code>Kedro-Viz &lt;kedro-viz:kedro-viz_visualisation&gt;</code> is a tool that enables you to visualise your Kedro pipeline and metrics generated from your data science experiments. It is a standalone web application that runs on a web browser, it can be run on a local machine or in a Databricks notebook.</p> <p>For Kedro-Viz to run with your Kedro project, you need to ensure that both the packages are installed in the same scope (notebook-scoped vs. cluster library). This means that if you <code>%pip install kedro</code> from inside your notebook then you should also <code>%pip install kedro-viz</code> from inside your notebook. If your cluster comes with Kedro installed on it as a library already then you should also add Kedro-Viz as a cluster library.</p> <p>To run Kedro-Viz in a Databricks notebook you must first launch the Kedro IPython extension:</p> <pre><code>%load_ext kedro.ipython\n</code></pre> <p>And load your Kedro project from where it is stored in either the Databricks workspace or in a Repo:</p> <pre><code>%reload_kedro &lt;project_root&gt;/iris-databricks\n</code></pre> <p>Kedro-Viz can then be launched in a new browser tab with the <code>%run_viz</code> line magic:</p> <pre><code>%run_viz\n</code></pre> <p>This command presents you with a link to the Kedro-Viz web application.</p> <p></p> <p>Clicking this link opens a new browser tab running Kedro-Viz for your project.</p> <p></p>"},{"location":"pages/development/automated_testing/","title":"Automated testing","text":"<p>An important step towards achieving high code quality and maintainability in your Kedro project is the use of automated tests. Let's look at how you can set this up.</p>"},{"location":"pages/development/automated_testing/#introduction","title":"Introduction","text":"<p>Software testing is the process of checking that the code you have written fulfills its requirements. Software testing can either be manual or automated. In the context of Kedro: - Manual testing is when you run part or all of your project and check that the results are what you expect. - Automated testing is writing new code (using libraries called testing frameworks) that runs part or all of your project and automatically checks the results against what you expect.</p> <p>As a project grows larger, new code will increasingly rely on existing code. As these interdependencies grow, making changes in one part of the code base can unexpectedly break the intended functionality in another part.</p> <p>The major disadvantage of manual testing is that it is time-consuming. Manual tests are usually run once, directly after new functionality has been added. It is impractical to repeat manual tests for the entire code base each time a change is made, which means this strategy often misses breaking changes.</p> <p>The solution to this problem is automated testing. Automated testing allows many tests across the whole code base to be run in seconds, every time a new feature is added or an old one is changed. In this way, breaking changes can be discovered during development rather than in production.</p>"},{"location":"pages/development/automated_testing/#set-up-automated-testing-with-pytest","title":"Set up automated testing with <code>pytest</code>","text":"<p>There are many testing frameworks available for Python. One of the most popular is <code>pytest</code> (see the project's home page for a quick overview). <code>pytest</code> is often used in Python projects for its short, readable tests and powerful set of features.</p> <p>Let's look at how you can start working with <code>pytest</code> in your Kedro project.</p>"},{"location":"pages/development/automated_testing/#install-test-requirements","title":"Install test requirements","text":"<p>Before getting started with test requirements, it is important to ensure you have installed your project locally. This allows you to test different parts of your project by importing them into your test files.</p> <p>To install your project including all the project-specific dependencies and test requirements: 1. Add the following section to the <code>pyproject.toml</code> file located in the project root:</p> <pre><code>[project.optional-dependencies]\ndev = [\n    \"pytest-cov\",\n    \"pytest-mock\",\n    \"pytest\",\n]\n</code></pre> <ol> <li>Navigate to the root directory of the project and run:</li> </ol> <pre><code>pip install .\"[dev]\"\n</code></pre> <p>Alternatively, you can individually install test requirements as you would install other packages with <code>pip</code>, making sure you have installed your project locally and your project's virtual environment is active.</p> <ol> <li>To install your project, navigate to your project root and run the following command:</li> </ol> <pre><code>pip install -e .\n</code></pre> <p>NOTE: The option <code>-e</code> installs an editable version of your project, allowing you to make changes to the project files without needing to re-install them each time.</p> <ol> <li>Install test requirements one by one:</li> </ol> <pre><code>pip install pytest\n</code></pre>"},{"location":"pages/development/automated_testing/#create-a-tests-directory","title":"Create a <code>/tests</code> directory","text":"<p>Now that <code>pytest</code> is installed, you will need a place to put your tests. Create a <code>/tests</code> folder in the root directory of your project.</p> <pre><code>mkdir &lt;project_root&gt;/tests\n</code></pre>"},{"location":"pages/development/automated_testing/#test-directory-structure","title":"Test directory structure","text":"<p>The subdirectories in your project's <code>/tests</code> directory should mirror the directory structure of your project's <code>/src/&lt;package_name&gt;</code> directory. All files in the <code>/tests</code> folder should be named <code>test_&lt;file_being_tested&gt;.py</code>. See an example <code>/tests</code> folder below.</p> <pre><code>src\n\u2502   ...\n\u2514\u2500\u2500\u2500&lt;package_name&gt;\n\u2502   \u2514\u2500\u2500\u2500pipelines\n\u2502       \u2514\u2500\u2500\u2500dataprocessing\n\u2502           \u2502   ...\n\u2502           \u2502   nodes.py\n\u2502           \u2502   ...\n\u2502\ntests\n\u2514\u2500\u2500\u2500pipelines\n\u2502   \u2514\u2500\u2500\u2500dataprocessing\n\u2502       \u2502   ...\n\u2502       \u2502   test_nodes.py\n\u2502       \u2502   ...\n</code></pre>"},{"location":"pages/development/automated_testing/#create-an-example-test","title":"Create an example test","text":"<p>Now that you have a place to put your tests, you can create an example test in the new file <code>/src/tests/test_run.py</code>. This example test demonstrates how to programmatically execute a <code>kedro run</code> using the <code>KedroSession</code> class.</p> <pre><code>from pathlib import Path\nfrom kedro.framework.session import KedroSession\nfrom kedro.framework.startup import bootstrap_project\n\nclass TestKedroRun:\n    def test_kedro_run(self):\n        bootstrap_project(Path.cwd())\n\n        with KedroSession.create(project_path=Path.cwd()) as session:\n            assert session.run() is not None\n</code></pre> <p>This test is redundant, but it introduces a few of <code>pytest</code>'s core features and demonstrates the layout of a test file: - Tests are implemented in methods or functions beginning with <code>test_</code> and classes beginning with <code>Test</code>. - The <code>assert</code> statement is used to compare the result of the test with an expected value.</p> <p>Although this specific example does not utilise fixtures, they are an essential part of pytest for defining reusable resources across tests. See Fixtures</p> <p>Tests should be named as descriptively as possible, especially if you are working with other people. For example, it is easier to understand the purpose of a test with the name <code>test_node_passes_with_valid_input</code> than a test with the name <code>test_passes</code>.</p> <p>You can read more about the basics of using <code>pytest</code> on the getting started page. For help writing your own tests and using all of the features of <code>pytest</code>, see the project documentation.</p>"},{"location":"pages/development/automated_testing/#run-your-tests","title":"Run your tests","text":"<p>To run your tests, run <code>pytest</code> from within your project's root directory.</p> <pre><code>cd &lt;project_root&gt;\npytest\n</code></pre> <p>If you created the example test in the previous section, you should see the following output in your shell.</p> <pre><code>============================= test session starts ==============================\n...\ncollected 1 item\n\ntests/test_run.py .                                                  [100%]\n\n============================== 1 passed in 0.38s ===============================\n</code></pre> <p>This output indicates that one test ran successfully in the file <code>src/tests/test_run.py</code>.</p>"},{"location":"pages/development/automated_testing/#add-test-coverage-reports-with-pytest-cov","title":"Add test coverage reports with <code>pytest-cov</code>","text":"<p>It can be useful to see how much of your project is covered by tests. For this, you can install and configure the <code>pytest-cov</code> plugin for <code>pytest</code>, which is based on the popular <code>coverage.py</code> library.</p>"},{"location":"pages/development/automated_testing/#install-pytest-cov","title":"Install <code>pytest-cov</code>","text":"<p>Install <code>pytest</code> as you would install other packages with pip, making sure your project's virtual environment is active.</p> <pre><code>pip install pytest-cov\n</code></pre>"},{"location":"pages/development/automated_testing/#configure-pytest-to-use-pytest-cov","title":"Configure <code>pytest</code> to use <code>pytest-cov</code>","text":"<p>To configure <code>pytest</code> to generate a coverage report using <code>pytest-cov</code>, you can add the following lines to your <code>&lt;project_root&gt;/pyproject.toml</code> file (creating it if it does not exist).</p> <pre><code>[tool.pytest.ini_options]\naddopts = \"\"\"\n--cov-report term-missing \\\n--cov src/&lt;package_name&gt; -ra\"\"\"\n</code></pre>"},{"location":"pages/development/automated_testing/#run-pytest-with-pytest-cov","title":"Run <code>pytest</code> with <code>pytest-cov</code>","text":"<p>Running <code>pytest</code> in the spaceflights starter with <code>pytest-cov</code> installed results in the following additional report.</p> <pre><code>Name                                                     Stmts   Miss  Cover   Missing\n--------------------------------------------------------------------------------------\nsrc/spaceflights/__init__.py                                 1      1     0%   4\nsrc/spaceflights/__main__.py                                30     30     0%   4-47\nsrc/spaceflights/pipeline_registry.py                        7      7     0%   2-16\nsrc/spaceflights/pipelines/__init__.py                       0      0   100%\nsrc/spaceflights/pipelines/data_processing/__init__.py       1      1     0%   3\nsrc/spaceflights/pipelines/data_processing/nodes.py         25     25     0%   1-67\nsrc/spaceflights/pipelines/data_processing/pipeline.py       5      5     0%   1-8\nsrc/spaceflights/pipelines/data_science/__init__.py          1      1     0%   3\nsrc/spaceflights/pipelines/data_science/nodes.py            20     20     0%   1-55\nsrc/spaceflights/pipelines/data_science/pipeline.py          8      8     0%   1-40\nsrc/spaceflights/settings.py                                 0      0   100%\n--------------------------------------------------------------------------------------\nTOTAL                                                       98     98     0%\n</code></pre> <p>This is the simplest report that <code>coverage.py</code> (via <code>pytest-cov</code>) will produce. It gives an overview of how many of the executable statements in each project file are covered by tests. For detail on the full set of features offered, see the <code>coverage.py</code> docs.</p>"},{"location":"pages/development/commands_reference/","title":"Kedro's command line interface","text":"<p>Kedro's command line interface (CLI) is used to give commands to Kedro via a terminal shell (such as the terminal app on macOS, or cmd.exe or PowerShell on Windows). You need to use the CLI to set up a new Kedro project, and to run it.</p>"},{"location":"pages/development/commands_reference/#autocompletion-optional","title":"Autocompletion (optional)","text":"<p>If you are using macOS or Linux, you can set up your shell to autocomplete <code>kedro</code> commands. If you don't know the type of shell you are using, first type the following:</p> <pre><code>echo $0\n</code></pre> If you are using Bash (click to expand)  Add the following to your <code>~/.bashrc</code> (or just run it on the command line):   <pre><code>eval \"$(_KEDRO_COMPLETE=bash_source kedro)\"\n</code></pre> If you are using Z shell (ZSh) (click to expand)  Add the following to <code>~/.zshrc</code>:   <pre><code>eval \"$(_KEDRO_COMPLETE=zsh_source kedro)\"\n</code></pre> If you are using Fish (click to expand)  Add the following to <code>~/.config/fish/completions/foo-bar.fish</code>:   <pre><code>eval (env _KEDRO_COMPLETE=fish_source kedro)\n</code></pre>"},{"location":"pages/development/commands_reference/#invoke-kedro-cli-from-python-optional","title":"Invoke Kedro CLI from Python (optional)","text":"<p>You can invoke the Kedro CLI as a Python module:</p> <pre><code>python -m kedro\n</code></pre>"},{"location":"pages/development/commands_reference/#kedro-commands","title":"Kedro commands","text":"<p>Here is a list of Kedro CLI commands, as a shortcut to the descriptions below. Project-specific commands are called from within a project directory and apply to that particular project. Global commands can be run anywhere and don't apply to any particular project:</p> <ul> <li>Global Kedro commands</li> <li><code>kedro --help</code></li> <li><code>kedro --version</code></li> <li><code>kedro info</code></li> <li> <p><code>kedro new</code></p> </li> <li> <p>Project-specific Kedro commands</p> </li> <li><code>kedro catalog list</code></li> <li><code>kedro catalog resolve</code></li> <li><code>kedro catalog rank</code></li> <li><code>kedro catalog create</code></li> <li><code>kedro ipython</code></li> <li><code>kedro jupyter lab</code></li> <li><code>kedro jupyter notebook</code></li> <li><code>kedro micropkg package &lt;pipeline_name&gt;</code> (deprecated from version 0.20.0)</li> <li><code>kedro micropkg pull &lt;package_name&gt;</code> (deprecated from version 0.20.0)</li> <li><code>kedro package</code></li> <li><code>kedro pipeline create &lt;pipeline_name&gt;</code></li> <li><code>kedro pipeline delete &lt;pipeline_name&gt;</code></li> <li><code>kedro registry describe &lt;pipeline_name&gt;</code></li> <li><code>kedro registry list</code></li> <li><code>kedro run</code></li> </ul>"},{"location":"pages/development/commands_reference/#global-kedro-commands","title":"Global Kedro commands","text":"<p>The following are Kedro commands that apply globally and can be run from any directory location.</p> <pre><code>You only need to use one of those given below (e.g. specify `kedro -V` **OR** `kedro --version`).\n</code></pre>"},{"location":"pages/development/commands_reference/#get-help-on-kedro-commands","title":"Get help on Kedro commands","text":"<pre><code>kedro\nkedro -h\nkedro --help\n</code></pre>"},{"location":"pages/development/commands_reference/#confirm-the-kedro-version","title":"Confirm the Kedro version","text":"<pre><code>kedro -V\nkedro --version\n</code></pre>"},{"location":"pages/development/commands_reference/#confirm-kedro-information","title":"Confirm Kedro information","text":"<pre><code>kedro info\n</code></pre> <p>Returns output similar to the following, depending on the version of Kedro used and plugins installed.</p> <pre><code> _            _\n| | _____  __| |_ __ ___\n| |/ / _ \\/ _` | '__/ _ \\\n|   &lt;  __/ (_| | | | (_) |\n|_|\\_\\___|\\__,_|_|  \\___/\nv0.19.12\n\nKedro is a Python framework for\ncreating reproducible, maintainable\nand modular data science code.\n\nInstalled plugins:\nkedro_viz: 10.1.0 (entry points:global,hooks,line_magic)\n\n</code></pre>"},{"location":"pages/development/commands_reference/#create-a-new-kedro-project","title":"Create a new Kedro project","text":"<pre><code>kedro new\n</code></pre>"},{"location":"pages/development/commands_reference/#customise-or-override-project-specific-kedro-commands","title":"Customise or override project-specific Kedro commands","text":"<pre><code>All project related CLI commands should be run from the project\u2019s root directory.\n</code></pre> <p>Kedro's command line interface (CLI) allows you to associate a set of commands and dependencies with a target, which you can then execute from inside the project directory.</p> <p>The commands a project supports are specified on the framework side. If you want to customise any of the Kedro commands you can do this either by adding a file called <code>cli.py</code> or by injecting commands into it via the <code>plugin</code> framework. Find the template for the <code>cli.py</code> file below.</p> Click to expand <pre><code>\"\"\"Command line tools for manipulating a Kedro project.\nIntended to be invoked via `kedro`.\"\"\"\nimport click\nfrom kedro.framework.cli.project import (\n    ASYNC_ARG_HELP,\n    CONFIG_FILE_HELP,\n    CONF_SOURCE_HELP,\n    FROM_INPUTS_HELP,\n    FROM_NODES_HELP,\n    LOAD_VERSION_HELP,\n    NODE_ARG_HELP,\n    PARAMS_ARG_HELP,\n    PIPELINE_ARG_HELP,\n    RUNNER_ARG_HELP,\n    TAG_ARG_HELP,\n    TO_NODES_HELP,\n    TO_OUTPUTS_HELP,\n)\nfrom kedro.framework.cli.utils import (\n    CONTEXT_SETTINGS,\n    _config_file_callback,\n    _split_params,\n    _split_load_versions,\n    env_option,\n    split_string,\n    split_node_names,\n)\nfrom kedro.framework.session import KedroSession\nfrom kedro.utils import load_obj\n\n\n@click.group(context_settings=CONTEXT_SETTINGS, name=__file__)\ndef cli():\n    \"\"\"Command line tools for manipulating a Kedro project.\"\"\"\n\n\n@cli.command()\n@click.option(\n    \"--from-inputs\", type=str, default=\"\", help=FROM_INPUTS_HELP, callback=split_string\n)\n@click.option(\n    \"--to-outputs\", type=str, default=\"\", help=TO_OUTPUTS_HELP, callback=split_string\n)\n@click.option(\n    \"--from-nodes\", type=str, default=\"\", help=FROM_NODES_HELP, callback=split_node_names\n)\n@click.option(\n    \"--to-nodes\", type=str, default=\"\", help=TO_NODES_HELP, callback=split_node_names\n)\n@click.option(\"--nodes\", \"-n\", \"node_names\", type=str, multiple=True, help=NODE_ARG_HELP)\n@click.option(\n    \"--runner\", \"-r\", type=str, default=None, multiple=False, help=RUNNER_ARG_HELP\n)\n@click.option(\"--async\", \"is_async\", is_flag=True, multiple=False, help=ASYNC_ARG_HELP)\n@env_option\n@click.option(\"--tags\", \"-t\", type=str, multiple=True, help=TAG_ARG_HELP)\n@click.option(\n    \"--load-versions\",\n    \"-lv\",\n    type=str,\n    multiple=True,\n    help=LOAD_VERSION_HELP,\n    callback=_split_load_versions,\n)\n@click.option(\"--pipeline\", \"-p\", type=str, default=None, help=PIPELINE_ARG_HELP)\n@click.option(\n    \"--config\",\n    \"-c\",\n    type=click.Path(exists=True, dir_okay=False, resolve_path=True),\n    help=CONFIG_FILE_HELP,\n    callback=_config_file_callback,\n)\n@click.option(\n    \"--conf-source\",\n    type=click.Path(exists=True, file_okay=False, resolve_path=True),\n    help=CONF_SOURCE_HELP,\n)\n@click.option(\n    \"--params\",\n    type=click.UNPROCESSED,\n    default=\"\",\n    help=PARAMS_ARG_HELP,\n    callback=_split_params,\n)\ndef run(\n    tags,\n    env,\n    runner,\n    is_async,\n    node_names,\n    to_nodes,\n    from_nodes,\n    from_inputs,\n    to_outputs,\n    load_versions,\n    pipeline,\n    config,\n    conf_source,\n    params,\n):\n    \"\"\"Run the pipeline.\"\"\"\n\n    runner = load_obj(runner or \"SequentialRunner\", \"kedro.runner\")\n    tags = tuple(tags)\n    node_names = tuple(node_names)\n\n    with KedroSession.create(\n        env=env, conf_source=conf_source, extra_params=params\n    ) as session:\n        session.run(\n            tags=tags,\n            runner=runner(is_async=is_async),\n            node_names=node_names,\n            from_nodes=from_nodes,\n            to_nodes=to_nodes,\n            from_inputs=from_inputs,\n            to_outputs=to_outputs,\n            load_versions=load_versions,\n            pipeline_name=pipeline,\n        )\n\n\n</code></pre>"},{"location":"pages/development/commands_reference/#project-setup","title":"Project setup","text":""},{"location":"pages/development/commands_reference/#install-all-package-dependencies","title":"Install all package dependencies","text":"<p>The following runs <code>pip</code> to install all package dependencies specified in <code>requirements.txt</code>:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>For further information, see the documentation on installing project-specific dependencies.</p>"},{"location":"pages/development/commands_reference/#run-the-project","title":"Run the project","text":"<p>Call the <code>run()</code> method of the <code>KedroSession</code> defined in <code>kedro.framework.session</code>.</p> <pre><code>kedro run\n</code></pre> <p><code>KedroContext</code> can be extended in <code>run.py</code> (<code>src/&lt;package_name&gt;/run.py</code>). In order to use the extended <code>KedroContext</code>, you need to set <code>context_path</code> in the <code>pyproject.toml</code> configuration file.</p>"},{"location":"pages/development/commands_reference/#modifying-a-kedro-run","title":"Modifying a <code>kedro run</code>","text":"<p>Kedro has options to modify pipeline runs. Below is a list of CLI arguments supported out of the box. Note that the names inside angular brackets (<code>&lt;&gt;</code>) are placeholders, and you should replace these values with the the names of relevant nodes, datasets, envs, etc. in your project.</p> CLI command Description <code>kedro run --from-inputs=&lt;dataset_name1&gt;,&lt;dataset_name2&gt;</code> A list of dataset names which should be used as a starting point <code>kedro run --to-outputs=&lt;dataset_name1&gt;,&lt;dataset_name2&gt;</code> A list of dataset names which should be used as an end point <code>kedro run --from-nodes=&lt;node_name1&gt;,&lt;node_name2&gt;</code> A list of node names which should be used as a starting point <code>kedro run --to-nodes=&lt;node_name1&gt;,&lt;node_name1&gt;</code> A list of node names which should be used as an end point <code>kedro run --nodes=&lt;node_name1&gt;,&lt;node_name2&gt;</code> Run only nodes with specified names. <code>kedro run --runner=&lt;runner_name&gt;</code> Run the pipeline with a specific runner <code>kedro run --async</code> Load and save node inputs and outputs asynchronously with threads <code>kedro run --env=&lt;env_name&gt;</code> Run the pipeline in the env_name environment. Defaults to local if not provided <code>kedro run --tags=&lt;tag_name1&gt;,&lt;tag_name2&gt;</code> Run only nodes which have any of these tags attached. <code>kedro run --load-versions=&lt;dataset_name&gt;:YYYY-MM-DDThh.mm.ss.sssZ</code> Specify particular dataset versions (timestamp) for loading. <code>kedro run --pipeline=&lt;pipeline_name&gt;</code> Run the whole pipeline by its name <code>kedro run --namespace=&lt;namespace&gt;</code> Run only nodes with the specified namespace <code>kedro run --config=&lt;config_file_name&gt;.yml</code> Specify all command line options in a named YAML configuration file <code>kedro run --conf-source=&lt;path_to_config_directory&gt;</code> Specify a new source directory for configuration files <code>kedro run --conf-source=&lt;path_to_compressed file&gt;</code> Only possible when using the <code>OmegaConfigLoader</code>. Specify a compressed config file in <code>zip</code> or <code>tar</code> format. <code>kedro run --params=&lt;param_key1&gt;=&lt;value1&gt;,&lt;param_key2&gt;=&lt;value2&gt;</code> Does a parametrised run with <code>{\"param_key1\": \"value1\", \"param_key2\": 2}</code>. These will take precedence over parameters defined in the <code>conf</code> directory. Additionally, dot (<code>.</code>) syntax can be used to address nested keys like <code>parent.child:value</code> <p>You can also combine these options together, so the following command runs all the nodes from <code>split</code> to <code>predict</code> and <code>report</code>:</p> <pre><code>kedro run --from-nodes=split --to-nodes=predict,report\n</code></pre> <p>This functionality is extended to the <code>kedro run --config=config.yml</code> command, which allows you to specify run commands in a configuration file.</p> <p>A parameterised run is best used for dynamic parameters, i.e. running the same pipeline with different inputs, for static parameters that do not change we recommend following the Kedro project setup methodology.</p>"},{"location":"pages/development/commands_reference/#deploy-the-project","title":"Deploy the project","text":"<p>The following packages your application as one <code>.whl</code> file within the <code>dist/</code> folder of your project. It packages the project configuration separately in a <code>tar.gz</code> file:</p> <pre><code>kedro package\n</code></pre> <p>See the Python documentation for further information about packaging.</p>"},{"location":"pages/development/commands_reference/#pull-a-micro-package","title":"Pull a micro-package","text":"<p>Since Kedro 0.17.7 you can pull a micro-package into your Kedro project as follows:</p> <pre><code>_This command is deprecated and will be removed from Kedro in version 0.20.0._\n</code></pre> <pre><code>kedro micropkg pull &lt;link-to-micro-package-sdist-file&gt;\n</code></pre> <p>The above command will take the bundled <code>.tar.gz</code> file and do the following:</p> <ul> <li>Place source code in <code>src/&lt;package_name&gt;/pipelines/&lt;pipeline_name&gt;</code></li> <li>Place parameters in <code>conf/base/parameters_&lt;pipeline_name&gt;.yml</code></li> <li>Pull out tests and place in <code>src/tests/pipelines/&lt;pipeline_name&gt;</code></li> </ul> <p><code>kedro micropkg pull</code> works with PyPI, local and cloud storage:</p> <ul> <li>PyPI: <code>kedro micropkg pull &lt;my-pipeline&gt;</code> with <code>&lt;my-pipeline&gt;</code> being a package on PyPI</li> <li>Local storage: <code>kedro micropkg pull dist/&lt;my-pipeline&gt;-0.1.tar.gz</code></li> <li>Cloud storage: <code>kedro micropkg pull s3://&lt;my-bucket&gt;/&lt;my-pipeline&gt;-0.1.tar.gz</code></li> </ul>"},{"location":"pages/development/commands_reference/#project-quality","title":"Project quality","text":""},{"location":"pages/development/commands_reference/#project-development","title":"Project development","text":""},{"location":"pages/development/commands_reference/#modular-pipelines","title":"Modular pipelines","text":""},{"location":"pages/development/commands_reference/#create-a-new-modular-pipeline-in-your-project","title":"Create a new modular pipeline in your project","text":"<pre><code>kedro pipeline create &lt;pipeline_name&gt;\n</code></pre>"},{"location":"pages/development/commands_reference/#package-a-micro-package","title":"Package a micro-package","text":"<p>The following command packages all the files related to a micro-package, e.g. a modular pipeline, into a Python source distribution file:</p> <pre><code>_This command is deprecated and will be removed from Kedro in version 0.20.0._\n</code></pre> <pre><code>kedro micropkg package &lt;package_module_path&gt;\n</code></pre> <p>Further information is available in the micro-packaging documentation.</p>"},{"location":"pages/development/commands_reference/#pull-a-micro-package-in-your-project","title":"Pull a micro-package in your project","text":"<p>The following command pulls all the files related to a micro-package, e.g. a modular pipeline, from either PyPI or a storage location of a Python source distribution file.</p> <pre><code>_This command is deprecated and will be removed from Kedro in version 0.20.0._\n</code></pre> <pre><code>kedro micropkg pull &lt;package_name&gt; (or path to a sdist file)\n</code></pre> <p>Further information is available in the micro-packaging documentation.</p>"},{"location":"pages/development/commands_reference/#delete-a-modular-pipeline","title":"Delete a modular pipeline","text":"<p>The following command deletes all the files related to a modular pipeline in your Kedro project.</p> <pre><code>kedro pipeline delete &lt;pipeline_name&gt;\n</code></pre> <p>Further information is available in the micro-packaging documentation.</p>"},{"location":"pages/development/commands_reference/#registered-pipelines","title":"Registered pipelines","text":""},{"location":"pages/development/commands_reference/#describe-a-registered-pipeline","title":"Describe a registered pipeline","text":"<pre><code>kedro registry describe &lt;pipeline_name&gt;\n</code></pre> <p>The output includes all the nodes in the pipeline. If no pipeline name is provided, this command returns all nodes in the <code>__default__</code> pipeline.</p>"},{"location":"pages/development/commands_reference/#list-all-registered-pipelines-in-your-project","title":"List all registered pipelines in your project","text":"<pre><code>kedro registry list\n</code></pre>"},{"location":"pages/development/commands_reference/#datasets","title":"Datasets","text":""},{"location":"pages/development/commands_reference/#list-datasets-per-pipeline-per-type","title":"List datasets per pipeline per type","text":"<pre><code>kedro catalog list\n</code></pre> <p>The results include datasets that are/aren't used by a specific pipeline.</p> <p>The command also accepts an optional <code>--pipeline</code> argument that allows you to specify the pipeline name(s) (comma-separated values) in order to filter datasets used only by those named pipeline(s). For example:</p> <pre><code>kedro catalog list --pipeline=ds,de\n</code></pre>"},{"location":"pages/development/commands_reference/#resolve-dataset-factories-in-the-catalog","title":"Resolve dataset factories in the catalog","text":"<pre><code>kedro catalog resolve\n</code></pre> <p>This command resolves dataset factories in the catalog file with any explicit entries in the pipeline. The output includes datasets explicitly mentioned in your catalog files and any datasets mentioned in the project's pipelines that match a dataset factory.</p>"},{"location":"pages/development/commands_reference/#rank-dataset-factories-in-the-catalog","title":"Rank dataset factories in the catalog","text":"<pre><code>kedro catalog rank\n</code></pre> <p>The output includes a list of any dataset factories in the catalog, ranked by the priority on which they are matched against.</p>"},{"location":"pages/development/commands_reference/#data-catalog","title":"Data Catalog","text":""},{"location":"pages/development/commands_reference/#create-a-data-catalog-yaml-configuration-file","title":"Create a Data Catalog YAML configuration file","text":"<p>The following command creates a Data Catalog YAML configuration file with <code>MemoryDataset</code> datasets for each dataset in a registered pipeline, if it is missing from the <code>DataCatalog</code>.</p> <pre><code>kedro catalog create --pipeline=&lt;pipeline_name&gt;\n</code></pre> <p>The command also accepts an optional <code>--env</code> argument that allows you to specify a configuration environment (defaults to <code>base</code>).</p> <p>The command creates the following file: <code>&lt;conf_root&gt;/&lt;env&gt;/catalog_&lt;pipeline_name&gt;.yml</code></p>"},{"location":"pages/development/commands_reference/#notebooks","title":"Notebooks","text":"<p>To start a Jupyter Notebook:</p> <pre><code>kedro jupyter notebook\n</code></pre> <p>To start JupyterLab:</p> <pre><code>kedro jupyter lab\n</code></pre> <p>To start an IPython shell:</p> <pre><code>kedro ipython\n</code></pre> <p>The Kedro IPython extension makes the following variables available in your IPython or Jupyter session:</p> <ul> <li><code>catalog</code> (type {py:class}<code>~kedro.io.DataCatalog</code>): Data Catalog instance that contains all defined datasets; this is a shortcut for <code>context.catalog</code></li> <li><code>context</code> (type {py:class}<code>~kedro.framework.context.KedroContext</code>): Kedro project context that provides access to Kedro's library components</li> <li><code>pipelines</code> (type <code>dict[str, Pipeline]</code>): Pipelines defined in your pipeline registry</li> <li><code>session</code> (type {py:class}<code>~kedro.framework.session.session.KedroSession</code>): Kedro session that orchestrates a pipeline run</li> </ul> <p>To reload these variables (e.g. if you updated <code>catalog.yml</code>) use the <code>%reload_kedro</code> line magic, which can also be used to see the error message if any of the variables above are undefined.</p>"},{"location":"pages/development/debugging/","title":"Debugging","text":"<p>Note Our debugging documentation has moved. Please see our existing guides:</p> <ul> <li>Debugging a Kedro project within a notebook or IPython shell for information on how to debug using the <code>%load_node</code> line magic and an interactive debugger.</li> <li>Debugging in VS Code for information on how to set up VS Code's built-in debugger.</li> <li>Debugging in PyCharm for information on using PyCharm's debugging tool.</li> <li>Debugging in the CLI with Kedro Hooks for information on how to automatically launch an interactive debugger in the CLI when an error occurs in your pipeline run.</li> </ul>"},{"location":"pages/development/linting/","title":"Code formatting and linting","text":""},{"location":"pages/development/linting/#introduction","title":"Introduction","text":"<p>Code formatting guidelines set a standard for the layout of your code, for stylistic elements such as use of line breaks and whitespace. Format doesn't have any impact on how the code works, but using a consistent style makes your code more readable, and makes it more likely to be reused.</p> <p>Linting tools check your code for errors such as a missing bracket or line indent. This can save time and frustration because you can catch errors in advance of running the code.</p> <p>As a project grows and goes through various stages of development it becomes important to maintain code quality. Using a consistent format and linting your code ensures that it is consistent, readable, and easy to debug and maintain.</p>"},{"location":"pages/development/linting/#set-up-python-tools","title":"Set up Python tools","text":"<p>There are a variety of Python tools available to use with your Kedro projects. This guide shows you how to use <code>ruff</code>.</p> <p><code>ruff</code> is a fast linter and formatter that replaces <code>flake8</code>, <code>pylint</code>, <code>pyupgrade</code>, <code>isort</code>, <code>black</code> and more.   - It helps to make your code compliant to PEP 8.   - It reformats code and sorts imports alphabetically and automatically separating them into sections by type.</p>"},{"location":"pages/development/linting/#install-the-tools","title":"Install the tools","text":"<p>To install <code>ruff</code> add the following section to the <code>pyproject.toml</code> file located in the project root:</p> <pre><code>[project.optional-dependencies]\ndev = [\"ruff\"]\n</code></pre> <p>Then to install your project including all the project-specific dependencies and the linting tools, navigate to the root directory of the project and run:</p> <pre><code>pip install .\"[dev]\"\n</code></pre> <p>Alternatively, you can individually install the linting tools using the following shell commands:</p> <pre><code>pip install ruff\n</code></pre>"},{"location":"pages/development/linting/#configure-ruff","title":"Configure <code>ruff</code>","text":"<p><code>ruff</code> read configurations from <code>pyproject.toml</code> within your project root. You can enable different rule sets within the <code>[tool.ruff]</code> section. For example, the rule set <code>F</code> is equivalent to <code>Pyflakes</code>.</p> <p>To start with <code>ruff</code>, we recommend adding this section to enable a few basic rules sets.</p> <pre><code>[tool.ruff]\nselect = [\n    \"F\",  # Pyflakes\n    \"E\",  # Pycodestyle\n    \"W\",  # Pycodestyle\n    \"UP\",  # pyupgrade\n    \"I\",  # isort\n    \"PL\", # Pylint\n]\n</code></pre> <pre><code>It is a good practice to [split your line when it is too long](https://beta.ruff.rs/docs/rules/line-too-long/), so it can be read easily even in a small screen. `ruff` treats this slightly different from `black`, when using together we recommend to disable this rule, i.e. `E501` to avoid conflicts.\n</code></pre>"},{"location":"pages/development/linting/#run-the-tools","title":"Run the tools","text":"<p>Use the following commands to run lint checks:</p> <pre><code>ruff format --check &lt;project_root&gt;\nruff check &lt;project_root&gt;\n</code></pre> <p>You can also have <code>ruff format</code> automatically format your code by omitting the <code>--check</code> flag.</p>"},{"location":"pages/development/linting/#automated-formatting-and-linting-with-pre-commit-hooks","title":"Automated formatting and linting with <code>pre-commit</code> hooks","text":"<p>You can automate the process of formatting and linting with <code>pre-commit</code> hooks. These hooks are run before committing your code to your repositories to automatically point out formatting issues, making code reviews easier and less time-consuming.</p>"},{"location":"pages/development/linting/#install-pre-commit","title":"Install <code>pre-commit</code>","text":"<p>You can install <code>pre-commit</code> along with other dependencies by including it in the <code>requirements.txt</code> file of your Kedro project by adding the following line:</p> <pre><code>pre-commit\n</code></pre> <p>You can also install <code>pre-commit</code> using the following command:</p> <pre><code>pip install pre-commit\n</code></pre>"},{"location":"pages/development/linting/#add-pre-commit-configuration-file","title":"Add <code>pre-commit</code> configuration file","text":"<p>Create a file named <code>.pre-commit-config.yaml</code> in your Kedro project root directory. You can add entries for the hooks you want to run before each <code>commit</code>. Below is a sample <code>YAML</code> file with entries for <code>ruff</code>:</p> <pre><code>repos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    # Ruff version.\n    rev: '' # Replace with latest version, for example 'v0.1.8'\n    hooks:\n      - id: ruff\n        args: [--fix]\n      - id: ruff-format\n</code></pre> <p>See GitHub for the latest configuration for ruff's pre-commit.</p>"},{"location":"pages/development/linting/#install-git-hook-scripts","title":"Install git hook scripts","text":"<p>Run the following command to complete installation:</p> <pre><code>pre-commit install\n</code></pre> <p>This enables <code>pre-commit</code> hooks to run automatically every time you execute <code>git commit</code>.</p>"},{"location":"pages/development/set_up_pycharm/","title":"Set up PyCharm","text":"<p>This section will present a quick guide on how to configure PyCharm as a development environment for working on Kedro projects.</p> <p>Open a new project directory in PyCharm. You will need to add your Project Interpreter, so go to PyCharm | Preferences for macOS or File | Settings for Windows and Linux:</p> <p></p> <p>Choose Project Interpreter: </p> <p></p> <p>Click the cog on the right-hand side and click Add:</p> <p></p> <p>Select Conda Environment:</p> <p></p> <p>Choose Existing environment and navigate your way to find your existing environment. If you don't see your <code>conda</code> environment in the dropdown menu then you need to open a <code>terminal</code> / <code>command prompt</code> with your <code>conda</code> environment activated and run:</p> <pre><code># macOS / Linux\nwhich python\n# Windows\npython -c \"import sys; print(sys.executable)\"\n</code></pre> <p>Paste the interpreter path into the file picker and click OK: </p> <p></p> <p>Finally, in the Project Explorer right-click on <code>src</code> and then go to Mark Directory as | Sources Root:</p> <p></p>"},{"location":"pages/development/set_up_pycharm/#set-up-run-configurations","title":"Set up Run configurations","text":"<p>PyCharm Run configurations allow you to execute preconfigured scripts rapidly in your IDE with a click of a button. This may be useful for testing, running and packaging your Kedro projects.</p> <p>Here we will walk you through an example of how to set up Run configuration for the Kedro CLI <code>run</code> command. It is also applicable to other Kedro commands, such as <code>test</code> or <code>install</code>.</p> <p>Go to Run | Edit Configurations:</p> <p></p> <p>Add a new Python Run configuration:</p> <p></p> <p>Specify the Run / Debug Configuration name in the Name field, and edit the new Run configuration as follows:</p> <ul> <li>Pick Module from the dropdown</li> <li>Enter <code>kedro</code> in the Module Name field</li> <li>Enter <code>run</code> in the Parameters field</li> <li>Enter the path of your project directory into the Working directory field</li> <li>Pick <code>Emulate terminal in output console</code> from the Modify options dropdown, and then click OK</li> </ul> <p></p> <pre><code>**Emulate terminal in output console** enables PyCharm to show [rich terminal output](../logging/index.md).\n</code></pre> <p>To execute the Run configuration, select it from the Run / Debug Configurations dropdown in the toolbar (if that toolbar is not visible, you can enable it by going to View &gt; Toolbar). Click the green triangle:</p> <p></p> <p>You may also select Run from the toolbar and execute from there. </p> <p></p> <p>For other <code>kedro</code> commands, follow same steps but replace <code>run</code> in the <code>Parameters</code> field with the other commands that are to be used (e.g., <code>jupyter</code>, <code>package</code>, <code>registry</code> etc.).</p>"},{"location":"pages/development/set_up_pycharm/#debugging","title":"Debugging","text":"<p>To debug, simply click the line number in the source where you want execution to break:</p> <p></p> <p>Then click the bug button in the toolbar () and execution should stop at the breakpoint:</p> <p></p> <p>For more information about debugging with PyCharm take a look at the debugging guide on jetbrains.com.</p>"},{"location":"pages/development/set_up_pycharm/#advanced-remote-ssh-interpreter","title":"Advanced: Remote SSH interpreter","text":"<pre><code>This section uses features supported in PyCharm Professional Edition only.\n</code></pre> <p>Firstly, add an SSH interpreter. Go to Preferences | Project Interpreter as above and proceed to add a new interpreter. Select SSH Interpreter and fill in details of the remote computer:</p> <p></p> <p>Click Next and add the SSH password or SSH private key:</p> <p></p> <p>Click Next and add the path of the remote interpreter. Assuming a Unix-like OS, this can be found by running <code>which python</code> within the appropriate <code>conda</code> environment on the remote computer.</p> <p></p> <p>Click Finish. Go to Run / Debug Configurations to add a Remote Run. Select the remote interpreter that you have just created. For the script path, get the path of the Kedro CLI on the remote computer by running <code>which kedro</code> (macOS / Linux) in the appropriate environment.</p> <p></p> <p>Click OK and then select Remote Run from the toolbar and click Run to execute remotely.</p> <p></p> <p>To debug remotely, click the debugger button as described above.</p>"},{"location":"pages/development/set_up_pycharm/#advanced-docker-interpreter","title":"Advanced: Docker interpreter","text":"<pre><code>This section uses features supported by PyCharm Professional Edition only.\n</code></pre> <p>First, add a Docker interpreter. Go to Preferences | Project Interpreter as above and proceed to add a new interpreter. Select Docker Interpreter and then choose the target Docker image:</p> <p></p> <p>Click OK and check that the required packages appear:</p> <p></p> <p>Click OK and wait for PyCharm to index your new Python interpreter.</p> <p>Click OK. Go to Run / Debug Configurations to add a Python Run. For the script path, get the path to the Kedro CLI on an instantiated image by running <code>which kedro</code> (macOS / Linux) in a container environment. Specify <code>run</code> as the parameter. Specify your working directory as the directory that contains your Kedro project on your local machine. Optional: Edit the Docker container volume mount setting if it does not match the directory that contains your Kedro project directory.</p> <p></p> <p>Click OK and then select your run configuration from the toolbar and click Run to execute.</p> <p>To debug, click the debugger button as described above.</p>"},{"location":"pages/development/set_up_pycharm/#configure-python-console","title":"Configure Python Console","text":"<p>You can configure PyCharm's IPython to load Kedro's Extension.</p> <p>Click PyCharm | Preferences for macOS or File | Settings, inside Build, Execution, Deployment and Console, enter the Python Console configuration.</p> <p>You can append the configuration necessary to use Kedro IPython to the Starting script:</p> <pre><code>%load_ext kedro.ipython\n</code></pre> <p>With this configuration, when you create a Python Console you should be able to use context, session and catalog.</p> <p></p>"},{"location":"pages/development/set_up_pycharm/#configuring-the-kedro-catalog-validation-schema","title":"Configuring the Kedro catalog validation schema","text":"<p>You can enable the Kedro catalog validation schema in your PyCharm IDE to enable real-time validation, autocompletion and see information about the different fields in your <code>catalog</code> as you write it. To enable this, open a <code>catalog.yml</code> file and you should see \"No JSON Schema\" in the bottom right corner of your window. Click it and select \"Edit Schema Mapping\".</p> <p></p> <p>Add a new mapping using the \"+\" button in the top left of the window and select the name you want for it. Enter this URL <code>https://raw.githubusercontent.com/kedro-org/kedro-plugins/main/kedro-datasets/static/jsonschema/kedro-catalog-0.19.json</code> in the \"Schema URL\" field and select \"JSON Schema Version 7\" in the \"Schema version\" field.</p> <p>Add the following file path pattern to the mapping: <code>conf/**/*catalog*</code>.</p> <p></p> <p>Different schemas for different Kedro versions can be found in the <code>kedro-datasets</code> repository.</p>"},{"location":"pages/development/set_up_vscode/","title":"Set up Visual Studio Code","text":"<p>Start by opening a new project directory in VS Code and installing the Python plugin under Tools and languages:</p> <p></p> <p>Python is an interpreted language; to run Python code you must tell VS Code which interpreter to use. From within VS Code, select a Python 3 interpreter by opening the Command Palette (<code>Cmd + Shift + P</code> for macOS), start typing the Python: Select Interpreter command to search, then select the command.</p> <p>At this stage, you should be able to see the <code>conda</code> environment that you have created. Select the environment:</p> <p></p>"},{"location":"pages/development/set_up_vscode/#kedro-vs-code-extension","title":"Kedro VS Code Extension","text":"<p>Kedro VS Code extension supports Kedro 0.19+. It helps you navigate around your Kedro project by finding the definition of your datasets, find references to them in your code, and more.</p> <p></p>"},{"location":"pages/development/set_up_vscode/#setting-up-venv-virtualenv-in-vs-code","title":"Setting up <code>venv</code> / <code>virtualenv</code> in VS Code","text":"<p>We're going to show you how to get your virtual environments to show up in your Python interpreter in VS Code. You do this by opening <code>settings.json</code> and adding the following:</p> <pre><code>\"python.venvPath\": \"/path/containing/your/venvs/\"\n</code></pre> <p>It is useful to note that if you create a <code>venv</code> / <code>virtualenv</code> in your project directory named <code>venv</code>, VS Code (much like PyCharm) automatically loads it as the Python interpreter (unless you manually define your Python interpreter to something else as described above).</p>"},{"location":"pages/development/set_up_vscode/#setting-up-tasks","title":"Setting up tasks","text":"<p>Here we will show you how to setup tasks for such Kedro CLI commands as <code>run</code>, <code>test</code>, <code>install</code>, <code>package</code>, etc.</p> <p>You'll start by finding the path of your Kedro CLI script in the terminal:</p> <pre><code># macOS / Linux\nwhich kedro\n\n# Windows (in **Anaconda Command Prompt**)\nwhere kedro\n</code></pre> <p>We're going to need you to modify your <code>tasks.json</code>. To do this, go to Terminal &gt; Configure Tasks... on your menu and open up <code>tasks.json</code> in the editor. Modify it with the following:</p> <pre><code>{\n    // See https://go.microsoft.com/fwlink/?LinkId=733558\n    // Kedro tasks\n    \"version\": \"2.0.0\",\n    \"tasks\": [\n        {\n            \"label\": \"Install\",\n            \"type\": \"shell\",\n            \"command\": \"/path/to/kedro/script\",\n            \"args\": [\n                \"install\"\n            ]\n        },\n        {\n            \"label\": \"Test\",\n            \"group\": \"test\",\n            \"type\": \"shell\",\n            \"command\": \"/path/to/kedro/script\",\n            \"args\": [\n                \"test\"\n            ]\n        },\n        {\n            \"label\": \"Run\",\n            \"type\": \"shell\",\n            \"command\": \"/path/to/kedro/script\",\n            \"args\": [\n                \"run\"\n            ]\n        },\n\n        // This is the default build task\n        {\n            \"label\": \"Package\",\n            \"group\": {\n                \"kind\": \"build\",\n                \"isDefault\": true\n            },\n            \"type\": \"shell\",\n            \"command\": \"/path/to/kedro/script\",\n            \"args\": [\n                \"package\"\n            ],\n            // Will run `Test` before packaging\n            \"dependsOn\": [\n                \"Test\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>To start a build, go to Terminal &gt; Run Build Task... or press <code>Cmd + Shift + B</code> for macOS. You can run other tasks by going to Terminal &gt; Run and choosing which task you want to run.</p> <p></p>"},{"location":"pages/development/set_up_vscode/#setting-a-custom-kedro-project-path","title":"Setting a custom Kedro project path","text":"<p>Starting with Kedro VS Code extension version 0.3.0, you can now specify a custom path to your Kedro project. This is useful when:</p> <ul> <li>Your Kedro project is not at the root of your workspace</li> <li>You want to work with a Kedro project that is outside your current workspace</li> <li>You have multiple Kedro projects and want to switch between them</li> </ul>"},{"location":"pages/development/set_up_vscode/#set-up-a-custom-path-using-the-command-palette","title":"Set up a custom path using the command palette","text":"<ol> <li>Open the Command Palette by pressing <code>Cmd + Shift + P</code> (macOS) or <code>Ctrl + Shift + P</code> (Windows/Linux)</li> <li>Type <code>Kedro: Set Project Path</code> and select it</li> <li>Enter the absolute path to your Kedro project (for example, <code>/Users/username/projects/my-kedro-project</code>)</li> </ol>"},{"location":"pages/development/set_up_vscode/#set-up-a-custom-path-using-the-vscode-settings-ui","title":"Set up a custom path using the VSCode settings UI","text":"<ol> <li>Open VS Code settings by pressing <code>Cmd + ,</code> (macOS) or <code>Ctrl + ,</code> (Windows/Linux)</li> <li>Search for <code>kedro</code> in the settings search bar</li> <li>Find the <code>Kedro: Project Path</code> setting</li> <li>Enter the absolute path to your Kedro project in the field</li> </ol>"},{"location":"pages/development/set_up_vscode/#multi-root-workspace-integration","title":"Multi-root workspace integration","text":"<p>If the Kedro project path you specify is not part of your current workspace, the extension will automatically add it to your workspace as part of a multi-root workspace. This allows you to:</p> <ul> <li>See the project files in the Explorer</li> <li>Navigate the project structure</li> <li>Use all Kedro extension features with the specified project</li> </ul>"},{"location":"pages/development/set_up_vscode/#example-directory-structure","title":"Example directory structure","text":"<p>If your Kedro project is nested within other folders, setting a custom project path can help the extension locate it. For example:</p> <pre><code>root\n\u2502   file001.txt\n\u2502\n\u2514\u2500\u2500\u2500folder1\n\u2502   \u2502   file011.txt\n\u2502   \u2502   file012.txt\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500kedroProject  &lt;-- Set this path\n\u2502       \u2502   pyproject.toml\n\u2502       \u2502   README.md\n\u2502       \u2502   ...\n\u2502\n\u2514\u2500\u2500\u2500folder2\n    \u2502   file020.txt\n    \u2502   file021.txt\n</code></pre> <p>In this case, you would set the Kedro project path to the absolute path of the <code>kedroProject</code> directory, such as <code>/Users/username/root/folder1/kedroProject</code>.</p>"},{"location":"pages/development/set_up_vscode/#switching-between-multiple-projects","title":"Switching between multiple projects","text":"<p>If you work with multiple Kedro projects, you can switch between them by updating the project path setting. The extension will automatically detect the change and reconfigure itself to work with the newly specified project.</p>"},{"location":"pages/development/set_up_vscode/#troubleshooting","title":"Troubleshooting","text":"<p>If the extension doesn't recognise your Kedro project after setting a custom path:</p> <ol> <li>Ensure the path points to a valid Kedro project (containing <code>pyproject.toml</code> with Kedro dependencies)</li> <li>Check that the path is an absolute path, not a relative one</li> <li>Reload VS Code if the changes don\u2019t take effect.</li> </ol>"},{"location":"pages/development/set_up_vscode/#real-time-catalog-validation-with-kedro-lsp","title":"Real time catalog validation with Kedro LSP","text":"<p>With the latest Kedro VS Code extension, you can automatically check your <code>catalog*.yml</code> or <code>catalog*.yaml</code> files without installing additional YAML plugins or schemas. The extension now uses a Language Server Protocol (LSP) approach to catch configuration issues as you edit.</p>"},{"location":"pages/development/set_up_vscode/#how-it-works","title":"How it works","text":"<ul> <li>Parsing &amp; Dataset Checks: The extension reads your catalog file and tries to load each dataset to ensure the configuration is correct.</li> <li>Immediate Feedback: If any dataset has an invalid type or missing dependency, you\u2019ll see red underlines in the editor and an entry in VS Code\u2019s Problems panel.</li> <li>Incremental &amp; Periodic Validation: The extension re-checks your catalogs whenever you open or edit a file, and it can also run in the background, so you\u2019re always up-to-date on any potential issues.</li> </ul>"},{"location":"pages/development/set_up_vscode/#viewing-errors-in-the-problems-panel","title":"Viewing errors in the problems panel","text":"<p>VS Code\u2019s Problems panel provides a convenient overview of all catalog issues:</p> <ol> <li>Go to View &gt; Problems or press <code>Ctrl+Shift+M</code> (or <code>Cmd+Shift+M</code> on macOS).</li> <li>Expand any reported errors to see details about what\u2019s wrong (for example, \u201cClass not found.\u201d).</li> <li>Click an error to jump directly to the problematic line in the catalog file.</li> </ol> <p></p> <p>This simplifies fixing dataset errors caused by things such as typos in your dataset type or missing modules before running any Kedro pipelines.</p>"},{"location":"pages/development/set_up_vscode/#visualise-the-pipeline-with-kedro-viz","title":"Visualise the pipeline with Kedro-Viz","text":"<p>To visualize your Kedro project using Kedro-Viz in Visual Studio Code, follow these steps:</p> <ol> <li> <p>Open the Command Palette: Press <code>Cmd</code> + <code>Shift</code> + <code>P</code> (on macOS) or <code>Ctrl</code> + <code>Shift</code> + <code>P</code> (on Windows/Linux).</p> </li> <li> <p>Run Kedro-Viz: Type <code>kedro: Run Kedro Viz</code> and select the command. This will launch Kedro-Viz and display your pipeline visually within the extension.</p> </li> </ol> <p>Note To update the Kedro-Viz flowchart after making any changes to your Kedro project, please hit <code>Cmd</code> + <code>Shift</code> + <code>P</code> to open the VSCode command and look for <code>kedro: restart server</code>.</p> <p>Navigate to Node Functions: Click on a node in the Kedro-Viz flowchart, and it will automatically navigate to the corresponding node function in your code. </p> <p>Navigate to DataCatalog: Clicking on a data node in the flowchart will open the corresponding dataset in the Data Catalog. </p>"},{"location":"pages/development/set_up_vscode/#debugging","title":"Debugging","text":"<p>To debug, you may need to create an <code>.env</code> file in your project root. Add the full path to the <code>./src/</code> folder to the PYTHONPATH environment variable in the <code>.env</code> file:</p> <pre><code>#\u00a0In macOS / Linux:\nPYTHONPATH=/path/to/project/src:$PYTHONPATH\n\n# In Windows\nPYTHONPATH=C:/path/to/project/src;%PYTHONPATH%\n</code></pre> <p>You can find more information about setting up environmental variables in the VS Code documentation.</p> <p>Click on the Run and Debug icon on the left activity bar (press <code>Cmd + Shift + D</code> for macOS). If there is no existing configuration, click on create a launch.json file else click on the dropdown arrow in the top-left (shown below) and click Add Configuration . </p> <p>Note If you encounter the following error: <code>Cannot read property 'openConfigFile' of undefined</code>, you can manually create a <code>launch.json</code> file in the <code>.vscode</code> directory and paste the configuration from below.</p> <p>Edit the <code>launch.json</code> that opens in the editor with:</p> <pre><code>{\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Kedro Run\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"console\": \"integratedTerminal\",\n            \"module\": \"kedro\",\n            \"args\": [\"run\"]\n            // Any other arguments should be passed as a comma-seperated-list\n            // e.g \"args\": [\"run\", \"--pipeline\", \"pipeline_name\"]\n        }\n    ]\n}\n</code></pre> <p>To add a breakpoint in your <code>pipeline.py</code> script, for example, click on the left hand side of the line of code:</p> <p></p> <p>Click on Debug button on the left pane:</p> <p></p> <p>Then select the debug config Python: Kedro Run and click Debug (the green play button):</p> <p></p> <p>Execution should stop at the breakpoint:</p> <p></p>"},{"location":"pages/development/set_up_vscode/#advanced-remote-interpreter-debugging","title":"Advanced: Remote interpreter debugging","text":"<p>It is possible to debug remotely using VS Code. The following example assumes SSH access is available on the remote computer (running a Unix-like OS) running the code that will be debugged.</p> <p>First install the <code>ptvsd</code> Python library on both the local and remote computer using the following command (execute it on both computers in the appropriate <code>conda</code> environment):</p> <pre><code>python -m pip install --upgrade ptvsd\n</code></pre> <p>Go to the Debugger Configurations as described in the debugging section above. Add the following to the <code>configurations</code> array in <code>launch.json</code>:</p> <pre><code>{\n    \"name\": \"Kedro Remote Debugger\",\n    \"type\": \"python\",\n    \"request\": \"attach\",\n    \"pathMappings\": [\n        {\n            // You may also manually specify the directory containing your source code.\n            \"localRoot\": \"${workspaceFolder}\",\n            \"remoteRoot\": \"/path/to/your/project\"\n        }\n    ],\n    \"port\": 3000,  // Set to the remote port.\n    \"host\": \"127.0.0.1\"  // Set to your remote host's public IP address.\n}\n</code></pre> <p>Change the <code>remoteRoot</code> path to the path of your project on the remote computer. Open the file <code>src/&lt;package_name&gt;/__main__.py</code> and enter the following near the top:</p> <pre><code>import ptvsd\n\n# Allow other computers to attach to ptvsd at this IP address and port.\nptvsd.enable_attach(address=(\"127.0.0.1\", 3000), redirect_output=True)\n\n# Pause the program until a remote debugger is attached\nprint(\"Waiting for debugger to attach...\")\nptvsd.wait_for_attach()\n</code></pre> <p>Ensure both computers (the computer you are working on and the remote computer executing your code) have the same source code. For example, you can use <code>scp</code> to sync your code:</p> <pre><code>scp -r &lt;project_root&gt; &lt;your_username&gt;@&lt;remote_server&gt;:projects/\n</code></pre> <p>\u2757The example above assumes there is a directory called <code>projects</code> in the home directory of the user account on the remote computer. This is where the project will be copied to. This can be set up as a deploy task as described above:</p> <pre><code>// Add to `tasks` array in `tasks.json`\n{\n    \"label\": \"Deploy\",\n    \"type\": \"shell\",\n    \"command\": \"scp -r &lt;project_root&gt; &lt;your_username&gt;@&lt;remote_server&gt;:projects/\",\n}\n</code></pre> <p>Note There is also a third-party plugin for VS Code that supports remote workspaces.</p> <p>Start executing the pipeline on your remote computer:</p> <pre><code>while :; do kedro run; done\n</code></pre> <p>You should see the following message in the terminal and execution will stop:</p> <pre><code>Waiting for debugger to attach...\n</code></pre> <p>Open a new terminal session and create an SSH tunnel from your local computer to the remote one (leave this process running):</p> <pre><code>ssh -vNL 3000:127.0.0.1:3000 &lt;your_username&gt;@&lt;remote_server&gt;\n</code></pre> <p>Go to the Debugging section in VS Code and select the newly created remote debugger profile:</p> <p></p> <p>You must set a breakpoint in VS Code as described in the debugging section above and start the debugger by clicking the green play triangle:</p> <p>Find more information on debugging in VS Code.</p>"},{"location":"pages/extend_kedro/","title":"Extend Kedro","text":"<ul> <li>Common Use Cases</li> <li>Plugins</li> <li>Architecture Overview</li> <li>Create a Starter</li> </ul>"},{"location":"pages/extend_kedro/architecture_overview/","title":"Kedro architecture overview","text":"<p>There are different ways to leverage Kedro in your work, you can:</p> <ul> <li>Commit to using all of Kedro (framework, project, starters and library); which is preferable to take advantage of the full value proposition of Kedro</li> <li>You can use parts of Kedro, like the DataCatalog (I/O), OmegaConfigLoader, Pipelines and Runner, by using it as a Python library; this best supports a workflow where you don't want to adopt the Kedro project template</li> <li>Or, you can develop extensions for Kedro e.g. custom starters, plugins, Hooks and more</li> </ul> <p>At a high level, Kedro consists of five main parts:</p> <p></p>"},{"location":"pages/extend_kedro/architecture_overview/#kedro-project","title":"Kedro project","text":"<p>As a data pipeline developer, you will interact with a Kedro project, which consists of:</p> <ul> <li>The <code>conf/</code> directory, which contains configuration for the project, such as data catalog configuration, parameters, etc.</li> <li>The <code>src</code> directory, which contains the source code for the project, including:</li> <li>The <code>pipelines</code>  directory, which contains the source code for your pipelines.</li> <li><code>settings.py</code> file contains the settings for the project, such as library component registration, custom hooks registration, etc. All the available settings are listed and explained in the project settings chapter.</li> <li><code>pipeline_registry.py</code> file defines the project pipelines, i.e. pipelines that can be run using <code>kedro run --pipeline</code>.</li> <li><code>__main__.py</code> file serves as the main entry point of the project in package mode.</li> <li><code>pyproject.toml</code> identifies the project root by providing project metadata, including:</li> <li><code>package_name</code>: A valid Python package name for your project package.</li> <li><code>project_name</code>: A human readable name for your project.</li> <li><code>kedro_init_version</code>: Kedro version with which the project was generated.</li> </ul>"},{"location":"pages/extend_kedro/architecture_overview/#kedro-framework","title":"Kedro framework","text":"<p>Kedro framework serves as the interface between a Kedro project and Kedro library components. The major building blocks of the Kedro framework include:</p> <ul> <li>{py:mod}<code>~kedro.framework.session</code> is responsible for managing the lifecycle of a Kedro run.</li> <li>{py:mod}<code>~kedro.framework.context</code> holds the configuration and Kedro's main functionality, and also serves as the main entry point for interactions with core library components.</li> <li>{py:mod}<code>~kedro.framework.hooks</code> defines all hook specifications available to extend Kedro.</li> <li>{py:mod}<code>~kedro.framework.cli</code> defines built-in Kedro CLI commands and utilities to load custom CLI commands from plugins.</li> </ul>"},{"location":"pages/extend_kedro/architecture_overview/#kedro-starter","title":"Kedro starter","text":"<p>You can use a Kedro starter to generate a Kedro project that contains boilerplate code. We maintain a set of official starters but you can also use a custom starter of your choice.</p>"},{"location":"pages/extend_kedro/architecture_overview/#kedro-library","title":"Kedro library","text":"<p>Kedro library consists of independent units, each responsible for one aspect of computation in a data pipeline:</p> <ul> <li>{py:class}<code>~kedro.config.OmegaConfigLoader</code> provides utility to parse and load configuration defined in a Kedro project.</li> <li>{py:mod}<code>~kedro.pipeline</code> provides a collection of abstractions to model data pipelines.</li> <li>{py:mod}<code>~kedro.runner</code> provides an abstraction for different execution strategy of a data pipeline.</li> <li>{py:mod}<code>~kedro.io</code> provides a collection of abstractions to handle I/O in a project, including <code>DataCatalog</code> and many <code>Dataset</code> implementations.</li> </ul>"},{"location":"pages/extend_kedro/architecture_overview/#kedro-extension","title":"Kedro extension","text":"<p>You can also extend Kedro behaviour in your project using a Kedro extension, which can be a custom starter, a Python library with extra hooks implementations, extra CLI commands such as Kedro-Viz or a custom library component implementation.</p> <p>If you create a Kedro extension, we welcome all kinds of contributions. Check out our guide to contributing to Kedro. Dataset contributions to <code>kedro-datasets</code> are the most frequently accepted, since they do not require any changes to the framework itself. However, we do not discourage contributions to any of the other <code>kedro-plugins</code>.</p>"},{"location":"pages/extend_kedro/common_use_cases/","title":"Common use cases","text":"<p>Kedro has a few built-in mechanisms for you to extend its behaviour. This document explains how to select which mechanism to employ for the most common use cases.</p>"},{"location":"pages/extend_kedro/common_use_cases/#use-case-1-how-to-add-extra-behaviour-to-kedros-execution-timeline","title":"Use Case 1: How to add extra behaviour to Kedro's execution timeline","text":"<p>The execution timeline of a Kedro pipeline can be thought of as a sequence of actions performed by various Kedro library components, such as the {py:mod}<code>datasets &lt;kedro-datasets:kedro_datasets&gt;</code>, {py:class}<code>~kedro.io.DataCatalog</code>, {py:class}<code>~kedro.pipeline.Pipeline</code>, {py:class}<code>~kedro.pipeline.node.Node</code> and {py:class}<code>~kedro.framework.context.KedroContext</code>.</p> <p>At different points in the lifecycle of these components, you might want to add extra behaviour: for example, you could add extra computation for profiling purposes before and after a node runs, or before and after the I/O actions of a dataset, namely the <code>load</code> and <code>save</code> actions.</p> <p>This can now achieved by using Hooks, to define the extra behaviour and when in the execution timeline it should be introduced.</p>"},{"location":"pages/extend_kedro/common_use_cases/#use-case-2-how-to-integrate-kedro-with-additional-data-sources","title":"Use Case 2: How to integrate Kedro with additional data sources","text":"<p>You can use {py:mod}<code>datasets &lt;kedro-datasets:kedro_datasets&gt;</code> to interface with various different data sources. If the data source you plan to use is not supported out of the box by Kedro, you can create a custom dataset.</p>"},{"location":"pages/extend_kedro/common_use_cases/#use-case-3-how-to-add-or-modify-cli-commands","title":"Use Case 3: How to add or modify CLI commands","text":"<p>If you want to customise a built-in Kedro command, such as <code>kedro run</code>, for a specific project, add a <code>cli.py</code> file that defines a custom <code>run()</code> function. You should add the <code>cli.py</code> file at the same level as <code>settings.py</code>, which is usually the <code>src/PROJECT_NAME</code> directory. See the template for the <code>cli.py</code> file.</p> <p>If you want to customise a Kedro command from a command group, such as <code>kedro pipeline</code> or <code>kedro jupyter</code>, you need to import the corresponding click command group from the Kedro framework <code>cli</code>. For <code>kedro pipeline</code> commands this would be <code>from kedro.framework.cli.pipeline import pipeline</code>, and for <code>kedro jupyter</code> commands <code>from kedro.framework.cli.jupyter import jupyter</code>. Note that you must still add the <code>cli</code> click group from the snippet above, even if you don't modify it.</p> <p>You can then add or overwrite any command by adding it to the click group, as in the snippet below:</p> <pre><code>@jupyter.command(\"notebook\")\n@env_option(\n    help=\"Open a notebook\"\n)\ndef notebook_run(...):\n    == ADD YOUR CUSTOM NOTEBOOK COMMAND CODE HERE ==\n</code></pre> <p>To inject additional CLI commands intended to be reused across projects, see our plugin system. An example of one such command is the <code>kedro viz run</code> command introduced by the Kedro-Viz plugin. This command is intended to work on every Kedro project which is why it comes from a standalone plugin.</p> <pre><code>Your plugin's implementation can take advantage of other extension mechanisms such as Hooks.\n</code></pre>"},{"location":"pages/extend_kedro/common_use_cases/#use-case-4-how-to-customise-the-initial-boilerplate-of-your-project","title":"Use Case 4: How to customise the initial boilerplate of your project","text":"<p>Sometimes you might want to tailor the starting boilerplate of a Kedro project to your specific needs. For example, your organisation might have a standard CI script that you want to include in every new Kedro project. To this end, see the guide for creating Kedro starters.</p>"},{"location":"pages/extend_kedro/plugins/","title":"Kedro plugins","text":"<p>Kedro plugins allow you to create new features for Kedro and inject additional commands into the CLI. Plugins are developed as separate Python packages that exist outside of any Kedro project.</p>"},{"location":"pages/extend_kedro/plugins/#overview","title":"Overview","text":"<p>Kedro's extension mechanism is built on <code>pluggy</code>, a solid plugin management library that was created for the pytest ecosystem. <code>pluggy</code> relies on entry points, a Python mechanism for packages to provide components that can be discovered by other packages using <code>importlib.metadata</code>.</p>"},{"location":"pages/extend_kedro/plugins/#example-of-a-simple-plugin","title":"Example of a simple plugin","text":"<p>Here is a simple example of a plugin that prints the pipeline as JSON:</p> <p><code>kedrojson/plugin.py</code></p> <pre><code>import click\nfrom kedro.framework.project import pipelines\n\n\n@click.group(name=\"JSON\")\ndef commands():\n    pass\n\n\n@commands.command(name=\"to_json\")\n@click.pass_obj\ndef to_json(metadata):\n    \"\"\"Display the pipeline in JSON format\"\"\"\n    pipeline = pipelines[\"__default__\"]\n    print(pipeline.to_json())\n</code></pre> <p>From version 0.18.14, Kedro replaced <code>setup.py</code> with <code>pyproject.toml</code>. The plugin needs to provide entry points in either file. If you are using <code>setup.py</code>, please refer to the <code>0.18.13</code> version of documentations.</p> <p>To add the entry point to <code>pyproject.toml</code>, the plugin needs to provide the following <code>entry_points</code> configuration:</p> <pre><code>[project.entry-points.\"kedro.project_commands\"]\nkedrojson = \"kedrojson.plugin:commands\"\n</code></pre> <p>Once the plugin is installed, you can run it as follows:</p> <pre><code>kedro to_json\n</code></pre>"},{"location":"pages/extend_kedro/plugins/#working-with-click","title":"Working with <code>click</code>","text":"<p>Commands must be provided as <code>click</code> <code>Groups</code></p> <p>The <code>click Group</code> will be merged into the main CLI Group. In the process, the options on the group are lost, as is any processing that was done as part of its callback function.</p>"},{"location":"pages/extend_kedro/plugins/#project-context","title":"Project context","text":"<p>When they run, plugins may request information about the current project by creating a session and loading its context:</p> <pre><code>from pathlib import Path\n\nfrom kedro.framework.startup import _get_project_metadata\nfrom kedro.framework.session import KedroSession\n\n\nproject_path = Path.cwd()\nsession = KedroSession.create(project_path=project_path)\ncontext = session.load_context()\n</code></pre>"},{"location":"pages/extend_kedro/plugins/#initialisation","title":"Initialisation","text":"<p>If the plugin initialisation needs to occur prior to Kedro starting, it can declare the <code>entry_point</code> key <code>kedro.init</code>. This entry point must refer to a function that currently has no arguments, but for future proofing you should declare it with <code>**kwargs</code>.</p>"},{"location":"pages/extend_kedro/plugins/#global-and-project-commands","title":"<code>global</code> and <code>project</code> commands","text":"<p>Plugins may also add commands to the Kedro CLI, which supports two types of commands:</p> <ul> <li>global - available both inside and outside a Kedro project. Global commands use the <code>entry_point</code> key <code>kedro.global_commands</code>.</li> <li>project - available only when a Kedro project is detected in the current directory. Project commands use the <code>entry_point</code> key <code>kedro.project_commands</code>.</li> </ul>"},{"location":"pages/extend_kedro/plugins/#suggested-command-convention","title":"Suggested command convention","text":"<p>We use the following command convention: <code>kedro &lt;plugin-name&gt; &lt;command&gt;</code>, with <code>kedro &lt;plugin-name&gt;</code> acting as a top-level command group. This is our suggested way of structuring your plugin but it is not necessary for your plugin to work.</p>"},{"location":"pages/extend_kedro/plugins/#advanced-lazy-loading-of-plugin-commands","title":"Advanced: Lazy loading of plugin commands","text":"<p>If you are developing a plugin with a large set of CLI commands or with certain large libraries that are slow to import but are used by a small subset of commands, consider using lazy loading of these commands. This can significantly improve the performance of the plugin as well as the overall performance of Kedro CLI. To do this, you can follow the instructions on the <code>click</code> documentation on lazy loading of commands. From Kedro 0.19.7, the Kedro commands are declared as lazy loaded command groups that you can use as a reference for the implementation.</p> <p>Consider the previous example of the <code>kedrojson</code> plugin. Suppose the plugin has two commands, <code>kedro to_json pipelines</code> and <code>kedro to_json nodes</code>. The <code>to_json pipelines</code> command is used more frequently than the <code>to_json nodes</code> command and the <code>to_json nodes</code> command requires a large library to be imported. In this case, you can define your commands to be lazily loaded with delayed imports as follows:</p> <p>In <code>kedrojson/plugin.py</code>:</p> <pre><code>import click\nfrom kedro.framework.project import pipelines\nfrom kedro.framework.cli.utils import LazyGroup\n\n@click.group()\ndef commands():\n    pass\n\n@commands.group(\n    name=\"to_json\",\n    cls=LazyGroup,\n    lazy_subcommands={\n        \"nodes\": \"kedrojson.plugin.nodes\",\n        \"pipelines\": \"kedrojson.plugin.pipelines\"\n        }\n)\ndef to_json():\n    \"\"\"Convert Kedro nodes and pipelines to JSON\"\"\"\n    pass\n\n@click.command(name=\"nodes\")\ndef nodes():\n    \"\"\"Convert Kedro nodes to JSON\"\"\"\n    import some_large_library\n    print(\"Converting nodes to JSON\")\n    ...\n\n@click.command(\"pipelines\")\ndef pipelines():\n    \"\"\"Convert Kedro pipelines to JSON\"\"\"\n    print(\"Converting pipelines to JSON\")\n    ...\n</code></pre> <p>The loading of the individual <code>nodes</code> and <code>pipelines</code> commands and subsequently the imports of the large libraries will be delayed until the respective commands are called.</p>"},{"location":"pages/extend_kedro/plugins/#hooks","title":"Hooks","text":"<p>You can develop hook implementations and have them automatically registered to the project context when the plugin is installed.</p> <p>To enable this for your custom plugin, simply add the following entry in <code>pyproject.toml</code></p> <p>To use <code>pyproject.toml</code>, specify</p> <pre><code>[project.entry-points.\"kedro.hooks\"]\nplugin_name = \"plugin_name.plugin:hooks\"\n</code></pre> <p>where <code>plugin.py</code> is the module where you declare hook implementations:</p> <pre><code>import logging\n\nfrom kedro.framework.hooks import hook_impl\n\n\nclass MyHooks:\n    @hook_impl\n    def after_catalog_created(self, catalog):\n        logging.info(\"Reached after_catalog_created hook\")\n\n\nhooks = MyHooks()\n</code></pre> <pre><code>`hooks` should be an instance of the class defining the Hooks.\n</code></pre>"},{"location":"pages/extend_kedro/plugins/#cli-hooks","title":"CLI Hooks","text":"<p>You can also develop Hook implementations to extend Kedro's CLI behaviour in your plugin. To find available CLI Hooks, please visit our {py:mod}<code>~kedro.framework.cli.hooks</code> API documentation. To register CLI Hooks developed in your plugin with Kedro, add the following entry in your project's <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"kedro.cli_hooks\"]\nplugin_name = \"plugin_name.plugin:cli_hooks\"\n</code></pre> <p>(where <code>plugin.py</code> is the module where you declare Hook implementations):</p> <pre><code>import logging\n\nfrom kedro.framework.cli.hooks import cli_hook_impl\n\n\nclass MyCLIHooks:\n    @cli_hook_impl\n    def before_command_run(self, project_metadata, command_args):\n        logging.info(\n            \"Command %s will be run for project %s\", command_args, project_metadata\n        )\n\n\ncli_hooks = MyCLIHooks()\n</code></pre>"},{"location":"pages/extend_kedro/plugins/#contributing-process","title":"Contributing process","text":"<p>When you are ready to submit your code:</p> <ol> <li>Create a separate repository using our naming convention for <code>plugin</code>s (<code>kedro-&lt;plugin-name&gt;</code>)</li> <li>Choose a command approach: <code>global</code> and / or <code>project</code> commands:</li> <li>All <code>global</code> commands should be provided as a single <code>click</code> group</li> <li>All <code>project</code> commands should be provided as another <code>click</code> group</li> <li>The <code>click</code> groups are declared through the entry points mechanism</li> <li>Include a <code>README.md</code> describing your plugin's functionality and all dependencies that should be included</li> <li>Use GitHub tagging to tag your plugin as a <code>kedro-plugin</code> so that we can find it</li> </ol>"},{"location":"pages/extend_kedro/plugins/#supported-kedro-plugins","title":"Supported Kedro plugins","text":"<ul> <li>Kedro-Datasets, a collection of all of Kedro's data connectors. These data connectors are implementations of the <code>AbstractDataset</code></li> <li>Kedro-Docker, a tool for packaging and shipping Kedro projects within containers</li> <li>Kedro-Airflow, a tool for converting your Kedro project into an Airflow project</li> <li>Kedro-Viz, a tool for visualising your Kedro pipelines</li> </ul>"},{"location":"pages/extend_kedro/plugins/#community-developed-plugins","title":"Community-developed plugins","text":"<p>There are many community-developed plugins available and a comprehensive list of plugins is published on the <code>awesome-kedro</code> GitHub repository. The list below is a small snapshot of some of those under active maintenance.</p> <pre><code>Your plugin needs to have an [Apache 2.0 compatible license](https://www.apache.org/legal/resolved.html#category-a) to be considered for this list.\n</code></pre> <ul> <li>kedro-mlflow, by Yolan Honor\u00e9-Roug\u00e9 and Takieddine Kadiri, facilitates MLflow integration within a Kedro project. Its main features are modular configuration, automatic parameters tracking, datasets versioning, Kedro pipelines packaging and serving and automatic synchronisation between training and inference pipelines for high reproducibility of machine learning experiments and ease of deployment. A tutorial is provided in the kedro-mlflow-tutorial repo. You can find more information in the kedro-mlflow documentation.</li> <li>kedro-kubeflow, by GetInData, lets you run and schedule pipelines on Kubernetes clusters using Kubeflow Pipelines</li> <li>kedro-airflow-k8s, by GetInData, enables running a Kedro pipeline with Airflow on a Kubernetes cluster</li> <li>kedro-vertexai, by GetInData, enables running a Kedro pipeline with Vertex AI Pipelines service</li> <li>kedro-azureml, by GetInData, enables running a Kedro pipeline with Azure ML Pipelines service</li> <li>kedro-sagemaker, by GetInData, enables running a Kedro pipeline with Amazon SageMaker service</li> <li>kedro-partitioned, by Gabriel Daiha Alves and Nickolas da Rocha Machado, extends the functionality on processing partitioned data.</li> </ul>"},{"location":"pages/faq/faq/","title":"FAQs","text":"<p>This is a growing set of technical FAQs. The product FAQs on the Kedro website explain how Kedro can answer the typical use cases and requirements of data scientists, data engineers, machine learning engineers and product owners.</p>"},{"location":"pages/faq/faq/#installing-kedro","title":"Installing Kedro","text":"<ul> <li> <p>How do I install a development version of Kedro?</p> </li> <li> <p>How can I check the version of Kedro installed?   To check the version installed, type <code>kedro -V</code> in your terminal window.</p> </li> <li>Do I need Git installed to use Kedro?   Yes, users are expected to have Git installed when working with Kedro. This is a prerequisite for the <code>kedro new</code> flow. If Git is not installed, use the following workaround: <code>kedro new -s https://github.com/kedro-org/kedro-starters/archive/0.18.6.zip --directory=pandas-iris</code></li> </ul>"},{"location":"pages/faq/faq/#kedro-documentation","title":"Kedro documentation","text":"<ul> <li>{doc}<code>Where can I find the documentation about Kedro-Viz&lt;kedro-viz:kedro-viz_visualisation&gt;</code>?</li> <li>{py:mod}<code>Where can I find the documentation for Kedro's datasets &lt;kedro-datasets:kedro_datasets&gt;</code>?</li> </ul>"},{"location":"pages/faq/faq/#working-with-notebooks","title":"Working with Notebooks","text":"<ul> <li>How can I debug a Kedro project in a Jupyter notebook?</li> <li>How do I connect a Kedro project kernel to other Jupyter clients like JupyterLab?</li> <li>How can I use the Kedro IPython extension in a notebook where launching a new kernel is not an option?</li> <li>How to fix Line magic function <code>%reload_kedro</code> not found?</li> </ul>"},{"location":"pages/faq/faq/#kedro-project-development","title":"Kedro project development","text":"<ul> <li>How do I write my own Kedro starter projects?</li> </ul>"},{"location":"pages/faq/faq/#configuration","title":"Configuration","text":"<ul> <li>How do I change the setting for a configuration source folder?</li> <li>How do I change the configuration source folder at run time?</li> <li>How do I specify parameters at run time?</li> <li>How do I read configuration from a compressed file?</li> <li>How do I access configuration in code?</li> <li>How do I load credentials in code?</li> <li>How do I load parameters in code?</li> <li>How do I specify additional configuration environments?</li> <li>How do I change the default overriding configuration environment?</li> <li>How do I use only one configuration environment?</li> <li>How do I use Kedro without the rich library?</li> </ul>"},{"location":"pages/faq/faq/#advanced-topics","title":"Advanced topics","text":"<ul> <li>How do I change which configuration files are loaded?</li> <li>How do I use a custom configuration loader?</li> <li>How do I ensure non default configuration files get loaded?</li> <li>How do I bypass the configuration loading rules?</li> <li>How do I do templating with the <code>OmegaConfigLoader</code>?</li> <li>How to use global variables with the <code>OmegaConfigLoader</code>?</li> <li>How do I use resolvers in the <code>OmegaConfigLoader</code>?</li> <li>How do I load credentials through environment variables?</li> <li>How do I use Kedro with different project structure?</li> </ul>"},{"location":"pages/faq/faq/#nodes-and-pipelines","title":"Nodes and pipelines","text":"<ul> <li>How can I create a new blank pipeline?</li> <li>How can I reuse my pipelines?</li> <li>Can I use generator functions in a node?</li> </ul>"},{"location":"pages/faq/faq/#what-is-data-engineering-convention","title":"What is data engineering convention?","text":"<p>Bruce Philp and Guilherme Braccialli are the brains behind a layered data-engineering convention as a model of managing data. You can find an in-depth walk through of their convention as a blog post on Medium.</p> <p>Refer to the following table below for a high level guide to each layer's purpose</p> <p>Note:The data layers don\u2019t have to exist locally in the <code>data</code> folder within your project, but we recommend that you structure your S3 buckets or other data stores in a similar way.</p> <p></p> Folder in data Description Raw Initial start of the pipeline, containing the sourced data model(s) that should never be changed, it forms your single source of truth to work from. These data models are typically un-typed in most cases e.g. csv, but this will vary from case to case Intermediate Optional data model(s), which are introduced to type your <code>raw</code> data model(s), e.g. converting string based values into their current typed representation Primary Domain specific data model(s) containing cleansed, transformed and wrangled data from either <code>raw</code> or <code>intermediate</code>, which forms your layer that you input into your feature engineering Feature Analytics specific data model(s) containing a set of features defined against the <code>primary</code> data, which are grouped by feature area of analysis and stored against a common dimension Model input Analytics specific data model(s) containing all <code>feature</code> data against a common dimension and in the case of live projects against an analytics run date to ensure that you track the historical changes of the features over time Models Stored, serialised pre-trained machine learning models Model output Analytics specific data model(s) containing the results generated by the model based on the <code>model input</code> data Reporting Reporting data model(s) that are used to combine a set of <code>primary</code>, <code>feature</code>, <code>model input</code> and <code>model output</code> data used to drive the dashboard and the views constructed. It encapsulates and removes the need to define any blending or joining of data, improve performance and replacement of presentation layer without having to redefine the data models"},{"location":"pages/get_started/","title":"First steps","text":"<p>This nested nav needs to be reviewed. This section explains the first steps to set up and explore Kedro.</p>"},{"location":"pages/get_started/#set-up-kedro","title":"Set up Kedro","text":"<p>Learn how to install Kedro by following the Install guide.</p>"},{"location":"pages/get_started/#create-a-new-kedro-project","title":"Create a new Kedro project","text":"<p>Learn how to create a new Kedro project by following the New Project guide.</p>"},{"location":"pages/get_started/#kedro-concepts","title":"Kedro concepts","text":"<p>Understand the core concepts of Kedro by reviewing the Kedro Concepts guide.</p>"},{"location":"pages/get_started/#create-a-minimal-kedro-project","title":"Create a Minimal Kedro Project","text":"<p>Explore a minimal Kedro project by visiting the Minimal Kedro Project guide.</p>"},{"location":"pages/get_started/install/","title":"Set up Kedro","text":""},{"location":"pages/get_started/install/#installation-prerequisites","title":"Installation prerequisites","text":"<ul> <li> <p>Python: Kedro supports macOS, Linux, and Windows and is built for Python 3.9+. You'll select a version of Python when you create a virtual environment for your Kedro project.</p> </li> <li> <p>Virtual environment: You should create a new virtual environment for each new Kedro project you work on to isolate its Python dependencies from those of other projects.</p> </li> <li> <p>git: You must install <code>git</code> onto your machine if you do not already have it. Type <code>git -v</code> into your terminal window to confirm it is installed; it will return the version of <code>git</code> available or an error message. You can download <code>git</code> from the official website.</p> </li> </ul>"},{"location":"pages/get_started/install/#python-version-support-policy","title":"Python version support policy","text":"<ul> <li>The core Kedro Framework supports all Python versions that are actively maintained by the CPython core team. When a Python version reaches end of life, support for that version is dropped from Kedro. This is not considered a breaking change.</li> <li>The Kedro Datasets package follows the NEP 29 Python version support policy. This means that <code>kedro-datasets</code> generally drops Python version support before <code>kedro</code>. This is because <code>kedro-datasets</code> has a lot of dependencies that follow NEP 29 and the more conservative version support approach of the Kedro Framework makes it hard to manage those dependencies properly.</li> </ul>"},{"location":"pages/get_started/install/#create-a-virtual-environment-for-your-kedro-project","title":"Create a virtual environment for your Kedro project","text":"<p>We strongly recommend using <code>venv</code> as your virtual environment manager if you don't already use it.</p> <pre><code>[Read more about virtual environments for Python projects](https://realpython.com/python-virtual-environments-a-primer/) or [watch an explainer video about them](https://youtu.be/YKfAwIItO7M).\n</code></pre>"},{"location":"pages/get_started/install/#how-to-create-a-new-virtual-environment-using-venv","title":"How to create a new virtual environment using <code>venv</code>","text":"<p>The recommended approach. If you use Python 3, you should already have the <code>venv</code> module installed with the standard library. Create a directory for working with your project and navigate to it. For example:</p> <pre><code>mkdir your-kedro-project &amp;&amp; cd your-kedro-project\n</code></pre> <p>Next, create a new virtual environment in this directory with <code>venv</code>:</p> <pre><code>python -m venv .venv\n</code></pre> <p>Activate this virtual environment:</p> <pre><code>source .venv/bin/activate # macOS / Linux\n.\\.venv\\Scripts\\activate  # Windows\n</code></pre> <p>To exit the environment:</p> <pre><code>deactivate\n</code></pre>"},{"location":"pages/get_started/install/#how-to-create-a-new-virtual-environment-using-conda","title":"How to create a new virtual environment using <code>conda</code>","text":"<p>Another popular option is to use Conda. After you install it, execute this from your terminal:</p> <pre><code>conda create --name kedro-environment python=3.10 -y\n</code></pre> <p>The example below uses Python 3.10, and creates a virtual environment called <code>kedro-environment</code>. You can opt for a different version of Python (any version &gt;= 3.9 and &lt;3.12) for your project, and you can name it anything you choose.</p> <p>The <code>conda</code> virtual environment is not dependent on your current working directory and can be activated from any directory:</p> <pre><code>conda activate kedro-environment\n</code></pre> <p>To confirm that a valid version of Python is installed in your virtual environment, type the following in your terminal (macOS and Linux):</p> <pre><code>python3 --version\n</code></pre> <p>On Windows:</p> <pre><code>python --version\n</code></pre> <p>To exit <code>kedro-environment</code>:</p> <pre><code>conda deactivate\n</code></pre>"},{"location":"pages/get_started/install/#optional-integrate-kedro-in-vs-code-with-the-official-extension","title":"Optional: Integrate Kedro in VS Code with the official extension","text":"<p>Working in an IDE can be a great productivity boost.</p> <p>For VS Code Users: Checkout Set up Visual Studio Code and Kedro VS Code Extension For PyCharm Users: Checkout Set up PyCharm</p>"},{"location":"pages/get_started/install/#how-to-install-kedro-using-pip","title":"How to install Kedro using <code>pip</code>","text":"<p>To install Kedro from the Python Package Index (PyPI):</p> <pre><code>pip install kedro\n</code></pre> <p>You can also install Kedro using <code>conda install -c conda-forge kedro</code>.</p>"},{"location":"pages/get_started/install/#how-to-verify-your-kedro-installation","title":"How to verify your Kedro installation","text":"<p>To check that Kedro is installed:</p> <pre><code>kedro info\n</code></pre> <p>You should see an ASCII art graphic and the Kedro version number. For example:</p> <p></p> <p>If you do not see the graphic displayed, or have any issues with your installation, check out the searchable archive of Slack discussions, or post a new query on the Slack organisation.</p>"},{"location":"pages/get_started/install/#how-to-upgrade-kedro","title":"How to upgrade Kedro","text":"<p>The best way to safely upgrade is to check our release notes for any notable breaking changes. Follow the steps in the migration guide included for that specific release.</p> <p>Once Kedro is installed, you can check your version as follows:</p> <pre><code>kedro --version\n</code></pre> <p>To later upgrade Kedro to a different version, simply run:</p> <pre><code>pip install kedro -U\n</code></pre> <p>When migrating an existing project to a newer Kedro version, make sure you also update the <code>kedro_init_version</code>:</p> <ul> <li>For projects generated with versions of Kedro &gt; 0.17.0, you'll do this in the <code>pyproject.toml</code> file from the project root directory.</li> <li>If your project was generated with a version of Kedro &lt;0.17.0, you will instead need to update the <code>ProjectContext</code>, which is found in <code>src/&lt;package_name&gt;/run.py</code>.</li> </ul>"},{"location":"pages/get_started/install/#summary","title":"Summary","text":"<ul> <li>Kedro can be used on Windows, macOS or Linux.</li> <li>Installation prerequisites include a virtual environment manager like <code>conda</code>, Python 3.9+, and <code>git</code>.</li> <li>You should install Kedro using <code>pip install kedro</code>.</li> </ul> <p>If you encounter any problems as you set up Kedro, ask for help on Kedro's Slack organisation or review the searchable archive of Slack discussions.</p>"},{"location":"pages/get_started/kedro_concepts/","title":"Kedro concepts","text":"<p>This page introduces the most basic elements of Kedro. You can find further information about these and more advanced Kedro concepts in the Kedro glossary.</p> <p>You may prefer to skip to the next section to create a Kedro project for hands-on Kedro experience.</p>"},{"location":"pages/get_started/kedro_concepts/#summary","title":"Summary","text":"<ul> <li>Kedro nodes are the building blocks of pipelines. A node is a wrapper for a Python function that names the inputs and outputs of that function.</li> <li>A pipeline organises the dependencies and execution order of a collection of nodes.</li> <li>Kedro has a registry of all data sources the project can use called the Data Catalog. There is inbuilt support for various file types and file systems.</li> <li>Kedro projects follow a default template that uses specific folders to store datasets, notebooks, configuration and source code.</li> </ul>"},{"location":"pages/get_started/kedro_concepts/#node","title":"Node","text":"<p>In Kedro, a node is a wrapper for a pure Python function that names the inputs and outputs of that function. Nodes are the building block of a pipeline, and the output of one node can be the input of another.</p> <p>Here are two simple nodes as an example:</p> <pre><code>from kedro.pipeline import node\n\n\n# First node\ndef return_greeting():\n    return \"Hello\"\n\n\nreturn_greeting_node = node(func=return_greeting, inputs=None, outputs=\"my_salutation\")\n\n\n# Second node\ndef join_statements(greeting):\n    return f\"{greeting} Kedro!\"\n\n\njoin_statements_node = node(\n    join_statements, inputs=\"my_salutation\", outputs=\"my_message\"\n)\n</code></pre>"},{"location":"pages/get_started/kedro_concepts/#pipeline","title":"Pipeline","text":"<p>A pipeline organises the dependencies and execution order of a collection of nodes and connects inputs and outputs while keeping your code modular. The pipeline determines the node execution order by resolving dependencies and does not necessarily run the nodes in the order in which they are passed in.</p> <p>Here is a pipeline comprised of the nodes shown above:</p> <pre><code>from kedro.pipeline import pipeline\n\n# Assemble nodes into a pipeline\ngreeting_pipeline = pipeline([return_greeting_node, join_statements_node])\n</code></pre>"},{"location":"pages/get_started/kedro_concepts/#data-catalog","title":"Data Catalog","text":"<p>The Kedro Data Catalog is the registry of all data sources that the project can use to manage loading and saving data. It maps the names of node inputs and outputs as keys in a <code>DataCatalog</code>, a Kedro class that can be specialised for different types of data storage.</p> <p>{py:mod}<code>Kedro provides different built-in datasets &lt;kedro-datasets:kedro_datasets&gt;</code> for numerous file types and file systems, so you don\u2019t have to write the logic for reading/writing data.</p>"},{"location":"pages/get_started/kedro_concepts/#kedro-project-directory-structure","title":"Kedro project directory structure","text":"<p>One of the main advantages of working with Kedro projects is that they follow a default template that makes collaboration straightforward. Kedro uses semantic naming to set up a default project with specific folders to store datasets, notebooks, configuration and source code. We advise you to retain the default Kedro project structure to make it easy to share your projects with other Kedro users, although you can adapt the folder structure if you need to.</p> <p>Starting from Kedro 0.19, when you create a new project with <code>kedro new</code>, you can customise the structure by selecting which tools to include. Depending on your choices, the resulting structure may vary. Below, we outline the default project structure when all tools are selected and give an example with no tools selected.</p>"},{"location":"pages/get_started/kedro_concepts/#default-kedro-project-structure-all-tools-selected","title":"Default Kedro project structure (all tools selected)","text":"<p>If you select all tools during project creation, your project structure will look like this:</p> <pre><code>project-dir          # Parent directory of the template\n\u251c\u2500\u2500 conf             # Project configuration files\n\u251c\u2500\u2500 data             # Local project data (not committed to version control)\n\u251c\u2500\u2500 docs             # Project documentation\n\u251c\u2500\u2500 notebooks        # Project-related Jupyter notebooks (can be used for experimental code before moving the code to src)\n\u251c\u2500\u2500 src              # Project source code\n\u251c\u2500\u2500 tests            # Folder containing unit and integration tests\n\u251c\u2500\u2500 .gitignore       # Hidden file that prevents staging of unnecessary files to `git`\n\u251c\u2500\u2500 pyproject.toml   # Identifies the project root and contains configuration information\n\u251c\u2500\u2500 README.md        # Project README\n\u251c\u2500\u2500 requirements.txt # Project dependencies file\n</code></pre>"},{"location":"pages/get_started/kedro_concepts/#example-kedro-project-structure-no-tools-selected","title":"Example Kedro project structure (no tools selected)","text":"<p>If you select no tools, the resulting structure will be simpler:</p> <pre><code>project-dir          # Parent directory of the template\n\u251c\u2500\u2500 conf             # Project configuration files\n\u251c\u2500\u2500 notebooks        # Project-related Jupyter notebooks (can be used for experimental code before moving the code to src)\n\u251c\u2500\u2500 src              # Project source code\n\u251c\u2500\u2500 .gitignore       # Hidden file that prevents staging of unnecessary files to `git`\n\u251c\u2500\u2500 pyproject.toml   # Identifies the project root and contains configuration information\n\u251c\u2500\u2500 README.md        # Project README\n\u251c\u2500\u2500 requirements.txt # Project dependencies file\n</code></pre>"},{"location":"pages/get_started/kedro_concepts/#tool-selection-and-resulting-structure","title":"Tool selection and resulting structure","text":"<p>During <code>kedro new</code>, you can select which tools to include in your project. Each tool adds specific files or folders to the project structure:</p> <ul> <li>Lint (Ruff): Modifies the <code>pyproject.toml</code> file to include Ruff configuration settings for linting. It sets up <code>ruff</code> under <code>[tool.ruff]</code>, defines options like line length, selected rules, and ignored rules, and includes <code>ruff</code> as an optional <code>dev</code> dependency.</li> <li>Test (Pytest): Adds a <code>tests</code> folder for storing unit and integration tests, helping to maintain code quality and ensuring that changes in the codebase do not introduce bugs. For more information about testing in Kedro, visit the Automated Testing Guide.</li> <li>Log: Allows specific logging configurations by including a <code>logging.yml</code> file inside the <code>conf</code> folder. For more information about logging customisation in Kedro, visit the Logging Customisation Guide.</li> <li>Docs (Sphinx): Adds a <code>docs</code> folder with a Sphinx documentation setup. This folder is typically used to generate technical documentation for the project.</li> <li>Data Folder: Adds a <code>data</code> folder structure for managing project data. The <code>data</code> folder contains multiple subfolders to store project data. We recommend you put raw data into <code>raw</code> and move processed data to other subfolders, as outlined in this data engineering article.</li> <li>PySpark: Adds PySpark-specific configuration files.</li> </ul>"},{"location":"pages/get_started/kedro_concepts/#conf","title":"<code>conf</code>","text":"<p>The <code>conf</code> folder contains two subfolders for storing configuration information: <code>base</code> and <code>local</code>.</p>"},{"location":"pages/get_started/kedro_concepts/#confbase","title":"<code>conf/base</code>","text":"<p>Use the <code>base</code> subfolder for project-specific settings to share across different installations (for example, with other users).</p> <p>The folder contains three files for the example, but you can add others as you require:</p> <ul> <li><code>catalog.yml</code> - Configures the Data Catalog with the file paths and load/save configuration needed for different datasets</li> <li><code>logging.yml</code> - Uses Python's default <code>logging</code> library to set up logging (only added if the Log tool is selected).</li> <li><code>parameters.yml</code> - Allows you to define parameters for machine learning experiments, for example, train/test split and the number of iterations</li> </ul>"},{"location":"pages/get_started/kedro_concepts/#conflocal","title":"<code>conf/local</code>","text":"<p>The <code>local</code> subfolder is specific to each user and installation and its contents is ignored by <code>git</code> (through inclusion in <code>.gitignore</code>).</p> <p>Use the <code>local</code> subfolder for settings that should not be shared, such as access credentials, custom editor configuration, personal IDE configuration and other sensitive or personal content.</p> <p>By default, Kedro creates one file, <code>credentials.yml</code>, in <code>conf/local</code>.</p>"},{"location":"pages/get_started/kedro_concepts/#src","title":"<code>src</code>","text":"<p>This subfolder contains the project's source code.</p>"},{"location":"pages/get_started/kedro_concepts/#customising-your-project-structure","title":"Customising your project structure","text":"<p>While the default Kedro structure is recommended for collaboration and standardisation, it is possible to adapt the folder structure if necessary. This flexibility allows you to tailor the project to your needs while maintaining a consistent and recognisable structure.</p> <p>The only technical requirement when organising code is that the <code>pipeline_registry.py</code> and <code>settings.py</code> files must remain in the <code>&lt;your_project&gt;/src/&lt;your_project&gt;</code> directory, where they are created by default.</p> <p>The <code>pipeline_registry.py</code> file must include a <code>register_pipelines()</code> function that returns a <code>dict[str, Pipeline]</code>, which maps pipeline names to their corresponding <code>Pipeline</code> objects.</p>"},{"location":"pages/get_started/minimal_kedro_project/","title":"Create a Minimal Kedro Project","text":"<p>This documentation aims to explain the essential components of a minimal Kedro project. While most users typically start with a project template or adapt an existing Python project, this guide begins with a blank project and gradually introduces the necessary elements. This will help you understand the core concepts and how to customise them to suit your specific needs.</p>"},{"location":"pages/get_started/minimal_kedro_project/#essential-components-of-a-kedro-project","title":"Essential Components of a Kedro Project","text":"<p>Kedro is a Python framework designed for creating reproducible data science code. A typical Kedro project consists of two parts, the mandatory structure and the opinionated project structure.</p>"},{"location":"pages/get_started/minimal_kedro_project/#1-recommended-structure","title":"1. Recommended Structure","text":"<p>Kedro projects follow a specific directory structure that promotes best practices for collaboration and maintenance. The default structure includes:</p> Directory/File Description <code>conf/</code> Contains configuration files such as <code>catalog.yml</code> and <code>parameters.yml</code>. <code>data/</code> Local project data, typically not committed to version control. <code>docs/</code> Project documentation files. <code>notebooks/</code> Jupyter notebooks for experimentation and prototyping. <code>src/</code> Source code for the project, including pipelines and nodes. <code>README.md</code> Project overview and instructions. <code>pyproject.toml</code> Metadata about the project, including dependencies. <code>.gitignore</code> Specifies files and directories to be ignored by Git."},{"location":"pages/get_started/minimal_kedro_project/#2-mandatory-files","title":"2. Mandatory Files","text":"<p>For a project to be recognised as a Kedro project and support running <code>kedro run</code>, it must contain three essential files: - <code>pyproject.toml</code>: Defines the python project - <code>settings.py</code>: Defines project settings, including library component registration. - <code>pipeline_registry.py</code>: Registers the project's pipelines.</p> <p>If you want to see some examples of these files, you can either create a project with <code>kedro new</code> or check out the project template on GitHub</p>"},{"location":"pages/get_started/minimal_kedro_project/#pyprojecttoml","title":"<code>pyproject.toml</code>","text":"<p>The <code>pyproject.toml</code> file is a crucial component of a Kedro project that serve as the standard way to store build metadata and tool settings for Python projects. It is essential for defining the project's configuration and ensuring proper integration with various tools and libraries.</p> <p>Particularly, Kedro requires <code>[tool.kedro]</code> section in <code>pyproject.toml</code>, this describes the project metadata in the project.</p> <p>Typically, it looks similar to this:</p> <pre><code>[tool.kedro]\npackage_name = \"package_name\"\nproject_name = \"project_name\"\nkedro_init_version = \"kedro_version\"\ntools = \"\"\nexample_pipeline = \"False\"\nsource_dir = \"src\"\n</code></pre> <p>This informs Kedro where to look for the source code, <code>settings.py</code> and <code>pipeline_registry.py</code> are.</p>"},{"location":"pages/get_started/minimal_kedro_project/#settingspy","title":"<code>settings.py</code>","text":"<p>The <code>settings.py</code> file is an important configuration file in a Kedro project that allows you to define various settings and hooks for your project. Here\u2019s a breakdown of its purpose and functionality: - Project Settings: This file is where you can configure project-wide settings, such as defining the logging level, setting environment variables, or specifying paths for data and outputs. - Hooks Registration: You can register custom hooks in <code>settings.py</code>, which are functions that can be executed at specific points in the Kedro pipeline lifecycle (e.g., before or after a node runs). This is useful for adding additional functionality, such as logging or monitoring. - Integration with Plugins: If you are using Kedro plugins, <code>settings.py</code> can also be utilized to configure them appropriately.</p> <p>Even if you do not have any settings, an empty <code>settings.py</code> is still required. Typically, they are stored at <code>src/&lt;package_name&gt;/settings.py</code>.</p>"},{"location":"pages/get_started/minimal_kedro_project/#pipeline_registrypy","title":"<code>pipeline_registry.py</code>","text":"<p>The <code>pipeline_registry.py</code> file is essential for managing the pipelines within your Kedro project. It provides a centralized way to register and access all pipelines defined in the project. Here are its key features: - Pipeline Registration: The file must contain a top-level function called <code>register_pipelines()</code> that returns a mapping from pipeline names to Pipeline objects. This function is crucial because it enables the Kedro CLI and other tools to discover and run the defined pipelines. - Autodiscovery of Pipelines: Since Kedro 0.18.3, you can use the <code>find_pipeline</code> function to automatically discover pipelines defined in your project without manually updating the registry each time you create a new pipeline.</p>"},{"location":"pages/get_started/minimal_kedro_project/#creating-a-minimal-kedro-project-step-by-step","title":"Creating a Minimal Kedro Project Step-by-Step","text":"<p>This guide will walk you through the process of creating a minimal Kedro project, allowing you to successfully run <code>kedro run</code> with just three files.</p>"},{"location":"pages/get_started/minimal_kedro_project/#step-1-install-kedro","title":"Step 1: Install Kedro","text":"<p>First, ensure that Python is installed on your machine. Then, install Kedro using pip:</p> <pre><code>pip install kedro\n</code></pre>"},{"location":"pages/get_started/minimal_kedro_project/#step-2-create-a-new-kedro-project","title":"Step 2: Create a New Kedro Project","text":"<p>Create a new directory for your project:</p> <pre><code>mkdir minikedro\n</code></pre> <p>Navigate into your newly created project directory:</p> <pre><code>cd minikiedro\n</code></pre>"},{"location":"pages/get_started/minimal_kedro_project/#step-3-create-pyprojecttoml","title":"Step 3: Create <code>pyproject.toml</code>","text":"<p>Create a new file named <code>pyproject.toml</code> in the project directory with the following content:</p> <pre><code>[tool.kedro]\npackage_name = \"minikedro\"\nproject_name = \"minikedro\"\nkedro_init_version = \"0.19.9\"\nsource_dir = \".\"\n</code></pre> <p>At this point, your workingn directory should look like this:</p> <pre><code>.\n\u251c\u2500\u2500 pyproject.toml\n</code></pre> <pre><code>Note we define `source_dir = \".\"`, usually we keep our source code inside a directory called `src`. For this example, we try to keep the structure minimal so we keep the source code in the root directory\n</code></pre>"},{"location":"pages/get_started/minimal_kedro_project/#step-4-create-settingspy-and-pipeline_registrypy","title":"Step 4: Create <code>settings.py</code> and <code>pipeline_registry.py</code>","text":"<p>Next, create a folder named minikedro, which should match the package_name defined in pyproject.toml:</p> <pre><code>mkdir minikedro\n</code></pre> <p>Inside this folder, create two empty files: <code>settings.py</code> and <code>pipeline_registry.py</code>:</p> <pre><code>touch minikedro/settings.py minikedro/pipeline_registry.py\n</code></pre> <p>Now your working directory should look like this:</p> <pre><code>.\n\u251c\u2500\u2500 minikedro\n\u2502   \u251c\u2500\u2500 pipeline_registry.py\n\u2502   \u2514\u2500\u2500 settings.py\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>Try running the following command in the terminal:</p> <pre><code>kedro run\n</code></pre> <p>You will encounter an error indicating that <code>pipeline_registry.py</code> is empty:</p> <pre><code>AttributeError: module 'minikedro.pipeline_registry' has no attribute 'register_pipelines'\n</code></pre>"},{"location":"pages/get_started/minimal_kedro_project/#step-5-create-a-simple-pipeline","title":"Step 5: Create a Simple Pipeline","text":"<p>To resolve this issue, add the following code to <code>pipeline_registry.py</code>, which defines a simple pipeline to run:</p> <pre><code>from kedro.pipeline import pipeline, node\n\ndef foo():\n    return \"dummy\"\n\ndef register_pipelines():\n    return {\"__default__\": pipeline([node(foo, None, \"dummy_output\")])}\n</code></pre> <p>If you attempt to run the pipeline again with <code>kedro run</code>, you will see another error:</p> <pre><code>MissingConfigException: Given configuration path either does not exist or is not a valid directory: /workspace/kedro/minikedro/conf/base\n</code></pre>"},{"location":"pages/get_started/minimal_kedro_project/#step-6-define-the-project-settings","title":"Step 6: Define the Project Settings","text":"<p>This error occurs because Kedro expects a configuration folder named <code>conf</code>, along with two environments called <code>base</code> and <code>local</code>.</p> <p>To fix this, add these two lines into <code>settings.py</code>:</p> <pre><code>CONF_SOURCE = \".\"\nCONFIG_LOADER_ARGS = {\"base_env\": \".\", \"default_run_env\": \".\"}\n</code></pre> <p>These lines override the default settings so that Kedro knows to look for configurations in the current directory instead of the expected <code>conf</code> folder. For more details, refer to How to change the setting for a configuration source folder and Advance Configuration without a full Kedro project</p> <p>Now, run the pipeline again:</p> <pre><code>kedro run\n</code></pre> <p>You should see that the pipeline runs successfully!</p>"},{"location":"pages/get_started/minimal_kedro_project/#conclusion","title":"Conclusion","text":"<p>Kedro provides a structured approach to developing data pipelines with clear separation of concerns through its components and directory structure. By following the steps outlined above, you can set up a minimal Kedro project that serves as a foundation for more complex data processing workflows. This guide explains essential concepts of Kedro projects. If you already have a Python project and want to integrate Kedro into it, these concepts will help you adjust and fit your own needs.</p>"},{"location":"pages/get_started/new_project/","title":"Create a new Kedro project","text":"<p>There are several ways to create a new Kedro project. This page explains the flow to create a basic project using <code>kedro new</code> to output a project directory containing the basic files and subdirectories that make up a Kedro project. Please note that users are expected to have <code>Git</code> installed, as it is a requirement for the <code>kedro new</code> flow.</p> <p>You can also create a new Kedro project with a starter that adds code for a common project use case. Starters are explained separately and the spaceflights tutorial illustrates their use.</p>"},{"location":"pages/get_started/new_project/#introducing-kedro-new","title":"Introducing <code>kedro new</code>","text":"<p>To create a basic Kedro project containing the default code needed to set up your own nodes and pipelines, navigate to your preferred directory and type:</p> <pre><code>kedro new\n</code></pre>"},{"location":"pages/get_started/new_project/#project-name","title":"Project name","text":"<p>The command line interface (CLI) first asks for a name for the project. This is the human-readable name, and it may contain alphanumeric symbols, spaces, underscores, and hyphens. It must be at least two characters long.</p> <p>It's best to keep the name simple because the choice is set as the value of <code>project_name</code> and is also used to generate the folder and package names for the project automatically. For example, if you enter \"Get Started\", the folder for the project (<code>repo_name</code>) is automatically set to be <code>get-started</code>, and the Python package name (<code>python_package</code>) for the project is set to be <code>get_started</code>.</p> Description Setting Example A human-readable name for the new project <code>project_name</code> <code>Get Started</code> Local directory to store the project <code>repo_name</code> <code>get-started</code> The Python package name for the project (short, all-lowercase) <code>python_package</code> <code>get_started</code>"},{"location":"pages/get_started/new_project/#project-tools","title":"Project tools","text":"<p>Next, the CLI asks which tools you'd like to include in the project:</p> <pre><code>Tools\n1) Lint: Basic linting with ruff\n2) Test: Basic testing with pytest\n3) Log: Additional, environment-specific logging options\n4) Docs: A Sphinx documentation setup\n5) Data Folder: A folder structure for data management\n6) PySpark: Configuration for working with PySpark\n7) Kedro-Viz: Kedro's native visualisation tool\n\nWhich tools would you like to include in your project? [1-7/1,3/all/none]:\n (none):\n</code></pre> <p>The options are described in more detail in the documentation about the new project tools.</p> <p>Select the tools by number, or <code>all</code> or follow the default to add <code>none</code>.</p>"},{"location":"pages/get_started/new_project/#project-examples","title":"Project examples","text":"<p>The CLI offers the option to include starter example code in the project:</p> <pre><code>Would you like to include an example pipeline? :\n (no):\n</code></pre> <p>If you say <code>yes</code>, the example code included depends upon your previous choice of tools, as follows:</p> <ul> <li>Default spaceflights starter (<code>spaceflights-pandas</code>): Added if you selected any combination of linting, testing, custom logging, documentation, and data structure, unless you also selected PySpark or Kedro Viz.</li> <li>PySpark spaceflights starter (<code>spaceflights-pyspark</code>): Added if you selected PySpark with any other tools, unless you also selected Kedro Viz.</li> <li>Kedro Viz spaceflights starter (<code>spaceflights-pandas-viz</code>): Added if Kedro Viz was one of your tools choices, unless you also selected PySpark.</li> <li>Full feature spaceflights starter (<code>spaceflights-pyspark-viz</code>): Added if you selected all available tools, including PySpark and Kedro Viz.</li> </ul> <p>Each starter example is tailored to demonstrate the capabilities and integrations of the selected tools, offering a practical insight into how they can be utilised in your project.</p>"},{"location":"pages/get_started/new_project/#quickstart-examples","title":"Quickstart examples","text":"<ol> <li>To create a default Kedro project called <code>My-Project</code> with no tools and no example code:</li> </ol> <pre><code>kedro new \u2b90\nMy-Project \u2b90\nnone \u2b90\nno \u2b90\n</code></pre> <p>You can also enter this in a single line as follows:</p> <pre><code>kedro new --name=My-Project --tools=none --example=n\n</code></pre> <ol> <li>To create a spaceflights project called <code>spaceflights</code> with Kedro Viz features and example code:</li> </ol> <pre><code>kedro new \u2b90\nspaceflights \u2b90\n7 \u2b90\nyes \u2b90\n</code></pre> <p>You can also enter this in a single line as follows:</p> <pre><code>kedro new --name=spaceflights --tools=viz --example=y\n</code></pre> <ol> <li>To create a project, called <code>testproject</code> containing linting, documentation, and PySpark, but no example code:</li> </ol> <pre><code>kedro new \u2b90\ntestproject \u2b90\n1,4,6 \u2b90\nno \u2b90\n</code></pre> <p>You can also enter this in a single line as follows:</p> <pre><code>kedro new --name=testproject --tools=lint,docs,pyspark --example=n\n</code></pre>"},{"location":"pages/get_started/new_project/#telemetry-consent","title":"Telemetry consent","text":"<p>The <code>--telemetry</code> flag offers the option to register consent to have user analytics collected in the moment of the creation of the project. This option bypasses the prompt to collect analytics that would otherwise appear on the moment the <code>kedro</code> command is invoked for the first time inside the project. In case the <code>--telemetry</code> flag is not used, the user will be prompted to accept or reject analytics collection as usual.</p> <p>When creating your new Kedro project, use the values <code>yes</code> or <code>no</code> to register consent to have user analytics collected for this specific project. Selecting <code>yes</code> means you consent to your data being collected, whereas <code>no</code> means you do not consent and no data will be collected.</p>"},{"location":"pages/get_started/new_project/#run-the-new-project","title":"Run the new project","text":"<p>Whichever options you selected for tools and example code, once <code>kedro new</code> has completed, the next step is to navigate to the project folder (<code>cd &lt;project-name&gt;</code>) and install dependencies with <code>pip</code> as follows:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Now run the project:</p> <pre><code>kedro run\n</code></pre> <pre><code>`kedro run` requires at least one pipeline with nodes. Please define a pipeline before running this command and ensure it is registred in `pipeline_registry.py`.\n</code></pre>"},{"location":"pages/get_started/new_project/#visualise-a-kedro-project","title":"Visualise a Kedro project","text":"<p>This section swiftly introduces project visualisation using Kedro-Viz. See the {doc}<code>Kedro-Viz documentation&lt;kedro-viz:kedro-viz_visualisation&gt;</code> for more detail.</p> <p>The Kedro-Viz package needs to be installed into your virtual environment separately as it is not part of the standard Kedro installation:</p> <pre><code>pip install kedro-viz\n</code></pre> <p>To start Kedro-Viz, navigate to the project folder (<code>cd &lt;project-name&gt;</code>) and enter the following in your terminal:</p> <pre><code>kedro viz run\n</code></pre> <p>This command automatically opens a browser tab to serve the visualisation at <code>http://127.0.0.1:4141/</code>.</p> <p>To exit the visualisation, close the browser tab. To regain control of the terminal, enter <code>^+c</code> on Mac or <code>Ctrl+c</code> on Windows or Linux machines.</p>"},{"location":"pages/get_started/new_project/#where-next","title":"Where next?","text":"<p>You have completed the section on Kedro project creation for new users. Here are some useful resources to learn more:</p> <ul> <li> <p>Understand more about Kedro: The following page explains the fundamental Kedro concepts.</p> </li> <li> <p>Learn hands-on: If you prefer to learn hands-on, move on to the spaceflights tutorial. The tutorial illustrates how to set up a working project, add dependencies, create nodes, register pipelines, set up the Data Catalog, add documentation, and package the project.</p> </li> <li> <p>How-to guide for notebook users: The documentation section following the tutorial explains how to combine Kedro with a Jupyter notebook.</p> </li> </ul> <p>If you've worked through the documentation listed and are unsure where to go next, review the Kedro repositories on GitHub and Kedro's Slack channels.</p>"},{"location":"pages/get_started/new_project/#flowchart-of-general-choice-of-tools","title":"Flowchart of general choice of tools","text":"<p>Here is a flowchart to help guide your choice of tools and examples you can select:</p> <p><code>{figure} ../meta/images/new-project-tools.png :alt: mermaid-General overview diagram for setting up a new Kedro project with tools</code> % Mermaid code, see https://github.com/kedro-org/kedro/wiki/Render-Mermaid-diagrams % flowchart TD %     A[Start] --&gt; B[Enter Project Name]; %     B --&gt; C[Select Tools]; % %     C --&gt;|None| D[None]; %     C --&gt;|Any combination| E[lint, test, logging, docs, data, PySpark, viz]; %     C --&gt;|All| F[All]; % %     D --&gt; G[Include Example Pipeline?] %     E --&gt; G; %     F --&gt; G % %     G --&gt;|Yes| H[New Project Created]; %     G --&gt;|No| H;</p>"},{"location":"pages/hooks/","title":"Hooks","text":"<p>Hooks are a mechanism to add extra behaviour to Kedro's main execution in an easy and consistent manner. Some examples might include:</p> <ul> <li>Adding a log statement after the data catalog is loaded.</li> <li>Adding data validation to the inputs before a node runs, and to the outputs after a node has run. This makes it possible to integrate with other tools like Great-Expectations.</li> <li> <p>Adding machine learning metrics tracking, e.g. using MLflow, throughout a pipeline run.</p> </li> <li> <p>Introduction</p> </li> <li>Common Use Cases</li> <li>Examples</li> </ul>"},{"location":"pages/hooks/common_use_cases/","title":"Common use cases","text":""},{"location":"pages/hooks/common_use_cases/#use-hooks-to-extend-a-nodes-behaviour","title":"Use Hooks to extend a node's behaviour","text":"<p>You can use the <code>before_node_run</code> and <code>after_node_run</code> Hooks to add extra behavior before and after a node's execution. Furthermore, you can apply extra behavior to not only an individual node or an entire Kedro pipeline, but also to a subset of nodes, based on their tags or namespaces: for example, suppose we want to add the following extra behavior to a node:</p> <pre><code>from kedro.pipeline.node import Node\n\n\ndef say_hello(node: Node):\n    \"\"\"An extra behaviour for a node to say hello before running.\"\"\"\n    print(f\"Hello from {node.name}\")\n</code></pre> <p>Then you can either add it to a single node based on the node's name:</p> <pre><code># src/&lt;package_name&gt;/hooks.py\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass ProjectHooks:\n    @hook_impl\n    def before_node_run(self, node: Node):\n        # adding extra behaviour to a single node\n        if node.name == \"hello\":\n            say_hello(node)\n</code></pre> <p>Or add it to a group of nodes based on their tags:</p> <pre><code># src/&lt;package_name&gt;/hooks.py\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass ProjectHooks:\n    @hook_impl\n    def before_node_run(self, node: Node):\n        if \"hello\" in node.tags:\n            say_hello(node)\n</code></pre> <p>Or add it to all nodes in the entire pipeline:</p> <pre><code># src/&lt;package_name&gt;/hooks.py\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass ProjectHooks:\n    @hook_impl\n    def before_node_run(self, node: Node):\n        # adding extra behaviour to all nodes in the pipeline\n        say_hello(node)\n</code></pre> <p>If your use case takes advantage of a decorator, for example to retry a node's execution using a library such as tenacity, you can still decorate the node's function directly:</p> <pre><code>from tenacity import retry\n\n\n@retry\ndef my_flaky_node_function():\n    ...\n</code></pre> <p>Or applying it in the <code>before_node_run</code> Hook as follows:</p> <pre><code># src/&lt;package_name&gt;/hooks.py\nfrom tenacity import retry\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass ProjectHooks:\n    @hook_impl\n    def before_node_run(self, node: Node):\n        # adding retrying behaviour to nodes tagged as flaky\n        if \"flaky\" in node.tags:\n            node.func = retry(node.func)\n</code></pre>"},{"location":"pages/hooks/common_use_cases/#use-hooks-to-customise-the-dataset-load-and-save-methods","title":"Use Hooks to customise the dataset load and save methods","text":"<p>We recommend using the <code>before_dataset_loaded</code>/<code>after_dataset_loaded</code> and <code>before_dataset_saved</code>/<code>after_dataset_saved</code> Hooks to customise the dataset <code>load</code> and <code>save</code> methods where appropriate.</p> <p>For example, you can add logging about the dataset load runtime as follows:</p> <pre><code>import logging\nimport time\nfrom typing import Any\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass LoggingHook:\n    \"\"\"A hook that logs how many time it takes to load each dataset.\"\"\"\n\n    def __init__(self):\n        self._timers = {}\n\n    @property\n    def _logger(self):\n        return logging.getLogger(__name__)\n\n    @hook_impl\n    def before_dataset_loaded(self, dataset_name: str, node: Node) -&gt; None:\n        start = time.time()\n        self._timers[dataset_name] = start\n\n    @hook_impl\n    def after_dataset_loaded(self, dataset_name: str, data: Any, node: Node) -&gt; None:\n        start = self._timers[dataset_name]\n        end = time.time()\n        self._logger.info(\n            \"Loading dataset %s before node '%s' takes %0.2f seconds\",\n            dataset_name,\n            node.name,\n            end - start,\n        )\n</code></pre>"},{"location":"pages/hooks/common_use_cases/#use-hooks-to-load-external-credentials","title":"Use Hooks to load external credentials","text":"<p>We recommend using the <code>after_context_created</code> Hook to add credentials to the session's config loader instance from any external credentials manager. In this example we show how to load credentials from Azure KeyVault.</p> <p>Here is the example KeyVault instance, note the KeyVault and secret names:</p> <p></p> <p>These credentials will be used to access these datasets in the data catalog:</p> <pre><code>weather:\n type: spark.SparkDataset\n filepath: s3a://your_bucket/data/01_raw/weather*\n file_format: csv\n credentials: s3_creds\n\ncars:\n type: pandas.CSVDataset\n filepath: https://your_data_store.blob.core.windows.net/data/01_raw/cars.csv\n file_format: csv\n credentials: abs_creds\n</code></pre> <p>We can then use the following hook implementation to fetch and inject these credentials:</p> <pre><code># hooks.py\n\nfrom kedro.framework.hooks import hook_impl\nfrom azure.keyvault.secrets import SecretClient\nfrom azure.identity import DefaultAzureCredential\n\n\nclass AzureSecretsHook:\n    @hook_impl\n    def after_context_created(self, context) -&gt; None:\n        keyVaultName = \"keyvault-0542abb\"  # or os.environ[\"KEY_VAULT_NAME\"] if you would like to provide it through environment variables\n        KVUri = f\"https://{keyVaultName}.vault.azure.net\"\n\n        my_credential = DefaultAzureCredential()\n        client = SecretClient(vault_url=KVUri, credential=my_credential)\n\n        secrets = {\n            \"abs_creds\": \"azure-blob-store\",\n            \"s3_creds\": \"s3-bucket-creds\",\n        }\n        azure_creds = {\n            cred_name: client.get_secret(secret_name).value\n            for cred_name, secret_name in secrets.items()\n        }\n\n        context.config_loader[\"credentials\"] = {\n            **context.config_loader[\"credentials\"],\n            **azure_creds,\n        }\n</code></pre> <p>Finally, register the Hook in <code>settings.py</code>:</p> <pre><code>from my_project.hooks import AzureSecretsHook\n\nHOOKS = (AzureSecretsHook(),)\n</code></pre> <pre><code>Note: `DefaultAzureCredential()` is Azure's recommended approach to authorise access to data in your storage accounts. For more information, consult the [documentation about how to authenticate to Azure and authorize access to blob data](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python).\n</code></pre>"},{"location":"pages/hooks/common_use_cases/#use-hooks-to-read-metadata-from-datacatalog","title":"Use Hooks to read <code>metadata</code> from <code>DataCatalog</code>","text":"<p>Use the <code>after_catalog_created</code> Hook to access <code>metadata</code> to extend Kedro.</p> <pre><code>class MetadataHook:\n    @hook_impl\n    def after_catalog_created(\n        self,\n        catalog: DataCatalog,\n    ):\n        for dataset_name, dataset in catalog.datasets.__dict__.items():\n            print(f\"{dataset_name} metadata: \\n  {str(dataset.metadata)}\")\n</code></pre>"},{"location":"pages/hooks/common_use_cases/#use-hooks-to-debug-your-pipeline","title":"Use Hooks to debug your pipeline","text":"<p>You can use Hooks to launch a post-mortem debugging session with <code>pdb</code> using Kedro Hooks when an error occurs during a pipeline run. ipdb could be integrated in the same manner.</p>"},{"location":"pages/hooks/common_use_cases/#debugging-a-node","title":"Debugging a node","text":"<p>To start a debugging session when an error is raised within your <code>node</code> that is not caught, implement the <code>on_node_error</code> Hook specification:</p> <pre><code>import pdb\nimport sys\nimport traceback\n\nfrom kedro.framework.hooks import hook_impl\n\n\nclass PDBNodeDebugHook:\n    \"\"\"A hook class for creating a post mortem debugging with the PDB debugger\n    whenever an error is triggered within a node. The local scope from when the\n    exception occured is available within this debugging session.\n    \"\"\"\n\n    @hook_impl\n    def on_node_error(self):\n        _, _, traceback_object = sys.exc_info()\n\n        #  Print the traceback information for debugging ease\n        traceback.print_tb(traceback_object)\n\n        # Drop you into a post mortem debugging session\n        pdb.post_mortem(traceback_object)\n</code></pre> <p>You can then register this <code>PDBNodeDebugHook</code> in your project's <code>settings.py</code>:</p> <pre><code>HOOKS = (PDBNodeDebugHook(),)\n</code></pre>"},{"location":"pages/hooks/common_use_cases/#debugging-a-pipeline","title":"Debugging a pipeline","text":"<p>To start a debugging session when an error is raised within your <code>pipeline</code> that is not caught, implement the <code>on_pipeline_error</code> Hook specification:</p> <pre><code>import pdb\nimport sys\nimport traceback\n\nfrom kedro.framework.hooks import hook_impl\n\n\nclass PDBPipelineDebugHook:\n    \"\"\"A hook class for creating a post mortem debugging with the PDB debugger\n    whenever an error is triggered within a pipeline. The local scope from when the\n    exception occured is available within this debugging session.\n    \"\"\"\n\n    @hook_impl\n    def on_pipeline_error(self):\n        # We don't need the actual exception since it is within this stack frame\n        _, _, traceback_object = sys.exc_info()\n\n        #  Print the traceback information for debugging ease\n        traceback.print_tb(traceback_object)\n\n        # Drop you into a post mortem debugging session\n        pdb.post_mortem(traceback_object)\n</code></pre> <p>You can then register this <code>PDBPipelineDebugHook</code> in your project's <code>settings.py</code>:</p> <pre><code>HOOKS = (PDBPipelineDebugHook(),)\n</code></pre>"},{"location":"pages/hooks/examples/","title":"Hooks examples","text":""},{"location":"pages/hooks/examples/#add-memory-consumption-tracking","title":"Add memory consumption tracking","text":"<p>This example illustrates how to track memory consumption using <code>memory_profiler</code>.</p> <ul> <li>Install dependencies:</li> </ul> <pre><code>pip install memory_profiler\n</code></pre> <ul> <li>Implement <code>before_dataset_loaded</code> and <code>after_dataset_loaded</code></li> </ul> <pre><code># src/&lt;package_name&gt;/hooks.py\nimport logging\n\nfrom kedro.framework.hooks import hook_impl\nfrom memory_profiler import memory_usage\n\n\ndef _normalise_mem_usage(mem_usage):\n    # memory_profiler &lt; 0.56.0 returns list instead of float\n    return mem_usage[0] if isinstance(mem_usage, (list, tuple)) else mem_usage\n\n\nclass MemoryProfilingHooks:\n    def __init__(self):\n        self._mem_usage = {}\n\n    @hook_impl\n    def before_dataset_loaded(self, dataset_name: str) -&gt; None:\n        before_mem_usage = memory_usage(\n            -1,\n            interval=0.1,\n            max_usage=True,\n            retval=True,\n            include_children=True,\n        )\n        before_mem_usage = _normalise_mem_usage(before_mem_usage)\n        self._mem_usage[dataset_name] = before_mem_usage\n\n    @hook_impl\n    def after_dataset_loaded(self, dataset_name: str) -&gt; None:\n        after_mem_usage = memory_usage(\n            -1,\n            interval=0.1,\n            max_usage=True,\n            retval=True,\n            include_children=True,\n        )\n        # memory_profiler &lt; 0.56.0 returns list instead of float\n        after_mem_usage = _normalise_mem_usage(after_mem_usage)\n\n        logging.getLogger(__name__).info(\n            \"Loading %s consumed %2.2fMiB memory\",\n            dataset_name,\n            after_mem_usage - self._mem_usage[dataset_name],\n        )\n</code></pre> <ul> <li>Register Hooks implementation by updating the <code>HOOKS</code> variable in <code>settings.py</code> as follows:</li> </ul> <pre><code>HOOKS = (MemoryProfilingHooks(),)\n</code></pre> <p>Then re-run the pipeline:</p> <pre><code>$ kedro run\n</code></pre>"},{"location":"pages/hooks/examples/#add-data-validation","title":"Add data validation","text":"<p>This example adds data validation to node inputs and outputs using Great Expectations.</p> <ul> <li>Install dependencies:</li> </ul> <pre><code>pip install great-expectations\n</code></pre> <ul> <li>Implement <code>before_node_run</code> and <code>after_node_run</code> Hooks to validate inputs and outputs data respectively leveraging <code>Great Expectations</code>:</li> </ul>"},{"location":"pages/hooks/examples/#v2-api","title":"V2 API","text":"<pre><code># src/&lt;package_name&gt;/hooks.py\nfrom typing import Any, Dict\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.io import DataCatalog\n\nimport great_expectations as ge\n\n\nclass DataValidationHooks:\n    # Map expectation to dataset\n    DATASET_EXPECTATION_MAPPING = {\n        \"companies\": \"raw_companies_dataset_expectation\",\n        \"preprocessed_companies\": \"preprocessed_companies_dataset_expectation\",\n    }\n\n    @hook_impl\n    def before_node_run(\n        self, catalog: DataCatalog, inputs: Dict[str, Any], session_id: str\n    ) -&gt; None:\n        \"\"\"Validate inputs data to a node based on using great expectation\n        if an expectation suite is defined in ``DATASET_EXPECTATION_MAPPING``.\n        \"\"\"\n        self._run_validation(catalog, inputs, session_id)\n\n    @hook_impl\n    def after_node_run(\n        self, catalog: DataCatalog, outputs: Dict[str, Any], session_id: str\n    ) -&gt; None:\n        \"\"\"Validate outputs data from a node based on using great expectation\n        if an expectation suite is defined in ``DATASET_EXPECTATION_MAPPING``.\n        \"\"\"\n        self._run_validation(catalog, outputs, session_id)\n\n    def _run_validation(\n        self, catalog: DataCatalog, data: Dict[str, Any], session_id: str\n    ):\n        for dataset_name, dataset_value in data.items():\n            if dataset_name not in self.DATASET_EXPECTATION_MAPPING:\n                continue\n\n            dataset = catalog._get_dataset(dataset_name)\n            dataset_path = str(dataset._filepath)\n            expectation_suite = self.DATASET_EXPECTATION_MAPPING[dataset_name]\n\n            expectation_context = ge.data_context.DataContext()\n            batch = expectation_context.get_batch(\n                {\"path\": dataset_path, \"datasource\": \"files_datasource\"},\n                expectation_suite,\n            )\n            expectation_context.run_validation_operator(\n                \"action_list_operator\",\n                assets_to_validate=[batch],\n                session_id=session_id,\n            )\n</code></pre> <ul> <li>Register Hooks implementation, as described in the hooks documentation and run Kedro.</li> </ul> <p><code>Great Expectations</code> example report:</p> <p></p>"},{"location":"pages/hooks/examples/#v3-api","title":"V3 API","text":"<ul> <li>Create new checkpoint:</li> </ul> <pre><code>great_expectations checkpoint new raw_companies_dataset_checkpoint\n</code></pre> <ul> <li>Remove <code>data_connector_query</code> from the <code>batch_request</code> in the checkpoint config file:</li> </ul> <pre><code>yaml_config = f\"\"\"\nname: {my_checkpoint_name}\nconfig_version: 1.0\nclass_name: SimpleCheckpoint\nrun_name_template: \"%Y%m%d-%H%M%S-my-run-name-template\"\nvalidations:\n  - batch_request:\n      datasource_name: {my_datasource_name}\n      data_connector_name: default_runtime_data_connector_name\n      data_asset_name: my_runtime_asset_name\n      data_connector_query:\n        index: -1\n    expectation_suite_name: {my_expectation_suite_name}\n\"\"\"\n</code></pre> <pre><code># src/&lt;package_name&gt;/hooks.py\nfrom typing import Any, Dict\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.io import DataCatalog\n\nimport great_expectations as ge\n\n\nclass DataValidationHooks:\n    # Map checkpoint to dataset\n    DATASET_CHECKPOINT_MAPPING = {\n        \"companies\": \"raw_companies_dataset_checkpoint\",\n    }\n\n    @hook_impl\n    def before_node_run(\n        self, catalog: DataCatalog, inputs: Dict[str, Any], session_id: str\n    ) -&gt; None:\n        \"\"\"Validate inputs data to a node based on using great expectation\n        if an expectation suite is defined in ``DATASET_EXPECTATION_MAPPING``.\n        \"\"\"\n        self._run_validation(catalog, inputs, session_id)\n\n    @hook_impl\n    def after_node_run(\n        self, catalog: DataCatalog, outputs: Dict[str, Any], session_id: str\n    ) -&gt; None:\n        \"\"\"Validate outputs data from a node based on using great expectation\n        if an expectation suite is defined in ``DATASET_EXPECTATION_MAPPING``.\n        \"\"\"\n        self._run_validation(catalog, outputs, session_id)\n\n    def _run_validation(\n        self, catalog: DataCatalog, data: Dict[str, Any], session_id: str\n    ):\n        for dataset_name, dataset_value in data.items():\n            if dataset_name not in self.DATASET_CHECKPOINT_MAPPING:\n                continue\n\n            data_context = ge.data_context.DataContext()\n\n            data_context.run_checkpoint(\n                checkpoint_name=self.DATASET_CHECKPOINT_MAPPING[dataset_name],\n                batch_request={\n                    \"runtime_parameters\": {\n                        \"batch_data\": dataset_value,\n                    },\n                    \"batch_identifiers\": {\n                        \"runtime_batch_identifier_name\": dataset_name\n                    },\n                },\n                run_name=session_id,\n            )\n</code></pre>"},{"location":"pages/hooks/examples/#add-observability-to-your-pipeline","title":"Add observability to your pipeline","text":"<p>This example adds observability to your pipeline using statsd and makes it possible to visualise dataset size and node execution time using Grafana.</p> <ul> <li>Install dependencies:</li> </ul> <pre><code>pip install statsd\n</code></pre> <ul> <li>Implement <code>before_node_run</code> and <code>after_node_run</code> Hooks to collect metrics (Dataset size and node execution time):</li> </ul> <pre><code># src/&lt;package_name&gt;/hooks.py\nimport sys\nfrom typing import Any, Dict\n\nimport statsd\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass PipelineMonitoringHooks:\n    def __init__(self):\n        self._timers = {}\n        self._client = statsd.StatsClient(prefix=\"kedro\")\n\n    @hook_impl\n    def before_node_run(self, node: Node) -&gt; None:\n        node_timer = self._client.timer(node.name)\n        node_timer.start()\n        self._timers[node.short_name] = node_timer\n\n    @hook_impl\n    def after_node_run(self, node: Node, inputs: Dict[str, Any]) -&gt; None:\n        self._timers[node.short_name].stop()\n        for dataset_name, dataset_value in inputs.items():\n            self._client.gauge(dataset_name + \"_size\", sys.getsizeof(dataset_value))\n\n    @hook_impl\n    def after_pipeline_run(self):\n        self._client.incr(\"run\")\n</code></pre> <ul> <li>Register the Hook implementation, as described in the Hooks documentation and run Kedro.</li> </ul> <p><code>Grafana</code> example page:</p> <p></p>"},{"location":"pages/hooks/examples/#add-metrics-tracking-to-your-model","title":"Add metrics tracking to your model","text":"<p>This examples adds metrics tracking using MLflow.</p> <ul> <li>Install dependencies:</li> </ul> <pre><code>pip install mlflow\n</code></pre> <ul> <li>Implement <code>before_pipeline_run</code>, <code>after_pipeline_run</code> and <code>after_node_run</code> Hooks to collect metrics using <code>MLflow</code>:</li> </ul> <pre><code># src/&lt;package_name&gt;/hooks.py\nfrom typing import Any, Dict\n\nimport mlflow\nimport mlflow.sklearn\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass ModelTrackingHooks:\n    \"\"\"Namespace for grouping all model-tracking hooks with MLflow together.\"\"\"\n\n    @hook_impl\n    def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n        \"\"\"Hook implementation to start an MLflow run\n        with the session_id of the Kedro pipeline run.\n        \"\"\"\n        mlflow.start_run(run_name=run_params[\"session_id\"])\n        mlflow.log_params(run_params)\n\n    @hook_impl\n    def after_node_run(\n        self, node: Node, outputs: Dict[str, Any], inputs: Dict[str, Any]\n    ) -&gt; None:\n        \"\"\"Hook implementation to add model tracking after some node runs.\n        In this example, we will:\n        * Log the parameters after the data splitting node runs.\n        * Log the model after the model training node runs.\n        * Log the model's metrics after the model evaluating node runs.\n        \"\"\"\n        if node._func_name == \"split_data\":\n            mlflow.log_params(\n                {\"split_data_ratio\": inputs[\"params:example_test_data_ratio\"]}\n            )\n\n        elif node._func_name == \"train_model\":\n            model = outputs[\"example_model\"]\n            mlflow.sklearn.log_model(model, \"model\")\n            mlflow.log_params(inputs[\"parameters\"])\n\n    @hook_impl\n    def after_pipeline_run(self) -&gt; None:\n        \"\"\"Hook implementation to end the MLflow run\n        after the Kedro pipeline finishes.\n        \"\"\"\n        mlflow.end_run()\n</code></pre> <ul> <li>Register the Hook implementation, as described in the Hooks documentation and run Kedro.</li> </ul> <p><code>MLflow</code> example page:</p> <p></p>"},{"location":"pages/hooks/examples/#modify-node-inputs-using-before_node_run-hook","title":"Modify node inputs using <code>before_node_run</code> hook","text":"<p>If the <code>before_node_run</code> hook is implemented and returns a dictionary, that dictionary is used to update the corresponding node inputs.</p> <p>For example, if a pipeline contains a node named <code>my_node</code>, which takes 2 inputs: <code>first_input</code> and <code>second_input</code>, to overwrite the value of <code>first_input</code> that is passed to <code>my_node</code>, we can implement the following hook:</p> <pre><code># src/&lt;package_name&gt;/hooks.py\nfrom typing import Any, Dict, Optional\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\nfrom kedro.io import DataCatalog\n\n\nclass NodeInputReplacementHook:\n    @hook_impl\n    def before_node_run(\n        self, node: Node, catalog: DataCatalog\n    ) -&gt; dict[str, Any] | None:\n        \"\"\"Replace `first_input` for `my_node`\"\"\"\n        if node.name == \"my_node\":\n            # return the string filepath to the `first_input` dataset\n            # instead of the underlying data\n            dataset_name = \"first_input\"\n            filepath = catalog._get_dataset(dataset_name)._filepath\n            return {\"first_input\": filepath}  # `second_input` is not affected\n        return None\n</code></pre> <p>Node input overwrites implemented in <code>before_node_run</code> affect only a specific node and do not modify the corresponding datasets in the <code>DataCatalog</code>.</p> <pre><code>In the example above, the `before_node_run` hook implementation must return datasets present in the `inputs` dictionary. If they are not in `inputs`, the node fails with the following error: `Node &lt;name&gt; expected X input(s) &lt;expected_inputs&gt;, but got the following Y input(s) instead: &lt;actual_inputs&gt;`.\n</code></pre> <p>Once you have implemented a new Hook you must register it as described in the Hooks documentation, and then run Kedro.</p>"},{"location":"pages/hooks/introduction/","title":"Introduction to Hooks","text":""},{"location":"pages/hooks/introduction/#concepts","title":"Concepts","text":"<p>A Hook consists of a Hook specification, and Hook implementation.</p>"},{"location":"pages/hooks/introduction/#hook-specifications","title":"Hook specifications","text":"<p>Kedro defines Hook specifications for particular execution points where users can inject additional behaviour. Currently, the following Hook specifications are provided in {py:mod}<code>~kedro.framework.hooks</code>:</p> <ul> <li><code>after_context_created</code></li> <li><code>after_catalog_created</code></li> <li><code>before_pipeline_run</code></li> <li><code>before_dataset_loaded</code></li> <li><code>after_dataset_loaded</code></li> <li><code>before_node_run</code></li> <li><code>after_node_run</code></li> <li><code>before_dataset_saved</code></li> <li><code>after_dataset_saved</code></li> <li><code>after_pipeline_run</code></li> <li><code>on_node_error</code></li> <li><code>on_pipeline_error</code></li> </ul> <p>The naming convention for non-error Hooks is <code>&lt;before/after&gt;_&lt;noun&gt;_&lt;past_participle&gt;</code>, in which:</p> <ul> <li><code>&lt;before/after&gt;</code> and <code>&lt;past_participle&gt;</code> refers to when the Hook executed, e.g. <code>before &lt;something&gt; was run</code> or <code>after &lt;something&gt; was created</code>.</li> <li><code>&lt;noun&gt;</code> refers to the relevant component in the Kedro execution timeline for which this Hook adds extra behaviour, e.g. <code>catalog</code>, <code>node</code> and <code>pipeline</code>.</li> </ul> <p>The naming convention for error hooks is <code>on_&lt;noun&gt;_error</code>, in which:</p> <ul> <li><code>&lt;noun&gt;</code> refers to the relevant component in the Kedro execution timeline that throws the error.</li> </ul> <p>{py:mod}<code>~kedro.framework.hooks</code> lists the full specifications for which you can inject additional behaviours by providing an implementation.</p> <p>This diagram illustrates the execution order of hooks during <code>kedro run</code>: </p>"},{"location":"pages/hooks/introduction/#cli-hooks","title":"CLI Hooks","text":"<p>Kedro defines a small set of CLI hooks that inject additional behaviour around execution of a Kedro CLI command:</p> <ul> <li><code>before_command_run</code></li> <li><code>after_command_run</code></li> </ul> <p>This is what the <code>kedro-telemetry</code> plugin relies on under the hood in order to be able to collect CLI usage statistics.</p>"},{"location":"pages/hooks/introduction/#hook-implementation","title":"Hook implementation","text":"<p>To add Hooks to your Kedro project, you must:</p> <ul> <li>Create or modify the file <code>src/&lt;package_name&gt;/hooks.py</code> to define a Hook implementation for the particular Hook specification that describes the point at which you want to inject additional behaviour</li> <li>Register that Hook implementation in the <code>src/&lt;package_name&gt;/settings.py</code> file under the <code>HOOKS</code> key</li> </ul>"},{"location":"pages/hooks/introduction/#define-the-hook-implementation","title":"Define the Hook implementation","text":"<p>The Hook implementation should have the same name as the specification. The Hook must provide a concrete implementation with a subset of the corresponding specification's parameters (you do not need to use them all).</p> <p>To declare a Hook implementation, use the <code>@hook_impl</code> decorator.</p> <p>For example, the full signature of the {py:meth}<code>after_catalog_created() &lt;kedro.framework.hooks.specs.DataCatalogSpecs.after_catalog_created&gt;</code> Hook specification is:</p> <pre><code>@hook_spec\ndef after_catalog_created(\n    self,\n    catalog: DataCatalog,\n    conf_catalog: Dict[str, Any],\n    conf_creds: Dict[str, Any],\n    save_version: str,\n    load_versions: Dict[str, str],\n) -&gt; None:\n    pass\n</code></pre> <p>However, if you just want to use this Hook to list the contents of a data catalog after it is created, your Hook implementation can be as simple as:</p> <pre><code># src/&lt;package_name&gt;/hooks.py\nimport logging\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.io import DataCatalog\n\n\nclass DataCatalogHooks:\n    @property\n    def _logger(self):\n        return logging.getLogger(__name__)\n\n    @hook_impl\n    def after_catalog_created(self, catalog: DataCatalog) -&gt; None:\n        self._logger.info(catalog.list())\n</code></pre> <pre><code>The name of a module that contains Hooks implementation is arbitrary and is not restricted to `hooks.py`.\n</code></pre> <p>We recommend that you group related Hook implementations under a namespace, preferably a class, within a <code>hooks.py</code> file that you create in your project.</p>"},{"location":"pages/hooks/introduction/#registering-the-hook-implementation-with-kedro","title":"Registering the Hook implementation with Kedro","text":"<p>Hook implementations should be registered with Kedro using the <code>src/&lt;package_name&gt;/settings.py</code> file under the <code>HOOKS</code> key.</p> <p>You can register more than one implementation for the same specification. They will be called in LIFO (last-in, first-out) order.</p> <p>The following example sets up a Hook so that the <code>after_data_catalog_created</code> implementation is called, every time, after a data catalog is created.</p> <pre><code># src/&lt;package_name&gt;/settings.py\nfrom &lt;package_name&gt;.hooks import ProjectHooks, DataCatalogHooks\n\nHOOKS = (ProjectHooks(), DataCatalogHooks())\n</code></pre> <p>Kedro also has auto-discovery enabled by default. This means that any installed plugins that declare a Hooks entry-point will be registered. To learn more about how to enable this for your custom plugin, see our plugin development guide.</p> <pre><code>Auto-discovered Hooks will run *first*, followed by the ones specified in `settings.py`.\n</code></pre>"},{"location":"pages/hooks/introduction/#auto-registered-hook-with-plugin","title":"Auto-registered Hook with plugin","text":"<p>You can auto-register a Hook (pip-installable) by creating a Kedro plugin. Kedro provides <code>kedro.hooks</code> entrypoints to extend this easily.</p>"},{"location":"pages/hooks/introduction/#disable-auto-registered-plugins-hooks","title":"Disable auto-registered plugins' Hooks","text":"<p>Auto-registered plugins' Hooks can be disabled via <code>settings.py</code> as follows:</p> <pre><code># src/&lt;package_name&gt;/settings.py\n\nDISABLE_HOOKS_FOR_PLUGINS = (\"&lt;plugin_name&gt;\",)\n</code></pre> <p>where <code>&lt;plugin_name&gt;</code> is the name of an installed plugin for which the auto-registered Hooks must be disabled.</p>"},{"location":"pages/hooks/introduction/#hook-execution-order","title":"Hook execution order","text":"<p>Hooks follow a Last-In-First-Out (LIFO) order, which means the first registered Hook will be executed last.</p> <p>Hooks are registered in the following order:</p> <ol> <li>Project Hooks in <code>settings.py</code> - If you have <code>HOOKS = (hook_a, hook_b,)</code>, <code>hook_b</code> will be executed before <code>hook_a</code></li> <li>Plugin Hooks registered in <code>kedro.hooks</code>, which follows alphabetical order</li> </ol> <p>In general, Hook execution order is not guaranteed and you should not rely on it. If you need to make sure a particular Hook is executed first or last, you can use the <code>tryfirst</code> or <code>trylast</code> argument for <code>hook_impl</code>.</p>"},{"location":"pages/hooks/introduction/#under-the-hood","title":"Under the hood","text":"<p>Under the hood, we use pytest's pluggy to implement Kedro's Hook mechanism. We recommend reading their documentation to find out more about the underlying implementation.</p>"},{"location":"pages/hooks/introduction/#plugin-hooks","title":"Plugin Hooks","text":"<p>Plugin Hooks are registered using <code>importlib_metadata</code>'s <code>EntryPoints</code> API.</p>"},{"location":"pages/integrations/deltalake_versioning/","title":"Data versioning with Delta Lake","text":"<p>Delta Lake is an open-source storage layer that brings reliability to data lakes by adding a transactional storage layer on top of the data stored in cloud storage. It allows for ACID transactions, data versioning, and rollback capabilities. Delta table is the default table format in Databricks, and it can be used outside of it as well. It is typically used for data lakes, where data is ingested either incrementally or in batch.</p> <p>This tutorial explores how to use Delta tables in your Kedro workflow and how to leverage the data versioning capabilities of Delta Lake.</p>"},{"location":"pages/integrations/deltalake_versioning/#prerequisites","title":"Prerequisites","text":"<p>In this example, you will use the <code>spaceflights-pandas</code> starter project which has example pipelines and datasets to work with. If you haven't already, you can create a new Kedro project using the following command:</p> <pre><code>kedro new --starter spaceflights-pandas\n</code></pre> <p>Kedro offers various connectors in the <code>kedro-datasets</code> package to interact with Delta tables: <code>pandas.DeltaTableDataset</code>, <code>spark.DeltaTableDataset</code>, <code>spark.SparkDataset</code>, <code>databricks.ManagedTableDataset</code>, and <code>ibis.FileDataset</code> support the delta table format. In this tutorial, we will use the <code>pandas.DeltaTableDataset</code> connector to interact with Delta tables using Pandas DataFrames. To install <code>kedro-datasets</code> alongwith dependencies required for Delta Lake, add the following line to your <code>requirements.txt</code>:</p> <pre><code>kedro-datasets[pandas-deltatabledataset]\n</code></pre> <p>Now, you can install the project dependencies by running:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"pages/integrations/deltalake_versioning/#using-delta-tables-in-catalog","title":"Using Delta tables in catalog","text":""},{"location":"pages/integrations/deltalake_versioning/#save-dataset-as-a-delta-table","title":"Save dataset as a Delta table","text":"<p>To use Delta tables in your Kedro project, you can update the <code>base/catalog.yml</code> to use <code>type: pandas.DeltaTableDataset</code> for the datasets you want to save as Delta tables. For this example, let us update the <code>model_input_table</code> dataset in the <code>base/catalog.yml</code> file:</p> <pre><code>model_input_table:\n  type: pandas.DeltaTableDataset\n  filepath: data/03_primary/model_input_table\n  save_args:\n    mode: overwrite\n</code></pre> <p>You can add <code>save_args</code> to the configuration to specify the mode of saving the Delta table. The <code>mode</code> parameter can be \"overwrite\" or \"append\" depending on whether you want to overwrite the existing Delta table or append to it. You can also specify additional saving options that are accepted by the <code>write_deltalake</code> function in the <code>delta-rs</code> library which is used by <code>pandas.DeltaTableDataset</code> to interact with the Delta table format.</p> <p>When you run the Kedro project with <code>kedro run</code> command, the Delta table will be saved to the location specified in the <code>filepath</code> argument as a folder of <code>parquet</code> files. This folder also contains a <code>_delta_log</code> directory which stores the transaction log of the Delta table. The following runs of the pipeline will create new versions of the Delta table in the same location and new entries in the <code>_delta_log</code> directory. You can run the Kedro project with the following command to generate the <code>model_input_table</code> dataset:</p> <pre><code>kedro run\n</code></pre> <p>Suppose the upstream datasets <code>companies</code>, <code>shuttles</code>, or <code>reviews</code> is updated. You can run the following command to generate a new version of the <code>model_input_table</code> dataset:</p> <pre><code>kedro run --to-outputs=model_input_table\n</code></pre> <p>To inspect the updated dataset and logs:</p> <pre><code>$ tree data/03_primary\ndata/03_primary\n\u2514\u2500\u2500 model_input_table\n    \u251c\u2500\u2500 _delta_log\n    \u2502   \u251c\u2500\u2500 00000000000000000000.json\n    \u2502   \u2514\u2500\u2500 00000000000000000001.json\n    \u251c\u2500\u2500 part-00001-0d522679-916c-4283-ad06-466c27025bcf-c000.snappy.parquet\n    \u2514\u2500\u2500 part-00001-42733095-97f4-46ef-bdfd-3afef70ee9d8-c000.snappy.parquet\n</code></pre>"},{"location":"pages/integrations/deltalake_versioning/#load-a-specific-dataset-version","title":"Load a specific dataset version","text":"<p>To load a specific version of the dataset, you can specify the version number in the <code>load_args</code> parameter in the catalog entry:</p> <pre><code>model_input_table:\n  type: pandas.DeltaTableDataset\n  filepath: data/03_primary/model_input_table\n  load_args:\n    version: 1\n</code></pre>"},{"location":"pages/integrations/deltalake_versioning/#inspect-the-dataset-in-interactive-mode","title":"Inspect the dataset in interactive mode","text":"<p>You can inspect the history and the metadata of the Delta table in an interactive Python session. To start the IPython session with Kedro components loaded, run:</p> <pre><code>kedro ipython\n</code></pre> <p>You can load the Delta table dataset using the <code>catalog.datasets</code> attribute and inspect the dataset:</p> <pre><code>In [1]: model_input_table = catalog.datasets['model_input_table']\n</code></pre> <p>You can inspect the history of the Delta table by accessing the <code>history</code> attribute:</p> <pre><code>In [2]: model_input_table.history\nOut [2]:\n[\n    {\n        'timestamp': 1739891304488,\n        'operation': 'WRITE',\n        'operationParameters': {'mode': 'Overwrite'},\n        'operationMetrics': {\n            'execution_time_ms': 8,\n            'num_added_files': 1,\n            'num_added_rows': 6027,\n            'num_partitions': 0,\n            'num_removed_files': 1\n        },\n        'clientVersion': 'delta-rs.0.23.1',\n        'version': 1\n    },\n    {\n        'timestamp': 1739891277424,\n        'operation': 'WRITE',\n        'operationParameters': {'mode': 'Overwrite'},\n        'clientVersion': 'delta-rs.0.23.1',\n        'operationMetrics': {\n            'execution_time_ms': 48,\n            'num_added_files': 1,\n            'num_added_rows': 6027,\n            'num_partitions': 0,\n            'num_removed_files': 0\n        },\n        'version': 0\n    }\n]\n</code></pre> <p>You can also inspect the loaded version of the table with the following method:</p> <pre><code>In [3]: model_input_table.get_loaded_version()\nOut [3]: 1\n</code></pre>"},{"location":"pages/integrations/deltalake_versioning/#using-delta-tables-with-spark","title":"Using Delta tables with Spark","text":"<p>You can also use <code>PySpark</code> to interact with Delta tables in your Kedro project. To set up Delta tables with Spark, consult the documentation on the integration of Spark with Kedro.</p> <p>We recommend the following workflow, which makes use of the transcoding feature in Kedro:</p> <ul> <li>To create a Delta table, use a <code>spark.SparkDataset</code> with <code>file_format=\"delta\"</code>. You can also use this type of dataset to read from a Delta table or overwrite it.</li> <li>To perform Delta table deletes, updates, and merges, load the data using a <code>DeltaTableDataset</code> and perform the write operations within the node function.</li> </ul> <p>As a result, we end up with a catalog that looks like this:</p> <pre><code>temperature:\n  type: spark.SparkDataset\n  filepath: data/01_raw/data.csv\n  file_format: \"csv\"\n  load_args:\n    header: True\n    inferSchema: True\n  save_args:\n    sep: '|'\n    header: True\n\nweather@spark:\n  type: spark.SparkDataset\n  filepath: s3a://my_bucket/03_primary/weather\n  file_format: \"delta\"\n  save_args:\n    mode: \"overwrite\"\n    versionAsOf: 0\n\nweather@delta:\n  type: spark.DeltaTableDataset\n  filepath: s3a://my_bucket/03_primary/weather\n</code></pre> <pre><code>The `DeltaTableDataset` does not support `save()` operation. Instead, pick the operation you want to perform (`DeltaTable.update()`, `DeltaTable.delete()`, `DeltaTable.merge()`) and write it in your node code instead.\n</code></pre> <pre><code>If you have defined an implementation for the Kedro `before_dataset_saved`/`after_dataset_saved` hook, the hook will not be triggered. This is because the save operation happens within the `node` itself, via the DeltaTable API.\n</code></pre> <pre><code>pipeline(\n    [\n        node(\n            func=process_barometer_data, inputs=\"temperature\", outputs=\"weather@spark\"\n        ),\n        node(\n            func=update_meterological_state,\n            inputs=\"weather@delta\",\n            outputs=\"first_operation_complete\",\n        ),\n        node(\n            func=estimate_weather_trend,\n            inputs=[\"first_operation_complete\", \"weather@delta\"],\n            outputs=\"second_operation_complete\",\n        ),\n    ]\n)\n</code></pre> <p><code>first_operation_complete</code> is a <code>MemoryDataset</code> and it signals that any Delta operations which occur \"outside\" the Kedro DAG are complete. This can be used as input to a downstream node, to preserve the shape of the DAG. Otherwise, if no downstream nodes need to run after this, the node can simply not return anything:</p> <pre><code>pipeline(\n    [\n        node(func=..., inputs=\"temperature\", outputs=\"weather@spark\"),\n        node(func=..., inputs=\"weather@delta\", outputs=None),\n    ]\n)\n</code></pre> <p>The following diagram is the visual representation of the workflow explained above:</p> <p></p> <pre><code>This pattern of creating \"dummy\" datasets to preserve the data flow also applies to other \"out of DAG\" execution operations such as SQL operations within a node.\n</code></pre>"},{"location":"pages/integrations/iceberg_versioning/","title":"Data versioning with Iceberg","text":"<p>Apache Iceberg is an open table format for analytic datasets. Iceberg tables offer features such as schema evolution, hidden partitioning, partition layout evolution, time travel, and version rollback. This guide explains how to use Iceberg tables with Kedro. For this tutorial, we will use <code>pyiceberg</code> which is a library that allows you to interact with Iceberg tables using Python, without the need of a JVM. It is important to note that <code>pyiceberg</code> is a fast evolving project and does not support the full range of features that Iceberg tables offer. You can use this tutorial as inspiration to extend the functionality using different compute engines such as Spark, or dataframe technologies such as Apache Arrow, DuckDB, and more by creating your own custom datasets.</p>"},{"location":"pages/integrations/iceberg_versioning/#prerequisites","title":"Prerequisites","text":"<p>You will need to create a <code>spaceflights-pandas</code> starter project which contains example pipelines to work with called <code>kedro-iceberg</code>. If you haven't already, you can create a new Kedro project using the following command:</p> <pre><code>kedro new --starter spaceflights-pandas --name kedro-iceberg\n</code></pre> <p>To interact with Iceberg tables, you will also need the <code>pyiceberg</code> package installed. You can install it by adding the following line to your <code>requirements.txt</code>:</p> <pre><code>pyiceberg[pyarrow]~=0.8.0\n</code></pre> <p>Depending on your choice of storage, you may also need to install the optional dependencies. Consult the installation guide for <code>PyIceberg</code> to update the line above with the necessary optional dependencies.</p> <p>Now you can install the project requirements with the following command:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"pages/integrations/iceberg_versioning/#set-up-the-iceberg-catalog","title":"Set up the Iceberg catalog","text":"<p>Iceberg tables are managed by a catalog which is responsible for managing the metadata of the tables. In production, this could be a Hive, Glue, or other catalog supported by Apache Iceberg. Iceberg also supports various storage options such as S3, HDFS, and more. There are multiple ways you can configure the catalog, credentials, and object storage to suit your needs by referring to the configuration guide. For this tutorial, we will use the <code>SQLCatalog</code> which stores the metadata in a local <code>sqlite</code> database and uses the local filesystem for storage.</p> <p>Create a temporary location for Iceberg tables by running the following command:</p> <pre><code>mkdir -p /tmp/warehouse\n</code></pre> <p>There are multiple ways to configure the catalog, and for this tutorial, you can use the <code>~/.pyiceberg.yaml</code> file. By default, <code>pyiceberg</code> looks for the <code>.pyiceberg.yaml</code> file in your home directory, that is, it looks for <code>~/.pyiceberg.yaml</code>. You can create or update the existing file <code>.pyiceberg.yaml</code> in your home directory with the following content:</p> <pre><code>catalog:\n  default:\n    type: sql\n    uri: sqlite:////tmp/warehouse/pyiceberg_catalog.db\n    warehouse: file:///tmp/warehouse/warehouse\n</code></pre> <p>You can check if the configuration is loading by opening a Python shell with <code>ipython</code> command and running the following code:</p> <pre><code>from pyiceberg.catalog import load_catalog\ncatalog = load_catalog(name=\"default\")\n</code></pre>"},{"location":"pages/integrations/iceberg_versioning/#define-a-custom-dataset-to-use-iceberg-tables","title":"Define a custom dataset to use Iceberg tables","text":"<p>To use the Iceberg tables with Kedro, you will need to define a custom dataset that uses the <code>pyiceberg</code> library. Create a new file  called <code>pyiceberg_dataset.py</code> in the <code>src/kedro_iceberg/</code> directory of your project and copy the following code:</p> <pre><code>import pyarrow as pa\nfrom kedro.io.core import AbstractDataset, DatasetError\nfrom pyiceberg.catalog import load_catalog\nfrom pyiceberg.exceptions import NoSuchTableError\n\nDEFAULT_LOAD_ARGS = {\"load_version\": None}\nDEFAULT_SAVE_ARGS = {\"mode\": \"overwrite\"}\n\nclass PyIcebergDataset(AbstractDataset):\n    def __init__(\n            self,\n            catalog,\n            namespace,\n            table_name,\n            load_args=DEFAULT_LOAD_ARGS,\n            scan_args=None,\n            save_args=DEFAULT_SAVE_ARGS,\n    ):\n        self.table_name = table_name\n        self.namespace = namespace\n        self.catalog = load_catalog(catalog)\n        self.load_args = load_args\n        self.table = self._load_table(namespace, table_name)\n        self.save_args = save_args\n        self.scan_args = scan_args\n\n\n    def load(self):\n        self.table = self.catalog.load_table((self.namespace, self.table_name))\n        if self.scan_args:\n            scan = self.table.scan(**self.scan_args)\n        else:\n            scan = self.table.scan()\n        return scan.to_pandas()\n\n    def _load_table(self, namespace, table_name):\n        try:\n            return self.catalog.load_table((namespace, table_name))\n        except NoSuchTableError:\n            return None\n\n    def save(self, data) -&gt; None:\n        arrow = pa.Table.from_pandas(data)\n        if not self.table:\n            self.catalog.create_namespace_if_not_exists(self.namespace)\n            self.table = self.catalog.create_table((self.namespace, self.table_name), schema=arrow.schema)\n        if self.save_args.get(\"mode\") == \"overwrite\":\n            self.table.overwrite(arrow)\n        elif self.save_args.get(\"mode\") == \"append\":\n            self.table.append(arrow)\n        else:\n            raise DatasetError(\"Mode not supported\")\n\n    def _describe(self) -&gt; dict:\n        return {}\n\n    def exists(self):\n        return self.catalog.table_exists((self.namespace, self.table_name))\n\n    def inspect(self):\n        return self.table.inspect\n</code></pre> <p>This dataset allows you to load Iceberg tables as <code>pandas</code> dataframes. You can also load a subset of the table by passing the <code>scan_args</code> parameter to the dataset. The <code>save()</code> method allows you to save a dataframe as an Iceberg table. You can specify the mode of saving the table by passing the <code>save_args</code> parameter to the dataset. The <code>inspect()</code> method returns an <code>InspectTable</code> object which contains metadata about the table. You can also update the code to extend the functionality of the dataset to support more features of Iceberg tables. Refer to the Iceberg API documentation to see what you can do with the <code>Table</code> object.</p>"},{"location":"pages/integrations/iceberg_versioning/#using-iceberg-tables-in-the-catalog","title":"Using Iceberg tables in the catalog","text":""},{"location":"pages/integrations/iceberg_versioning/#save-the-dataset-as-an-iceberg-table","title":"Save the dataset as an Iceberg table","text":"<p>Now update your catalog in <code>conf/base/catalog.yml</code> to update the <code>model_input_table</code> dataset to use the custom <code>PyIcebergDataset</code>:</p> <pre><code>model_input_table:\n  type: kedro_iceberg.pyiceberg_dataset.PyIcebergDataset\n  catalog: default\n  namespace: default\n  table_name: model_input_table\n</code></pre> <p>Now run your Kedro project with the following command:</p> <pre><code>kedro run\n</code></pre> <p>You can inspect the <code>model_input_table</code> dataset created as an Iceberg table by running the following command:</p> <pre><code>tree /tmp/warehouse\n</code></pre> <p>The output should look something like:</p> <pre><code>/tmp/warehouse\n\u251c\u2500\u2500 pyiceberg_catalog.db\n\u2514\u2500\u2500 warehouse\n    \u2514\u2500\u2500 default.db\n        \u2514\u2500\u2500 model_input_table\n            \u251c\u2500\u2500 data\n            \u2502   \u251c\u2500\u2500 00000-0-a3d0f3e6-a9b4-44e4-8dac-95d4b5c14b29.parquet\n            \u2502   \u2514\u2500\u2500 00000-0-baa30a43-2cad-4507-967e-84c744d69c9b.parquet\n            \u2514\u2500\u2500 metadata\n                \u251c\u2500\u2500 00000-e66a465e-cdfa-458e-aaf3-aed48ac49157.metadata.json\n                \u251c\u2500\u2500 00001-d1fa7797-ef6f-438e-83c0-bdaabf1bd8de.metadata.json\n                \u251c\u2500\u2500 00002-f0b5294e-a0be-450c-95fe-4cee96c9a311.metadata.json\n                \u251c\u2500\u2500 836ecf2e-e339-44d8-933a-bd978991ea3e-m0.avro\n                \u251c\u2500\u2500 a3d0f3e6-a9b4-44e4-8dac-95d4b5c14b29-m0.avro\n                \u251c\u2500\u2500 baa30a43-2cad-4507-967e-84c744d69c9b-m0.avro\n                \u251c\u2500\u2500 snap-3087457244520966174-0-836ecf2e-e339-44d8-933a-bd978991ea3e.avro\n                \u251c\u2500\u2500 snap-3885749350984242152-0-a3d0f3e6-a9b4-44e4-8dac-95d4b5c14b29.avro\n                \u2514\u2500\u2500 snap-7387825159950300388-0-baa30a43-2cad-4507-967e-84c744d69c9b.avro\n</code></pre> <p>Suppose the upstream datasets <code>companies</code>, <code>shuttles</code>, or <code>reviews</code> are updated. You can run the following command to generate a new version of the <code>model_input_table</code> dataset:</p> <pre><code>kedro run --to-outputs=model_input_table\n</code></pre> <p>You can use the <code>find /tmp/warehouse/</code> command to inspect the updated dataset and logs.</p>"},{"location":"pages/integrations/iceberg_versioning/#load-a-specific-dataset-version","title":"Load a specific dataset version","text":"<p>To load a specific version of the dataset, you can specify the <code>snapshot_id</code> of the version you want in the <code>scan_args</code> of the table in the configuration. You can get the <code>snapshot_id</code> from the history of the table. The section below explains how to inspect the history of the table in interactive mode.</p> <pre><code>model_input_table:\n  type: kedro_iceberg.pyiceberg_dataset.PyIcebergDataset\n  catalog: default\n  namespace: default\n  table_name: model_input_table\n  table_type: pandas\n  scan_args:\n    snapshot_id: &lt;snapshot_id&gt;\n</code></pre>"},{"location":"pages/integrations/iceberg_versioning/#inspect-the-dataset-in-interactive-mode","title":"Inspect the dataset in interactive mode","text":"<p>You can inspect the history, metadata, schema, and more of the Iceberg table in an interactive Python session. To start the IPython session with Kedro components loaded, run:</p> <pre><code>kedro ipython\n</code></pre> <p>Load the instance of the <code>PyIcebergDataset</code> using the <code>catalog.datasets</code> attribute:</p> <pre><code>In [1]: model_input_table = catalog.datasets['model_input_table']\n</code></pre> <p>You can inspect the history of the Delta table by accessing the <code>InspectTable</code> object with the <code>inspect()</code> method:</p> <pre><code>In [2]: inspect_table = model_input_table.inspect()\n</code></pre> <p>Now you can call the <code>history()</code> method on the <code>InspectTable</code> object to get the history of the table:</p> <pre><code>In [3]: inspect_table.history()\nOut [3]:\n\npyarrow.Table\nmade_current_at: timestamp[ms] not null\nsnapshot_id: int64 not null\nparent_id: int64\nis_current_ancestor: bool not null\n----\nmade_current_at: [[2025-02-26 11:42:36.871,2025-02-26 12:08:38.826,2025-02-26 12:08:38.848]]\nsnapshot_id: [[9089827653240705573,5091346767047746426,7107920212859354452]]\nparent_id: [[null,9089827653240705573,5091346767047746426]]\nis_current_ancestor: [[true,true,true]]\n</code></pre> <p>Alternatively, you can also call the <code>history()</code> method from the <code>pyiceberg.table.Table</code> object directly which shows a more consise output:</p> <pre><code>In [4]: model_input_table.table.history()\nOut [4]:\n[\n    SnapshotLogEntry(snapshot_id=7387825159950300388, timestamp_ms=1741190825900),\n    SnapshotLogEntry(snapshot_id=3087457244520966174, timestamp_ms=1741190833531),\n    SnapshotLogEntry(snapshot_id=3885749350984242152, timestamp_ms=1741190833554)\n]\n</code></pre> <p>Similarly, you can call other methods on the <code>InspectTable</code> object to get more information about the table, such as <code>snapshots()</code>, <code>schema()</code>, <code>partitions()</code>, and more.</p>"},{"location":"pages/integrations/kedro_dvc_versioning/","title":"Data and pipeline versioning with Kedro and DVC","text":"<p>This document explains how to use DVC to version datasets and pipelines in your Kedro project. DVC is a tool to develop reproducible machine learning projects. It can be installed on Visual Studio Code, any system terminal, and used as a Python library.</p> <p>This tutorial assumes you have experience with the Git CLI and Kedro CLI commands but does not require any prior knowledge of DVC.</p>"},{"location":"pages/integrations/kedro_dvc_versioning/#versioning-data-with-dvc-files","title":"Versioning data with .dvc files","text":""},{"location":"pages/integrations/kedro_dvc_versioning/#initialising-the-repository","title":"Initialising the repository","text":"<p>For this example, you will be using a Kedro <code>spaceflights-pandas</code> starter project, which includes pre-configured datasets and pipelines. To create this starter project locally, use the command:</p> <p><code>kedro new --starter=spaceflights-pandas --name=space-dvc</code></p> <p>For more information about starter projects, see the Kedro starters documentation page.</p> <p>To use DVC as a Python library, install it using <code>pip</code> or <code>conda</code>, for example:</p> <pre><code>`pip install dvc`\n</code></pre> <p>Since DVC works alongside Git to track data changes, initialise the Kedro project as a git repository:</p> <pre><code>`git init`\n</code></pre> <p>Then, initialise DVC in the project:</p> <pre><code>`dvc init`\n</code></pre> <p>This will create the <code>.dvc</code> directory inside the project. You should see a message such as:</p> <pre><code>Initialized DVC repository.\n\nYou can now commit the changes to git.\n\n+---------------------------------------------------------------------+\n|                                                                     |\n|        DVC has enabled anonymous aggregate usage analytics.         |\n|     Read the analytics documentation (and how to opt-out) here:     |\n|             &lt;https://dvc.org/doc/user-guide/analytics&gt;              |\n|                                                                     |\n+---------------------------------------------------------------------+\n</code></pre> <p>Since you initialised a new Git repository with <code>git init</code> on the previous step, you can now make an initial commit containing all of the files in the project:</p> <pre><code>git add .\ngit commit -m \"First commit, initial structure from the starter\"\n</code></pre>"},{"location":"pages/integrations/kedro_dvc_versioning/#tracking-your-data-with-dvc","title":"Tracking your data with DVC","text":"<p>DVC helps manage large datasets that should not be stored directly in Git. Instead of adding dataset files to Git, DVC generates small metadata files that Git tracks instead.</p> <p>These metadata files store information about the actual dataset, such as its hash and location. More information about the structure of the <code>.dvc</code> file can be found in the DVC documentation.</p> <p>Verify that your project catalog contains this dataset definition:</p> <pre><code>companies:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/companies.csv\n</code></pre> <p>Due to the location of the dataset files in the project template, you must ensure the following line is present in the <code>.gitignore</code> file to allow <code>.dvc</code> files to be committed:</p> <pre><code>!*.dvc\n</code></pre> <p>We want to use DVC to track and version our dataset file, so you remove it from Git and commit the change:</p> <p><code>bash  git rm -r --cached 'data/01_raw/companies.csv'  git commit -m \"Stop tracking data/01_raw/companies.csv\"</code></p> <p>We can then start tracking it with DVC:</p> <p><code>bash  dvc add data/01_raw/companies.csv</code></p> <p>This generates the <code>companies.csv.dvc</code> file which can be committed to git. This small, human-readable metadata file acts as a placeholder for the original data for Git tracking.</p> <p>Once updated, add the <code>.dvc</code> file to Git and commit the changes:</p> <pre><code>git add data/01_raw/companies.csv.dvc\ngit commit -m \"Track companies.csv dataset with DVC\"\n</code></pre>"},{"location":"pages/integrations/kedro_dvc_versioning/#going-back-to-a-previous-version-of-the-data","title":"Going back to a previous version of the data","text":"<p>First, let's create a different version of the <code>companies.csv</code> file by adding a dummy line to it.</p> <pre><code>echo \"000,100%,Example,1.0,f\" &gt;&gt; data/01_raw/companies.csv\n</code></pre> <p>By using the command <code>tail data/01_raw/companies.csv</code>, you can verify that the line has been added to the file:</p> <pre><code>6957,,Rwanda,1.0,t\n7554,100%,,1105.0,f\n34243,95%,Uzbekistan,1.0,f\n12502,89%,Denmark,1.0,f\n20213,,Russian Federation,1.0,f\n2235,100%,Guinea,1.0,f\n2353,100%,Senegal,2.0,t\n49772,100%,Jersey,1.0,f\n16548,90%,,2.0,f\n000,100%,Example,1.0,f\n</code></pre> <p>Then you can track the changes with DVC, and commit them to Git:</p> <pre><code>dvc add data/01_raw/companies.csv\ngit add data/01_raw/companies.csv.dvc\ngit commit -m \"Track dataset changes with DVC\"\n</code></pre> <p>DVC integrates with Git to manage different dataset versions. If you need to restore a previous version of a dataset, first identify the commit containing the desired version. You can use:</p> <pre><code>git log -- data/01_raw/companies.csv.dvc\n</code></pre> <p>To display the commit hashes associated with this file. Once you find the desired commit, run:</p> <pre><code>git checkout &lt;commit_hash&gt; data/01_raw/companies.csv.dvc\ndvc checkout\n</code></pre> <p>The first command will restore the <code>.dvc</code> metadata file to its previous version. The second uses the metadata file to restore the corresponding dataset.</p> <pre><code>Building workspace index\nComparing indexes\nApplying changes\nM       data/01_raw/companies.csv\n</code></pre> <p>Using the command <code>tail data/01_raw/companies.csv</code> again shows that the dataset file has been restored to a previous version.</p> <pre><code>1618,100%,,1.0,t\n6957,,Rwanda,1.0,t\n7554,100%,,1105.0,f\n34243,95%,Uzbekistan,1.0,f\n12502,89%,Denmark,1.0,f\n20213,,Russian Federation,1.0,f\n2235,100%,Guinea,1.0,f\n2353,100%,Senegal,2.0,t\n49772,100%,Jersey,1.0,f\n16548,90%,,2.0,f\n</code></pre>"},{"location":"pages/integrations/kedro_dvc_versioning/#advanced-use-cases","title":"Advanced use cases","text":""},{"location":"pages/integrations/kedro_dvc_versioning/#how-to-store-data-remotely","title":"How to store data remotely","text":"<p>DVC remotes provide access to external storage locations to track and share your data and ML models with the <code>dvc push</code> and <code>dvc pull</code> commands. Those will be shared between devices or team members who are working on a project. It supports several different storage types, like Amazon S3, Azure Blob Storage or Google Cloud Storage, as well as self-hosted options. For more detail on this subject, see the DVC documentation on remote storage.</p> <p>For example:</p> <pre><code>dvc remote add myremote s3://mybucket\nkedro run\ngit add .\ngit commit -m \"Update\"\ndvc push\n</code></pre>"},{"location":"pages/integrations/kedro_dvc_versioning/#how-to-go-back-to-a-previous-version-of-the-data-stored-remotely","title":"How to go back to a previous version of the data, stored remotely","text":"<pre><code>git checkout &lt;commit hash&gt; data/01_raw/companies.csv.dvc\ndvc checkout\ndvc pull\n</code></pre>"},{"location":"pages/integrations/kedro_dvc_versioning/#how-to-version-with-dvc-data-pipelines","title":"How to version with DVC data pipelines","text":"<p>While the previous method allows you to version datasets, it comes with some limitations, as DVC requires the files to be tracked to be added manually:</p> <ul> <li>Intermediate and output datasets must be added to DVC individually.</li> <li>Parameters and code changes are not explicitly tracked.</li> <li>Artifacts and metrics can be cumbersome to track.</li> </ul> <p>To address these issues, you can define Kedro pipelines as DVC stages in the dvc.yaml file. The list of stages is typically the most important part of a dvc.yaml file. The file can also be used to configure artifacts, metrics, parameters, and plots, either as part of a stage definition or on their own.</p> <p>For more information on the configuration of those files, see the documentation on dvc.yaml.</p>"},{"location":"pages/integrations/kedro_dvc_versioning/#how-to-define-kedro-pipelines-as-dvc-stages","title":"How to define Kedro pipelines as DVC stages","text":"<p>Here is an example configuration for dvc.yaml:</p> <pre><code>stages:\n  data_processing:\n    cmd: kedro run --pipeline data_processing\n    deps:\n      - data/01_raw/companies.csv\n      - data/01_raw/reviews.csv\n      - data/01_raw/shuttles.xlsx\n    outs:\n      - data/02_intermediate/preprocessed_companies.parquet\n      - data/02_intermediate/preprocessed_shuttles.parquet\n      - data/03_primary/model_input_table.parquet\n\n  data_science:\n    cmd: kedro run --pipeline data_science\n    deps:\n      - data/03_primary/model_input_table.parquet\n    outs:\n      - data/06_models/regressor.pickle\n</code></pre> <p>Run the pipeline with:</p> <pre><code>dvc repro\n</code></pre>"},{"location":"pages/integrations/kedro_dvc_versioning/#how-to-update-a-dataset","title":"How to update a dataset","text":"<p>If one of the datasets is updated, you can rerun only the pipelines affected by the change.</p> <p>The command <code>dvc repro</code> executes pipelines where outputs or dependencies have changed.</p>"},{"location":"pages/integrations/kedro_dvc_versioning/#how-to-track-code-changes","title":"How to track code changes","text":"<p>You can track changes to your code by adding the relevant files to the <code>deps</code> section in <code>dvc.yaml</code>.</p> <pre><code>stages:\n  data_processing:\n    cmd: kedro run --pipeline data_processing\n    deps:\n      - data/01_raw/companies.csv\n      - data/01_raw/reviews.csv\n      - data/01_raw/shuttles.xlsx\n      - src/space_dvc/pipelines/data_processing/nodes.py\n      - src/space_dvc/pipelines/data_processing/pipeline.py\n    outs:\n      - data/02_intermediate/preprocessed_companies.parquet\n      - data/02_intermediate/preprocessed_shuttles.parquet\n      - data/03_primary/model_input_table.parquet\n</code></pre> <p>After applying the desired code changes, run <code>dvc repro</code>. The output should confirm the updates on the <code>dvc.lock</code> file, if any:</p> <pre><code>Updating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage.\n</code></pre> <p>After that, they can be pushed to remote storage with the <code>dvc push</code> command.</p>"},{"location":"pages/integrations/kedro_dvc_versioning/#how-to-track-parameters","title":"How to track parameters","text":"<p>To track parameters, you can include them under the <code>params</code> section in <code>dvc.yaml</code>.</p> <pre><code>stages:\n  data_science:\n    cmd: kedro run --pipeline data_science\n    deps:\n      - data/03_primary/model_input_table.parquet\n      - src/space_dvc/pipelines/data_science/nodes.py\n      - src/space_dvc/pipelines/data_science/pipeline.py\n    params:\n      - conf/base/parameters_data_science.yaml:\n          - model_options\n    outs:\n      - data/06_models/regressor.pickle\n</code></pre> <p>Run the pipeline and push the changes:</p> <pre><code>dvc repro\ndvc push\n</code></pre>"},{"location":"pages/integrations/kedro_dvc_versioning/#how-to-run-experiments-with-different-parameters","title":"How to run experiments with different parameters","text":"<p>To experiment with different parameter values, update the parameter in <code>parameters.yaml</code> and then run the pipelines with <code>dvc repro</code>.</p> <p>Compare parameter changes between runs with <code>dvc params diff</code>:</p> <pre><code>Path                                   Param                       HEAD    workspace\nconf/base/parameters_data_science.yml  model_options.features      -       ['engines', 'passenger_capacity', 'crew', 'd_check_complete', 'moon_clearance_complete', 'iata_approved', 'company_rating', 'review_scores_rating']\nconf/base/parameters_data_science.yml  model_options.random_state  -       3\nconf/base/parameters_data_science.yml  model_options.test_size     -       0.2\n</code></pre>"},{"location":"pages/integrations/mlflow/","title":"How to add MLflow to your Kedro workflow","text":"<p>MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools for tracking experiments, packaging code into reproducible runs, and sharing and deploying models. MLflow supports machine learning frameworks such as TensorFlow, PyTorch, and scikit-learn.</p> <p>Adding MLflow to a Kedro project enables you to track and manage your machine learning experiments and models. For example, you can log metrics, parameters, and artifacts from your Kedro pipeline runs to MLflow, then compare and reproduce the results. When collaborating with others on a Kedro project, MLflow's model registry and deployment tools help you to share and deploy machine learning models.</p>"},{"location":"pages/integrations/mlflow/#prerequisites","title":"Prerequisites","text":"<p>You will need the following:</p> <ul> <li>A working Kedro project in a virtual environment. The examples in this document assume the <code>spaceflights-pandas-viz</code> starter.   If you're unfamiliar with the Spaceflights project, check out our tutorial.</li> <li>The MLflow client installed into the same virtual environment. For the purposes of this tutorial,   you can use MLflow {external+mlflow:doc}<code>in its simplest configuration &lt;tracking&gt;</code>.</li> </ul> <p>To set yourself up, create a new Kedro project:</p> <pre><code>$ kedro new --starter=spaceflights-pandas-viz --name spaceflights-mlflow\n$ cd spaceflights-mlflow\n$ python -m venv &amp;&amp; source .venv/bin/activate\n(.venv) $ pip install -r requirements.txt\n</code></pre> <p>And then launch the UI locally from the root of your directory as follows:</p> <pre><code>(.venv) $ pip install mlflow\n(.venv) $ mlflow ui --backend-store-uri ./mlflow_runs\n</code></pre> <p>This will make MLflow record metadata and artifacts for each run to a local directory called <code>mlflow_runs</code>.</p> <p>Note If you want to use a more sophisticated setup, have a look at the documentation of MLflow tracking server, the official MLflow tracking server 5 minute overview, and the MLflow tracking server documentation.</p>"},{"location":"pages/integrations/mlflow/#simple-use-cases","title":"Simple use cases","text":"<p>Although MLflow works best when working with machine learning (ML) and AI pipelines, you can track your regular Kedro runs as experiments in MLflow even if they do not use ML.</p> <p>This section explains how you can use the <code>kedro-mlflow</code> plugin to track your Kedro pipelines in MLflow in a straightforward way.</p>"},{"location":"pages/integrations/mlflow/#easy-tracking-of-kedro-runs-in-mlflow-using-kedro-mlflow","title":"Easy tracking of Kedro runs in MLflow using <code>kedro-mlflow</code>","text":"<p>To start using <code>kedro-mlflow</code>, install it first:</p> <pre><code>pip install kedro-mlflow\n</code></pre> <p>In recent versions of Kedro, this will already register the <code>kedro-mlflow</code> Hooks for you.</p> <p>Next, create a <code>mlflow.yml</code> configuration file in your <code>conf/local</code> directory that configures where the MLflow runs are stored, consistent with how you launched the <code>mlflow ui</code> command:</p> <pre><code>server:\n  mlflow_tracking_uri: mlflow_runs\n</code></pre> <p>From this point, when you execute <code>kedro run</code> you will see the logs coming from <code>kedro-mlflow</code>:</p> <pre><code>[06/04/24 09:52:53] INFO     Kedro project spaceflights-mlflow                                     session.py:324\n                    INFO     Registering new custom resolver: 'km.random_name'                  mlflow_hook.py:65\n                    INFO     The 'tracking_uri' key in mlflow.yml is relative          kedro_mlflow_config.py:260\n                             ('server.mlflow_(tracking|registry)_uri = mlflow_runs').\n                             It is converted to a valid uri:\n                             'file:///Users/juan_cano/Projects/QuantumBlackLabs/kedro-\n                             mlflow-playground/spaceflights-mlflow/mlflow_runs'\n</code></pre> <p>If you open your tracking server UI you will observe a result like this:</p> <p></p> <p>Notice that <code>kedro-mlflow</code> used a subset of the <code>run_params</code> as tags for the MLflow run, and logged the Kedro parameters as MLflow parameters.</p> <p>Check out the official kedro-mlflow tutorial for more detailed steps.</p>"},{"location":"pages/integrations/mlflow/#artifact-tracking-in-mlflow-using-kedro-mlflow","title":"Artifact tracking in MLflow using <code>kedro-mlflow</code>","text":"<p><code>kedro-mlflow</code> provides some out-of-the-box artifact tracking capabilities that connect your Kedro project with your MLflow deployment, such as <code>MlflowArtifactDataset</code>, which can be used to wrap any of your existing Kedro datasets.</p> <p>Use of this dataset has the advantage that the preview capabilities of the MLflow UI can be used.</p> <p>Warning This will work for datasets that are outputs of a node, and will have no effect for datasets that are free inputs (hence are only loaded).</p> <p>For example, if you modify the a <code>matplotlib.MatplotlibWriter</code> dataset like this:</p> <pre><code> # conf/base/catalog.yml\n\n dummy_confusion_matrix:\n-  type: matplotlib.MatplotlibWriter\n-  filepath: data/08_reporting/dummy_confusion_matrix.png\n-  versioned: true\n+  type: kedro_mlflow.io.artifacts.MlflowArtifactDataset\n+  dataset:\n+    type: matplotlib.MatplotlibWriter\n+    filepath: data/08_reporting/dummy_confusion_matrix.png\n</code></pre> <p>Then the image would be logged as part of the artifacts of the run and you would be able to preview it in the MLflow web UI:</p> <p></p> <p>Warning If you get a <code>Failed while saving data to dataset MlflowMatplotlibWriter</code> error, it's probably because you had already executed <code>kedro run</code> while the dataset was marked as <code>versioned: true</code>. The solution is to clean up the old <code>data/08_reporting/dummy_confusion_matrix.png</code> directory.</p> <p>Check out {external+kedro-mlflow:doc}<code>the official kedro-mlflow documentation on versioning Kedro datasets &lt;source/04_experimentation_tracking/03_version_datasets&gt;</code> for more information.</p>"},{"location":"pages/integrations/mlflow/#model-registry-in-mlflow-using-kedro-mlflow","title":"Model registry in MLflow using <code>kedro-mlflow</code>","text":"<p>If your Kedro pipeline trains a machine learning model, you can track those models in MLflow so that you can manage and deploy them. The <code>kedro-mlflow</code> plugin introduces a special artifact, <code>MlflowModelTrackingDataset</code>, that you can use to load and save your models as MLflow artifacts.</p> <p>For example, if you have a dataset corresponding to a scikit-learn model, you can modify it as follows:</p> <pre><code> regressor:\n-  type: pickle.PickleDataset\n-  filepath: data/06_models/regressor.pickle\n-  versioned: true\n+  type: kedro_mlflow.io.models.MlflowModelTrackingDataset\n+  flavor: mlflow.sklearn\n</code></pre> <p>The <code>kedro-mlflow</code> Hook will log the model as part of the run in {external+mlflow:doc}<code>the standard MLflow Model format &lt;models&gt;</code>.</p> <p>If you also want to register it (hence store it in the MLflow Model Registry) you can add a <code>registered_model_name</code> parameter:</p> <pre><code>regressor:\n  type: kedro_mlflow.io.models.MlflowModelTrackingDataset\n  flavor: mlflow.sklearn\n  save_args:\n    registered_model_name: spaceflights-regressor\n</code></pre> <p>Then you will see it listed as a Registered Model:</p> <p></p> <p>To load a model from a specific run, you can specify the <code>run_id</code>. For that, you can make use of {ref}<code>runtime parameters &lt;runtime-params&gt;</code>:</p> <pre><code># Add the intermediate datasets to run only the inference\nX_test:\n  type: pandas.ParquetDataset\n  filepath: data/05_model_input/X_test.parquet\n\ny_test:\n  type: pandas.CSVDataset  # https://github.com/pandas-dev/pandas/issues/54638\n  filepath: data/05_model_input/y_test.csv\n\nregressor:\n  type: kedro_mlflow.io.models.MlflowModelTrackingDataset\n  flavor: mlflow.sklearn\n  run_id: ${runtime_params:mlflow_run_id,null}\n  save_args:\n    registered_model_name: spaceflights-regressor\n</code></pre> <p>And specify the MLflow run id on the command line as follows:</p> <pre><code>$ kedro run --to-outputs=X_test,y_test\n...\n$ kedro run --from-nodes=evaluate_model_node --params mlflow_run_id=4cba84...\n</code></pre> <p>Note Notice that MLflow runs are immutable for reproducibility purposes, therefore you cannot save a model in an existing run.</p>"},{"location":"pages/integrations/mlflow/#advanced-use-cases","title":"Advanced use cases","text":""},{"location":"pages/integrations/mlflow/#track-additional-metadata-of-kedro-runs-in-mlflow-using-hooks","title":"Track additional metadata of Kedro runs in MLflow using Hooks","text":"<p>So far, <code>kedro-mlflow</code> has proven abundantly useful already. And yet, you might have the need to track additional metadata in the run.</p> <p>One possible way of doing it is using the {py:meth}<code>~kedro.framework.hooks.specs.PipelineSpecs.before_pipeline_run</code> Hook to log the <code>run_params</code> passed to the Hook. An implementation would look as follows:</p> <pre><code># src/spaceflights_mlflow/hooks.py\n\nimport typing as t\nimport logging\n\nimport mlflow\nfrom kedro.framework.hooks import hook_impl\n\nlogger = logging.getLogger(__name__)\n\n\nclass ExtraMLflowHooks:\n    @hook_impl\n    def before_pipeline_run(self, run_params: dict[str, t.Any]):\n        logger.info(\"Logging extra metadata to MLflow\")\n        mlflow.set_tags({\n            \"pipeline\": run_params[\"pipeline_name\"] or \"__default__\",\n            \"custom_version\": \"0.1.0\",\n        })\n</code></pre> <p>And then enable your custom hook in <code>settings.py</code>:</p> <pre><code># src/spaceflights_mlflow/settings.py\n...\nfrom .hooks import ExtraMLflowHooks\n\nHOOKS = (ExtraMLflowHooks(),)\n...\n</code></pre> <p>After enabling this custom Hook, you can execute <code>kedro run</code>, and see something like this in the logs:</p> <pre><code>...\n[06/04/24 10:44:25] INFO     Logging extra metadata to MLflow                                         hooks.py:13\n...\n</code></pre> <p>If you open your tracking server UI you will observe a result like this:</p> <p></p>"},{"location":"pages/integrations/mlflow/#tracking-kedro-in-mlflow-using-the-python-api","title":"Tracking Kedro in MLflow using the Python API","text":"<p>If you are running Kedro programmatically using the Python API, you can log your runs using the MLflow \"fluent\" API.</p> <p>For example, taking the {doc}<code>lifecycle management example &lt;/kedro_project_setup/session&gt;</code> as a starting point:</p> <pre><code>from pathlib import Path\n\nimport mlflow\nfrom kedro.framework.session import KedroSession\nfrom kedro.framework.startup import bootstrap_project\n\nbootstrap_project(Path.cwd())\n\nmlflow.set_experiment(\"Kedro Spaceflights test\")\n\nwith KedroSession.create() as session:\n    with mlflow.start_run():\n        mlflow.set_tag(\"session_id\", session.session_id)\n        session.run()\n</code></pre> <p>If you want more flexibility or to log extra parameters, you might need to run the Kedro pipelines manually yourself.</p>"},{"location":"pages/integrations/pyspark_integration/","title":"PySpark integration","text":"<p>This page outlines some best practices when building a Kedro pipeline with <code>PySpark</code>. It assumes a basic understanding of both Kedro and <code>PySpark</code>.</p>"},{"location":"pages/integrations/pyspark_integration/#centralise-spark-configuration-in-confbasesparkyml","title":"Centralise Spark configuration in <code>conf/base/spark.yml</code>","text":"<p>Spark allows you to specify many different configuration options. We recommend storing all of these options in a file located at <code>conf/base/spark.yml</code>. Below is an example of the content of the file to specify the <code>maxResultSize</code> of the Spark's driver and to use the <code>FAIR</code> scheduler:</p> <pre><code>spark.driver.maxResultSize: 3g\nspark.scheduler.mode: FAIR\n</code></pre> <p>Note Optimal configuration for Spark depends on the setup of your Spark cluster.</p>"},{"location":"pages/integrations/pyspark_integration/#initialise-a-sparksession-using-a-hook","title":"Initialise a <code>SparkSession</code> using a hook","text":"<p>Before any <code>PySpark</code> operations are performed, you should initialise your <code>SparkSession</code> using an <code>after_context_created</code> hook. This ensures that a <code>SparkSession</code> has been initialised before the Kedro pipeline is run.</p> <p>Below is an example implementation to initialise the <code>SparkSession</code> in <code>src/&lt;package_name&gt;/hooks.py</code> by reading configuration from the <code>spark.yml</code> configuration file created in the previous section:</p> <pre><code>from kedro.framework.hooks import hook_impl\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\n\nclass SparkHooks:\n    @hook_impl\n    def after_context_created(self, context) -&gt; None:\n        \"\"\"Initialises a SparkSession using the config\n        defined in project's conf folder.\n        \"\"\"\n\n        # Load the spark configuration in spark.yaml using the config loader\n        parameters = context.config_loader[\"spark\"]\n        spark_conf = SparkConf().setAll(parameters.items())\n\n        # Initialise the spark session\n        spark_session_conf = (\n            SparkSession.builder.appName(context.project_path.name)\n            .enableHiveSupport()\n            .config(conf=spark_conf)\n        )\n        _spark_session = spark_session_conf.getOrCreate()\n        _spark_session.sparkContext.setLogLevel(\"WARN\")\n</code></pre> <p>You should modify this code to adapt it to your cluster's setup, e.g. setting master to <code>yarn</code> if you are running Spark on YARN.</p> <p>Call <code>SparkSession.builder.getOrCreate()</code> to obtain the <code>SparkSession</code> anywhere in your pipeline. <code>SparkSession.builder.getOrCreate()</code> is a global singleton.</p> <p>We don't recommend storing Spark session on the context object, as it cannot be serialised and therefore prevents the context from being initialised for some plugins.</p> <p>You will also need to register <code>SparkHooks</code> by updating the <code>HOOKS</code> variable in <code>src/&lt;package_name&gt;/settings.py</code> as follows:</p> <pre><code>from &lt;package_name&gt;.hooks import SparkHooks\n\nHOOKS = (SparkHooks(),)\n</code></pre>"},{"location":"pages/integrations/pyspark_integration/#use-kedros-built-in-spark-datasets-to-load-and-save-raw-data","title":"Use Kedro's built-in Spark datasets to load and save raw data","text":"<p>We recommend using Kedro's built-in Spark datasets to load raw data into Spark's DataFrame, as well as to write them back to storage. Some of our built-in Spark datasets include:</p> <ul> <li>{class}<code>spark.DeltaTableDataset &lt;kedro-datasets:kedro_datasets.spark.DeltaTableDataset&gt;</code></li> <li>{class}<code>spark.SparkDataset &lt;kedro-datasets:kedro_datasets.spark.SparkDataset&gt;</code></li> <li>{class}<code>spark.SparkJDBCDataset &lt;kedro-datasets:kedro_datasets.spark.SparkJDBCDataset&gt;</code></li> <li>{class}<code>spark.SparkHiveDataset &lt;kedro-datasets:kedro_datasets.spark.SparkHiveDataset&gt;</code></li> </ul> <p>The example below illustrates how to use <code>spark.SparkDataset</code> to read a CSV file located in S3 into a <code>DataFrame</code> in <code>conf/base/catalog.yml</code>:</p> <pre><code>weather:\n  type: spark.SparkDataset\n  filepath: s3a://your_bucket/data/01_raw/weather-\n  file_format: csv\n  load_args:\n    header: True\n    inferSchema: True\n  save_args:\n    sep: '|'\n    header: True\n</code></pre> <p>Or using the Python API:</p> <pre><code>import pyspark.sql\nfrom kedro.io import DataCatalog\nfrom kedro_datasets.spark import SparkDataset\n\nspark_ds = SparkDataset(\n    filepath=\"s3a://your_bucket/data/01_raw/weather*\",\n    file_format=\"csv\",\n    load_args={\"header\": True, \"inferSchema\": True},\n    save_args={\"sep\": \"|\", \"header\": True},\n)\ncatalog = DataCatalog({\"weather\": spark_ds})\n\ndf = catalog.load(\"weather\")\nassert isinstance(df, pyspark.sql.DataFrame)\n</code></pre>"},{"location":"pages/integrations/pyspark_integration/#spark-and-delta-lake-interaction","title":"Spark and Delta Lake interaction","text":"<p>Delta Lake is an open-source project that enables building a Lakehouse architecture on top of data lakes. It provides ACID transactions and unifies streaming and batch data processing on top of existing data lakes, such as S3, ADLS, GCS, and HDFS. To setup PySpark with Delta Lake, have a look at the recommendations in Delta Lake's documentation. You may have to update the <code>SparkHooks</code> in your <code>src/&lt;package_name&gt;/hooks.py</code> to set up the <code>SparkSession</code> with Delta Lake support:</p> <pre><code>from kedro.framework.hooks import hook_impl\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n+ from delta import configure_spark_with_delta_pip\n\nclass SparkHooks:\n    @hook_impl\n    def after_context_created(self, context) -&gt; None:\n        \"\"\"Initialises a SparkSession using the config\n        defined in project's conf folder.\n        \"\"\"\n\n        # Load the spark configuration in spark.yaml using the config loader\n        parameters = context.config_loader[\"spark\"]\n        spark_conf = SparkConf().setAll(parameters.items())\n\n        # Initialise the spark session\n        spark_session_conf = (\n            SparkSession.builder.appName(context.project_path.name)\n            .enableHiveSupport()\n            .config(conf=spark_conf)\n        )\n-       _spark_session = spark_session_conf.getOrCreate()\n+       _spark_session = configure_spark_with_delta_pip(spark_session_conf).getOrCreate()\n        _spark_session.sparkContext.setLogLevel(\"WARN\")\n</code></pre> <p>Refer to the more detailed section on Kedro and Delta Lake integration in the Delta Lake integration guide.</p>"},{"location":"pages/integrations/pyspark_integration/#use-memorydataset-for-intermediary-dataframe","title":"Use <code>MemoryDataset</code> for intermediary <code>DataFrame</code>","text":"<p>For nodes operating on <code>DataFrame</code> that doesn't need to perform Spark actions such as writing the <code>DataFrame</code> to storage, we recommend using the default <code>MemoryDataset</code> to hold the <code>DataFrame</code>. In other words, there is no need to specify it in the <code>DataCatalog</code> or <code>catalog.yml</code>. This allows you to take advantage of Spark's optimiser and lazy evaluation.</p>"},{"location":"pages/integrations/pyspark_integration/#use-memorydataset-with-copy_modeassign-for-non-dataframe-spark-objects","title":"Use <code>MemoryDataset</code> with <code>copy_mode=\"assign\"</code> for non-<code>DataFrame</code> Spark objects","text":"<p>Sometimes, you might want to use Spark objects that aren't <code>DataFrame</code> as inputs and outputs in your pipeline. For example, suppose you have a <code>train_model</code> node to train a classifier using Spark ML's <code>RandomForrestClassifier</code> and a <code>predict</code> node to make predictions using this classifier. In this scenario, the <code>train_model</code> node will output a <code>RandomForestClassifier</code> object, which then becomes the input for the <code>predict</code> node. Below is the code for this pipeline:</p> <pre><code>from typing import Any, Dict\n\nfrom kedro.pipeline import node, pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.sql import DataFrame\n\n\ndef train_model(training_data: DataFrame) -&gt; RandomForestClassifier:\n    \"\"\"Node for training a random forest model to classify the data.\"\"\"\n    classifier = RandomForestClassifier(numTrees=10)\n    return classifier.fit(training_data)\n\n\ndef predict(model: RandomForestClassifier, testing_data: DataFrame) -&gt; DataFrame:\n    \"\"\"Node for making predictions given a pre-trained model and a testing dataset.\"\"\"\n    predictions = model.transform(testing_data)\n    return predictions\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        [\n            node(train_model, inputs=[\"training_data\"], outputs=\"example_classifier\"),\n            node(\n                predict,\n                inputs=dict(model=\"example_classifier\", testing_data=\"testing_data\"),\n                outputs=\"example_predictions\",\n            ),\n        ]\n    )\n</code></pre> <p>To make the pipeline work, you will need to specify <code>example_classifier</code> as follows in the <code>catalog.yml</code>:</p> <pre><code>example_classifier:\n  type: MemoryDataset\n  copy_mode: assign\n</code></pre> <p>The <code>assign</code> copy mode ensures that the <code>MemoryDataset</code> will be assigned the Spark object itself, not a deep copy version of it, since deep copy doesn't work with Spark object generally.</p>"},{"location":"pages/integrations/pyspark_integration/#tips-for-maximising-concurrency-using-threadrunner","title":"Tips for maximising concurrency using <code>ThreadRunner</code>","text":"<p>Under the hood, every Kedro node that performs a Spark action (e.g. <code>save</code>, <code>collect</code>) is submitted to the Spark cluster as a Spark job through the same <code>SparkSession</code> instance. These jobs may be running concurrently if they were submitted by different threads. In order to do that, you will need to run your Kedro pipeline with the {py:class}<code>~kedro.runner.ThreadRunner</code>:</p> <pre><code>kedro run --runner=ThreadRunner\n</code></pre> <p>To further increase the concurrency level, if you are using Spark &gt;= 0.8, you can also give each node a roughly equal share of the Spark cluster by turning on fair sharing and therefore giving them a roughly equal chance of being executed concurrently. By default, they are executed in a FIFO manner, which means if a job takes up too much resources, it could hold up the execution of other jobs. In order to turn on fair sharing, put the following in your <code>conf/base/spark.yml</code> file, which was created in the Initialise a <code>SparkSession</code> section:</p> <pre><code>spark.scheduler.mode: FAIR\n</code></pre> <p>For more information, see the Spark documentation on jobs scheduling within an application.</p>"},{"location":"pages/introduction/","title":"Introduction to Kedro","text":"<p>Kedro is an open-source Python framework to create reproducible, maintainable, and modular data science code. It uses software engineering best practices to help you build production-ready data science pipelines.</p> <p>Kedro is hosted by the LF AI &amp; Data Foundation, and you can find the Kedro source code on GitHub.</p> <p>In the following chapters, you will learn how to set up Kedro and discover the key Kedro concepts. You can then review the spaceflights tutorial to get hands-on experience with a Kedro project.</p> <p>For new and intermediate Kedro users, there's a comprehensive section on working with Kedro and Jupyter notebooks.</p> <p>Use the left-hand table of contents to explore the documentation available for more advanced Kedro usage and deployment. We also recommend the glossary and the API reference documentation.</p> <p>Note We have designed the preliminary documentation and the spaceflights tutorial for anyone new to Kedro. The more knowledge of Python you have, the easier you will find the learning curve.  </p> <p>There are many excellent online resources for learning Python; you should choose those that reference Python 3, as Kedro is built for Python 3.9+. There are curated lists of online resources, such as the official Python programming language website and this list of free programming books and tutorials.</p>"},{"location":"pages/kedro_project_setup/","title":"Project setup","text":"<ul> <li>Dependencies</li> <li>Session</li> <li>Settings</li> </ul>"},{"location":"pages/kedro_project_setup/dependencies/","title":"Dependencies","text":"<p>Both <code>pip install kedro</code> and <code>conda install -c conda-forge kedro</code> install the core Kedro module, which includes the CLI tool, project template, pipeline abstraction, framework, and support for configuration.</p> <p>When you create a project, you then introduce additional dependencies for the tasks it performs.</p>"},{"location":"pages/kedro_project_setup/dependencies/#declare-project-specific-dependencies","title":"Declare project-specific dependencies","text":"<p>When you create a new Kedro project, Kedro generates a <code>requirements.txt</code> file in the root directory of the project. The file contains the core dependencies and those related to the tools you choose to include in the project. Specifying the project's exact dependencies in a <code>requirements.txt</code> file makes it easier to run the project in the future, and avoids version conflicts downstream.</p>"},{"location":"pages/kedro_project_setup/dependencies/#install-project-specific-dependencies","title":"Install project-specific dependencies","text":"<p>When someone clones your project, they can install the project-specific dependencies by navigating to the root directory of the project and running the following command:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"pages/kedro_project_setup/dependencies/#install-dependencies-related-to-the-data-catalog","title":"Install dependencies related to the Data Catalog","text":"<p>The Data Catalog is your way of interacting with different data types in Kedro. You can use <code>kedro-datasets</code> to interact with the data used in your projects. Depending on the datasets that you use in your Data Catalog, you might need to include additional dependencies in your <code>requirements.txt</code>. The modular dependencies in this category include <code>pandas</code>, <code>numpy</code>, <code>pyspark</code>, <code>matplotlib</code>, <code>pillow</code>, <code>dask</code>, and more.</p>"},{"location":"pages/kedro_project_setup/dependencies/#install-dependencies-at-a-group-level","title":"Install dependencies at a group-level","text":"<p>Data types are broken into groups e.g. <code>pandas</code>, <code>spark</code> and <code>pickle</code>. Each group has a collection of data types e.g.<code>pandas.CSVDataset</code>, <code>pandas.ParquetDataset</code> and more. You can install dependencies for an entire group of dependencies as follows:</p> <pre><code>pip install \"kedro-datasets[&lt;group&gt;]\"\n</code></pre> <p>This installs Kedro and dependencies related to the data type group. An example of this could be a workflow that depends on the data types in <code>pandas</code>. Run <code>pip install \"kedro-datasets[pandas]\"</code> to install Kedro and the dependencies for the data types in the <code>pandas</code> group.</p>"},{"location":"pages/kedro_project_setup/dependencies/#install-dependencies-at-a-type-level","title":"Install dependencies at a type-level","text":"<p>To limit installation to dependencies specific to a data type:</p> <pre><code>pip install \"kedro-datasets[&lt;group&gt;-&lt;dataset&gt;]\"\n</code></pre> <p>For example, your workflow might require the <code>pandas.ExcelDataset</code>, so to install its dependencies, run <code>pip install \"kedro-datasets[pandas-exceldataset]\"</code>.</p> <pre><code>From `kedro-datasets` version 3.0.0 onwards, the names of the optional dataset-level dependencies have been normalised to follow [PEP 685](https://peps.python.org/pep-0685/). The '.' character has been replaced with a '-' character and the names are in lowercase. For example, if you had `kedro-datasets[pandas.ExcelDataset]` in your requirements file, it would have to be changed to `kedro-datasets[pandas-exceldataset]`.\n</code></pre>"},{"location":"pages/kedro_project_setup/dependencies/#reproducible-environments","title":"Reproducible environments","text":"<p>To ensure that the project dependencies and the transitive dependencies are pinned to specific versions, use <code>pip-tools</code> to compile <code>requirements.txt</code> file into a <code>requirements.lock</code> file. To install <code>pip-tools</code> in your virtual environment, run the following command:</p> <pre><code>pip install pip-tools\n</code></pre> <p>To add or remove dependencies to a project, edit the <code>requirements.txt</code> file, then run the following:</p> <pre><code>pip-compile &lt;project_root&gt;/requirements.txt --output-file &lt;project_root&gt;/requirements.lock\n</code></pre> <p>This will pip compile the requirements listed in the <code>requirements.txt</code> file into a <code>requirements.lock</code> that specifies a list of pinned project dependencies(those with a strict version). You can also use this command with additional CLI arguments such as <code>--generate-hashes</code> to use <code>pip</code>'s Hash Checking Mode or <code>--upgrade-package</code> to update specific packages to the latest or specific versions. Check out the <code>pip-tools</code> documentation for more information.</p> <pre><code>The `requirements.txt` file contains \"source\" requirements, while `requirements.lock` contains the compiled version of those and requires no manual updates. If you need to update the dependencies, update the `requirements.txt` file and re-run the `pip-compile` command.\n</code></pre>"},{"location":"pages/kedro_project_setup/session/","title":"Lifecycle management with <code>KedroSession</code>","text":""},{"location":"pages/kedro_project_setup/session/#overview","title":"Overview","text":"<p>A <code>KedroSession</code> allows you to:</p> <ul> <li>Manage the lifecycle of a Kedro run</li> <li>Persist runtime parameters with corresponding session IDs</li> <li>Traceback runtime parameters, such as CLI command flags and environment variables</li> </ul> <p><code>KedroSession</code> decouples Kedro's library components, managed by <code>KedroContext</code>, and any session data (both static and dynamic data). As a result, Kedro components and plugins can access session data without the need to import the <code>KedroContext</code> object and library components.</p> <p>The main methods and properties of <code>KedroSession</code> are:</p> <ul> <li><code>create()</code>: Create a new instance of <code>KedroSession</code> with  session data</li> <li><code>load_context()</code>: Instantiate <code>KedroContext</code> object</li> <li><code>close()</code>: Close the current session \u2014 although we recommend that you use the session object as a context manager, which will call <code>close()</code> automatically, as opposed to calling the method explicitly</li> <li><code>run()</code>: Run the pipeline with the arguments provided; see  Running pipelines for details</li> </ul>"},{"location":"pages/kedro_project_setup/session/#create-a-session","title":"Create a session","text":"<p>The following code creates a <code>KedroSession</code> object as a context manager and runs a pipeline inside the context, with session data provided. The session automatically closes after exit:</p> <pre><code>from kedro.framework.session import KedroSession\nfrom kedro.framework.startup import bootstrap_project\nfrom pathlib import Path\n\nbootstrap_project(Path.cwd())\nwith KedroSession.create() as session:\n    session.run()\n</code></pre> <p>You can provide the following optional arguments in <code>KedroSession.create()</code>:</p> <ul> <li><code>project_path</code>: Path to the project root directory</li> <li><code>save_on_close</code>: A boolean value to indicate whether or not to save the session to disk when it's closed</li> <li><code>env</code>: Environment for the <code>KedroContext</code></li> <li><code>extra_params</code>: Optional dictionary containing extra project parameters for the underlying <code>KedroContext</code>; if specified, this will update (and therefore take precedence over) parameters retrieved from the project configuration</li> </ul>"},{"location":"pages/kedro_project_setup/session/#bootstrap_project-and-configure_project","title":"<code>bootstrap_project</code> and <code>configure_project</code>","text":"<p>% Mermaid code, see https://github.com/kedro-org/kedro/wiki/Render-Mermaid-diagrams % graph LR %  subgraph Kedro Startup Flowchart %    A[bootstrap_project] --&gt;|Read pyproject.toml| B %    A --&gt;|Add project root to sys.path| B[configure_project] %    C[Initialize KedroSession] %    B --&gt; |Read settings.py| C %    B --&gt; |Read pipeline_registry.py| C %  end</p> <p>Both <code>bootstrap_project</code> and <code>configure_project</code> handle the setup of a Kedro project, but there are subtle differences: <code>bootstrap_project</code> is used for project mode, and <code>configure_project</code> is used for packaged mode.</p> <p>Kedro's CLI runs the functions at startup as part of <code>kedro run</code> so in most cases you don't need to call these functions. If you want to interact with a Kedro project programmatically in an interactive session such as Notebook, use <code>%reload_kedro</code> line magic with Jupyter or IPython.</p>"},{"location":"pages/kedro_project_setup/session/#bootstrap_project","title":"<code>bootstrap_project</code>","text":"<p>This function uses <code>configure_project</code>, and additionally reads metadata from <code>pyproject.toml</code> and adds the project root to <code>sys.path</code> so the project can be imported as a Python package. It is typically used to work directly with the source code of a Kedro project.</p>"},{"location":"pages/kedro_project_setup/session/#configure_project","title":"<code>configure_project</code>","text":"<p>This function reads <code>settings.py</code> and <code>pipeline_registry.py</code> and registers the configuration before Kedro's run starts. If you have a packaged Kedro project, you only need to run <code>configure_project</code> before executing your pipeline.</p>"},{"location":"pages/kedro_project_setup/session/#valueerror-package-name-not-found","title":"ValueError: Package name not found","text":"<p>ValueError: Package name not found. Make sure you have configured the project using 'bootstrap_project'. This should happen automatically if you are using Kedro command line interface.</p> <p>If you are using <code>multiprocessing</code>, you need to be careful about this. Depending on your Operating System, you may have different default. If the processes are <code>spawn</code>, Python will re-import all the modules in each process and thus you need to run <code>configure_project</code> again at the start of the new process. For example, this is how Kedro handles this in <code>ParallelRunner</code>:</p> <pre><code>if multiprocessing.get_start_method() == \"spawn\" and package_name:\n        _bootstrap_subprocess(package_name, logging_config)\n</code></pre>"},{"location":"pages/kedro_project_setup/settings/","title":"Project settings","text":""},{"location":"pages/kedro_project_setup/settings/#application-settings","title":"Application settings","text":"<p>A Kedro project's <code>settings.py</code> file contains the application settings for the project, including registration of Hooks and library components. This page explains how settings work, and which settings are available.</p> <pre><code>Application settings is distinct from [run time configuration](../configuration/configuration_basics.md), which is stored in the `conf` folder and can vary by configuration environment, and [pyproject.toml](#project-metadata) , which provides project metadata and build configuration.\n</code></pre> <p>By default, all code in <code>settings.py</code> is commented out. When settings are not supplied, Kedro chooses sensible default values. You only need to edit <code>settings.py</code> if you wish to change to values other than the defaults.</p> Setting Default value Use <code>HOOKS</code> <code>tuple()</code> Inject additional behaviour into the execution timeline with project Hooks. <code>DISABLE_HOOKS_FOR_PLUGINS</code> <code>tuple()</code> Disable auto-registration of Hooks from plugins. <code>SESSION_STORE_CLASS</code> <code>kedro.framework.session.session.BaseSessionStore</code> Customise how session data is stored. <code>SESSION_STORE_ARGS</code> <code>dict()</code> Keyword arguments for the <code>SESSION_STORE_CLASS</code> constructor. <code>CONTEXT_CLASS</code> <code>kedro.framework.context.KedroContext</code> Customise how Kedro library components are managed. <code>CONF_SOURCE</code> <code>\"conf\"</code> Directory that holds configuration. <code>CONFIG_LOADER_CLASS</code> <code>kedro.config.ConfigLoader</code> Customise how project configuration is handled. <code>CONFIG_LOADER_ARGS</code> <code>dict()</code> Keyword arguments for the <code>CONFIG_LOADER_CLASS</code> constructor. <code>DATA_CATALOG_CLASS</code> <code>kedro.io.DataCatalog</code> Customise how the Data Catalog is handled."},{"location":"pages/kedro_project_setup/settings/#project-metadata","title":"Project metadata","text":"<p>The <code>pyproject.toml</code> file is the standard way to store build metadata and tool settings for Python projects. Every Kedro project comes with a default pre-populated <code>pyproject.toml</code> file in your project root directory with the following keys specified under the <code>[tool.kedro]</code> section:</p> <pre><code>[tool.kedro]\npackage_name = \"package_name\"\nproject_name = \"project_name\"\nkedro_init_version = \"kedro_version\"\ntools = \"\"\nexample_pipeline = \"False\"\nsource_dir = \"src\"\n</code></pre> <p>The <code>package_name</code> should be a valid Python package name and the <code>project_name</code> should be a human-readable name. They are both mandatory keys for your project. <code>kedro_init_version</code> specifies the version of Kedro the project was created with. When you upgrade to a newer Kedro version, this value should also be updated.</p> <p>You can also use <code>pyproject.toml</code> to specify settings for functionalities such as micro-packaging. You can also store the settings for the other tools you've used in your project, such as <code>pytest</code> for automated testing. Consult the respective documentation for the tools you have used to check how you can configure the settings with the <code>pyproject.toml</code> file for your project.</p>"},{"location":"pages/kedro_project_setup/settings/#use-kedro-without-the-src-folder","title":"Use Kedro without the <code>src</code> folder","text":"<p>Kedro uses the <code>src</code> layout by default. It is possible to change this, for example, to use a flat layout, you can change the <code>pyproject.toml</code> as follow.</p> <pre><code>+++ source_dir = \"\"\n--- source_dir = \"src\"\n</code></pre>"},{"location":"pages/logging/","title":"Logging","text":"<p>Kedro uses Python's <code>logging</code> library. Configuration is provided as a dictionary according to the Python logging configuration schema in Kedro's default logging configuration, as described below.</p> <p>By default, Python only shows logging messages at level <code>WARNING</code> and above. Kedro's logging configuration specifies that <code>INFO</code> level messages from Kedro should also be emitted. This makes it easier to track the progress of your pipeline when you perform a <code>kedro run</code>.</p>"},{"location":"pages/logging/#default-logging-configuration","title":"Default logging configuration","text":"<p>Kedro's default logging configuration defines a handler called <code>rich</code> that uses the Rich logging handler to format messages. We also use the Rich traceback handler to render exceptions.</p>"},{"location":"pages/logging/#how-to-perform-logging-in-your-kedro-project","title":"How to perform logging in your Kedro project","text":"<p>To add logging to your own code (e.g. in a node):</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__)\nlogger.warning(\"Issue warning\")\nlogger.info(\"Send information\")\nlogger.debug(\"Useful information for debugging\")\n</code></pre> <p>You can use Rich's console markup in your logging calls:</p> <pre><code>logger.error(\"[bold red blink]Important error message![/]\", extra={\"markup\": True})\n</code></pre>"},{"location":"pages/logging/#how-to-customise-kedro-logging","title":"How to customise Kedro logging","text":"<p>To customise logging in your Kedro project, you need to specify the path to a project-specific logging configuration file. Change the environment variable <code>KEDRO_LOGGING_CONFIG</code> to override the default logging configuration. Point the variable instead to your project-specific configuration, which we recommend you store inside the project's<code>conf</code> folder, and name <code>logging.yml</code>.</p> <p>For example, you can set <code>KEDRO_LOGGING_CONFIG</code> by typing the following into your terminal:</p> <pre><code>export KEDRO_LOGGING_CONFIG=&lt;project_root&gt;/conf/logging.yml\n</code></pre> <p>After setting the environment variable, any subsequent Kedro commands use the logging configuration file at the specified path.</p> <pre><code>If the `KEDRO_LOGGING_CONFIG` environment variable is not set, Kedro will use the [default logging configuration](https://github.com/kedro-org/kedro/blob/main/kedro/framework/project/default_logging.yml).\n</code></pre>"},{"location":"pages/logging/#change-the-verbosity-of-specific-parts-of-kedro","title":"Change the verbosity of specific parts of Kedro","text":"<p>You can also customise logging at runtime and redefine the logging configuration provided in the <code>logging.yml</code> when using jupyter notebook. The example below demonstrates how you can change the logging level from default <code>INFO</code> to <code>WARNING</code> for the <code>kedro.io.data_catalog</code> component logger specifically, the logging for the rest of the components will remain unchanged. The same can be done for higher/lower-level components without affecting the top-level.</p> <p>Add the following to a cell in your notebook:</p> <pre><code>import logging\n\n\nlogging.getLogger(\"kedro.io.data_catalog\").setLevel(logging.WARNING)\n</code></pre>"},{"location":"pages/logging/#custom-conf_source-with-logging","title":"Custom <code>CONF_SOURCE</code> with logging","text":"<p>When you customise the <code>CONF_SOURCE</code> setting in your Kedro project, it determines where Kedro looks for configuration files, including the logging configuration file. However, changing <code>CONF_SOURCE</code> does not automatically update the path to <code>logging.yml</code>. To use a custom location or filename for the logging configuration, you must explicitly set the <code>KEDRO_LOGGING_CONFIG</code> environment variable.</p> <p>By default, Kedro looks for a file named <code>logging.yml</code> in the <code>conf</code> directory. If you move or rename your logging configuration file after changing <code>CONF_SOURCE</code>, specify the new path using the <code>KEDRO_LOGGING_CONFIG</code> environment variable:</p> <pre><code>export KEDRO_LOGGING_CONFIG=&lt;project_root&gt;/custom_config_folder/custom_logging_name.yml\n</code></pre> <p>Please note that adjusting <code>CONF_SOURCE</code> or renaming <code>logging.yml</code> without updating the logging configuration accordingly can lead to Kedro not locating the file, which will result in the default logging settings being used instead.</p>"},{"location":"pages/logging/#how-to-show-debug-level-messages","title":"How to show DEBUG level messages","text":"<p>To see <code>DEBUG</code> level messages, change the level of logging in your project-specific logging configuration file (<code>logging.yml</code>). We provide a <code>logging.yml</code> template:</p> Click to expand the <code>logging.yml</code> template <code> <pre><code>version: 1\n\ndisable_existing_loggers: False\n\nformatters:\n  simple:\n    format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\nhandlers:\n  console:\n    class: logging.StreamHandler\n    level: INFO\n    formatter: simple\n    stream: ext://sys.stdout\n\n  info_file_handler:\n    class: logging.handlers.RotatingFileHandler\n    level: INFO\n    formatter: simple\n    filename: info.log\n    maxBytes: 10485760 # 10MB\n    backupCount: 20\n    encoding: utf8\n    delay: True\n\n  rich:\n    class: kedro.logging.RichHandler\n    rich_tracebacks: True\n    # Advance options for customisation.\n    # See https://docs.kedro.org/en/stable/logging/index.html#how-to-perform-logging-in-your-kedro-project\n    # tracebacks_show_locals: False\n\nloggers:\n  kedro:\n    level: INFO\n\n  your_python_package:\n    level: INFO\n\nroot:\n  handlers: [rich]\n</code></pre> </code> <p>You need to change the line:</p> <pre><code>loggers:\n  kedro:\n    level: INFO\n\n  your_python_package:\n-   level: INFO\n+   level: DEBUG\n</code></pre> <pre><code>The name of a logger corresponds to a key in the `loggers` section of the logging configuration file (e.g. `kedro`). See [Python's logging documentation](https://docs.python.org/3/library/logging.html#logger-objects) for more information.\n</code></pre> <p>By changing the level value to <code>DEBUG</code> for the desired logger (e.g. <code>&lt;your_python_package&gt;</code>), you will start seeing <code>DEBUG</code> level messages in the log output.</p>"},{"location":"pages/logging/#advanced-logging","title":"Advanced logging","text":"<p>In addition to the <code>rich</code> handler defined in Kedro's framework, we provide two additional handlers in the template.</p> <ul> <li><code>console</code>: show logs on standard output (typically your terminal screen) without any rich formatting</li> <li><code>info_file_handler</code>: write logs of level <code>INFO</code> and above to <code>info.log</code></li> </ul> <p>The following section illustrates some common examples of how to change your project's logging configuration.</p>"},{"location":"pages/logging/#how-to-customise-the-rich-handler","title":"How to customise the <code>rich</code> handler","text":"<p>Kedro's <code>kedro.logging.RichHandler</code> is a subclass of <code>rich.logging.RichHandler</code> and supports the same set of arguments. By default, <code>rich_tracebacks</code> is set to <code>True</code> to use <code>rich</code> to render exceptions. However, you can disable it by setting <code>rich_tracebacks: False</code>.</p> <pre><code>If you want to disable `rich`'s tracebacks, you must set `KEDRO_LOGGING_CONFIG` to point to your local config i.e. `conf/logging.yml`.\n</code></pre> <p>When <code>rich_tracebacks</code> is set to <code>True</code>, the configuration is propagated to <code>rich.traceback.install</code>. If an argument is compatible with <code>rich.traceback.install</code>, it will be passed to the traceback's settings.</p> <p>For instance, you can enable the display of local variables inside <code>logging.yml</code> to aid with debugging.</p> <pre><code>  rich:\n    class: kedro.logging.RichHandler\n    rich_tracebacks: True\n+   tracebacks_show_locals: True\n</code></pre> <p>A comprehensive list of available options can be found in the RichHandler documentation.</p>"},{"location":"pages/logging/#how-to-enable-file-based-logging","title":"How to enable file-based logging","text":"<p>File-based logging in Python projects aids troubleshooting and debugging. It offers better visibility into application's behaviour and it's easy to search. However, it does not work well with read-only systems such as Databricks Repos.</p> <p>To enable file-based logging,  add <code>info_file_handler</code> in your <code>root</code> logger as follows in your <code>conf/logging.yml</code> as follows:</p> <pre><code> root:\n-  handlers: [rich]\n+  handlers: [rich, info_file_handler]\n</code></pre> <p>By default it only tracks <code>INFO</code> level messages, but it can be configured to capture any level of logs.</p>"},{"location":"pages/logging/#how-to-use-plain-console-logging","title":"How to use plain console logging","text":"<p>To use plain rather than rich logging, swap the <code>rich</code> handler for the <code>console</code> one as follows:</p> <pre><code> root:\n-  handlers: [rich]\n+  handlers: [console]\n</code></pre>"},{"location":"pages/logging/#how-to-enable-rich-logging-in-a-dumb-terminal","title":"How to enable rich logging in a dumb terminal","text":"<p>Rich detects whether your terminal is capable of displaying richly formatted messages. If your terminal is \"dumb\" then formatting is automatically stripped out so that the logs are just plain text. This is likely to happen if you perform <code>kedro run</code> on CI (e.g. GitHub Actions or CircleCI).</p> <p>If you find that the default wrapping of the log messages is too narrow but do not wish to switch to using the <code>console</code> logger on CI then the simplest way to control the log message wrapping is through altering the <code>COLUMNS</code> and <code>LINES</code> environment variables. For example:</p> <pre><code>export COLUMNS=120 LINES=25\n</code></pre> <pre><code>You must provide a value for both `COLUMNS` and `LINES` even if you only wish to change the width of the log message. Rich's default values for these variables are `COLUMNS=80` and `LINE=25`.\n</code></pre>"},{"location":"pages/logging/#how-to-enable-rich-logging-in-jupyter","title":"How to enable rich logging in Jupyter","text":"<p>Rich also formats the logs in JupyterLab and Jupyter Notebook. The size of the output console does not adapt to your window but can be controlled through the <code>JUPYTER_COLUMNS</code> and <code>JUPYTER_LINES</code> environment variables. The default values (115 and 100 respectively) should be suitable for most users, but if you require a different output console size then you should alter the values of <code>JUPYTER_COLUMNS</code> and <code>JUPYTER_LINES</code>.</p>"},{"location":"pages/logging/#how-to-use-logging-without-the-rich-library","title":"How to use logging without the rich library","text":"<p>If you prefer not to have the <code>rich</code> library in your Kedro project, you have the option to uninstall it. However, it's important to note that versions of the <code>cookiecutter</code> library above 2.3 have a dependency on rich. You will need to downgrade <code>cookiecutter</code> to a version below 2.3 to have Kedro work without <code>rich</code>.</p> <p>To uninstall the rich library, run:</p> <pre><code>pip uninstall rich\n</code></pre> <p>To downgrade cookiecutter to a version that does not require rich, you can specify a version below 2.3. For example:</p> <pre><code>pip install cookiecutter==2.2.0\n</code></pre> <p>These changes will affect the visual appearance and formatting of Kedro's logging, prompts, and the output of the <code>kedro ipython</code> command. While using a version of <code>cookiecutter</code> below 2.3, the appearance of the prompts will be plain even with <code>rich</code> installed.</p>"},{"location":"pages/nodes_and_pipelines/","title":"Nodes and pipelines","text":"<ul> <li>Nodes</li> <li>Pipeline Introduction</li> <li>Modular Pipelines</li> <li>Namespaces</li> <li>Pipeline Registry</li> <li>Micro-Packaging</li> <li>Run a Pipeline</li> <li>Slice a Pipeline</li> </ul>"},{"location":"pages/nodes_and_pipelines/micro_packaging/","title":"Micro-packaging","text":"<pre><code>_Micro-packaging is deprecated and will be removed from Kedro version 0.20.0._\n</code></pre> <p>Micro-packaging allows users to share Kedro micro-packages across codebases, organisations and beyond. A micro-package can be any part of Python code in a Kedro project including pipelines and utility functions.</p>"},{"location":"pages/nodes_and_pipelines/micro_packaging/#package-a-micro-package","title":"Package a micro-package","text":"<p>You can package a micro-package by executing: <code>kedro micropkg package &lt;micropkg_name&gt;</code>.</p> <p><code>&lt;micropkg_name&gt;</code> should be a Python module path like what would be used in an <code>import</code> statement, for example</p> <p><code>kedro micropkg package pipelines.data_processing</code></p> <ul> <li>This will generate a new source distribution for this micro-package.</li> <li>By default, the tar file will be saved into <code>dist/</code> directory inside your project.</li> <li>You can customise the target with the <code>--destination</code> (<code>-d</code>) option.</li> </ul> <p>When you package your micro-package, such as a modular pipeline for example, Kedro will also automatically package files from 3 locations:</p> <pre><code>\u251c\u2500\u2500 conf\n\u2502   \u2514\u2500\u2500 base\n\u2502       \u2514\u2500\u2500 parameters_{{pipeline_name*}}  &lt;-- All parameter file(s)\n\u251c\u2500\u2500 tests\n\u2502   \u251c\u2500\u2500 init__.py\n\u2502   \u2514\u2500\u2500 pipelines\n\u2502       \u2514\u2500\u2500 {{pipeline_name}}              &lt;-- Pipeline tests\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 my_project\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 pipelines\n            \u2514\u2500\u2500 {{pipeline_name}}          &lt;-- Pipeline folder\n</code></pre> <p>Kedro will also include any requirements found in <code>src/&lt;package_name&gt;/pipelines/&lt;micropkg_name&gt;/requirements.txt</code> in the micro-package tar file. These requirements will later be taken into account when pulling a micro-package via <code>kedro micropkg pull</code>.</p> <pre><code>Kedro will not package the catalog config files even if those are present in `conf/&lt;env&gt;/catalog_&lt;micropkg_name&gt;.yml`.\n</code></pre> <p>If you plan to publish your packaged micro-package to some Python package repository like PyPI, you need to make sure that your micro-package name doesn't clash with any of the existing packages in that repository. However, there is no need to rename any of your source files if that is the case. Simply alias your package with a new name by running <code>kedro micropkg package --alias &lt;new_package_name&gt; &lt;micropkg_name&gt;</code>.</p> <p>In addition to PyPI, you can also share the packaged tar file directly, or via a cloud storage such as AWS S3.</p>"},{"location":"pages/nodes_and_pipelines/micro_packaging/#package-multiple-micro-packages","title":"Package multiple micro-packages","text":"<p>To package multiple micro-packages in bulk, run <code>kedro micropkg package --all</code>. This will package all micro-packages specified in the <code>tool.kedro.micropkg.package</code> manifest section of the project's <code>pyproject.toml</code> file:</p> <pre><code>[tool.kedro.micropkg.package]\ncleaning_utils = {alias = \"aliased_util\", destination = \"somewhere/else\", env = \"uat\"}\nsecond_pipeline = {}\n</code></pre> <ul> <li>The keys (<code>first_pipeline</code>, <code>second_pipeline</code>) are the names of the micro-package folders within the codebase.</li> <li>The values are the options accepted by the <code>kedro micropkg package &lt;micropkg_name&gt;</code> CLI command.</li> </ul> <pre><code>Make sure `destination` is specified as a POSIX path even when working on a Windows machine.\n</code></pre> <pre><code>The examples above apply to any generic Python package, modular pipelines fall under this category and can be easily addressed via the `pipelines.pipeline_name` syntax.\n</code></pre>"},{"location":"pages/nodes_and_pipelines/micro_packaging/#pull-a-micro-package","title":"Pull a micro-package","text":"<p>You can pull a micro-package from a tar file by executing <code>kedro micropkg pull &lt;package_name&gt;</code>.</p> <ul> <li>The <code>&lt;package_name&gt;</code> must either be a package name on PyPI or a path to the source distribution file.</li> <li>Kedro will unpack the tar file, and install the files in following locations in your Kedro project:</li> <li>All the micro-package code in <code>src/&lt;package_name&gt;/&lt;micropkg_name&gt;/</code></li> <li>Configuration files in <code>conf/&lt;env&gt;/parameters_&lt;micropkg_name&gt;.yml</code>, where <code>&lt;env&gt;</code> defaults to <code>base</code>.</li> <li>To place parameters from a different config environment, run <code>kedro micropkg pull &lt;micropkg_name&gt; --env &lt;env_name&gt;</code></li> <li>Unit tests in <code>src/tests/&lt;micropkg_name&gt;</code></li> <li>Kedro will also parse any requirements packaged with the micro-package and add them to project level <code>requirements.in</code>.</li> <li>It is advised to compile an updated list of requirements after pulling a micro-package using <code>pip-compile</code>.</li> </ul> <pre><code>If a micro-package has embedded requirements and a project `requirements.in` file does not already exist, it will be generated based on the project `requirements.txt` before appending the micro-package requirements.\n</code></pre> <p>You can pull a micro-package from different locations, including local storage, PyPI and the cloud:</p> Operation Command Pulling from a local directory <code>kedro micropkg pull dist/&lt;pipeline_name&gt;-0.1-py3-none-any.tar.gz</code> Pull from cloud storage <code>kedro micropkg pull s3://my_bucket/&lt;pipeline_name&gt;-0.1-py3-none-any.tar.gz</code> Pull from PyPI-like endpoint <code>kedro micropkg pull &lt;pypi_package_name&gt;</code>"},{"location":"pages/nodes_and_pipelines/micro_packaging/#providing-fsspec-arguments","title":"Providing <code>fsspec</code> arguments","text":"<ul> <li>If you are pulling the micro-package from a location that isn't PyPI, Kedro uses <code>fsspec</code> to locate and pull down your micro-package.</li> <li>You can use the <code>--fs-args</code> option to point to a YAML that contains the required configuration.</li> </ul> <pre><code>kedro micropkg pull https://&lt;url-to-pipeline.tar.gz&gt; --fs-args micropkg_pull_args.yml\n</code></pre> <pre><code># `micropkg_pull_args.yml`\nclient_kwargs:\n  headers:\n    Authorization: token &lt;token&gt;\n</code></pre>"},{"location":"pages/nodes_and_pipelines/micro_packaging/#pull-multiple-micro-packages","title":"Pull multiple micro-packages","text":"<ul> <li>To pull multiple micro-packages in bulk, run <code>kedro micropkg pull --all</code>.</li> <li>This will pull and unpack all micro-packages specified in the <code>tool.kedro.micropkg.pull</code> manifest section of the project's <code>pyproject.toml</code> file:</li> </ul> <pre><code>[tool.kedro.micropkg.pull]\n\"src/dist/first-pipeline-0.1-py3-none-any.tar.gz\" = {}\n\"https://www.url.to/second-pipeline.tar.gz\" = {alias = \"aliased_pipeline\", destination = \"pipelines\", fs-args = \"pipeline_pull_args.yml\"}\n</code></pre> <ul> <li>The keys (tar references in this case) are the package paths</li> <li>The values are the options that <code>kedro micropkg pull &lt;package_path&gt;</code> CLI command accepts.</li> </ul> <pre><code>As per the [TOML specification](https://toml.io/en/v1.0.0#keys), a key that contains any character outside `A-Za-z0-9_-` must be quoted.\n</code></pre>"},{"location":"pages/nodes_and_pipelines/modular_pipelines/","title":"Modular pipelines","text":"<p>In many typical Kedro projects, a single (\u201cmain\u201d) pipeline increases in complexity as the project evolves. To keep your project fit for purpose, we recommend you separate your code into different pipelines (modules) that are logically isolated and can be reused. Each pipeline should ideally be organised in its own folder, promoting easy copying and reuse within and between projects. Simply put: one pipeline, one folder.</p> <p>Kedro supports this concept of modular pipelines with the following tools: - How to create a new blank pipeline using the <code>kedro pipeline create</code> command - How to structure your pipeline creation - How to use custom new pipeline templates - How to share your pipelines</p>"},{"location":"pages/nodes_and_pipelines/modular_pipelines/#how-to-create-a-new-blank-pipeline-using-the-kedro-pipeline-create-command","title":"How to create a new blank pipeline using the <code>kedro pipeline create</code> command","text":"<p>To create a new modular pipeline, use the following command:</p> <pre><code>kedro pipeline create &lt;pipeline_name&gt;\n</code></pre> <p>After running this command, a new pipeline with boilerplate folders and files will be created in your project. For your convenience, Kedro gives you a pipeline-specific <code>nodes.py</code>, <code>pipeline.py</code>, parameters file and appropriate <code>tests</code> structure. It also adds the appropriate <code>__init__.py</code> files. You can see the generated folder structure below:</p> <pre><code>\u251c\u2500\u2500 conf\n\u2502   \u2514\u2500\u2500 base\n\u2502       \u2514\u2500\u2500 parameters_{{pipeline_name}}.yml  &lt;-- Pipeline-specific parameters\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 my_project\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 pipelines\n    \u2502       \u251c\u2500\u2500 __init__.py\n    \u2502       \u2514\u2500\u2500 {{pipeline_name}}      &lt;-- This folder defines the modular pipeline\n    \u2502           \u251c\u2500\u2500 __init__.py        &lt;-- So that Python treats this pipeline as a module\n    \u2502           \u251c\u2500\u2500 nodes.py           &lt;-- To declare your nodes\n    \u2502           \u2514\u2500\u2500 pipeline.py        &lt;-- To structure the pipeline itself\n    \u2514\u2500\u2500 tests\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 pipelines\n            \u251c\u2500\u2500 __init__.py\n            \u2514\u2500\u2500 {{pipeline_name}}      &lt;-- Pipeline-specific tests\n                \u251c\u2500\u2500 __init__.py\n                \u2514\u2500\u2500 test_pipeline.py\n\n</code></pre> <p>If you want to delete an existing pipeline, you can use <code>kedro pipeline delete &lt;pipeline_name&gt;</code> to do so.</p> <pre><code>To see the full list of available CLI options, you can run `kedro pipeline create --help`.\n</code></pre>"},{"location":"pages/nodes_and_pipelines/modular_pipelines/#how-to-structure-your-pipeline-creation","title":"How to structure your pipeline creation","text":"<p>After creating the pipeline with <code>kedro pipeline create</code>, you will find template code in <code>pipeline.py</code> that you need to fill with your actual pipeline code:</p> <pre><code># src/my_project/pipelines/{{pipeline_name}}/pipeline.py\nfrom kedro.pipeline import Pipeline, pipeline\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([])\n</code></pre> <p>Here, you are creating a <code>create_pipeline()</code> function that returns a <code>Pipeline</code> class instance with the help of the <code>pipeline</code> function. You should keep the function name as <code>create_pipeline()</code> because this allows kedro to automatically discover the pipeline. Otherwise, the pipeline would need to be registered manually.</p> <p>Before filling <code>pipeline.py</code> with nodes, we recommend storing all node functions in <code>nodes.py</code>. From our previous example, we should add the functions <code>mean()</code>, <code>mean_sos()</code> and <code>variance()</code> into <code>nodes.py</code>:</p> <pre><code># src/my_project/pipelines/{{pipeline_name}}/nodes.py\ndef mean(xs, n):\n    return sum(xs) / n\n\ndef mean_sos(xs, n):\n    return sum(x**2 for x in xs) / n\n\ndef variance(m, m2):\n    return m2 - m * m\n</code></pre> <p>Then we can assemble a pipeline from those nodes as follows:</p> <pre><code># src/my_project/pipelines/{{pipeline_name}}/pipelines.py\nfrom kedro.pipeline import Pipeline, pipeline, node\n\nfrom .nodes import mean, mean_sos, variance\n# Import node functions from nodes.py located in the same folder\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        [\n            node(len, \"xs\", \"n\"),\n            node(mean, [\"xs\", \"n\"], \"m\", name=\"mean_node\", tags=\"tag1\"),\n            node(mean_sos, [\"xs\", \"n\"], \"m2\", name=\"mean_sos\", tags=[\"tag1\", \"tag2\"]),\n            node(variance, [\"m\", \"m2\"], \"v\", name=\"variance_node\"),\n        ],  # A list of nodes and pipelines combined into a new pipeline\n        tags=\"tag3\",  # Optional, each pipeline node will be tagged\n        namespace=\"\",  # Optional\n        inputs={},  # Optional\n        outputs={},  # Optional\n        parameters={},  # Optional\n    )\n</code></pre> <p>Here it was shown that pipeline creation function have few optional parameters, you can use: - tags on a pipeline level to apply them for all nodes inside of pipeline - namespace, inputs, outputs and parameters to reuse pipelines. More about that you can find at Reuse pipelines with namespaces</p>"},{"location":"pages/nodes_and_pipelines/modular_pipelines/#how-to-use-custom-new-pipeline-templates","title":"How to use custom new pipeline templates","text":"<p>If you want to generate a pipeline with a custom Cookiecutter template, you can save it in <code>&lt;project_root&gt;/templates/pipeline</code>. The <code>kedro pipeline create</code> command will pick up the custom template in your project as the default. You can also specify the path to your custom Cookiecutter pipeline template with the <code>--template</code> flag like this:</p> <pre><code>kedro pipeline create &lt;pipeline_name&gt; --template &lt;path_to_template&gt;\n</code></pre> <p>A template folder passed to <code>kedro pipeline create</code> using the <code>--template</code> argument will take precedence over any local templates. Kedro supports having a single pipeline template in your project. If you need to have multiple pipeline templates, consider saving them in a separate folder and pointing to them with the <code>--template</code> flag.</p>"},{"location":"pages/nodes_and_pipelines/modular_pipelines/#creating-custom-pipeline-templates","title":"Creating custom pipeline templates","text":"<p>It is your responsibility to create functional Cookiecutter templates for custom pipelines. Please ensure you understand the basic structure of a pipeline. Your template should render to a valid, importable Python module containing a <code>create_pipeline</code> function at the top level that returns a <code>Pipeline</code> object. You will also need appropriate <code>config</code> and <code>tests</code> subdirectories that will be copied to the project <code>config</code> and <code>tests</code> directories when the pipeline is created. The <code>config</code> and <code>tests</code> directories need to follow the same layout as in the default template and cannot be customised, although the contents of the parameters and actual test file can be changed. File and folder names or structure do not matter beyond that and can be customised according to your needs. You can use the default template that Kedro uses as a starting point.</p> <p>Pipeline templates are rendered using Cookiecutter, and must also contain a <code>cookiecutter.json</code> See the <code>cookiecutter.json</code> file in the Kedro default template for an example. It is important to note that if you are embedding your custom pipeline template within a Kedro starter template, you must tell Cookiecutter not to render this template when creating a new project from the starter. To do this, you must add <code>_copy_without_render: [\"templates\"]</code> to the <code>cookiecutter.json</code> file for the starter and not the <code>cookiecutter.json</code> for the pipeline template.</p>"},{"location":"pages/nodes_and_pipelines/modular_pipelines/#providing-pipeline-specific-dependencies","title":"Providing pipeline specific dependencies","text":"<ul> <li>A pipeline might have external dependencies specified in a local <code>requirements.txt</code> file.</li> <li>Pipeline specific dependencies are scooped up during the micro-packaging process.</li> <li>These dependencies need to be manually installed using <code>pip</code>:</li> </ul> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"pages/nodes_and_pipelines/modular_pipelines/#how-to-share-your-pipelines","title":"How to share your pipelines","text":"<p>Warning: Micro-packaging is deprecated and will be removed from Kedro version 0.20.0.</p> <p>Pipelines are shareable between Kedro codebases via micro-packaging, but you must follow a couple of rules to ensure portability:</p> <ul> <li>A pipeline that you want to share needs to be separated in terms of its folder structure. <code>kedro pipeline create</code> command makes this easy.</li> <li>Pipelines should not depend on the main Python package, as this would break portability to another project.</li> <li>Catalog references are not packaged when sharing/consuming pipelines, i.e. the <code>catalog.yml</code> file is not packaged.</li> <li>Kedro will only look for top-level configuration in <code>conf/</code>; placing a configuration folder within the pipeline folder will have no effect.</li> <li>We recommend that you document the configuration required (parameters and catalog) in the local <code>README.md</code> file for any downstream consumers.</li> </ul>"},{"location":"pages/nodes_and_pipelines/namespaces/","title":"Reuse pipelines and group nodes with namespaces","text":"<p>In this section, we introduce namespaces - a powerful tool for grouping and isolating nodes. Namespaces are useful in two key scenarios:</p> <ul> <li> <p>Reusing a Kedro pipeline: If you need to reuse a pipeline with some modifications of inputs, outputs or parameters, Kedro does not allow direct duplication because all nodes within a project must have unique names. Using namespaces helps resolve this issue by isolating identical pipelines while also enhancing visualisation in Kedro-Viz.</p> </li> <li> <p>Grouping specific nodes:  Namespaces provide a simple way to group selected nodes, making it possible to execute them together in deployment while also improving their visual representation in Kedro-Viz.</p> </li> </ul>"},{"location":"pages/nodes_and_pipelines/namespaces/#how-to-reuse-your-pipelines","title":"How to reuse your pipelines","text":"<p>If you want to create a new pipeline that performs similar tasks with different inputs/outputs/parameters as your <code>existing_pipeline</code>, you can use the same <code>pipeline()</code> creation function as described in How to structure your pipeline creation. This function allows you to overwrite inputs, outputs, and parameters. Your new pipeline creation code should look like this:</p> <pre><code>def create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n       existing_pipeline, # Name of the existing Pipeline object\n       inputs = {\"old_input_df_name\" : \"new_input_df_name\"},  # Mapping existing Pipeline input to new input\n       outputs = {\"old_output_df_name\" : \"new_output_df_name\"},  # Mapping existing Pipeline output to new output\n       parameters = {\"params: model_options\": \"params: new_model_options\"},  # Updating parameters\n    )\n</code></pre> <p>This means you can create multiple pipelines based on the <code>existing_pipeline</code> pipeline to test different approaches with various input datasets and model training parameters. For example, for the <code>data_science</code> pipeline from our Spaceflights tutorial, you can restructure the <code>src/project_name/pipelines/data_science/pipeline.py</code> file by separating the <code>data_science</code> pipeline creation code into a separate <code>base_data_science</code> pipeline object, then reusing it inside the <code>create_pipeline()</code> function:</p> <pre><code>#src/project_name/pipelines/data_science/pipeline.py\n\nfrom kedro.pipeline import Pipeline, node, pipeline\nfrom .nodes import evaluate_model, split_data, train_model\n\nbase_data_science = pipeline(\n        [\n            node(\n                func=split_data,\n                inputs=[\"model_input_table\", \"params:model_options\"],\n                outputs=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n                name=\"split_data_node\",\n            ),\n            node(\n                func=train_model,\n                inputs=[\"X_train\", \"y_train\"],\n                outputs=\"regressor\",\n                name=\"train_model_node\",\n            ),\n            node(\n                func=evaluate_model,\n                inputs=[\"regressor\", \"X_test\", \"y_test\"],\n                outputs=None,\n                name=\"evaluate_model_node\",\n            ),\n        ]\n    )  # Creating a base data science pipeline that will be reused with different model training parameters\n\n# data_science pipeline creation function\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        [base_data_science],  # Creating a new data_science pipeline based on base_data_science pipeline\n        parameters={\"params:model_options\": \"params:model_options_1\"},  # Using a new set of parameters to train model\n    )\n</code></pre> <p>To use a new set of parameters, you should create a second parameters file to ovewrite parameters specified in  <code>conf/base/parameters.yml</code>. To overwrite the parameter <code>model_options</code>, create a file  <code>conf/base/parameters_data_science.yml</code> and add a parameter called <code>model_options_1</code>:</p> <pre><code>#conf/base/parameters.yml\nmodel_options_1:\n  test_size: 0.15\n  random_state: 3\n  features:\n    - passenger_capacity\n    - crew\n    - d_check_complete\n    - moon_clearance_complete\n    - company_rating\n</code></pre> <pre><code>In Kedro, you cannot run pipelines with the same node names. In this example, both pipelines have nodes with the same names, so it's impossible to execute them together. However, `base_data_science` is not registered and will not be executed with the `kedro run` command. The `data_science` pipeline, on the other hand, will be executed during `kedro run` because it will be autodiscovered by Kedro, as it was created inside the `create_pipeline()` function.\n</code></pre> <p>If you want to execute <code>base_data_science</code> and <code>data_science</code> pipelines together or reuse <code>base_data_science</code> a few more times, you need to modify the node names. The easiest way to do this is by using namespaces.</p>"},{"location":"pages/nodes_and_pipelines/namespaces/#what-is-a-namespace","title":"What is a namespace","text":"<p>A namespace is a way to isolate nodes, inputs, outputs, and parameters inside your pipeline. If you put <code>namespace=\"namespace_name\"</code> attribute inside the <code>pipeline()</code> creation function, it will add the <code>namespace_name.</code> prefix to all nodes, inputs, outputs, and parameters inside your new pipeline.</p> <pre><code>If you don't want to change the names of your inputs, outputs, or parameters with the `namespace_name.` prefix while using a namespace, you should list these objects inside the corresponding parameters of the `pipeline()` creation function. For example:\n\n```python\npipeline(\n    [node(...), node(...), node(...)],\n    namespace=\"your_namespace_name\",\n    inputs={\"first_input_to_not_be_prefixed\", \"second_input_to_not_be_prefixed\"},\n    outputs={\"first_output_to_not_be_prefixed\", \"second_output_to_not_be_prefixed\"},\n    parameters={\"first_parameter_to_not_be_prefixed\", \"second_parameter_to_not_be_prefixed\"},\n)\n</code></pre> <p>Let's extend our previous example and try to reuse the <code>base_data_science</code> pipeline one more time by creating another pipeline based on it. First, we should use the <code>kedro pipeline create</code> command to create a new blank pipeline named <code>data_science_2</code>:</p> <pre><code>kedro pipeline create data_science_2\n</code></pre> <p>Then, we need to modify the <code>src/project_name/pipelines/data_science_2/pipeline.py</code> file to create a pipeline in a similar way to the example above. We will import <code>base_data_science</code> from the code above and use a namespace to isolate our nodes:</p> <pre><code>#src/project_name/pipelines/data_science_2/pipeline.py\nfrom kedro.pipeline import Pipeline, pipeline\nfrom ..data_science.pipeline import base_data_science  # Import pipeline to create a new one based on it\n\ndef create_pipeline() -&gt; Pipeline:\n    return pipeline(\n        base_data_science, # Creating a new data_science_2 pipeline based on base_data_science pipeline\n        namespace = \"ds_2\", # With that namespace, \"ds_2.\" prefix will be added to inputs, outputs, params, and node names\n        parameters={\"params:model_options\": \"params:model_options_2\"}, # Using a new set of parameters to train model\n        inputs={\"model_input_table\"}, # Inputs remain the same, without namespace prefix\n    )\n</code></pre> <p>To use a new set of parameters, copy <code>model_options</code> from <code>conf/base/parameters_data_science.yml</code> to <code>conf/base/parameters_data_science_2.yml</code> and modify it slightly to try new model training parameters, such as test size and a different feature set. Call it <code>model_options_2</code>:</p> <pre><code>#conf/base/parameters.yml\nmodel_options_2:\n  test_size: 0.3\n  random_state: 3\n  features:\n    - d_check_complete\n    - moon_clearance_complete\n    - iata_approved\n    - company_rating\n</code></pre> <p>In this example, all nodes inside the <code>data_science_2</code> pipeline will be prefixed with <code>ds_2</code>: <code>ds_2.split_data</code>, <code>ds_2.train_model</code>, <code>ds_2.evaluate_model</code>. Parameters will be used from <code>model_options_2</code> because we overwrite <code>model_options</code> with them. The input for that pipeline will be <code>model_input_table</code> as it was previously, because we mentioned that in the inputs parameter (without that, the input would be modified to <code>ds_2.model_input_table</code>, but we don't have that table in the pipeline).</p> <p>Since the node names are unique now, we can run the project with:</p> <pre><code>kedro run\n</code></pre> <p>Logs show that <code>data_science</code> and <code>data_science_2</code> pipelines were executed successfully with different R2 results. Now, we can see how Kedro-viz renders namespaced pipelines in collapsible \"super nodes\":</p> <pre><code>kedro viz run\n</code></pre> <p>After running viz, we can see two equal pipelines: <code>data_science</code> and <code>data_science_2</code>:</p> <p></p> <p>We can collapse all namespaced pipelines (in our case, it's only <code>data_science_2</code>) with a special button and see that the <code>data_science_2</code> pipeline was collapsed into one super node called <code>Ds 2</code>:</p> <p></p> <pre><code>You can use `kedro run --namespace=namespace_name` to run only the specific namespace\n</code></pre>"},{"location":"pages/nodes_and_pipelines/namespaces/#how-to-namespace-all-pipelines-in-a-project","title":"How to namespace all pipelines in a project","text":"<p>If we want to make all pipelines in this example fully namespaced, we should:</p> <p>Modify the <code>data_processing</code> pipeline by adding to the <code>pipeline()</code> creation function in <code>src/project_name/pipelines/data_processing/pipeline.py</code> with the following code:</p> <pre><code>        namespace=\"data_processing\",\n        inputs={\"companies\", \"shuttles\", \"reviews\"},  # Inputs remain the same, without namespace prefix\n        outputs={\"model_input_table\"},  # Outputs remain the same, without namespace prefix\n</code></pre> <p>Modify the <code>data_science</code> pipeline by adding namespace and inputs in the same way as it was done in <code>data_science_2</code> pipeline:</p> <pre><code>def create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        base_data_science,\n        namespace=\"ds_1\",\n        parameters={\"params:model_options\": \"params:model_options_1\"},\n        inputs={\"model_input_table\"},\n    )\n</code></pre> <p>After executing the pipeline with <code>kedro run</code>, the visualisation with <code>kedro viz run</code> after collapsing will look like this:</p> <p></p>"},{"location":"pages/nodes_and_pipelines/namespaces/#group-nodes-with-namespaces","title":"Group nodes with namespaces","text":"<p>You can use namespaces in your Kedro projects, not only to reuse pipelines but to also group your nodes for better high level visualisation in Kedro Viz and for deployment purposes. In production environments, it might be inefficient to map each node to a container. Using namespaces as a grouping mechanism, you can map each namespaced pipeline to a container or a task in your deployment environment.</p> <p>For example, in your spaceflights project, you can assign a namespace to the <code>data_processing</code> pipeline like this:</p> <pre><code>#src/project_name/pipelines/data_science/pipeline.py\n\nfrom kedro.pipeline import Pipeline, node, pipeline\nfrom .nodes import create_model_input_table, preprocess_companies, preprocess_shuttles\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        [\n            node(\n                func=preprocess_companies,\n                inputs=\"companies\",\n                outputs=\"preprocessed_companies\",\n                name=\"preprocess_companies_node\",\n            ),\n            node(\n                func=preprocess_shuttles,\n                inputs=\"shuttles\",\n                outputs=\"preprocessed_shuttles\",\n                name=\"preprocess_shuttles_node\",\n            ),\n            node(\n                func=create_model_input_table,\n                inputs=[\"preprocessed_shuttles\", \"preprocessed_companies\", \"reviews\"],\n                outputs=\"model_input_table\",\n                name=\"create_model_input_table_node\",\n            ),\n        ],\n        namespace=\"data_processing\",\n    )\n</code></pre> <p>The pipeline will expect the inputs and outputs to be prefixed with the namespace name, that is, <code>data_processing.</code>.</p> <pre><code>From Kedro 0.19.12, you can use the `grouped_nodes_by_namespace` property of the `Pipeline` object to get a dictionary which groups nodes by their top level namespace. Plugin developers are encouraged to use this property to obtain the mapping of namespaced group of nodes to a container or a task in the deployment environment.\n</code></pre> <p>You can further nest namespaces by assigning namespaces on the node level with the <code>namespace</code> argument of the <code>node()</code> function. Namespacing at node level should only be done to enhance visualisation by creating collapsible pipeline parts on Kedro Viz. In this case, only the node name will be prefixed with <code>namespace_name</code>, while inputs, outputs, and parameters will remain unchanged. This behaviour differs from namespacing at the pipeline level.</p> <p>For example, if you want to group the first two nodes of the <code>data_processing</code> pipeline from Spaceflights tutorial into the same collapsible namespace for visualisation, you can update your pipeline like this:</p> <pre><code>#src/project_name/pipelines/data_science/pipeline.py\n\nfrom kedro.pipeline import Pipeline, node, pipeline\nfrom .nodes import create_model_input_table, preprocess_companies, preprocess_shuttles\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        [\n            node(\n                func=preprocess_companies,\n                inputs=\"companies\",\n                outputs=\"preprocessed_companies\",\n                name=\"preprocess_companies_node\",\n                namespace=\"preprocessing\", # Assigning the node to the \"preprocessing\" nested namespace\n            ),\n            node(\n                func=preprocess_shuttles,\n                inputs=\"shuttles\",\n                outputs=\"preprocessed_shuttles\",\n                name=\"preprocess_shuttles_node\",\n                namespace=\"preprocessing\", # Assigning the node to the \"preprocessing\" nested namespace\n            ),\n            node(\n                func=create_model_input_table,\n                inputs=[\"preprocessed_shuttles\", \"preprocessed_companies\", \"reviews\"],\n                outputs=\"model_input_table\",\n                name=\"create_model_input_table_node\",\n            ),\n        ],\n        namespace=\"data_processing\",\n    )\n</code></pre> <p>As you can see in the above example, the entire pipeline is namespaced as <code>data_processing</code>, while the first two nodes are also namespaced as <code>data_processing.preprocessing</code>. This will allow you to collapse the nested <code>preprocessing</code> namespace in Kedro-Viz for better visualisation, but the inputs and outputs of the pipeline will still expect the prefix <code>data_processing.</code>.</p> <p>You can execute the whole namespaced pipeline with:</p> <pre><code>kedro run --namespace=data_processing\n</code></pre> <p>Or, you can run the first two nodes with:</p> <pre><code>kedro run --namespace=data_processing.preprocessing\n</code></pre> <p>Open the visualisation with <code>kedro viz run</code> to see the collapsible pipeline parts, which you can toggle with \"Collapse pipelines\" button on the left panel.</p> <p> </p> <pre><code>The use of `namespace` at node level is not recommended for grouping your nodes for deployment as this behaviour differs from defining `namespace` at `pipeline()` level. When defining namespaces at the node level, they behave similarly to tags and do not guarantee execution consistency.\n</code></pre>"},{"location":"pages/nodes_and_pipelines/nodes/","title":"Nodes","text":"<p>In this section, we introduce the concept of a node, for which the relevant API documentation is {py:mod}<code>~kedro.pipeline.node</code>.</p> <p>Nodes are the building blocks of pipelines, and represent tasks. Pipelines are used to combine nodes to build workflows, which range from simple machine learning workflows to end-to-end (E2E) production workflows.</p> <p>You must first import libraries from Kedro and other standard tools to run the code snippets below.</p> <pre><code>from kedro.pipeline import *\nfrom kedro.io import *\nfrom kedro.runner import *\n\nimport pickle\nimport os\n</code></pre>"},{"location":"pages/nodes_and_pipelines/nodes/#how-to-create-a-node","title":"How to create a node","text":"<p>A node is created by specifying a function, input variable names and output variable names. Let's consider a simple function that adds two numbers:</p> <pre><code>def add(x, y):\n    return x + y\n</code></pre> <p>The function has two inputs (<code>x</code> and <code>y</code>) and a single output (the sum of the inputs).</p> <p>Here is how a node is created with this function:</p> <pre><code>adder_node = node(func=add, inputs=[\"a\", \"b\"], outputs=\"sum\")\nadder_node\n</code></pre> <p>Here is the output:</p> <pre><code>Out[1]: Node(add, ['a', 'b'], 'sum', None)\n</code></pre> <p>You can also add labels to nodes, which will be used to describe them in logs:</p> <pre><code>adder_node = node(func=add, inputs=[\"a\", \"b\"], outputs=\"sum\")\nprint(str(adder_node))\n\nadder_node = node(func=add, inputs=[\"a\", \"b\"], outputs=\"sum\", name=\"adding_a_and_b\")\nprint(str(adder_node))\n</code></pre> <p>This gives the following output:</p> <pre><code>add([a,b]) -&gt; [sum]\nadding_a_and_b: add([a,b]) -&gt; [sum]\n</code></pre> <p>Let's break down the node definition:</p> <ul> <li><code>add</code> is the Python function that will execute when the node runs</li> <li><code>['a', 'b']</code> specify the input variable names</li> <li><code>sum</code> specifies the return variable name. The value returned by <code>add</code> will be bound in this variable</li> <li><code>name</code> is an optional label for the node, which can be used to provide description of the business logic it provides</li> </ul>"},{"location":"pages/nodes_and_pipelines/nodes/#node-definition-syntax","title":"Node definition syntax","text":"<p>A syntax describes function inputs and outputs. This syntax allows different Python functions to be reused in nodes, and supports dependency resolution in pipelines.</p>"},{"location":"pages/nodes_and_pipelines/nodes/#syntax-for-input-variables","title":"Syntax for input variables","text":"Input syntax Meaning Example function parameters How function is called when node runs <code>None</code> No input <code>def f()</code> <code>f()</code> <code>'a'</code> Single input <code>def f(arg1)</code> <code>f(a)</code> <code>['a', 'b']</code> Multiple inputs <code>def f(arg1, arg2)</code> <code>f(a, b)</code> <code>['a', 'b', 'c']</code> Variable inputs <code>def f(arg1, *args)</code>. <code>f(arg1, arg2, arg3)</code> <code>dict(arg1='x', arg2='y')</code> Keyword inputs <code>def f(arg1, arg2)</code> <code>f(arg1=x, arg2=y)</code>"},{"location":"pages/nodes_and_pipelines/nodes/#syntax-for-output-variables","title":"Syntax for output variables","text":"Output syntax Meaning Example return statement <code>None</code> No output Does not return <code>'a'</code> Single output <code>return a</code> <code>['a', 'b']</code> List output <code>return [a, b]</code> <code>dict(key1='a', key2='b')</code> Dictionary output <code>return dict(key1=a, key2=b)</code> <p>Any combinations of the above are possible, except nodes of the form <code>node(f, None, None)</code> (at least a single input or output must be provided).</p>"},{"location":"pages/nodes_and_pipelines/nodes/#args-node-functions","title":"<code>*args</code> node functions","text":"<p>It is common to have functions that take an arbitrary number of inputs, like a function that combines multiple dataframes. You can use the <code>*args</code> argument in the node function, while simply declaring the names of the datasets in the node's inputs.</p>"},{"location":"pages/nodes_and_pipelines/nodes/#kwargs-only-node-functions","title":"<code>**kwargs</code>-only node functions","text":"<p>Sometimes, when creating reporting nodes for instance, you need to know the names of the datasets that your node receives, but you might not have this information in advance. This can be solved by defining a <code>**kwargs</code>-only function:</p> <pre><code>def reporting(**kwargs):\n    result = []\n    for name, data in kwargs.items():\n        res = example_report(name, data)\n        result.append(res)\n    return combined_report(result)\n</code></pre> <p>Then, when it comes to constructing the <code>Node</code>, simply pass a dictionary to the node inputs:</p> <pre><code>from kedro.pipeline import node\n\n\nuk_reporting_node = node(\n    reporting,\n    inputs={\"uk_input1\": \"uk_input1\", \"uk_input2\": \"uk_input2\", ...},\n    outputs=\"uk\",\n)\n\nge_reporting_node = node(\n    reporting,\n    inputs={\"ge_input1\": \"ge_input1\", \"ge_input2\": \"ge_input2\", ...},\n    outputs=\"ge\",\n)\n</code></pre> <p>Alternatively, you can also make use of a helper function that creates the mapping for you, so you can reuse it across your codebase.</p> <pre><code> from kedro.pipeline import node\n\n\n+mapping = lambda x: {k: k for k in x}\n+\n uk_reporting_node = node(\n     reporting,\n-    inputs={\"uk_input1\": \"uk_input1\", \"uk_input2\": \"uk_input2\", ...},\n+    inputs=mapping([\"uk_input1\", \"uk_input2\", ...]),\n     outputs=\"uk\",\n )\n\n ge_reporting_node = node(\n     reporting,\n-    inputs={\"ge_input1\": \"ge_input1\", \"ge_input2\": \"ge_input2\", ...},\n+    inputs=mapping([\"ge_input1\", \"ge_input2\", ...]),\n     outputs=\"ge\",\n )\n</code></pre>"},{"location":"pages/nodes_and_pipelines/nodes/#how-to-tag-a-node","title":"How to tag a node","text":"<p>Tags might be useful to run part of a pipeline without changing the code. For instance, <code>kedro run --tags=ds</code> will only run nodes that have a <code>ds</code> tag attached.</p> <p>To tag a node, you can simply specify the <code>tags</code> argument:</p> <pre><code>node(func=add, inputs=[\"a\", \"b\"], outputs=\"sum\", name=\"adding_a_and_b\", tags=\"node_tag\")\n</code></pre> <p>Moreover, you can tag all nodes in a <code>Pipeline</code>. If the pipeline definition contains the <code>tags=</code> argument, Kedro will attach the corresponding tag to every node within that pipeline.</p> <p>To run a pipeline using a tag:</p> <pre><code>kedro run --tags=pipeline_tag\n</code></pre> <p>This will run only the nodes found within the pipeline tagged with <code>pipeline_tag</code>.</p> <pre><code>Node or tag names must ONLY contain letters, digits, hyphens, underscores and/or periods. Other symbols are not permitted.\n</code></pre>"},{"location":"pages/nodes_and_pipelines/nodes/#how-to-run-a-node","title":"How to run a node","text":"<p>To run a node, you must instantiate its inputs. In this case, the node expects two inputs:</p> <pre><code>adder_node.run(dict(a=2, b=3))\n</code></pre> <p>The output is as follows:</p> <pre><code>Out[2]: {'sum': 5}\n</code></pre> <pre><code>You can also call a node as a regular Python function: `adder_node(dict(a=2, b=3))`. This will call `adder_node.run(dict(a=2, b=3))` behind the scenes.\n</code></pre>"},{"location":"pages/nodes_and_pipelines/nodes/#how-to-use-generator-functions-in-a-node","title":"How to use generator functions in a node","text":"<pre><code>This documentation section uses the `pandas-iris` starter that is unavailable in Kedro version 0.19.0 and beyond. The latest version of Kedro that supports `pandas-iris` is Kedro 0.18.14: install that or an earlier version to work through this example `pip install kedro==0.18.14`).\n\nTo check the version installed, type `kedro -V` in your terminal window.\n</code></pre> <p>Generator functions were introduced with PEP 255 and are a special kind of function in Python that returns lazy iterators. They are often used for lazy-loading or lazy-saving of data, which can be particularly useful when dealing with large datasets that do not fit entirely into memory. In the context of Kedro, generator functions can be used in nodes to efficiently process and handle such large datasets.</p>"},{"location":"pages/nodes_and_pipelines/nodes/#set-up-the-project","title":"Set up the project","text":"<p>Set up a Kedro project using the legacy <code>pandas-iris</code> starter. Create the project with this command, assuming Kedro version 0.18.14:</p> <pre><code>kedro new --starter=pandas-iris --checkout=0.18.14\n</code></pre>"},{"location":"pages/nodes_and_pipelines/nodes/#loading-data-with-generators","title":"Loading data with generators","text":"<p>To use generator functions in Kedro nodes, you need to update the <code>catalog.yml</code> file to include the <code>chunksize</code> argument for the relevant dataset that will be processed using the generator.</p> <p>You need to add a new dataset in your <code>catalog.yml</code> as follows:</p> <pre><code>+ X_test:\n+  type: pandas.CSVDataset\n+  filepath: data/05_model_input/X_test.csv\n+  load_args:\n+    chunksize: 10\n</code></pre> <p>With <code>pandas</code> built-in support, you can use the <code>chunksize</code> argument to read data using generator.</p>"},{"location":"pages/nodes_and_pipelines/nodes/#saving-data-with-generators","title":"Saving data with generators","text":"<p>To use generators to save data lazily, you need do three things: - Update the <code>make_prediction</code> function definition to use <code>yield</code> instead of <code>return</code>. - Create a custom dataset called <code>ChunkWiseCSVDataset</code> - Update <code>catalog.yml</code> to use a newly created <code>ChunkWiseCSVDataset</code>.</p> <p>Copy the following code to <code>nodes.py</code>. The main change is to use a new model <code>DecisionTreeClassifier</code> to make prediction by chunks in <code>make_predictions</code>.</p> Click to open <pre><code>import logging\nfrom typing import Any, Dict, Tuple, Iterator, Generator\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport pandas as pd\n\n\ndef split_data(\n    data: pd.DataFrame, parameters: Dict[str, Any]\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n    \"\"\"Splits data into features and target training and test sets.\n\n    Args:\n        data: Data containing features and target.\n        parameters: Parameters defined in parameters.yml.\n    Returns:\n        Split data.\n    \"\"\"\n\n    data_train = data.sample(\n        frac=parameters[\"train_fraction\"], random_state=parameters[\"random_state\"]\n    )\n    data_test = data.drop(data_train.index)\n\n    X_train = data_train.drop(columns=parameters[\"target_column\"])\n    X_test = data_test.drop(columns=parameters[\"target_column\"])\n    y_train = data_train[parameters[\"target_column\"]]\n    y_test = data_test[parameters[\"target_column\"]]\n\n    label_encoder = LabelEncoder()\n    label_encoder.fit(pd.concat([y_train, y_test]))\n    y_train = label_encoder.transform(y_train)\n\n    return X_train, X_test, y_train, y_test\n\n\ndef make_predictions(\n    X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series\n) -&gt; Generator[pd.Series, None, None]:\n    \"\"\"Use a DecisionTreeClassifier model to make prediction.\"\"\"\n    model = DecisionTreeClassifier()\n    model.fit(X_train, y_train)\n\n    for chunk in X_test:\n        y_pred = model.predict(chunk)\n        y_pred = pd.DataFrame(y_pred)\n        yield y_pred\n\n\ndef report_accuracy(y_pred: pd.Series, y_test: pd.Series):\n    \"\"\"Calculates and logs the accuracy.\n\n    Args:\n        y_pred: Predicted target.\n        y_test: True target.\n    \"\"\"\n    accuracy = accuracy_score(y_test, y_pred)\n    logger = logging.getLogger(__name__)\n    logger.info(\"Model has accuracy of %.3f on test data.\", accuracy)\n</code></pre> <p>The <code>ChunkWiseCSVDataset</code> is a variant of the <code>pandas.CSVDataset</code> where the main change is to the <code>_save</code> method that appends data instead of overwriting it. You need to create a file <code>src/&lt;package_name&gt;/chunkwise.py</code> and put this class inside it. Below is an example of the <code>ChunkWiseCSVDataset</code> implementation:</p> <pre><code>import pandas as pd\n\nfrom kedro.io.core import (\n    get_filepath_str,\n)\nfrom kedro_datasets.pandas import CSVDataset\n\n\nclass ChunkWiseCSVDataset(CSVDataset):\n    \"\"\"``ChunkWiseCSVDataset`` loads/saves data from/to a CSV file using an underlying\n    filesystem. It uses pandas to handle the CSV file.\n    \"\"\"\n\n    _overwrite = True\n\n    def _save(self, data: pd.DataFrame) -&gt; None:\n        save_path = get_filepath_str(self._get_save_path(), self._protocol)\n        # Save the header for the first batch\n        if self._overwrite:\n            data.to_csv(save_path, index=False, mode=\"w\")\n            self._overwrite = False\n        else:\n            data.to_csv(save_path, index=False, header=False, mode=\"a\")\n</code></pre> <p>After that, you need to update the <code>catalog.yml</code> to use this new dataset.</p> <pre><code>+ y_pred:\n+  type: &lt;package_name&gt;.chunkwise.ChunkWiseCSVDataset\n+  filepath: data/07_model_output/y_pred.csv\n</code></pre> <p>With these changes, when you run <code>kedro run</code> in your terminal, you should see <code>y_pred</code> being saved multiple times in the logs as the generator lazily processes and saves the data in smaller chunks.</p> <pre><code>...\n                    INFO     Loading data from 'y_train' (MemoryDataset)...                                                                                         data_catalog.py:475\n                    INFO     Running node: make_predictions: make_predictions([X_train,X_test,y_train]) -&gt; [y_pred]                                                         node.py:331\n                    INFO     Saving data to 'y_pred' (ChunkWiseCSVDataset)...                                                                                       data_catalog.py:514\n                    INFO     Saving data to 'y_pred' (ChunkWiseCSVDataset)...                                                                                       data_catalog.py:514\n                    INFO     Saving data to 'y_pred' (ChunkWiseCSVDataset)...                                                                                       data_catalog.py:514\n                    INFO     Completed 2 out of 3 tasks                                                                                                         sequential_runner.py:85\n                    INFO     Loading data from 'y_pred' (ChunkWiseCSVDataset)...                                                                                    data_catalog.py:475\n...                                                                              runner.py:105\n</code></pre>"},{"location":"pages/nodes_and_pipelines/pipeline_introduction/","title":"Pipeline objects","text":"<p>We previously introduced Nodes as building blocks that represent tasks, and can be combined in a pipeline to build your workflow. A pipeline organises the dependencies and execution order of your collection of nodes, and connects inputs and outputs while keeping your code modular. The pipeline resolves dependencies to determine the node execution order, and does not necessarily run the nodes in the order in which they are passed in.</p> <p>To benefit from Kedro's automatic dependency resolution is that you can chain your nodes into a {py:class}<code>~kedro.pipeline.Pipeline</code>, which is a list of nodes that use a shared set of variables. That class can be created using the {py:class}<code>~kedro.pipeline.modular_pipeline.pipeline</code> method, based on nodes or other pipelines (in which case all nodes from that pipeline will be used).</p> <p>The following sections explain how to create and use Kedro pipelines:</p> <ul> <li>How to build a pipeline</li> <li>How to use <code>describe</code> to discover what nodes are part of the pipeline</li> <li>How to merge multiple pipelines</li> <li>How to receive information about the nodes in a pipeline</li> <li>How to receive information about pipeline inputs and outputs</li> <li>How to tag a pipeline</li> <li>How to avoid creating bad pipelines</li> <li>How to store pipeline code in a kedro project</li> </ul>"},{"location":"pages/nodes_and_pipelines/pipeline_introduction/#how-to-build-a-pipeline","title":"How to build a pipeline","text":"<p>In the following example, we construct a simple pipeline that computes the variance of a set of numbers. In practice, pipelines can use more complicated node definitions, and the variables they use usually correspond to entire datasets:</p> <pre><code>from kedro.pipeline import pipeline, node\n\ndef mean(xs, n):\n    return sum(xs) / n\n\ndef mean_sos(xs, n):\n    return sum(x**2 for x in xs) / n\n\ndef variance(m, m2):\n    return m2 - m * m\n\n\nvariance_pipeline = pipeline(\n    [\n        node(len, \"xs\", \"n\"),\n        node(mean, [\"xs\", \"n\"], \"m\", name=\"mean_node\"),\n        node(mean_sos, [\"xs\", \"n\"], \"m2\", name=\"mean_sos\"),\n        node(variance, [\"m\", \"m2\"], \"v\", name=\"variance_node\"),\n    ]\n)\n</code></pre> <p>Kedro determines the order of execution of these nodes based on the inputs and outputs specified for each node. In this example: 1. The first node computes the length of <code>xs</code> and outputs it as <code>n</code>. 2. The second node calculates the mean using <code>xs</code> and <code>n</code>, and outputs it as <code>m</code>. 3. The third node computes the mean sum of squares (mean_sos) using <code>xs</code> and <code>n</code>, and outputs it as <code>m2</code>. 4. The fourth node calculates the variance using the mean (<code>m</code>) and mean sum of squares (<code>m2</code>), and outputs it as <code>v</code>.</p> <p>Kedro's dependency resolution algorithm ensures that each node runs only after its required inputs are available from the outputs of previous nodes. This way, the nodes are executed in the correct order automatically, based on the defined dependencies.</p>"},{"location":"pages/nodes_and_pipelines/pipeline_introduction/#how-to-use-describe-to-discover-what-nodes-are-part-of-the-pipeline","title":"How to use <code>describe</code> to discover what nodes are part of the pipeline","text":"<p>You can use the <code>describe</code> method to get an overview of the nodes in your pipeline and their execution order.</p> <pre><code>print(variance_pipeline.describe())\n</code></pre> <p>The output is as follows:</p> <pre><code>#### Pipeline execution order ####\nName: None\nInputs: xs\n\nlen([xs]) -&gt; [n]\nmean_node\nmean_sos\nvariance_node\n\nOutputs: v\n##################################\n</code></pre>"},{"location":"pages/nodes_and_pipelines/pipeline_introduction/#how-to-merge-multiple-pipelines","title":"How to merge multiple pipelines","text":"<p>You can merge multiple pipelines as shown below. Note that, in this case, <code>pipeline_de</code> and <code>pipeline_ds</code> are expanded to a list of their underlying nodes and these are merged together:</p> <pre><code>pipeline_de = pipeline([node(len, \"xs\", \"n\"), node(mean, [\"xs\", \"n\"], \"m\")])\n\npipeline_ds = pipeline(\n    [node(mean_sos, [\"xs\", \"n\"], \"m2\"), node(variance, [\"m\", \"m2\"], \"v\")]\n)\n\nlast_node = node(print, \"v\", None)\n\npipeline_all = pipeline([pipeline_de, pipeline_ds, last_node])\nprint(pipeline_all.describe())\n</code></pre> <p>The output is as follows:</p> <pre><code>#### Pipeline execution order ####\nName: None\nInputs: xs\n\nlen([xs]) -&gt; [n]\nmean([n,xs]) -&gt; [m]\nmean_sos([n,xs]) -&gt; [m2]\nvariance([m,m2]) -&gt; [v]\nprint([v]) -&gt; None\n\nOutputs: None\n##################################\n</code></pre>"},{"location":"pages/nodes_and_pipelines/pipeline_introduction/#how-to-receive-information-about-the-nodes-in-a-pipeline","title":"How to receive information about the nodes in a pipeline","text":"<p>Pipelines provide access to their nodes in a topological order to enable custom functionality, e.g. pipeline visualisation. Each node has information about its inputs and outputs:</p> <pre><code>nodes = variance_pipeline.nodes\nnodes\n</code></pre> <p>The output is as follows:</p> <pre><code>[\n    Node(len, \"xs\", \"n\", None),\n    Node(mean, [\"xs\", \"n\"], \"m\", \"mean_node\"),\n    Node(mean_sos, [\"xs\", \"n\"], \"m2\", \"mean_sos\"),\n    Node(variance, [\"m\", \"m2\"], \"v\", \"variance node\"),\n]\n</code></pre> <p>To find out about the inputs:</p> <pre><code>nodes[0].inputs\n</code></pre> <p>You should see the following:</p> <pre><code>[\"xs\"]\n</code></pre>"},{"location":"pages/nodes_and_pipelines/pipeline_introduction/#how-to-receive-information-about-pipeline-inputs-and-outputs","title":"How to receive information about pipeline inputs and outputs","text":"<p>In a similar way to the above, you can use <code>inputs()</code> and <code>outputs()</code> to check the inputs and outputs of a pipeline:</p> <pre><code>variance_pipeline.inputs()\n</code></pre> <p>Gives the following:</p> <pre><code>Out[7]: {'xs'}\n</code></pre> <pre><code>variance_pipeline.outputs()\n</code></pre> <p>Displays the output:</p> <pre><code>Out[8]: {'v'}\n</code></pre>"},{"location":"pages/nodes_and_pipelines/pipeline_introduction/#how-to-tag-a-pipeline","title":"How to tag a pipeline","text":"<p>You can also tag your pipeline by providing the <code>tags</code> argument, which will tag all of the pipeline's nodes. In the following example, both nodes are tagged with <code>pipeline_tag</code>.</p> <pre><code>pipeline = pipeline(\n    [node(..., name=\"node1\"), node(..., name=\"node2\")], tags=\"pipeline_tag\"\n)\n</code></pre> <p>You can combine pipeline tagging with node tagging. In the following example, <code>node1</code> and <code>node2</code> are tagged with <code>pipeline_tag</code>, while <code>node2</code> also has a <code>node_tag</code>.</p> <pre><code>pipeline = pipeline(\n    [node(..., name=\"node1\"), node(..., name=\"node2\", tags=\"node_tag\")],\n    tags=\"pipeline_tag\",\n)\n</code></pre>"},{"location":"pages/nodes_and_pipelines/pipeline_introduction/#how-to-avoid-creating-bad-pipelines","title":"How to avoid creating bad pipelines","text":"<p>A pipelines can usually readily resolve its dependencies. In some cases, resolution is not possible. In this case, the pipeline is not well-formed.</p>"},{"location":"pages/nodes_and_pipelines/pipeline_introduction/#pipeline-with-bad-nodes","title":"Pipeline with bad nodes","text":"<p>In this case, we have a pipeline consisting of a single node with no input and output:</p> <pre><code>try:\n    pipeline([node(lambda: print(\"!\"), None, None)])\nexcept Exception as e:\n    print(e)\n</code></pre> <p>Gives the following output:</p> <pre><code>Invalid Node definition: it must have some `inputs` or `outputs`.\nFormat should be: node(function, inputs, outputs)\n</code></pre>"},{"location":"pages/nodes_and_pipelines/pipeline_introduction/#pipeline-with-circular-dependencies","title":"Pipeline with circular dependencies","text":"<p>For every two variables where the first depends on the second, there must not be a way in which the second also depends on the first, otherwise, a circular dependency will prevent us from compiling the pipeline.</p> <p>The first node captures the relationship of how to calculate <code>y</code> from <code>x</code> and the second captures the relationship of how to calculate <code>x</code> knowing <code>y</code>. The pair of nodes cannot co-exist in the same pipeline:</p> <pre><code>try:\n    pipeline(\n        [\n            node(lambda x: x + 1, \"x\", \"y\", name=\"first node\"),\n            node(lambda y: y - 1, \"y\", \"x\", name=\"second node\"),\n        ]\n    )\nexcept Exception as e:\n    print(e)\n</code></pre> <p>The output is as follows:</p> <pre><code>Circular dependencies exist among these items: ['first node: &lt;lambda&gt;([x]) -&gt; [y]', 'second node: &lt;lambda&gt;([y]) -&gt; [x]']\n</code></pre>"},{"location":"pages/nodes_and_pipelines/pipeline_introduction/#pipeline-nodes-named-with-the-dot-notation","title":"Pipeline nodes named with the dot notation","text":"<p>Nodes named with dot notation may behave strangely.</p> <pre><code>pipeline([node(lambda x: x, inputs=\"input1kedro\", outputs=\"output1.kedro\")])\n</code></pre> <p>Nodes that are created with input or output names that contain <code>.</code> risk a disconnected pipeline or improperly-formatted Kedro structure.</p> <p>This is because <code>.</code> has a special meaning internally and indicates a namespace pipeline. In the example, the outputs segment should be disconnected as the name implies there is an \"output1\" namespace pipeline. The input is not namespaced, but the output is via its dot notation. This leads to Kedro processing each separately. For this example, a better approach would've been writing both as <code>input1_kedro</code> and <code>output1_kedro</code>.</p> <p>We recommend use of characters like <code>_</code> instead of <code>.</code> as name separators.</p>"},{"location":"pages/nodes_and_pipelines/pipeline_introduction/#how-to-store-pipeline-code-in-a-kedro-project","title":"How to store pipeline code in a kedro project","text":"<p>When managing your Kedro project, we recommend grouping related tasks into individual pipelines to achieve modularity. A project typically contains many tasks, and organising frequently executed tasks together into separate pipelines helps maintain order and efficiency. Each pipeline should ideally be organised in its own folder, promoting easy copying and reuse within project. Simply put: one pipeline, one folder. To assist with this, Kedro introduces the concept of Modular Pipelines, which are described in the next section.</p>"},{"location":"pages/nodes_and_pipelines/pipeline_registry/","title":"The pipeline registry","text":"<p>Projects generated using Kedro 0.17.2 or later define their pipelines in <code>src/&lt;package_name&gt;/pipeline_registry.py</code>. This, in turn, populates the <code>pipelines</code> variable in {py:mod}<code>~kedro.framework.project</code> that the Kedro CLI and plugins use to access project pipelines. The <code>pipeline_registry</code> module must contain a top-level <code>register_pipelines()</code> function that returns a mapping from pipeline names to {py:class}<code>~kedro.pipeline.Pipeline</code> objects. For example, the pipeline registry in the Kedro starter for the completed spaceflights tutorial could define the following <code>register_pipelines()</code> function that exposes the data processing pipeline, the data science pipeline, and a third, default pipeline that combines both of the aforementioned pipelines:</p> <pre><code>import spaceflights.pipelines.data_processing as dp\nimport spaceflights.pipelines.data_science as ds\n\n\ndef register_pipelines() -&gt; Dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    data_processing_pipeline = dp.create_pipeline()\n    data_science_pipeline = ds.create_pipeline()\n\n    return {\n        \"__default__\": data_processing_pipeline + data_science_pipeline,\n        \"data_processing\": data_processing_pipeline,\n        \"data_science\": data_science_pipeline,\n    }\n</code></pre> <p>As a reminder, running <code>kedro run</code> without the <code>--pipeline</code> option runs the default pipeline.</p> <pre><code>The order in which you add the pipelines together is not significant (`data_science_pipeline + data_processing_pipeline` would produce the same result), since Kedro automatically detects the data-centric execution order for all the nodes in the resulting pipeline.\n</code></pre>"},{"location":"pages/nodes_and_pipelines/pipeline_registry/#pipeline-autodiscovery","title":"Pipeline autodiscovery","text":"<p>In the above example, you need to update the <code>register_pipelines()</code> function whenever you create a pipeline that should be returned as part of the project's pipelines. Since Kedro 0.18.3, you can achieve the same result with less code using {py:meth}<code>find_pipelines() &lt;kedro.framework.project.find_pipelines&gt;</code>. The updated pipeline registry contains no project-specific code:</p> <pre><code>def register_pipelines() -&gt; Dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    pipelines = find_pipelines()\n    pipelines[\"__default__\"] = sum(pipelines.values())\n    return pipelines\n</code></pre> <p>Under the hood, the <code>find_pipelines()</code> function traverses the <code>src/&lt;package_name&gt;/pipelines/</code> directory and returns a mapping from pipeline directory name to {py:class}<code>~kedro.pipeline.Pipeline</code> object by:</p> <ol> <li>Importing the <code>&lt;package_name&gt;.pipelines.&lt;pipeline_name&gt;</code> module</li> <li>Calling the <code>create_pipeline()</code> function exposed by the <code>&lt;package_name&gt;.pipelines.&lt;pipeline_name&gt;</code> module</li> <li>Validating that the constructed object is a {py:class}<code>~kedro.pipeline.Pipeline</code></li> </ol> <p>By default, if any of these steps fail, <code>find_pipelines()</code> (or <code>find_pipelines(raise_errors=False)</code>) raises an appropriate warning and skips the current pipeline but continues traversal. During development, this enables you to run your project with some pipelines, even if other pipelines are broken or works in progress.</p> <p>If you specify <code>find_pipelines(raise_errors=True)</code>, the autodiscovery process will fail upon the first error. In production, this ensures errors are caught up front, and pipelines do not get excluded accidentally.</p> <p>The mapping returned by <code>find_pipelines()</code> can be modified, meaning you are not limited to the pipelines returned by each of the <code>create_pipeline()</code> functions found above. For example, to add a data engineering pipeline that isn't part of the default pipeline, add it to the dictionary after constructing the default pipeline:</p> <pre><code>def register_pipelines() -&gt; Dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    pipelines = find_pipelines()\n    pipelines[\"__default__\"] = sum(pipelines.values())\n    pipelines[\"data_engineering\"] = pipeline(\n        pipelines[\"data_processing\"], tags=\"data_engineering\"\n    )\n    return pipelines\n</code></pre> <pre><code>In the case above, `kedro run --tags data_engineering` will not run the data engineering pipeline, as it is not part of the default pipeline. To run the data engineering pipeline, you need to specify `kedro run --pipeline data_engineering --tags data_engineering`.\n</code></pre> <p>On the other hand, you can also modify pipelines before assigning <code>pipelines[\"__default__\"] = sum(pipelines.values())</code> which includes it in the default pipeline. For example, you can update the <code>data_processing</code> pipeline with the <code>data_engineering</code> tag in the <code>pipeline_registry.py</code> and also include this change in the default pipeline:</p> <pre><code>def register_pipelines() -&gt; Dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    pipelines = find_pipelines()\n    pipelines[\"data_processing\"] = pipeline(\n        pipelines[\"data_processing\"], tags=\"data_engineering\"\n    )\n    pipelines[\"__default__\"] = sum(pipelines.values())\n    return pipelines\n</code></pre>"},{"location":"pages/nodes_and_pipelines/run_a_pipeline/","title":"Run a pipeline","text":""},{"location":"pages/nodes_and_pipelines/run_a_pipeline/#runners","title":"Runners","text":"<p>Runners are the execution mechanisms used to run pipelines. They all inherit from <code>AbstractRunner</code>.</p>"},{"location":"pages/nodes_and_pipelines/run_a_pipeline/#sequentialrunner","title":"<code>SequentialRunner</code>","text":"<p>Use <code>SequentialRunner</code> to execute pipeline nodes one-by-one based on their dependencies.</p> <p>We recommend using <code>SequentialRunner</code> in cases where:</p> <ul> <li>the pipeline has limited branching</li> <li>the pipeline is fast</li> <li>the resource-consuming steps require most of a scarce resource (e.g., significant RAM, disk memory or CPU)</li> </ul> <p>Kedro uses <code>SequentialRunner</code> by default, so to execute the pipeline sequentially:</p> <pre><code>kedro run\n</code></pre> <p>You can also explicitly use <code>SequentialRunner</code> as follows:</p> <pre><code>kedro run --runner=SequentialRunner\n</code></pre>"},{"location":"pages/nodes_and_pipelines/run_a_pipeline/#parallelrunner","title":"<code>ParallelRunner</code>","text":""},{"location":"pages/nodes_and_pipelines/run_a_pipeline/#multiprocessing","title":"Multiprocessing","text":"<p>You can alternatively run the nodes within the pipeline concurrently, using a <code>ParallelRunner</code> as follows:</p> <pre><code>kedro run --runner=ParallelRunner\n</code></pre>"},{"location":"pages/nodes_and_pipelines/run_a_pipeline/#multithreading","title":"Multithreading","text":"<p>While <code>ParallelRunner</code> uses multiprocessing, you can also run the pipeline with multithreading for concurrent execution by specifying <code>ThreadRunner</code> as follows:</p> <pre><code>kedro run --runner=ThreadRunner\n</code></pre> <pre><code>`SparkDataset` doesn't work correctly with `ParallelRunner`. To add concurrency to the pipeline with `SparkDataset`, you must use `ThreadRunner`.\n</code></pre> <p>For more information on how to maximise concurrency when using Kedro with PySpark, read our guide on how to build a Kedro pipeline with PySpark.</p>"},{"location":"pages/nodes_and_pipelines/run_a_pipeline/#custom-runners","title":"Custom runners","text":"<p>If the built-in Kedro runners do not meet your requirements, you can also define your own runner within your project. For example, you may want to add a dry runner, which lists which nodes would be run without executing them:</p> Click to expand <pre><code># in src/&lt;package_name&gt;/runner.py\nfrom typing import Any, Dict\nfrom kedro.io import AbstractDataset, DataCatalog, MemoryDataset\nfrom kedro.pipeline import Pipeline\nfrom kedro.runner.runner import AbstractRunner\nfrom pluggy import PluginManager\n\n\nclass DryRunner(AbstractRunner):\n    \"\"\"``DryRunner`` is an ``AbstractRunner`` implementation. It can be used to list which\n    nodes would be run without actually executing anything. It also checks if all the\n    necessary data exists.\n    \"\"\"\n\n    def __init__(self, is_async: bool = False, extra_dataset_patterns: Dict[str, Dict[str, Any]] = None):\n        \"\"\"Instantiates the runner class.\n\n        Args:\n            is_async: If True, the node inputs and outputs are loaded and saved\n                asynchronously with threads. Defaults to False.\n            extra_dataset_patterns: Extra dataset factory patterns to be added to the DataCatalog\n                during the run. This is used to set the default datasets.\n        \"\"\"\n        default_dataset_pattern = {\"{default}\": {\"type\": \"MemoryDataset\"}}\n        self._extra_dataset_patterns = extra_dataset_patterns or default_dataset_pattern\n        super().__init__(is_async=is_async, extra_dataset_patterns=self._extra_dataset_patterns)\n\n    def _run(\n        self,\n        pipeline: Pipeline,\n        catalog: DataCatalog,\n        hook_manager: PluginManager = None,\n        session_id: str = None,\n    ) -&gt; None:\n        \"\"\"The method implementing dry pipeline running.\n        Example logs output using this implementation:\n\n            kedro.runner.dry_runner - INFO - Actual run would execute 3 nodes:\n            node3: identity([A]) -&gt; [B]\n            node2: identity([C]) -&gt; [D]\n            node1: identity([D]) -&gt; [E]\n\n        Args:\n            pipeline: The ``Pipeline`` to run.\n            catalog: The ``DataCatalog`` from which to fetch data.\n            hook_manager: The ``PluginManager`` to activate hooks.\n            session_id: The id of the session.\n\n        \"\"\"\n        nodes = pipeline.nodes\n        self._logger.info(\n            \"Actual run would execute %d nodes:\\n%s\",\n            len(nodes),\n            pipeline.describe(),\n        )\n        self._logger.info(\"Checking inputs...\")\n        input_names = pipeline.inputs()\n\n        missing_inputs = [\n            input_name\n            for input_name in input_names\n            if not catalog._get_dataset(input_name).exists()\n        ]\n        if missing_inputs:\n            raise KeyError(f\"Datasets {missing_inputs} not found.\")\n</code></pre> <p>And use it with <code>kedro run</code> through the <code>--runner</code> flag:</p> <pre><code>$ kedro run --runner=&lt;package_name&gt;.runner.DryRunner\n</code></pre>"},{"location":"pages/nodes_and_pipelines/run_a_pipeline/#load-and-save-asynchronously","title":"Load and save asynchronously","text":"<pre><code>`ThreadRunner` doesn't support asynchronous load-input or save-output operations.\n</code></pre> <p>When processing a node, both <code>SequentialRunner</code> and <code>ParallelRunner</code> perform the following steps in order:</p> <ol> <li>Load data based on node input(s)</li> <li>Execute node function with the input(s)</li> <li>Save the output(s)</li> </ol> <p>If a node has multiple inputs or outputs (e.g., <code>node(func, [\"a\", \"b\", \"c\"], [\"d\", \"e\", \"f\"])</code>), you can reduce load and save time by using asynchronous mode. You can enable it by passing an <code>--async</code> flag to the run command as follows:</p> <pre><code>$ kedro run --async\n...\n2020-03-24 09:20:01,482 - kedro.runner.sequential_runner - INFO - Asynchronous mode is enabled for loading and saving data\n...\n</code></pre> <pre><code>All the datasets used in the run have to be [thread-safe](https://www.quora.com/What-is-thread-safety-in-Python) in order for asynchronous loading/saving to work properly.\n</code></pre>"},{"location":"pages/nodes_and_pipelines/run_a_pipeline/#run-a-pipeline-by-name","title":"Run a pipeline by name","text":"<p>To run the pipeline by its name, you need to add your new pipeline to the <code>register_pipelines()</code> function in <code>src/&lt;package_name&gt;/pipeline_registry.py</code>:</p> Click to expand <pre><code>def register_pipelines():\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    pipelines = find_pipelines()\n    pipelines[\"__default__\"] = sum(pipelines.values())\n    my_pipeline = pipeline(\n        [\n            # your definition goes here\n        ]\n    )\n    pipelines[\"my_pipeline\"] = my_pipeline\n    return pipelines\n</code></pre> <p>Then, from the command line, execute the following:</p> <pre><code>kedro run --pipeline=my_pipeline\n</code></pre> <pre><code>If you specify `kedro run` without the `--pipeline` option, it runs the `__default__` pipeline from the dictionary returned by `register_pipelines()`.\n</code></pre> <p>Further information about <code>kedro run</code> can be found in the Kedro CLI documentation.</p>"},{"location":"pages/nodes_and_pipelines/run_a_pipeline/#run-pipelines-with-io","title":"Run pipelines with IO","text":"<p>The above definition of pipelines only applies for non-stateful or \"pure\" pipelines that do not interact with the outside world. In practice, we would like to interact with APIs, databases, files and other sources of data. By combining IO and pipelines, we can tackle these more complex use cases.</p> <p>By using <code>DataCatalog</code> from the IO module we are still able to write pure functions that work with our data and outsource file saving and loading to <code>DataCatalog</code>.</p> <p>Through <code>DataCatalog</code>, we can control where inputs are loaded from, where intermediate variables get persisted and ultimately the location to which output variables are written.</p> <p>In a simple example, we define a <code>MemoryDataset</code> called <code>xs</code> to store our inputs, save our input list <code>[1, 2, 3]</code> into <code>xs</code>, then instantiate <code>SequentialRunner</code> and call its <code>run</code> method with the pipeline and data catalog instances:</p> Click to expand <pre><code>io = DataCatalog(dict(xs=MemoryDataset()))\n</code></pre> <pre><code>io.list()\n</code></pre>   `Output`:   <pre><code>Out[10]: ['xs']\n</code></pre> <pre><code>io.save(\"xs\", [1, 2, 3])\n</code></pre> <pre><code>SequentialRunner().run(pipeline, catalog=io)\n</code></pre>   `Output`:   <pre><code>Out[11]: {'v': 0.666666666666667}\n</code></pre>"},{"location":"pages/nodes_and_pipelines/run_a_pipeline/#configure-kedro-run-arguments","title":"Configure <code>kedro run</code> arguments","text":"<p>The Kedro CLI documentation lists the available CLI options for <code>kedro run</code>. You can alternatively supply a configuration file that contains the arguments to <code>kedro run</code>.</p> <p>Here is an example file named <code>config.yml</code>, but you can choose any name for the file:</p> <pre><code>$ kedro run --config=config.yml\n</code></pre> <p>where <code>config.yml</code> is formatted as below (for example):</p> <pre><code>run:\n  tags: tag1, tag2, tag3\n  pipeline: pipeline1\n  runner: ParallelRunner\n  node_names: node1, node2\n  env: env1\n</code></pre> <p>The syntax for the options is different when you're using the CLI compared to the configuration file. In the CLI you use dashes, for example for <code>kedro run --from-nodes=...</code>, but you have to use an underscore in the configuration file:</p> <pre><code>run:\n  from_nodes: ...\n</code></pre> <p>This is because the configuration file gets parsed by Click, a Python package to handle command line interfaces. Click passes the options defined in the configuration file to a Python function. The option names need to match the argument names in that function.</p> <p>Variable names and arguments in Python may only contain alpha-numeric characters and underscores, so it's not possible to have a dash in the option names when using the configuration file.</p> <pre><code>If you provide both a configuration file and a CLI option that clashes with the configuration file, the CLI option will take precedence.\n</code></pre>"},{"location":"pages/nodes_and_pipelines/slice_a_pipeline/","title":"Slice a pipeline","text":"<p>Sometimes it is desirable to run a subset, or a 'slice' of a pipeline's nodes. There are two primary ways to achieve this:</p> <ol> <li>Visually through Kedro-Viz: This approach allows you to visually choose and slice pipeline nodes, which then generates a run command for executing the slice within your Kedro project. Detailed steps on how to achieve this are available in the Kedro-Viz documentation: Slice a Pipeline.</li> </ol> <p></p> <ol> <li>Programmatically with the Kedro CLI. You can also use the Kedro CLI to pass parameters to <code>kedro run</code> command and slice a pipeline. In this page, we illustrate the programmatic options that Kedro provides.</li> </ol> <p>Let's look again at the example pipeline from the pipeline introduction documentation, which computes the variance of a set of numbers:</p> Click to expand <pre><code>def mean(xs, n):\n    return sum(xs) / n\n\n\ndef mean_sos(xs, n):\n    return sum(x**2 for x in xs) / n\n\n\ndef variance(m, m2):\n    return m2 - m * m\n\n\nfull_pipeline = pipeline(\n    [\n        node(len, \"xs\", \"n\"),\n        node(mean, [\"xs\", \"n\"], \"m\", name=\"mean_node\", tags=\"mean\"),\n        node(mean_sos, [\"xs\", \"n\"], \"m2\", name=\"mean_sos\", tags=[\"mean\", \"variance\"]),\n        node(variance, [\"m\", \"m2\"], \"v\", name=\"variance_node\", tags=\"variance\"),\n    ]\n)\n</code></pre> <p>The <code>Pipeline.describe()</code> method returns the following output:</p> Click to expand <pre><code>#### Pipeline execution order ####\nName: None\nInputs: xs\n\nlen([xs]) -&gt; [n]\nmean_node\nmean_sos\nvariance_node\n\nOutputs: v\n##################################\n</code></pre>"},{"location":"pages/nodes_and_pipelines/slice_a_pipeline/#slice-a-pipeline-by-providing-inputs","title":"Slice a pipeline by providing inputs","text":"<p>One way to slice a pipeline is to provide a set of pre-calculated inputs which should serve as a start of the pipeline. For example, in order to slice the pipeline to run from input <code>m2</code> downstream you can specify it like this:</p> Click to expand <pre><code>print(full_pipeline.from_inputs(\"m2\").describe())\n</code></pre>   `Output`:   <pre><code>#### Pipeline execution order ####\nName: None\nInputs: m, m2\n\nvariance_node\n\nOutputs: v\n##################################\n</code></pre> <p>Slicing the pipeline from inputs <code>m</code> and <code>xs</code> results in the following pipeline:</p> Click to expand <pre><code>print(full_pipeline.from_inputs(\"m\", \"xs\").describe())\n</code></pre>   `Output`:   <pre><code>#### Pipeline execution order ####\nName: None\nInputs: xs\n\nlen([xs]) -&gt; [n]\nmean_node\nmean_sos\nvariance_node\n\nOutputs: v\n##################################\n</code></pre> <p>As you can see, adding <code>m</code> in the <code>from_inputs</code> list does not guarantee that it will not be recomputed if another input like <code>xs</code> is specified.</p>"},{"location":"pages/nodes_and_pipelines/slice_a_pipeline/#slice-a-pipeline-by-specifying-nodes","title":"Slice a pipeline by specifying nodes","text":"<p>Another way of slicing a pipeline is to specify the nodes which should be used as a start of the new pipeline. For example:</p> Click to expand <pre><code>print(full_pipeline.from_nodes(\"mean_node\").describe())\n</code></pre>   `Output`:   <pre><code>#### Pipeline execution order ####\nName: None\nInputs: m2, n, xs\n\nmean_node\nvariance_node\n\nOutputs: v\n##################################\n</code></pre> <p>As you can see, this will slice the pipeline and run it from the specified node to all other nodes downstream.</p> <p>You can run the resulting pipeline slice by running the following command in your terminal window:</p> <pre><code>kedro run --from-nodes=mean_node\n</code></pre>"},{"location":"pages/nodes_and_pipelines/slice_a_pipeline/#slice-a-pipeline-by-specifying-final-nodes","title":"Slice a pipeline by specifying final nodes","text":"<p>Similarly, you can specify the nodes which should be used to end a pipeline. For example:</p> Click to expand <pre><code>print(full_pipeline.to_nodes(\"mean_node\").describe())\n</code></pre>   `Output`:   <pre><code>#### Pipeline execution order ####\nName: None\nInputs: xs\n\nlen([xs]) -&gt; [n]\nmean_node\n\nOutputs: m\n##################################\n</code></pre> <p>As you can see, this will slice the pipeline, so it runs from the beginning and ends with the specified node:</p> <pre><code>kedro run --to-nodes=mean_node\n</code></pre> <p>You can also slice a pipeline by specifying the start and finish nodes, and thus the set of nodes to be included in the pipeline slice:</p> <pre><code>kedro run --from-nodes=A --to-nodes=Z\n</code></pre> <p>or, when specifying multiple nodes:</p> <pre><code>kedro run --from-nodes=A,D --to-nodes=X,Y,Z\n</code></pre>"},{"location":"pages/nodes_and_pipelines/slice_a_pipeline/#slice-a-pipeline-with-tagged-nodes","title":"Slice a pipeline with tagged nodes","text":"<p>You can also slice a pipeline from the nodes that have specific tags attached to them. For example, for nodes that have both tag <code>mean</code> AND tag <code>variance</code>, you can run the following:</p> Click to expand <pre><code>print(full_pipeline.only_nodes_with_tags(\"mean\", \"variance\").describe())\n</code></pre>   `Output`:   <pre><code>#### Pipeline execution order ####\nInputs: n, xs\n\nmean_sos\n\nOutputs: m2\n##################################\n</code></pre> <p>To slice a pipeline from nodes that have tag <code>mean</code> OR tag <code>variance</code>:</p> Click to expand <pre><code>sliced_pipeline = full_pipeline.only_nodes_with_tags(\n    \"mean\"\n) + full_pipeline.only_nodes_with_tags(\"variance\")\nprint(sliced_pipeline.describe())\n</code></pre>   `Output`:   <pre><code>#### Pipeline execution order ####\nInputs: n, xs\n\nmean\nmean_sos\nvariance\n\nOutputs: v\n##################################\n</code></pre>"},{"location":"pages/nodes_and_pipelines/slice_a_pipeline/#slice-a-pipeline-by-running-specified-nodes","title":"Slice a pipeline by running specified nodes","text":"<p>Sometimes you might need to run only some of the nodes in a pipeline, as follows:</p> Click to expand <pre><code>print(full_pipeline.only_nodes(\"mean_node\", \"mean_sos\").describe())\n</code></pre>   `Output`:   <pre><code>#### Pipeline execution order ####\nName: None\nInputs: n, xs\n\nmean_node\nmean_sos\n\nOutputs: m, m2\n##################################\n</code></pre> <p>This will create a sliced pipeline, comprised of the nodes you specify in the method call.</p> <pre><code>All the inputs required by the specified nodes must exist, i.e. already produced or present in the data catalog.\n</code></pre>"},{"location":"pages/nodes_and_pipelines/slice_a_pipeline/#how-to-recreate-missing-outputs","title":"How to recreate missing outputs","text":"<p>Kedro can automatically generate a sliced pipeline from existing node outputs. This can be helpful if you want to avoid re-running nodes that take a long time:</p> Click to expand <pre><code>print(full_pipeline.describe())\n</code></pre>   `Output`:   <pre><code>#### Pipeline execution order ####\nName: None\nInputs: xs\n\nlen([xs]) -&gt; [n]\nmean_node\nmean_sos\nvariance_node\n\nOutputs: v\n##################################\n</code></pre> <p>To demonstrate this, let us save the intermediate output <code>n</code> using a <code>JSONDataset</code>.</p> Click to expand <pre><code>from kedro_datasets.pandas import JSONDataset\nfrom kedro.io import DataCatalog, MemoryDataset\n\nn_json = JSONDataset(filepath=\"./data/07_model_output/len.json\")\nio = DataCatalog(dict(xs=MemoryDataset([1, 2, 3]), n=n_json))\n</code></pre> <p>Because <code>n</code> was not saved previously, checking for its existence returns <code>False</code>:</p> Click to expand <pre><code>io.exists(\"n\")\n</code></pre>   `Output`:   <pre><code>Out[15]: False\n</code></pre> <p>Running the pipeline calculates <code>n</code> and saves the result to disk:</p> Click to expand <pre><code>SequentialRunner().run(full_pipeline, io)\n</code></pre>   `Output`:   <pre><code>Out[16]: {'v': 0.666666666666667}\n</code></pre> <pre><code>io.exists(\"n\")\n</code></pre>   `Output`:   <pre><code>Out[17]: True\n</code></pre> <p>We can avoid re-calculating <code>n</code> (and all other results that have already been saved) by using the <code>Runner.run_only_missing</code> method. Note that the first node of the original pipeline (<code>len([xs]) -&gt; [n]</code>) has not been run:</p> Click to expand <pre><code>SequentialRunner().run_only_missing(full_pipeline, io)\n</code></pre>   `Ouput`:   <pre><code>Out[18]: {'v': 0.666666666666667}\n</code></pre> <pre><code>try:\n    os.remove(\"./data/07_model_output/len.json\")\nexcept FileNotFoundError:\n    pass\n</code></pre>"},{"location":"pages/notebooks_and_ipython/","title":"Kedro for notebook users","text":"<p>If you are familiar with notebooks, you probably find their liberal development environment perfect for exploratory data analysis and experimentation.</p> <p>Kedro makes it easier to organise your code into a shareable project, and you may decide to transition to Kedro for collaboration purposes, or if your code becomes more complex.</p> <p>There is flexibility in the ways you can combine notebooks and Kedro. For example, it's possible to gradually introduce Kedro techniques into your notebook code. Likewise, it is possible to take a Kedro project and add a notebooks to explore data or experimental features.</p>"},{"location":"pages/notebooks_and_ipython/#add-kedro-features-to-a-notebook","title":"Add Kedro features to a notebook","text":"<p>How to add Kedro to your existing notebook project The page titled Add Kedro features to a notebook describes how to convert your notebook project to use Kedro in increments. It starts with the basics of configuration loading, then adds Kedro's data management approach, and finally introduces nodes and pipelines.</p>"},{"location":"pages/notebooks_and_ipython/#use-a-jupyter-notebook-for-kedro-project-experiments","title":"Use a Jupyter notebook for Kedro project experiments","text":"<p>How to add a notebook to your existing Kedro project The page titled Use a Jupyter notebook for Kedro project experiments describes how to set up a notebook to access the elements of a Kedro project for experimentation. If you have an existing Kedro project but want to use notebook features to explore your data and experiment with pipelines, this is the page to start.</p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/","title":"Use a Jupyter notebook for Kedro project experiments","text":"<p>This page explains how to use a Jupyter notebook to explore elements of a Kedro project. It shows how to use <code>kedro jupyter notebook</code> to set up a notebook that has access to the <code>catalog</code>, <code>context</code>, <code>pipelines</code> and <code>session</code> variables of the Kedro project so you can query them.</p> <p>This page also explains how to use line magic to display a Kedro-Viz visualisation of your pipeline directly in your notebook.</p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#example-project","title":"Example project","text":"<p>The example adds a notebook to experiment with the  <code>spaceflight-pandas-viz</code> starter. As an alternative, you can follow the example using a different starter or just add a notebook to your own project.</p> <p>We will assume the example project is called <code>spaceflights</code>, but you can call it whatever you choose.</p> <p>To create a project, you can run this command:</p> <pre><code>kedro new -n spaceflights --tools=viz --example=yes\n</code></pre> <p>You can find more options of <code>kedro new</code> from Create a new Kedro Project.</p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#loading-the-project-with-kedro-jupyter-notebook","title":"Loading the project with <code>kedro jupyter notebook</code>","text":"<p>Navigate to the project directory (<code>cd spaceflights</code>) and issue the following command in the terminal to launch Jupyter:</p> <pre><code>kedro jupyter notebook\n</code></pre> <p>You'll be asked if you want to opt into usage analytics on the first run of your new project. Once you've answered the question with <code>y</code> or <code>n</code>, your browser window will open with a Jupyter page that lists the folders in your project:</p> <p></p> <p>You can now create a new Jupyter notebook using the New dropdown and selecting the Kedro (iris) kernel:</p> <p></p> <p>This opens a new browser tab to display the empty notebook:</p> <p></p> <p>We recommend that you save your notebook in the <code>notebooks</code> folder of your Kedro project.</p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#what-does-kedro-jupyter-notebook-do","title":"What does <code>kedro jupyter notebook</code> do?","text":"<p>The <code>kedro jupyter notebook</code> command launches a notebook with a customised kernel that has been extended to make the following project variables available:</p> <ul> <li><code>catalog</code> (type {py:class}<code>~kedro.io.DataCatalog</code>): Data Catalog instance that contains all defined datasets; this is a shortcut for <code>context.catalog</code></li> <li><code>context</code> (type {py:class}<code>~kedro.framework.context.KedroContext</code>): Kedro project context that provides access to Kedro's library components</li> <li><code>pipelines</code> (type <code>dict[str, Pipeline]</code>): Pipelines defined in your pipeline registry</li> <li><code>session</code> (type {py:class}<code>~kedro.framework.session.session.KedroSession</code>): Kedro session that orchestrates a pipeline run</li> </ul> <p>In addtion, it also runs <code>%load_ext kedro.ipython</code> automatically when you launch the notebook.</p> <pre><code>If the Kedro variables are not available within your Jupyter notebook, you could have a malformed configuration file or missing dependencies. The full error message is shown on the terminal used to launch `kedro jupyter notebook` or run `%load_ext kedro.ipython` in a notebook cell.\n</code></pre>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#loading-the-project-with-the-kedroipython-extension","title":"Loading the project with the <code>kedro.ipython</code> extension","text":"<p>A quick way to explore the <code>catalog</code>, <code>context</code>, <code>pipelines</code>, and <code>session</code> variables in your project within a IPython compatible environment, such as Databricks notebooks, Google Colab, and more, is to use the <code>kedro.ipython</code> extension. This is tool-independent and useful in situations where launching a Jupyter interactive environment is not possible. You can use the <code>%load_ext</code> line magic to explicitly load the Kedro IPython extension:</p> <pre><code>In [1]: %load_ext kedro.ipython\n</code></pre> <p>If you have launched your interactive environment from outside your Kedro project, you will need to run a second line magic to set the project path. This is so that Kedro can load the <code>catalog</code>, <code>context</code>, <code>pipelines</code> and <code>session</code> variables:</p> <pre><code>In [2]: %reload_kedro &lt;project_root&gt;\n</code></pre> <p>The Kedro IPython extension remembers the project path so that future calls to <code>%reload_kedro</code> do not need to specify it:</p> <pre><code>In [1]: %load_ext kedro.ipython\nIn [2]: %reload_kedro &lt;project_root&gt;\nIn [3]: %reload_kedro\n</code></pre>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#exploring-the-kedro-project-in-a-notebook","title":"Exploring the Kedro project in a notebook","text":"<p>Here are some examples of how to work with the Kedro variables. To explore the full range of attributes and methods available, see the relevant {doc}<code>API documentation &lt;/api/kedro&gt;</code> or use the Python {py:func}<code>dir</code> function, for example <code>dir(catalog)</code>.</p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#catalog","title":"<code>catalog</code>","text":"<p><code>catalog</code> can be used to explore your project's Data Catalog using methods such as {py:meth}<code>catalog.list &lt;kedro.io.DataCatalog.list&gt;</code>, {py:meth}<code>catalog.load &lt;kedro.io.DataCatalog.load&gt;</code> and {py:meth}<code>catalog.save &lt;kedro.io.DataCatalog.save&gt;</code>.</p> <p>For example, add the following to a cell in your notebook to run <code>catalog.list</code>:</p> <pre><code>catalog.list()\n</code></pre> <p>When you run the cell:</p> <pre><code>[\n    'companies',\n    'reviews',\n    'shuttles',\n    'preprocessed_companies',\n    'preprocessed_shuttles',\n    'model_input_table',\n    'regressor',\n    'metrics',\n    'companies_columns',\n    'shuttle_passenger_capacity_plot_exp',\n    'shuttle_passenger_capacity_plot_go',\n    'dummy_confusion_matrix',\n    'parameters',\n    'params:model_options',\n    'params:model_options.test_size',\n    'params:model_options.random_state',\n    'params:model_options.features'\n]\n</code></pre>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#search-datasets-with-regex","title":"Search datasets with regex","text":"<p>If you do not remember the exact name of a dataset, you can provide a regular expression to search datasets.</p> <pre><code>catalog.list(\"pre*\")\n</code></pre> <p>When you run the cell:</p> <pre><code>['preprocessed_companies', 'preprocessed_shuttles']\n</code></pre> <p>Next try the following for <code>catalog.load</code>:</p> <pre><code>catalog.load(\"shuttles\")\n</code></pre> <p>The output:</p> <pre><code>[06/05/24 12:50:17] INFO     Loading data from reviews (CSVDataset)...\nOut[1]:\n\n       shuttle_id  review_scores_rating  review_scores_comfort  ...  review_scores_price  number_of_reviews  reviews_per_month\n0           45163                  91.0                   10.0  ...                  9.0                 26               0.77\n1           49438                  96.0                   10.0  ...                  9.0                 61               0.62\n2           10750                  97.0                   10.0  ...                 10.0                467               4.66\n3            4146                  95.0                   10.0  ...                  9.0                318               3.22\n\n</code></pre> <p>Now try the following:</p> <pre><code>catalog.load(\"parameters\")\n</code></pre> <p>You should see this:</p> <pre><code>INFO     Loading data from 'parameters' (MemoryDataset)...\n\n{\n    'model_options': {\n        'test_size': 0.2,\n        'random_state': 3,\n        'features': [\n            'engines',\n            'passenger_capacity',\n            'crew',\n            'd_check_complete',\n            'moon_clearance_complete',\n            'iata_approved',\n            'company_rating',\n            'review_scores_rating'\n        ]\n    }\n}\n</code></pre> <pre><code>If you enable [versioning](../data/data_catalog.md#dataset-versioning) you can load a particular version of a dataset, e.g. `catalog.load(\"preprocessed_shuttles\", version=\"2024-06-05T15.08.09.255Z\")`.\n</code></pre>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#context","title":"<code>context</code>","text":"<p><code>context</code> enables you to access Kedro's library components and project metadata. For example, if you add the following to a cell and run it:</p> <pre><code>context.project_path\n</code></pre> <p>You should see output like this, according to your username and path:</p> <pre><code>PosixPath('/Users/username/kedro_projects/spaceflights')\n</code></pre> <p>You can find out more in the API documentation of {py:class}<code>~kedro.framework.context.KedroContext</code>.</p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#pipelines","title":"<code>pipelines</code>","text":"<p><code>pipelines</code> is a dictionary containing your project's registered pipelines:</p> <pre><code>pipelines\n</code></pre> <p>The output will be a listing as follows:</p> <pre><code>{'__default__': Pipeline([\nNode(create_confusion_matrix, 'companies', 'dummy_confusion_matrix', None),\nNode(preprocess_companies, 'companies', ['preprocessed_companies', 'companies_columns'], 'preprocess_companies_node'),\nNode(preprocess_shuttles, 'shuttles', 'preprocessed_shuttles', 'preprocess_shuttles_node'),\n...\n</code></pre> <p>You can use this to explore your pipelines and the nodes they contain:</p> <pre><code>pipelines[\"__default__\"].all_outputs()\n</code></pre> <p>Should give the output:</p> <pre><code>{\n    'X_train',\n    'regressor',\n    'shuttle_passenger_capacity_plot_exp',\n    'y_test',\n    'model_input_table',\n    'y_train',\n    'X_test',\n    'metrics',\n    'companies_columns',\n    'preprocessed_shuttles',\n    'preprocessed_companies',\n    'shuttle_passenger_capacity_plot_go',\n    'dummy_confusion_matrix'\n}\n</code></pre>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#session","title":"<code>session</code>","text":"<p><code>session.run</code> allows you to run a pipeline. With no arguments, this will run your <code>__default__</code> project pipeline sequentially, much as a call to <code>kedro run</code> from the terminal:</p> <pre><code>session.run()\n</code></pre> <p>You can also specify the following optional arguments for <code>session.run</code>:</p> Argument name Accepted types Description <code>tags</code> <code>Iterable[str]</code> Construct the pipeline using nodes which have this tag attached. A node is included in the resulting pipeline if it contains any of those tags <code>runner</code> <code>AbstractRunner</code> An instance of Kedro {py:class}<code>~kedro.runner.AbstractRunner</code>. Can be an instance of a {py:class}<code>~kedro.runner.ParallelRunner</code> <code>node_names</code> <code>Iterable[str]</code> Run nodes with specified names <code>from_nodes</code> <code>Iterable[str]</code> A list of node names which should be used as a starting point <code>to_nodes</code> <code>Iterable[str]</code> A list of node names which should be used as an end point <code>from_inputs</code> <code>Iterable[str]</code> A list of dataset names which should be used as a starting point <code>to_outputs</code> <code>Iterable[str]</code> A list of dataset names which should be used as an end point <code>load_versions</code> <code>Dict[str, str]</code> A mapping of a dataset name to a specific dataset version (timestamp) for loading. Applies to versioned datasets <code>pipeline_name</code> <code>str</code> Name of the modular pipeline to run. Must be one of those returned by the <code>register_pipelines</code> function in <code>src/&lt;package_name&gt;/pipeline_registry.py</code> <p>You can execute one successful run per session, as there's a one-to-one mapping between a session and a run. If you wish to do more than one run, you'll have to run <code>%reload_kedro</code> line magic to get a new <code>session</code>.</p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#kedro-line-magics","title":"Kedro line magics","text":"<p>{external+ipython:doc}<code>Line magics &lt;interactive/magics&gt;</code> are commands that provide a concise way of performing tasks in an interactive session. Kedro provides several line magic commands to simplify working with Kedro projects in interactive environments.</p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#reload_kedro-line-magic","title":"<code>%reload_kedro</code> line magic","text":"<p>You can use <code>%reload_kedro</code> line magic within your Jupyter notebook to reload the Kedro variables (for example, if you need to update <code>catalog</code> following changes to your Data Catalog).</p> <p>You don't need to restart the kernel for the <code>catalog</code>, <code>context</code>, <code>pipelines</code> and <code>session</code> variables.</p> <p><code>%reload_kedro</code> accepts optional keyword arguments <code>env</code> and <code>params</code>. For example, to use configuration environment <code>prod</code>:</p> <pre><code>%reload_kedro --env=prod\n</code></pre> <p>For more details, run <code>%reload_kedro?</code>.</p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#load_node-line-magic","title":"<code>%load_node</code> line magic","text":"<pre><code>This is still an experimental feature and is currently only available for Jupyter Notebook (&gt;7.0), Jupyter Lab, IPython, and VS Code Notebook. If you encounter unexpected behaviour or would like to suggest feature enhancements, add it under [this github issue](https://github.com/kedro-org/kedro/issues/3580).\n</code></pre> <p>When using this feature in Jupyter Notebook you will need to have the following requirements and minimum versions installed:</p> <pre><code>ipylab&gt;=1.0.0\nnotebook&gt;=7.0.0\n</code></pre> <p>You can load the contents of a node in your project into a series of cells using the <code>%load_node</code> line magic. To use <code>%load_node</code>, the node you want to load needs to fulfil two requirements: - The node needs to have a name - The node's inputs need to be persisted</p> <p>The section about creating nodes with names explains how to ensure your node has a name. By default, Kedro saves data in memory. To persist the data, you need to declare the dataset in the Data Catalog.</p> <pre><code>The node name needs to be unique within the pipeline. In the absence of a user defined name, Kedro generates one using a combination of the function name, inputs and outputs.\n</code></pre> <p>The line magic will load your node's inputs, imports, and body:</p> <pre><code>%load_node &lt;my-node-name&gt;\n</code></pre> Click to see an example.  ![jupyter_ipython_load_node](../meta/images/jupyter_ipython_load_node.gif)   <p>To be able to access your node's inputs, make sure they are explicitly defined in your project's catalog.</p> <p>You can then run the generated cells to recreate how the node would run in your pipeline. You can use this to explore your node's inputs, behaviour, and outputs in isolation, or for debugging.</p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#run_viz-line-magic","title":"<code>%run_viz</code> line magic","text":"<pre><code>If you have not yet installed [Kedro-Viz](https://github.com/kedro-org/kedro-viz) for the project, run `pip install kedro-viz` in your terminal from within the project directory.\n</code></pre> <p>You can display an interactive visualisation of your pipeline directly in your notebook using the <code>%run_viz</code> line magic from within a cell:</p> <pre><code>%run_viz\n</code></pre> <p></p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#debugging-a-kedro-project-within-a-notebook","title":"Debugging a Kedro project within a notebook","text":"<p>You can use the built-in <code>%debug</code> line magic to launch an interactive debugger in your Jupyter notebook. Declare it before a single-line statement to step through the execution in debug mode. You can use the argument <code>--breakpoint</code> or <code>-b</code> to provide a breakpoint. Alternatively, use the command with no arguments after an error occurs to load the stack trace and begin debugging.</p> <p>The following sequence occurs when <code>%debug</code> runs after an error occurs:</p> <ul> <li>The stack trace of the last unhandled exception loads.</li> <li>The program stops at the point where the exception occurred.</li> <li>An interactive shell where the user can navigate through the stack trace opens.</li> </ul> <p>You can then inspect the value of expressions and arguments, or add breakpoints to the code.</p> <p>Here is example debugging workflow after discovering a node in your pipeline is failing: 1. Inspect the logs to find the name of the failing node. We can see below the problematic node is <code>split_data_node</code>.</p> Click to the pipeline failure logs.  ![pipeline_error_logs](../meta/images/pipeline_error_logs.png)   <ol> <li>In your notebook, run <code>%load_node &lt;name-of-failing-node&gt;</code> to load the contents of the problematic node with the <code>%load_node</code> line magic.</li> <li>Run the populated cells to examine the node's behaviour in isolation.</li> <li>If the node fails in error, use <code>%debug</code> to launch an interactive debugging session in your notebook.</li> </ol> Click to see this workflow in action.  ![jupyter_ipython_debug_command](../meta/images/jupyter_ipython_debug_command.gif)   <pre><code>The `%load_node` line magic is currently only available for Jupyter Notebook (&gt;7.0) and Jupyter Lab. If you are working within a different interactive environment, manually copy over the contents from your project files instead of using `%load_node` to automatically populate your node's contents, and continue from step 2.\n</code></pre> <p>You can also set up the debugger to run automatically when an exception occurs by using the <code>%pdb</code> line magic. This automatic behaviour can be enabled with <code>%pdb 1</code> or <code>%pdb on</code> before executing a program, and disabled with <code>%pdb 0</code> or <code>%pdb off</code>.</p> Click to see an example.   ![jupyter_ipython_pdb_command](../meta/images/jupyter_ipython_pdb_command.gif)   <p>Some examples of the possible commands that can be used to interact with the ipdb shell are as follows:</p> Command Description <code>list</code> Show the current location in the file <code>h(elp)</code> Show a list of commands, or find help on a specific command <code>q(uit)</code> Quit the debugger and the program <code>c(ontinue)</code> Quit the debugger, continue in the program <code>n(ext)</code> Go to the next step of the program <code>&lt;enter&gt;</code> Repeat the previous command <code>p(rint)</code> Print variables <code>s(tep)</code> Step into a subroutine <code>r(eturn)</code> Return out of a subroutine <code>b(reak)</code> Insert a breakpoint <code>a(rgs)</code> Print the argument list of the current function <p>For more information, use the <code>help</code> command in the debugger, or take at the ipdb repository for guidance.</p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#useful-to-know-for-advanced-users","title":"Useful to know (for advanced users)","text":"<p>Each Kedro project has its own Jupyter kernel so you can switch between Kedro projects from a single Jupyter instance by selecting the appropriate kernel.</p> <p>To ensure that a Jupyter kernel always points to the correct Python executable, if one already exists with the same name <code>kedro_&lt;package_name&gt;</code>, then it is replaced.</p> <p>You can use the <code>jupyter kernelspec</code> set of commands to manage your Jupyter kernels. For example, to remove a kernel, run <code>jupyter kernelspec remove &lt;kernel_name&gt;</code>.</p>"},{"location":"pages/notebooks_and_ipython/kedro_and_notebooks/#ipython-jupyterlab-and-other-jupyter-clients","title":"IPython, JupyterLab and other Jupyter clients","text":"<p>You can also connect an IPython shell to a Kedro project kernel as follows:</p> <pre><code>kedro ipython\n</code></pre> <p>The command launches an IPython shell with the extension already loaded and is the same command as  <code>ipython --ext kedro.ipython</code>. You first saw this in action in the spaceflights tutorial.</p> <p>Similarly, the following creates a custom Jupyter kernel that automatically loads the extension and launches JupyterLab with this kernel selected:</p> <pre><code>kedro jupyter lab\n</code></pre> <p>You can use any other Jupyter client to connect to a Kedro project kernel such as the Qt Console, which can be launched using the <code>spaceflights</code> kernel as follows:</p> <pre><code>jupyter qtconsole --kernel=spaceflights\n</code></pre> <p>This will automatically load the Kedro IPython in a console that supports graphical features such as embedded figures: </p>"},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/","title":"Add kedro to a notebook","text":""},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/#add-kedro-features-to-a-notebook","title":"Add Kedro features to a notebook","text":"<p>This page describes how to add Kedro features incrementally to a notebook.</p> <p>It starts with a notebook example which does NOT use Kedro. It then explains how to convert portions of the code to use Kedro features while remaining runnable within a notebook. For that part of the example, you need to have set up Kedro.</p> <p>NOTE: If you want to experiment with the code in a notebook, you can find it in the <code>notebook-example</code> folder on GitHub. Be sure to download the entire folder, or clone the entire repo, because the <code>add_kedro_to_spaceflights_notebook.ipynb</code> notebook relies upon files stored in the <code>notebook-example</code> folder.</p>"},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/#kedro-spaceflights","title":"Kedro spaceflights","text":"<p>The Kedro spaceflights tutorial introduces the basics of Kedro in a tutorial that runs as a Kedro project, that is, as a set of <code>.py</code> files. The premise is as follows:</p> <p>It is 2160, and the space tourism industry is booming. Globally, thousands of space shuttle companies take tourists to the Moon and back. You have been able to source data that lists the amenities offered in each space shuttle, customer reviews, and company information.</p> <p>Project: You want to construct a model that predicts the price for each trip to the Moon and the corresponding return flight.</p>"},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/#the-notebook-example","title":"The notebook example","text":"<p>The full example code is given below. To run this, you will need:</p> <pre><code>import pandas as pd\n\ncompanies = pd.read_csv(\"data/companies.csv\")\nreviews = pd.read_csv(\"data/reviews.csv\")\nshuttles = pd.read_excel(\"data/shuttles.xlsx\", engine=\"openpyxl\")\n</code></pre> <pre><code># Data processing\ncompanies[\"iata_approved\"] = companies[\"iata_approved\"] == \"t\"\ncompanies[\"company_rating\"] = (\n    companies[\"company_rating\"].str.replace(\"%\", \"\").astype(float)\n)\nshuttles[\"d_check_complete\"] = shuttles[\"d_check_complete\"] == \"t\"\nshuttles[\"moon_clearance_complete\"] = shuttles[\"moon_clearance_complete\"] == \"t\"\nshuttles[\"price\"] = (\n    shuttles[\"price\"].str.replace(\"$\", \"\").str.replace(\",\", \"\").astype(float)\n)\nrated_shuttles = shuttles.merge(reviews, left_on=\"id\", right_on=\"shuttle_id\")\nmodel_input_table = rated_shuttles.merge(companies, left_on=\"company_id\", right_on=\"id\")\nmodel_input_table = model_input_table.dropna()\nmodel_input_table.head()\n</code></pre> <pre><code># Model training\nfrom sklearn.model_selection import train_test_split\n\nX = model_input_table[\n    [\n        \"engines\",\n        \"passenger_capacity\",\n        \"crew\",\n        \"d_check_complete\",\n        \"moon_clearance_complete\",\n        \"iata_approved\",\n        \"company_rating\",\n        \"review_scores_rating\",\n    ]\n]\ny = model_input_table[\"price\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nmodel.predict(X_test)\n</code></pre> <pre><code># Model evaluation\nfrom sklearn.metrics import r2_score\n\ny_pred = model.predict(X_test)\nr2_score(y_test, y_pred)\n</code></pre>"},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/#use-kedro-for-data-processing","title":"Use Kedro for data processing","text":"<p>Even if you\u2019re not ready to work with a full Kedro project, you can still use its for data handling within an existing notebook project. This section shows you how.</p> <p>Kedro\u2019s Data Catalog is a registry of all data sources available for use by the project. It offers a separate place to declare details of the datasets your projects use. Kedro provides built-in datasets for different file types and file systems so you don\u2019t have to write any of the logic for reading or writing data.</p> <p>Kedro offers a range of datasets, including CSV, Excel, Parquet, Feather, HDF5, JSON, Pickle, SQL Tables, SQL Queries, Spark DataFrames, and more. They are supported with the APIs of pandas, spark, networkx, matplotlib, yaml, and beyond. It relies on\u00a0<code>fsspec</code>\u00a0to read and save data from a variety of data stores including local file systems, network file systems, cloud object stores, and Hadoop. You can pass arguments in to load and save operations, and use versioning and credentials for data access.</p> <p>To start using the Data Catalog, you'll need a <code>catalog.yml</code> to define datasets that can be used when writing your functions. There is one included in the same folder as your notebook:</p> <pre><code>companies:\n  type: pandas.CSVDataset\n  filepath: data/companies.csv\n\nreviews:\n  type: pandas.CSVDataset\n  filepath: data/reviews.csv\n\nshuttles:\n  type: pandas.ExcelDataset\n  filepath: data/shuttles.xlsx\n</code></pre> <p>By using Kedro to load the <code>catalog.yml</code> file, you can reference the Data Catalog in your notebook as you load the data for data processing.</p> <pre><code># Using Kedro's DataCatalog\n\nfrom kedro.io import DataCatalog\n\nimport yaml\n\n# load the configuration file\nwith open(\"catalog.yml\") as f:\n    conf_catalog = yaml.safe_load(f)\n\n# Create the DataCatalog instance from the configuration\ncatalog = DataCatalog.from_config(conf_catalog)\n\n# Load the datasets\ncompanies = catalog.load(\"companies\")\nreviews = catalog.load(\"reviews\")\nshuttles = catalog.load(\"shuttles\")\n</code></pre> <p>The rest of the spaceflights notebook code for data processing and model evaluation from above can now run as before.</p>"},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/#use-a-yaml-configuration-file","title":"Use a YAML configuration file","text":""},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/#use-a-configuration-file-for-magic-numbers","title":"Use a configuration file for \"magic numbers\"","text":"<p>When writing exploratory code, it\u2019s tempting to hard code values to save time, but it makes code harder to maintain in the longer-term. The example code for model evaluation above calls <code>sklearn.model_selection.train_test_split()</code>, passing in a model input table and outputs the test and train datasets. There are hard-code values supplied to <code>test_size</code> and <code>random_state</code>.</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)\n</code></pre> <p>Good software engineering practice suggests that we extract \u2018magic numbers\u2019 into named constants. These could be defined at the top of a file or in a utility file, in a format such as yaml.</p> <pre><code># params.yml\n\nmodel_options:\n  test_size: 0.3\n  random_state: 3\n</code></pre> <p>The <code>params.yml</code> file is included in the example folder so you can reference the values with notebook code as follows:</p> <pre><code>import yaml\n\nwith open(\"params.yml\", encoding=\"utf-8\") as yaml_file:\n    params = yaml.safe_load(yaml_file)\n</code></pre> <pre><code>test_size = params[\"model_options\"][\"test_size\"]\nrandom_state = params[\"model_options\"][\"random_state\"]\n</code></pre> <pre><code>features = [\n    \"engines\",\n    \"passenger_capacity\",\n    \"crew\",\n    \"d_check_complete\",\n    \"moon_clearance_complete\",\n    \"iata_approved\",\n    \"company_rating\",\n    \"review_scores_rating\",\n]\n</code></pre> <pre><code>X = model_input_table[features]\ny = model_input_table[\"price\"]\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=test_size, random_state=random_state\n)\n</code></pre> <p>The rest of the model evaluation code can now run as before.</p> <pre><code>from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nmodel.predict(X_test)\nfrom sklearn.metrics import r2_score\n\ny_pred = model.predict(X_test)\nr2_score(y_test, y_pred)\n</code></pre>"},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/#use-a-configuration-file-for-all-magic-values","title":"Use a configuration file for all \"magic values\"","text":"<p>If we extend the concept of magic numbers to encompass magic values in general, it seems possible that the variable <code>features</code> might also be reusable elsewhere. Extracting it from code into the configuration file named <code>parameters.yml</code> leads to the following:</p> <pre><code># parameters.yml\n\nmodel_options:\n  test_size: 0.3\n  random_state: 3\n  features:\n    - engines\n    - passenger_capacity\n    - crew\n    - d_check_complete\n    - moon_clearance_complete\n    - iata_approved\n    - company_rating\n    - review_scores_rating\n</code></pre> <p>The <code>parameters.yml</code> file is included in the example folder so you can reference the values with notebook code as follows:</p> <pre><code>import yaml\n\nwith open(\"parameters.yml\", encoding=\"utf-8\") as yaml_file:\n    parameters = yaml.safe_load(yaml_file)\n\ntest_size = parameters[\"model_options\"][\"test_size\"]\nrandom_state = parameters[\"model_options\"][\"random_state\"]\n</code></pre> <pre><code>X = model_input_table[parameters[\"model_options\"][\"features\"]]\ny = model_input_table[\"price\"]\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=test_size, random_state=random_state\n)\n</code></pre> <p>The rest of the model evaluation code can now run as before.</p> <pre><code>from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nmodel.predict(X_test)\nfrom sklearn.metrics import r2_score\n\ny_pred = model.predict(X_test)\nr2_score(y_test, y_pred)\n</code></pre>"},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/#use-kedro-configuration","title":"Use Kedro configuration","text":"<p>Kedro offers a configuration loader to abstract loading values from a yaml file. You can use Kedro configuration loading without a full Kedro project and this approach replaces the need to load the configuration file with <code>yaml.safe_load</code>.</p>"},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/#use-kedros-configuration-loader-to-load-magic-values","title":"Use Kedro's configuration loader to load \"magic values\"","text":"<p>To use Kedro's <code>OmegaConfigLoader</code> to load <code>parameters.yml</code> the code is as follows:</p> <pre><code>from kedro.config import OmegaConfigLoader\n\nconf_loader = OmegaConfigLoader(conf_source=\".\")\n</code></pre> <pre><code>conf_params = conf_loader[\"parameters\"]\ntest_size = conf_params[\"model_options\"][\"test_size\"]\nrandom_state = conf_params[\"model_options\"][\"random_state\"]\nX = model_input_table[conf_params[\"model_options\"][\"features\"]]\ny = model_input_table[\"price\"]\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=test_size, random_state=random_state\n)\n</code></pre> <p>The rest of the model evaluation code can now run as before.</p> <pre><code>from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nmodel.predict(X_test)\nfrom sklearn.metrics import r2_score\n\ny_pred = model.predict(X_test)\nr2_score(y_test, y_pred)\n</code></pre>"},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/#use-kedros-configuration-loader-to-load-the-data-catalog","title":"Use Kedro's configuration loader to load the Data Catalog","text":"<p>Earlier in the example, we saw how to use Kedro's Data Catalog to load a <code>yaml</code> file, with <code>safe_load</code> and pass it to the <code>DataCatalog</code> class.</p> <pre><code># Using Kedro's DataCatalog\n\nfrom kedro.io import DataCatalog\n\nimport yaml\n\n# load the configuration file\nwith open(\"catalog.yml\") as f:\n    conf_catalog = yaml.safe_load(f)\n\n# Create the DataCatalog instance from the configuration\ncatalog = DataCatalog.from_config(conf_catalog)\n\n# Load the datasets\n...\n</code></pre> <p>It's also possible to use Kedro's <code>OmegaConfigLoader</code>configuration loader to initialise the Data Catalog.</p> <p>To load <code>catalog.yml</code> the code is as follows:</p> <pre><code># Now we are using Kedro's ConfigLoader alongside the DataCatalog\n\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.io import DataCatalog\n\nconf_loader = OmegaConfigLoader(conf_source=\".\")\nconf_catalog = conf_loader[\"catalog\"]\n\n# Create the DataCatalog instance from the configuration\ncatalog = DataCatalog.from_config(conf_catalog)\n\n# Load the datasets\ncompanies = catalog.load(\"companies\")\nreviews = catalog.load(\"reviews\")\nshuttles = catalog.load(\"shuttles\")\n</code></pre>"},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/#where-next","title":"Where next?","text":"<p>At this point in the notebook, we've introduced Kedro data management (using the Data Catalog) and configuration loader. You have now \"Kedro-ised\" the notebook code to make it more reusable in future. You can go further if your ultimate goal is to migrate code out of the notebook and use it in a full-blown Kedro project.</p>"},{"location":"pages/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook/#refactor-your-code-into-functions","title":"Refactor your code into functions","text":"<p>Code in a Kedro project runs in one or more pipelines, where a pipeline is a series of \"nodes\", which wrap discrete functions. One option is to put everything into a single function. Let's try this.</p> <pre><code># Use Kedro for data management and configuration\n\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.io import DataCatalog\n\nconf_loader = OmegaConfigLoader(conf_source=\".\")\nconf_catalog = conf_loader[\"catalog\"]\nconf_params = conf_loader[\"parameters\"]\n\n# Create the DataCatalog instance from the configuration\ncatalog = DataCatalog.from_config(conf_catalog)\n\n# Load the datasets\ncompanies = catalog.load(\"companies\")\nreviews = catalog.load(\"reviews\")\nshuttles = catalog.load(\"shuttles\")\n\n# Load the configuration data\ntest_size = conf_params[\"model_options\"][\"test_size\"]\nrandom_state = conf_params[\"model_options\"][\"random_state\"]\n</code></pre> <pre><code>def big_function():\n    ####################\n    # Data processing  #\n    ####################\n    companies[\"iata_approved\"] = companies[\"iata_approved\"] == \"t\"\n    companies[\"company_rating\"] = (\n        companies[\"company_rating\"].str.replace(\"%\", \"\").astype(float)\n    )\n    shuttles[\"d_check_complete\"] = shuttles[\"d_check_complete\"] == \"t\"\n    shuttles[\"moon_clearance_complete\"] = shuttles[\"moon_clearance_complete\"] == \"t\"\n    shuttles[\"price\"] = (\n        shuttles[\"price\"].str.replace(\"$\", \"\").str.replace(\",\", \"\").astype(float)\n    )\n    rated_shuttles = shuttles.merge(reviews, left_on=\"id\", right_on=\"shuttle_id\")\n    model_input_table = rated_shuttles.merge(\n        companies, left_on=\"company_id\", right_on=\"id\"\n    )\n    model_input_table = model_input_table.dropna()\n    model_input_table.head()\n\n    X = model_input_table[conf_params[\"model_options\"][\"features\"]]\n    y = model_input_table[\"price\"]\n\n    ##################################\n    # Model training and evaluation  #\n    ##################################\n    from sklearn.model_selection import train_test_split\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n\n    from sklearn.linear_model import LinearRegression\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    model.predict(X_test)\n    from sklearn.metrics import r2_score\n\n    y_pred = model.predict(X_test)\n    print(r2_score(y_test, y_pred))\n</code></pre> <pre><code># Call the one big function\nbig_function()\n</code></pre> <p>In truth, this code is not much more maintainable than previous versions.</p> <p>Maybe we could do better with a series of smaller functions that map to the Kedro vision of a pipeline of nodes. A node should behave consistently, repeatably, and predictably, so that a given input  to a node always returns the same output. For those in the know, this is the definition of a pure function. Nodes/pure functions should be small single responsibility functions that perform a single specific task.</p> <p>Let's try this with our code. We'll split it into a set of functions to process the data, which are based on the code in <code>big_function</code> but where each function has a single responsibility. Then we'll add a set of data science functions which split the model training and evaluation code into three separate functions.</p> <pre><code>####################\n# Data processing  #\n####################\nimport pandas as pd\n\n\ndef _is_true(x: pd.Series) -&gt; pd.Series:\n    return x == \"t\"\n\n\ndef _parse_percentage(x: pd.Series) -&gt; pd.Series:\n    x = x.str.replace(\"%\", \"\")\n    x = x.astype(float) / 100\n    return x\n\n\ndef _parse_money(x: pd.Series) -&gt; pd.Series:\n    x = x.str.replace(\"$\", \"\").str.replace(\",\", \"\")\n    x = x.astype(float)\n    return x\n\n\ndef preprocess_companies(companies: pd.DataFrame) -&gt; pd.DataFrame:\n    companies[\"iata_approved\"] = _is_true(companies[\"iata_approved\"])\n    companies[\"company_rating\"] = _parse_percentage(companies[\"company_rating\"])\n    return companies\n\n\ndef preprocess_shuttles(shuttles: pd.DataFrame) -&gt; pd.DataFrame:\n    shuttles[\"d_check_complete\"] = _is_true(shuttles[\"d_check_complete\"])\n    shuttles[\"moon_clearance_complete\"] = _is_true(shuttles[\"moon_clearance_complete\"])\n    shuttles[\"price\"] = _parse_money(shuttles[\"price\"])\n    return shuttles\n\n\ndef create_model_input_table(\n    shuttles: pd.DataFrame, companies: pd.DataFrame, reviews: pd.DataFrame\n) -&gt; pd.DataFrame:\n    rated_shuttles = shuttles.merge(reviews, left_on=\"id\", right_on=\"shuttle_id\")\n    model_input_table = rated_shuttles.merge(\n        companies, left_on=\"company_id\", right_on=\"id\"\n    )\n    model_input_table = model_input_table.dropna()\n    return model_input_table\n\n\n##################################\n# Model training and evaluation  #\n##################################\n\nfrom typing import Dict, Tuple\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\n\ndef split_data(data: pd.DataFrame, parameters: Dict) -&gt; Tuple:\n    X = data[parameters[\"features\"]]\n    y = data[\"price\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=parameters[\"test_size\"], random_state=parameters[\"random_state\"]\n    )\n    return X_train, X_test, y_train, y_test\n\n\ndef train_model(X_train: pd.DataFrame, y_train: pd.Series) -&gt; LinearRegression:\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train)\n    return regressor\n\n\ndef evaluate_model(\n    regressor: LinearRegression, X_test: pd.DataFrame, y_test: pd.Series\n):\n    y_pred = regressor.predict(X_test)\n    print(r2_score(y_test, y_pred))\n</code></pre> <pre><code># Call data processing functions\npreprocessed_companies = preprocess_companies(companies)\npreprocessed_shuttles = preprocess_shuttles(shuttles)\nmodel_input_table = create_model_input_table(\n    preprocessed_shuttles, preprocessed_companies, reviews\n)\n\n# Call model evaluation functions\nX_train, X_test, y_train, y_test = split_data(\n    model_input_table, conf_params[\"model_options\"]\n)\nregressor = train_model(X_train, y_train)\nevaluate_model(regressor, X_test, y_test)\n</code></pre> <p>And that's it. The notebook code has been refactored into a series of functions. Let's reproduce it all in one big notebook cell for reference. Compare it to the notebook code at the top of this page that began this example.</p> <pre><code># Kedro setup for data management and configuration\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.io import DataCatalog\n\nconf_loader = OmegaConfigLoader(conf_source=\".\")\nconf_catalog = conf_loader[\"catalog\"]\nconf_params = conf_loader[\"parameters\"]\n\n# Create the DataCatalog instance from the configuration\ncatalog = DataCatalog.from_config(conf_catalog)\n\n# Load the datasets\ncompanies = catalog.load(\"companies\")\nreviews = catalog.load(\"reviews\")\nshuttles = catalog.load(\"shuttles\")\n\n# Load the configuration data\ntest_size = conf_params[\"model_options\"][\"test_size\"]\nrandom_state = conf_params[\"model_options\"][\"random_state\"]\n\n\n####################\n# Data processing  #\n####################\nimport pandas as pd\n\n\ndef _is_true(x: pd.Series) -&gt; pd.Series:\n    return x == \"t\"\n\n\ndef _parse_percentage(x: pd.Series) -&gt; pd.Series:\n    x = x.str.replace(\"%\", \"\")\n    x = x.astype(float) / 100\n    return x\n\n\ndef _parse_money(x: pd.Series) -&gt; pd.Series:\n    x = x.str.replace(\"$\", \"\").str.replace(\",\", \"\")\n    x = x.astype(float)\n    return x\n\n\ndef preprocess_companies(companies: pd.DataFrame) -&gt; pd.DataFrame:\n    companies[\"iata_approved\"] = _is_true(companies[\"iata_approved\"])\n    companies[\"company_rating\"] = _parse_percentage(companies[\"company_rating\"])\n    return companies\n\n\ndef preprocess_shuttles(shuttles: pd.DataFrame) -&gt; pd.DataFrame:\n    shuttles[\"d_check_complete\"] = _is_true(shuttles[\"d_check_complete\"])\n    shuttles[\"moon_clearance_complete\"] = _is_true(shuttles[\"moon_clearance_complete\"])\n    shuttles[\"price\"] = _parse_money(shuttles[\"price\"])\n    return shuttles\n\n\ndef create_model_input_table(\n    shuttles: pd.DataFrame, companies: pd.DataFrame, reviews: pd.DataFrame\n) -&gt; pd.DataFrame:\n    rated_shuttles = shuttles.merge(reviews, left_on=\"id\", right_on=\"shuttle_id\")\n    model_input_table = rated_shuttles.merge(\n        companies, left_on=\"company_id\", right_on=\"id\"\n    )\n    model_input_table = model_input_table.dropna()\n    return model_input_table\n\n\n##################################\n# Model training and evaluation  #\n##################################\n\nfrom typing import Dict, Tuple\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\n\ndef split_data(data: pd.DataFrame, parameters: Dict) -&gt; Tuple:\n    X = data[parameters[\"features\"]]\n    y = data[\"price\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=parameters[\"test_size\"], random_state=parameters[\"random_state\"]\n    )\n    return X_train, X_test, y_train, y_test\n\n\ndef train_model(X_train: pd.DataFrame, y_train: pd.Series) -&gt; LinearRegression:\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train)\n    return regressor\n\n\ndef evaluate_model(\n    regressor: LinearRegression, X_test: pd.DataFrame, y_test: pd.Series\n):\n    y_pred = regressor.predict(X_test)\n    print(r2_score(y_test, y_pred))\n\n\n# Call data processing functions\npreprocessed_companies = preprocess_companies(companies)\npreprocessed_shuttles = preprocess_shuttles(shuttles)\nmodel_input_table = create_model_input_table(\n    preprocessed_shuttles, preprocessed_companies, reviews\n)\n\n# Call model evaluation functions\nX_train, X_test, y_train, y_test = split_data(\n    model_input_table, conf_params[\"model_options\"]\n)\nregressor = train_model(X_train, y_train)\nevaluate_model(regressor, X_test, y_test)\n</code></pre>"},{"location":"pages/resources/","title":"FAQs and resources","text":"<ul> <li>FAQ</li> <li>Glossary</li> <li>Migration</li> </ul>"},{"location":"pages/resources/glossary/","title":"Kedro glossary","text":""},{"location":"pages/resources/glossary/#data-catalog","title":"Data Catalog","text":"<p>The Data Catalog is Kedro's registry of all data sources available for use in the data pipeline. It manages loading and saving of data. The Data Catalog maps the names of node inputs and outputs as keys in a Kedro dataset, which can be specialised for different types of data storage.</p> <p>Further information about the Data Catalog</p>"},{"location":"pages/resources/glossary/#data-engineering-vs-data-science","title":"Data engineering vs Data science","text":"<p>Data engineering is the process of wrangling data into a clean and reliable state. Data wrangling is about taking a messy or unrefined source of data and turning it into something useful by parsing and cleaning it.</p> <p>Data science extracts insights from data by using a combination of domain expertise, programming skills, and knowledge of mathematics and statistics. An old joke: \"A data scientist is someone who knows more statistics than a computer scientist and more computer science than a statistician\".</p>"},{"location":"pages/resources/glossary/#kedro","title":"Kedro","text":"<p>Kedro is an open-source Python framework for creating reproducible, maintainable and modular data science code. It applies software engineering best-practices to machine learning code, including modularity, separation of concerns and versioning.</p> <p>Introduction to Kedro</p>"},{"location":"pages/resources/glossary/#kedrocontext","title":"<code>KedroContext</code>","text":"<p>A Python class that holds the configuration and Kedro\u2019s main functionality.</p> <p>API documentation for {py:class}<code>~kedro.framework.context.KedroContext</code></p>"},{"location":"pages/resources/glossary/#kedrosession","title":"<code>KedroSession</code>","text":"<p>A KedroSession allows you to manage the lifecycle of a Kedro run, persist runtime parameters and trace back runtime parameters, such as CLI command flags and environment variables.</p> <p>Further information about <code>KedroSession</code></p>"},{"location":"pages/resources/glossary/#kedro-viz","title":"Kedro-Viz","text":"<p>You can use Kedro-Viz to visualise your Kedro data pipelines:</p> <ul> <li>See how your datasets and nodes are resolved in Kedro so that you can understand how your data pipeline is built</li> <li>Get a clear picture when you have lots of datasets and nodes by using tags to visualise sub-pipelines</li> <li>Search for nodes and datasets</li> </ul> <p>Further information from the Kedro-Viz repository and {doc}<code>Kedro-Viz documentation&lt;kedro-viz:kedro-viz_visualisation&gt;</code>.</p>"},{"location":"pages/resources/glossary/#layers-data-engineering-convention","title":"Layers (data engineering convention)","text":"<p>According to common data engineering convention, a pipeline can be broken up into different layers according to how data is processed. This convention makes it easier to collaborate with other team members because everyone has an idea of what type of data cleaning or processing has happened.</p> <p>Kedro-Viz makes it easy to visualise these data processing stages by adding a <code>layer</code> attribute to the <code>kedro-viz</code> section within the <code>metadata</code> of the datasets in the Data Catalog.</p>"},{"location":"pages/resources/glossary/#modular-pipeline","title":"Modular pipeline","text":"<p>(See also Pipeline)</p> <p>In many typical Kedro projects, a single (\u201cmain\u201d) pipeline increases in complexity as the project evolves. To keep your project fit for purpose, you can create modular pipelines, which are logically isolated and can be reused. Modular pipelines are easier to develop, test and maintain, and are portable so they can be copied and reused between projects.</p> <p>Further information about modular pipelines</p>"},{"location":"pages/resources/glossary/#node","title":"Node","text":"<p>A Kedro node is a wrapper for a pure Python function that names the inputs and outputs of that function.</p> <p>(A pure function is a one whose output value follows solely from its input values, without any observable side effects such as changes to state or mutable data).</p> <p>Nodes are the building block of a pipeline. Nodes can be linked when the output of one node is the input of another.</p> <p>Further information about nodes</p>"},{"location":"pages/resources/glossary/#node-execution-order","title":"Node execution order","text":"<p>The node execution order is determined by resolving the input and output data dependencies between the nodes. The pipeline determines the node execution order and does not necessarily run the nodes in the order in which they are passed in.</p>"},{"location":"pages/resources/glossary/#pipeline","title":"Pipeline","text":"<p>A Kedro pipeline organises the dependencies and execution order of a collection of nodes, and connects inputs and outputs. The pipeline determines the node execution order by resolving dependencies.</p> <p>Further information about pipelines</p> <p>Chonky pipeline: Chonky is generally used to describe animals that are plump, rounded or simply heavier than average. A chonky pipeline is, likewise, a pipeline that is more bulky than usual.</p>"},{"location":"pages/resources/glossary/#pipeline-slicing","title":"Pipeline slicing","text":"<p>This is when you run a subset, or a \u2018slice\u2019 of a pipeline\u2019s nodes. You can slice a pipeline in a variety of ways, such as:</p> <ul> <li>by providing specific inputs (<code>pipeline.from_inputs</code>)</li> <li>by specifying the nodes from which to start the pipeline (<code>pipeline.from_nodes</code>)</li> <li>by specifying a final node (<code>pipeline.to_nodes</code>)</li> <li>by tagging certain nodes (<code>pipeline.only_nodes_with_tags</code>)</li> <li>by specifying certain nodes (<code>pipeline.only_nodes</code>)</li> </ul> <p>Further information about pipeline slicing</p>"},{"location":"pages/resources/glossary/#runner","title":"Runner","text":"<p>Runners are different execution mechanisms to run pipelines with the specified data catalog.</p> <ul> <li>The sequential runner executes pipeline nodes one-by-one based on their dependencies</li> <li>The parallel runner allows for concurrency by use of multiprocessing</li> <li>The thread runner uses threading for concurrent execution</li> </ul> <p>Further information about runners</p>"},{"location":"pages/resources/glossary/#starters","title":"Starters","text":"<p>Kedro starters are used to create projects that contain code to run as-is, or to adapt and extend. They provide pre-defined example code and configuration that can be reused. A Kedro starter is a Cookiecutter template that contains the boilerplate code for a Kedro project.</p> <p>Further information about Kedro starters</p>"},{"location":"pages/resources/glossary/#tags","title":"Tags","text":"<p>You can apply tags to nodes or pipelines as a means of filtering which are executed.</p>"},{"location":"pages/resources/glossary/#workflow-dependencies","title":"Workflow dependencies","text":"<p>In Kedro, workflow dependencies are the packages that your project (the data and ML workflow) requires.</p>"},{"location":"pages/resources/migration/","title":"Migration guide","text":"<p>See the release notes on GitHub for comprehensive information about the content of each Kedro release.</p>"},{"location":"pages/resources/migration/#migrate-an-existing-project-that-uses-kedro-018-to-use-019","title":"Migrate an existing project that uses Kedro 0.18. to use 0.19.","text":""},{"location":"pages/resources/migration/#custom-syntax-for-params-was-removed","title":"Custom syntax for <code>--params</code> was removed","text":"<p>Kedro 0.19.0 removed the custom Kedro syntax for <code>--params</code>. To update, you need to use the OmegaConf syntax instead by replacing <code>:</code> with <code>=</code>.</p> <p>If you used this command to pass parameters to <code>kedro run</code>:</p> <pre><code>kedro run --params=param_key1:value1,param_key2:2.0\n</code></pre> <p>You should now use the following:</p> <pre><code>kedro run --params=param_key1=value1,param_key2=2.0\n</code></pre> <p>For more information see \"How to specify parameters at runtime\".</p>"},{"location":"pages/resources/migration/#create_default_data_set-was-removed-from-runner","title":"<code>create_default_data_set()</code> was removed from <code>Runner</code>","text":"<p>Kedro 0.19 removed the <code>create_default_data_set()</code> method in the <code>Runner</code>. To overwrite the default dataset creation, you need to use the new <code>Runner</code> class argument <code>extra_dataset_patterns</code> instead.</p> <p>On class instantiation, pass the <code>extra_dataset_patterns</code> argument, and overwrite the default <code>MemoryDataset</code> creation as follows:</p> <pre><code>from kedro.runner import ThreadRunner\n\nrunner = ThreadRunner(extra_dataset_patterns={\"{default}\": {\"type\": \"MyCustomDataset\"}})\n</code></pre>"},{"location":"pages/resources/migration/#project_version-was-removed","title":"<code>project_version</code> was removed","text":"<p>Kedro 0.19 removed <code>project_version</code> in <code>pyproject.toml</code>. Use <code>kedro_init_version</code> instead:</p> <pre><code>[tool.kedro]\npackage_name = \"my_project\"\nproject_name = \"my project\"\n- project_version = \"0.19.1\"\n+ kedro_init_version = \"0.19.1\"\n</code></pre>"},{"location":"pages/resources/migration/#datasets-changes-in-019","title":"Datasets changes in 0.19","text":""},{"location":"pages/resources/migration/#the-layer-attribute-in-catalogyml-has-moved","title":"The <code>layer</code> attribute in <code>catalog.yml</code> has moved","text":"<p>From 0.19, the <code>layer</code> attribute at the top level has been  moved inside the <code>metadata</code> -&gt; <code>kedro-viz</code> attribute. You need to update <code>catalog.yml</code> accordingly.</p> <p>The following <code>catalog.yml</code> entry changes from the following in 0.18.x code:</p> <pre><code>companies:\n  type: pandas.CSVDataSet\n  filepath: data/01_raw/companies.csv\n  layer: raw\n</code></pre> <p>to this in 0.19.x:</p> <pre><code>companies:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/companies.csv\n  metadata:\n    kedro-viz:\n      layer: raw\n</code></pre> <p>See {doc}<code>See the Kedro-Viz documentation for more information&lt;kedro-viz:kedro-viz_visualisation&gt;</code>.</p>"},{"location":"pages/resources/migration/#for-apidataset-the-requests-specific-arguments-in-catalogyml-have-moved","title":"For <code>APIDataset</code>, the <code>requests</code>-specific arguments in <code>catalog.yml</code> have moved","text":"<p>From 0.19, if you use <code>APIDataset</code>, you need to move all <code>requests</code>-specific arguments, such as <code>params</code>, <code>headers</code>, in the hierarchy to sit under <code>load_args</code>. The <code>url</code> and <code>method</code> arguments are not affected.</p> <p>For example the following <code>APIDataset</code> in <code>catalog.yml</code> changes from the following in 0.18.x code:</p> <pre><code>us_corn_yield_data:\n  type: api.APIDataSet\n  url: https://quickstats.nass.usda.gov\n  credentials: usda_credentials\n  params:\n    key: SOME_TOKEN\n    format: JSON\n</code></pre> <p>to this in 0.19.x:</p> <pre><code>us_corn_yield_data:\n  type: api.APIDataSet\n  url: https://quickstats.nass.usda.gov\n  credentials: usda_credentials\n  load_args:\n    params:\n      key: SOME_TOKEN\n      format: JSON\n</code></pre>"},{"location":"pages/resources/migration/#dataset-renaming","title":"Dataset renaming","text":"<p>In 0.19.0 we renamed dataset and error classes to follow the Kedro lexicon.</p> <ul> <li>Dataset classes ending with <code>DataSet</code> are replaced by classes that end with <code>Dataset</code>.</li> <li>Error classes starting with <code>DataSet</code> are replaced by classes that start with <code>Dataset</code>.</li> </ul> <p>All the classes below are also importable from <code>kedro.io</code>; only the module where they are defined is listed as the location.</p> Type Removed Alias Location <code>AbstractDataset</code> <code>AbstractDataSet</code> <code>kedro.io.core</code> <code>AbstractVersionedDataset</code> <code>AbstractVersionedDataSet</code> <code>kedro.io.core</code> <code>CachedDataset</code> <code>CachedDataSet</code> <code>kedro.io.cached_dataset</code> <code>LambdaDataset</code> <code>LambdaDataSet</code> <code>kedro.io.lambda_dataset</code> <code>MemoryDataset</code> <code>MemoryDataSet</code> <code>kedro.io.memory_dataset</code> <code>DatasetError</code> <code>DataSetError</code> <code>kedro.io.core</code> <code>DatasetAlreadyExistsError</code> <code>DataSetAlreadyExistsError</code> <code>kedro.io.core</code> <code>DatasetNotFoundError</code> <code>DataSetNotFoundError</code> <code>kedro.io.core</code>"},{"location":"pages/resources/migration/#all-other-dataset-classes-are-removed-from-the-core-kedro-repository-kedroextrasdatasets","title":"All other dataset classes are removed from the core Kedro repository (<code>kedro.extras.datasets</code>)","text":"<p>You now need to install and import datasets from the <code>kedro-datasets</code> package instead.</p>"},{"location":"pages/resources/migration/#configuration-changes-in-019","title":"Configuration changes in 0.19","text":"<p>The <code>ConfigLoader</code> and <code>TemplatedConfigLoader</code> classes were deprecated in Kedro 0.18.12 and were removed in Kedro 0.19.0. To use that release or later, you must now adopt the <code>OmegaConfigLoader</code>. The configuration migration guide outlines the primary distinctions between the old loaders and the OmegaConfigLoader, and provides step-by-step instructions on updating your code base to use the new class effectively.</p>"},{"location":"pages/resources/migration/#changes-to-the-default-environments","title":"Changes to the default environments","text":"<p>The default configuration environment has changed in 0.19 and needs to be declared in <code>settings.py</code> explicitly if you have custom arguments. For example, if you use <code>CONFIG_LOADER_ARGS</code>  in <code>settings.py</code> to read Spark configuration, you need to add <code>base_env</code> and <code>default_run_env</code> explicitly.</p> <p>Before 0.19.x:</p> <pre><code>CONFIG_LOADER_ARGS = {\n#       \"base_env\": \"base\",\n#       \"default_run_env\": \"local\",\n    \"config_patterns\": {\n        \"spark\": [\"spark*\", \"spark*/**\"],\n    }\n}\n</code></pre> <p>In 0.19.x:</p> <pre><code>CONFIG_LOADER_ARGS = {\n      \"base_env\": \"base\",\n      \"default_run_env\": \"local\",\n          \"config_patterns\": {\n              \"spark\": [\"spark*\", \"spark*/**\"],\n          }\n}\n</code></pre> <p>If you didn't use <code>CONFIG_LOADER_ARGS</code> in your code, this change is not needed because Kedro sets it by default.</p>"},{"location":"pages/resources/migration/#logging","title":"Logging","text":"<p><code>logging.yml</code> is now independent of Kedro's run environment and used only if <code>KEDRO_LOGGING_CONFIG</code> is set to point to it. The documentation on logging describes in detail how logging works in Kedro and how it can be customised.</p>"},{"location":"pages/starters/","title":"Customise a new project","text":"<p>As you saw from the First steps section, once you have set up Kedro, you can create a new project with <code>kedro new</code> and customise the code added to that project for its tooling and example code requirements.</p> <p>The pages in this section describe in detail the various options available.</p>"},{"location":"pages/starters/#tools-to-customise-a-new-kedro-project","title":"Tools to customise a new Kedro project","text":"<ul> <li>Tools and example code options</li> </ul>"},{"location":"pages/starters/#kedro-starters","title":"Kedro starters","text":"<ul> <li>Starters</li> </ul> <p>Use <code>kedro new</code> to create a basic project  In the simplest instance, you can create a project using <code>kedro new</code> and select from a range of tools and example code options to extend the basic project.</p> <p>Use <code>kedro new</code> with <code>--config</code>  Similarly, you can use <code>kedro new</code> but additionally pass in a configuration file, for example:</p> <pre><code>kedro new --config=config.yml\n</code></pre> <p>The file enables you to customise details such as the project folder name and package name.</p> <p>The configuration file must contain:</p> <ul> <li><code>output_dir</code>: The path in which to create the project directory, which can be set to <code>~</code> for the home directory or <code>.</code> for the current working directory.</li> <li><code>project_name</code></li> <li><code>repo_name</code></li> <li><code>python_package</code></li> </ul> <p>Additionally, the configuration file may contain:</p> <ul> <li><code>tools</code>: The tools to customise your project setup with. Select from comma-separated values <code>lint, test, log, docs, data, pyspark, viz</code> or <code>all/none</code>. Omitting this from your configuration file will result in the default selection of <code>none</code>.</li> <li><code>example_pipeline</code>: Indicate <code>yes</code> or <code>no</code> to select whether you would like your project to be populated with example code. Omitting this from your configuration file will result in the default selection of <code>no</code>.</li> </ul> <p>The <code>output_dir</code> can be specified as <code>~</code> for the home directory or <code>.</code> for the current working directory. Here is an example <code>config.yml</code>, which assumes that a directory named <code>~/code</code> already exists:</p> <pre><code>output_dir: ~/code\nproject_name: My First Kedro Project\nrepo_name: testing-kedro\npython_package: test_kedro\n</code></pre> <p>Note When the <code>--config</code> flag is used together with <code>--name</code>, <code>--tools</code>, or <code>--example</code>, the values provided directly on the CLI will overwrite those specified in the configuration file.</p> <p>Use <code>kedro new</code> with a <code>--starter</code>  You can create a new Kedro project with a starter that adds code for a common project use case.</p> <p>Important You cannot combine the use of a Kedro starter with the tools and example code options listed above.</p>"},{"location":"pages/starters/create_a_starter/","title":"Create a Kedro starter","text":"<p>Kedro starters are a useful way to create a new project that contains code to run as-is, or to adapt and extend.</p> <p>A team may find it useful to build Kedro starters to create reusable projects that bootstrap a common base and can be extended.</p> <p>Please note that users are expected to have <code>Git</code> installed for the <code>kedro new</code> flow, which is used in this section.</p>"},{"location":"pages/starters/create_a_starter/#install-the-cookiecutter-package","title":"Install the <code>cookiecutter</code> package","text":"<p>A Kedro starter is a Cookiecutter template that contains the boilerplate code for a Kedro project. First install <code>cookiecutter</code> as follows:</p> <pre><code>pip install cookiecutter\n</code></pre> <p>To create a Kedro starter, you need a base project to convert to a template, which forms the boilerplate for all projects that use it. You then need to decide which are:</p> <ul> <li>the common, boilerplate parts of the project</li> <li>the configurable elements, which need to be replaced by <code>cookiecutter</code> strings</li> </ul>"},{"location":"pages/starters/create_a_starter/#custom-project-creation-variables","title":"Custom project creation variables","text":"<p>When you create a new project using a Kedro starter, <code>kedro new</code> prompts you for a project name. This variable (<code>project_name</code>) is set in the default starter setup in<code>prompts.yml</code>.</p> <p>Kedro automatically generates the following two variables from the entered <code>project_name</code>:</p> <ul> <li><code>repo_name</code> - A name for the directory that holds the project repository</li> <li><code>python_package</code> - A Python package name for the project package (see Python package naming conventions)</li> </ul> <p>As a starter creator, you can customise the prompts triggered by <code>kedro new</code> by adding your own prompts into the <code>prompts.yml</code> file in the root of your template. This is an example of a custom prompt:</p> <pre><code>custom_prompt:\n    title: \"Prompt title\"\n    text: |\n      Prompt description that explains to the user what\n      information they should provide.\n</code></pre> <p>At the very least, the prompt <code>title</code> must be defined for the prompt to be valid. After Kedro receives the user's input for each prompt, it passes the value to Cookiecutter, so every key in <code>prompts.yml</code> must have a corresponding key in <code>cookiecutter.json</code>.</p> <p>If the input to the prompts needs to be validated, for example to make sure it only has alphanumeric characters, you can add regex validation rules via the <code>regex_validator</code> key. Consider using cookiecutter pre/post-generate hooks for more complex validation.</p> <p>If you want <code>cookiecutter</code> to provide sensible default values, in case a user doesn't provide any input, you can add those to <code>cookiecutter.json</code>. See the default starter <code>cookiecutter.json</code> as example.</p>"},{"location":"pages/starters/create_a_starter/#example-kedro-starter","title":"Example Kedro starter","text":"<p>To review an example Kedro starter, check out the <code>spaceflights-pandas</code> starter on GitHub.</p> <p>When a new <code>spaceflights-pandas</code> project is created with <code>kedro new --starter=spaceflights-pandas</code>, the user is asked to enter a <code>project_name</code> variable, which is then used to generate the <code>repo_name</code> and <code>python_package</code> variables by default.</p> <p>If you use a configuration file, you must supply all three variables in the file. You can see how these variables are used by inspecting the template:</p>"},{"location":"pages/starters/create_a_starter/#project_name","title":"<code>project_name</code>","text":"<p>The human-readable <code>project_name</code> variable is used in the README.md for the new project.</p>"},{"location":"pages/starters/create_a_starter/#repo_name","title":"<code>repo_name</code>","text":"<p>The top-level folder labelled <code>{{ cookiecutter.repo_name }}</code>, which forms the top-level folder to contain the starter project when it is created.</p>"},{"location":"pages/starters/create_a_starter/#python_package","title":"<code>python_package</code>","text":"<p>Within the parent folder, inside the <code>src</code> subfolder, is another configurable variable {{ cookiecutter.python_package }} which contains the source code for the example pipelines. The variable is also used within <code>__main__.py</code>.</p> <p>Here is the layout of the project as a Cookiecutter template:</p> <pre><code>{{ cookiecutter.repo_name }}     # Parent directory of the template\n\u251c\u2500\u2500 conf                         # Project configuration files\n\u251c\u2500\u2500 data                         # Local project data (not committed to version control)\n\u251c\u2500\u2500 docs                         # Project documentation\n\u251c\u2500\u2500 notebooks                    # Project related Jupyter notebooks (can be used for experimental code before moving the code to src)\n\u251c\u2500\u2500 pyproject.toml               #\n\u251c\u2500\u2500 README.md                    # Project README\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 src                          # Project source code\n    \u2514\u2500\u2500 {{ cookiecutter.python_package }}\n       \u251c\u2500\u2500 __init.py__\n       \u251c\u2500\u2500 pipelines\n       \u251c\u2500\u2500 pipeline_registry.py\n       \u251c\u2500\u2500 __main__.py\n       \u2514\u2500\u2500 settings.py\n\u2514\u2500\u2500 tests\n</code></pre>"},{"location":"pages/starters/create_a_starter/#extend-starter-aliases","title":"Extend starter aliases","text":"<p>You can add an alias by creating a plugin using <code>kedro.starters</code> entry point which enables you to call <code>kedro new --starter=your_starters</code>. That is, it can be used directly through the <code>starter</code> argument in <code>kedro new</code> rather than needing to explicitly provide the <code>template</code> and <code>directory</code> arguments.</p> <p>A custom starter alias behaves in the same way as an official Kedro starter alias and is also picked up by the command <code>kedro starter list</code>.</p> <p>You need to extend the starters by providing a list of <code>KedroStarterSpec</code>, in this example it is defined in a file called <code>plugin.py</code>.</p> <p>Example for a non-git repository starter:</p> <pre><code># plugin.py\nstarters = [\n    KedroStarterSpec(\n        alias=\"test_plugin_starter\",\n        template_path=\"your_local_directory/starter_folder\",\n    )\n]\n</code></pre> <p>Example for a git repository starter:</p> <pre><code># plugin.py\nstarters = [\n    KedroStarterSpec(\n        alias=\"test_plugin_starter\",\n        template_path=\"https://github.com/kedro-org/kedro-starters/\",\n        directory=\"spaceflights-pandas\",\n    )\n]\n</code></pre> <p>The <code>directory</code> argument is optional and should be used when you have multiple templates in one repository as for the official kedro-starters. If you only have one template, your top-level directory will be treated as the template.</p> <p>In your <code>pyproject.toml</code>, you need to register the specifications to <code>kedro.starters</code>:</p> <pre><code>[project.entry-points.\"kedro.starters\"]\nstarter = \"plugin:starters\"\n</code></pre> <p>After that you can use this starter with <code>kedro new --starter=test_plugin_starter</code>.</p> <pre><code>If your starter is stored on a git repository, Kedro defaults to use a tag or branch labelled with your version of Kedro, e.g. `0.18.12`. This means that you can host different versions of your starter template on the same repository, and the correct one will be used automatically. If you prefer not to follow this structure, you should override it with the `checkout` flag, e.g. `kedro new --starter=test_plugin_starter --checkout=main`.\n</code></pre>"},{"location":"pages/starters/new_project_tools/","title":"Tools to customise a new Kedro project","text":"<p>There are several ways to customise your new project with the tools and example code:</p> <ul> <li>Specify tools using inputs to <code>kedro new</code></li> <li>Specify tools using YAML configuration</li> </ul> <p>There is a flowchart to illustrate the choices available at the bottom of the page.</p>"},{"location":"pages/starters/new_project_tools/#specify-tools-as-inputs-to-kedro-new","title":"Specify tools as inputs to <code>kedro new</code>","text":"<p>Navigate to the directory in which you would like to create your new Kedro project, and run the following command:</p> <pre><code>kedro new\n</code></pre> <p>This will start the new project creation workflow.</p> <pre><code>You can also add flags to `kedro new` to skip some or all of the steps in the project creation workflow to skip queries about how you want to customise the project. The flags are described below.\n</code></pre>"},{"location":"pages/starters/new_project_tools/#project-name","title":"Project name","text":"<p>The first prompt asks you to input a project name.</p> <p>To skip this step and name the project directly, add it to <code>kedro new</code> as follows:</p> <pre><code>kedro new --name=spaceflights\n</code></pre>"},{"location":"pages/starters/new_project_tools/#tools","title":"Tools","text":"<p>You are then asked to select which tools to include. Choose from the list using comma separated values <code>(1,2,4)</code>, ranges of values <code>(1-3,5-7)</code>, a combination of the two <code>(1,3-5,7)</code>, or the key words <code>all</code> or <code>none</code>. Skipping the prompt by entering no value will result in the default selection of <code>none</code>.</p> <p>Further information about each of the tools is described below.</p> <pre><code>Project Tools\n=============\nThese optional tools can help you apply software engineering best practices.\nTo skip this step in future use --tools\nTo find out more: https://docs.kedro.org/en/stable/starters/new_project_tools.html\n\nTools\n1) Lint: Basic linting with Ruff\n2) Test: Basic testing with pytest\n3) Log: Additional, environment-specific logging options\n4) Docs: A Sphinx documentation setup\n5) Data Folder: A folder structure for data management\n6) PySpark: Configuration for working with PySpark\n7) Kedro-Viz: Kedro's native visualisation tool\n\nWhich tools would you like to include in your project? [1-7/1,3/all/none]:\n [none]:\n</code></pre> <p>A list of available tools can also be accessed by running <code>kedro new --help</code></p> <pre><code>...\n-t, --tools TEXT    Select which tools you'd like to include. By default,\n                      none are included.\n\n                      Tools\n\n                      1) Linting: Provides a basic linting setup with Ruff\n\n                      2) Testing: Provides basic testing setup with pytest\n\n                      3) Custom Logging: Provides more logging options\n\n                      4) Documentation: Basic documentation setup with Sphinx\n\n                      5) Data Structure: Provides a directory structure for\n                      storing data\n\n                      6) PySpark: Provides set up configuration for working\n                      with PySpark\n\n                      7) Kedro Viz: Provides Kedro's native visualisation tool\n\n                      Example usage:\n\n                      kedro new --tools=lint,test,log,docs,data,pyspark,viz\n                      (or any subset of these options)\n\n                      kedro new --tools=all\n\n                      kedro new --tools=none\n...\n</code></pre>"},{"location":"pages/starters/new_project_tools/#shortcut","title":"Shortcut","text":"<p>To skip this step and select tools directly, add the tools selection to <code>kedro new</code> as follows:</p> <pre><code>kedro new --tools=&lt;your tool selection&gt;\n</code></pre> <p>To specify your desired tools you must provide them by name as a comma separated list, for example <code>--tools=lint,test,viz</code>. The following tools are available for selection: <code>lint</code>, <code>test</code>, <code>log</code>, <code>docs</code>, <code>data</code>, <code>pyspark</code>, and <code>viz</code>.</p>"},{"location":"pages/starters/new_project_tools/#example-code","title":"Example code","text":"<p>In the final step you are asked whether you want to populate the project with an example spaceflights starter pipeline. If you select <code>yes</code>, the example code included depends upon your previous choice of tools, as follows:</p> <ul> <li>Default spaceflights starter (<code>spaceflights-pandas</code>): Added if you selected any combination of linting, testing, custom logging, documentation, and data structure, unless you also selected PySpark or Kedro Viz.</li> <li>PySpark spaceflights starter (<code>spaceflights-pyspark</code>): Added if you selected PySpark with any other tools, unless you also selected Kedro Viz.</li> <li>Kedro Viz spaceflights starter (<code>spaceflights-pandas-viz</code>): Added if Kedro Viz was one of your tools choices, unless you also selected PySpark.</li> <li>Full feature spaceflights starter (<code>spaceflights-pyspark-viz</code>): Added if you selected all available tools, including PySpark and Kedro Viz.</li> </ul> <p>Each starter example is tailored to demonstrate the capabilities and integrations of the selected tools, offering a practical insight into how they can be utilised in your project.</p>"},{"location":"pages/starters/new_project_tools/#shortcut_1","title":"Shortcut","text":"<p>To skip this step and make a choice of example code directly, add the your preference to <code>kedro new</code> as follows:</p> <pre><code>kedro new --example=y\n</code></pre>"},{"location":"pages/starters/new_project_tools/#specify-tools-using-yaml-configuration","title":"Specify tools using YAML configuration","text":"<p>As an alternative to the interactive project creation workflow, you can also supply values to <code>kedro new</code> by providing a YML configuration file to your <code>kedro new</code> command. Consider the following file:</p> <pre><code># config.yml\n\n\"project_name\":\n    \"My Project\"\n\n\"repo_name\":\n    \"my-project\"\n\n\"python_package\":\n    \"my project\"\n\n\"tools\":\n    \"lint, test, log, docs, data, pyspark, viz\"\n\n\"example_pipeline\":\n    \"y\"\n</code></pre> <p>To create a new project using the file to supply details to <code>kedro new</code>, run the following command:</p> <pre><code>kedro new --config=&lt;path/to/config.yml&gt;\n</code></pre> <pre><code>Note: When using a configuration file to create a new project, you must provide values for the project name, repository name, and package names. Specifying your tools selection is optional, omitting them results in the default selection of `none`.\n</code></pre> <pre><code>When the `--config` flag is used together with `--name`, `--tools`, or `--example`, the values provided directly on the CLI will overwrite those specified in the configuration file.\n</code></pre>"},{"location":"pages/starters/new_project_tools/#kedro-tools","title":"Kedro tools","text":"<p>Tools in Kedro serve as modular functionalities that enhance a foundational project template. They provide a means to tailor your Kedro project to meet your unique requirements. When creating a new project, you may select one or more of the available tools, or none at all.</p> <p>The available tools include: linting, testing, custom logging, documentation, data structure, PySpark, and Kedro-Viz.</p>"},{"location":"pages/starters/new_project_tools/#linting","title":"Linting","text":"<p>The Kedro linting tool introduces <code>ruff</code> as dependency in your new project's requirements. After project creation, make sure these are installed by running the following command from the project root:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>The linting tool will configure <code>ruff</code> with the following settings by default:</p> <pre><code>#pyproject.toml\n\nline-length = 88\nshow-fixes = true\nselect = [\n    \"F\",   # Pyflakes\n    \"W\",   # pycodestyle\n    \"E\",   # pycodestyle\n    \"I\",   # isort\n    \"UP\",  # pyupgrade\n    \"PL\",  # Pylint\n    \"T201\", # Print Statement\n]\nignore = [\"E501\"]  # Ruff format takes care of line-too-long\n</code></pre> <p>With these installed, you can then make use of the following commands to format and lint your code:</p> <pre><code>ruff format path/to/project/root\nruff check path/to/project/root\n</code></pre> <p>Though it has no impact on how your code works, linting is important for code quality because improves consistency, readability, debugging, and maintainability. To learn more about linting your Kedro projects, check our linting documentation.</p>"},{"location":"pages/starters/new_project_tools/#testing","title":"Testing","text":"<p>This tool introduces the <code>tests</code> directory to the new project's structure, containing the file <code>test_run.py</code> with an example unit test. <code>Pytest</code> is added as a dependency in your new project's requirements. After project creation, make sure it is installed by running the following command from your project root:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>The tool leverages <code>pytest</code> with the following configuration:</p> <pre><code>[tool.pytest.ini_options]\naddopts = \"\"\"\n--cov-report term-missing \\\n--cov src/{{ cookiecutter.python_package }} -ra\"\"\"\n\n[tool.coverage.report]\nfail_under = 0\nshow_missing = true\nexclude_lines = [\"pragma: no cover\", \"raise NotImplementedError\"]\n</code></pre> <p>To run your tests, use the following command:</p> <pre><code>pytest path/to/your/project/root/tests\n</code></pre> <p>Kedro promotes the use of unit tests to achieve high code quality and maintainability in your projects. To read more about unit testing with Kedro, check our testing documentation</p>"},{"location":"pages/starters/new_project_tools/#custom-logging","title":"Custom logging","text":"<p>Selecting the custom logging tool introduces the file <code>logging.yml</code> to your project's <code>conf</code> directory. This tool allows you to customise your logging configuration instead of using Kedro's default logging configuration. The populated <code>conf/logging.yml</code> provides two additional logging handlers: <code>console</code> and <code>info_file_handler</code>, as well as <code>rich</code> that is available in the default configuration, though only <code>info_file_handler</code> and <code>rich</code> are used.</p> <p>To use this provided logging configuration, remember to set the <code>KEDRO_LOGGING_CONFIG</code> environment variable to point to <code>logging.yml</code> by naviagting to your project root and running the following command:</p> <pre><code>export KEDRO_LOGGING_CONFIG=conf/logging.yml\n</code></pre> <p>To learn more about using logging in your project, or modifying the logging configuration, take a look at our logging documentation.</p>"},{"location":"pages/starters/new_project_tools/#documentation","title":"Documentation","text":"<p>Including the Documentation tool adds a <code>docs</code> directory to your project structure and includes the Sphinx setup files, <code>conf.py</code> and <code>index.rst</code>, with some added features such as auto generation of HTML documentation. The aim of this tool reflects Kedro's commitment to best practices in understanding code and facilitating collaboration by helping you create and maintain guides and API docs.</p> <p>If you did not initially select <code>docs</code> and want to implement it later you can do so by following the official documentation for guidance on adding documentation to a Kedro project.</p>"},{"location":"pages/starters/new_project_tools/#data-structure","title":"Data structure","text":"<p>The Data Structure tool provides a local standardised folder hierarchy for your project data, which includes predefined folders such as raw, intermediate, and processed data, as determined by data engineering convention. This is crucial if you want to include example pipelines during the creation of your project as it can not be omitted from the tool selections. Kedro's capabilities extend far beyond local data storage. Kedro seamlessly integrates with Data Lakes and various databases through fsspec URIs, catering to the needs of professional teams that store their data in blob/object storage or databases. We believe a well-organised data structure is key to efficient data management, allowing for scalable and maintainable data pipelines. You can learn more about Kedro's recommended project directory structure.</p>"},{"location":"pages/starters/new_project_tools/#pyspark","title":"PySpark","text":"<p>The <code>PySpark</code> tool modifies the project's <code>requirements.txt</code> to include PySpark dependencies and adjusts the project setup for Spark jobs, this will allow you to process datasets using Apache Spark for scalable data processing. PySpark aligns with Kedro's scalability principle, as it provides data processing capabilities for large datasets. See the PySpark integration documentation for more information on setup and usage.</p>"},{"location":"pages/starters/new_project_tools/#kedro-viz","title":"Kedro Viz","text":"<p>The <code>viz</code> tool will add visualisation to your project by including Kedro-Viz, which creates an interactive web-app to visualise your pipelines allowing for an intuitive understanding of data on your DAG. In addition, <code>viz</code> will also add setup for experiment tracking and plotting datasets. See the Kedro-Viz documentation for more information on using this tool.</p>"},{"location":"pages/starters/new_project_tools/#flowchart-illustration","title":"Flowchart illustration","text":"<p>Here is a flowchart to help illustrate some example choice of tools you can select:</p> <p></p> <p>% Mermaid code, see https://github.com/kedro-org/kedro/wiki/Render-Mermaid-diagrams % flowchart TD %     A[Start] --&gt; B[Enter Project Name: Example Project]; %     B --&gt; C3[Select Tools: None]; %     B --&gt; C1[Select Tools: lint, docs, PySpark]; %     B --&gt; C2[Select Tools: All]; % %     C1 --&gt; D1[Include Example Pipeline?]; %     C2 --&gt; D2[Include Example Pipeline?]; %     C3 --&gt; D3[Include Example Pipeline?]; % %     D1 --&gt;|Yes| E1[New Project Created\\nName: Example Project\\nTools: lint, docs, PySpark\\nExample: Yes]; %     D1 --&gt;|No| E2[New Project Created\\nName: Example Project\\nTools: lint, docs, PySpark\\nExample: No]; % %     D2 --&gt;|Yes| F1[New Project Created\\nName: Example Project\\nTools: All: lint, test, logging, docs, data, PySpark, viz \\nExample: Yes]; %     D2 --&gt;|No| F2[New Project Created\\nName: Example Project\\nTools: All: lint, test, logging, docs, data, PySpark, viz \\nExample: No]; % %     D3 --&gt;|Yes| G1[New Project Created\\nName: Example Project\\nTools: None\\nExample: Yes]; %     D3 --&gt;|No| G2[New Project Created\\nName: Example Project\\nTools: None\\nExample: No];</p>"},{"location":"pages/starters/starters/","title":"Kedro starters","text":"<p>A Kedro starter contains code in the form of a Cookiecutter template for a Kedro project. Using a starter is like using a pre-defined layout when creating a presentation or document.</p>"},{"location":"pages/starters/starters/#how-to-use-a-starter","title":"How to use a starter","text":"<p>To create a Kedro project using a starter, apply the <code>--starter</code> flag to <code>kedro new</code>. For example:</p> <pre><code>kedro new --starter=&lt;path-to-starter&gt;\n</code></pre> <pre><code>`path-to-starter` could be the path to a local directory, a URL to a remote VCS repository supported by `cookiecutter` or one of the aliases listed in ``kedro starter list``.\n</code></pre> <p>If you want to use <code>--starter</code> as remote VCS repository, run:</p> <pre><code>kedro new --starter git+https://github.com/kedro-org/kedro-starters.git --directory spaceflights-pandas\n</code></pre>"},{"location":"pages/starters/starters/#starter-aliases","title":"Starter aliases","text":"<p>We provide aliases for common starters maintained by the Kedro team so that you don't have to specify the full path. For example, to create a project using the <code>spaceflights-pandas</code> starter:</p> <pre><code>kedro new --starter=spaceflights-pandas\n</code></pre> <p>To list all the aliases we support:</p> <pre><code>kedro starter list\n</code></pre>"},{"location":"pages/starters/starters/#official-kedro-starters","title":"Official Kedro starters","text":"<p>The Kedro team maintains the following starters for a range of Kedro projects:</p> <ul> <li><code>astro-airflow-iris</code>: An example project using the Iris dataset with a minimal setup for deploying the pipeline on Airflow with Astronomer.</li> <li><code>databricks-iris</code>: An example project using the Iris dataset with a setup for Databricks deployment.</li> <li><code>spaceflights-pandas</code>: The spaceflights tutorial example code with <code>pandas</code> datasets.</li> <li><code>spaceflights-pandas-viz</code>: The spaceflights tutorial example code with <code>pandas</code> datasets and visualisation and experiment tracking <code>kedro-viz</code> features.</li> <li><code>spaceflights-pyspark</code>: The spaceflights tutorial example code with <code>pyspark</code> datasets.</li> <li><code>spaceflights-pyspark-viz</code>: The spaceflights tutorial example code with <code>pyspark</code> datasets and visualisation and experiment tracking <code>kedro-viz</code> features.</li> </ul>"},{"location":"pages/starters/starters/#archived-starters","title":"Archived starters","text":"<p>The following Kedro starters have been archived and are unavailable in Kedro version 0.19.0 and beyond.</p> <ul> <li><code>standalone-datacatalog</code></li> <li><code>pandas-iris</code></li> <li><code>pyspark-iris</code></li> <li><code>pyspark</code></li> </ul> <p>The latest version of Kedro that supports these starters is Kedro 0.18.14.</p> <ul> <li>To check the version of Kedro you have installed, type <code>kedro -V</code> in your terminal window.</li> <li>To install a specific version of Kedro, e.g. 0.18.14, type <code>pip install kedro==0.18.14</code>.</li> <li>To create a project with one of these starters using <code>kedro new</code>,  type the following (assuming Kedro version 0.18.14) <code>kedro new --starter=pandas-iris --checkout=0.18.14</code> (for example, to use the <code>pandas-iris</code> starter).</li> </ul>"},{"location":"pages/starters/starters/#starter-versioning","title":"Starter versioning","text":"<p>By default, Kedro will use the latest version available in the repository. If you want to use a specific version of a starter, you can pass a <code>--checkout</code> argument to the command:</p> <pre><code>kedro new --starter=spaceflights-pandas --checkout=0.1.0\n</code></pre> <p>The <code>--checkout</code> value can point to a branch, tag or commit in the starter repository.</p> <p>Under the hood, the value will be passed to the <code>--checkout</code> flag in Cookiecutter.</p>"},{"location":"pages/starters/starters/#use-a-starter-with-a-configuration-file","title":"Use a starter with a configuration file","text":"<p>By default, when you create a new project using a starter, <code>kedro new</code> asks you to enter the <code>project_name</code>, which it uses to set the <code>repo_name</code> and <code>python_package</code> name. This is the same behaviour as when you create a new empty project</p> <p>Kedro also allows you to specify a configuration file when you create a project using a Kedro starter. Use the <code>--config</code> flag alongside the starter:</p> <pre><code>kedro new --config=my_kedro_project.yml --starter=spaceflights-pandas\n</code></pre> <p>This option is useful when the starter requires more configuration than the default mode requires.</p>"},{"location":"pages/starters/starters/#create-a-custom-starter","title":"Create a custom starter","text":"<p>You can build your own starters for reuse within a project or team, as described in the how to create a Kedro starter documentation.</p> <p>To create a project from a custom starter, run:</p> <pre><code>kedro new --starter=&lt;path-to-starter&gt; --directory &lt;directory&gt;\n</code></pre>"},{"location":"pages/tutorial/add_another_pipeline/","title":"Create a data science pipeline","text":"<p>This section explains the following:</p> <ul> <li>How to add a second Kedro pipeline for data science code that extends the default project pipeline</li> <li>How to 'slice' the project to run just part of the entire pipeline</li> <li>(Optional) How to make a modular pipeline</li> <li>(Optional) How to specify the way the pipeline nodes are run: sequentially or in parallel</li> </ul>"},{"location":"pages/tutorial/add_another_pipeline/#data-science-nodes","title":"Data science nodes","text":"<p>The data science pipeline uses the <code>LinearRegression</code> implementation from the scikit-learn library.</p> <p>The data science pipeline is made up of the following:</p> <ul> <li>Two python files within <code>src/spaceflights/pipelines/data_science</code><ul> <li><code>nodes.py</code> (for the node functions that form the data processing)</li> <li><code>pipeline.py</code> (to build the pipeline)</li> </ul> </li> <li>A yaml file: <code>conf/base/parameters_data_science.yml</code> to define the parameters used when running the pipeline</li> <li><code>__init__.py</code> files in the required folders to ensure that Python can import the pipeline</li> </ul> <p>First, take a look at the functions for the data science nodes in <code>src/spaceflights/pipelines/data_science/nodes.py</code>:</p> Click to expand <pre><code>import logging\nfrom typing import dict, Tuple\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\n\ndef split_data(data: pd.DataFrame, parameters: dict[str, Any]) -&gt; Tuple:\n    \"\"\"Splits data into features and targets training and test sets.\n\n    Args:\n        data: Data containing features and target.\n        parameters: Parameters defined in parameters_data_science.yml.\n    Returns:\n        Split data.\n    \"\"\"\n    X = data[parameters[\"features\"]]\n    y = data[\"price\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=parameters[\"test_size\"], random_state=parameters[\"random_state\"]\n    )\n    return X_train, X_test, y_train, y_test\n\n\ndef train_model(X_train: pd.DataFrame, y_train: pd.Series) -&gt; LinearRegression:\n    \"\"\"Trains the linear regression model.\n\n    Args:\n        X_train: Training data of independent features.\n        y_train: Training data for price.\n\n    Returns:\n        Trained model.\n    \"\"\"\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train)\n    return regressor\n\n\ndef evaluate_model(\n    regressor: LinearRegression, X_test: pd.DataFrame, y_test: pd.Series\n):\n    \"\"\"Calculates and logs the coefficient of determination.\n\n    Args:\n        regressor: Trained model.\n        X_test: Testing data of independent features.\n        y_test: Testing data for price.\n    \"\"\"\n    y_pred = regressor.predict(X_test)\n    score = r2_score(y_test, y_pred)\n    logger = logging.getLogger(__name__)\n    logger.info(\"Model has a coefficient R^2 of %.3f on test data.\", score)\n</code></pre>"},{"location":"pages/tutorial/add_another_pipeline/#input-parameter-configuration","title":"Input parameter configuration","text":"<p>Parameters that are used by the <code>DataCatalog</code> when the pipeline executes are stored in <code>conf/base/parameters_data_science.yml</code>:</p> Click to expand <pre><code>model_options:\n  test_size: 0.2\n  random_state: 3\n  features:\n    - engines\n    - passenger_capacity\n    - crew\n    - d_check_complete\n    - moon_clearance_complete\n    - iata_approved\n    - company_rating\n    - review_scores_rating\n</code></pre> <p>Here, the parameters <code>test_size</code> and <code>random_state</code> are used as part of the train-test split, and <code>features</code> gives the names of columns in the model input table to use as features.</p> <p>More information about parameters is available in the configuration documentation.</p>"},{"location":"pages/tutorial/add_another_pipeline/#model-registration","title":"Model registration","text":"<p>The following definition in <code>conf/base/catalog.yml</code> registers the dataset that saves the trained model:</p> <pre><code>regressor:\n  type: pickle.PickleDataset\n  filepath: data/06_models/regressor.pickle\n  versioned: true\n</code></pre> <p>By setting <code>versioned</code> to <code>true</code>, versioning is enabled for <code>regressor</code>. This means that the pickled output of the <code>regressor</code> is saved every time the pipeline runs, which stores the history of the models built using this pipeline. You can learn more in the later section about dataset and ML model versioning.</p>"},{"location":"pages/tutorial/add_another_pipeline/#data-science-pipeline","title":"Data science pipeline","text":"<p>The data science pipeline is defined in <code>src/spaceflights/pipelines/data_science/pipeline.py</code>:</p> Click to expand <pre><code>from kedro.pipeline import Pipeline, node, pipeline\n\nfrom .nodes import evaluate_model, split_data, train_model\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        [\n            node(\n                func=split_data,\n                inputs=[\"model_input_table\", \"params:model_options\"],\n                outputs=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n                name=\"split_data_node\",\n            ),\n            node(\n                func=train_model,\n                inputs=[\"X_train\", \"y_train\"],\n                outputs=\"regressor\",\n                name=\"train_model_node\",\n            ),\n            node(\n                func=evaluate_model,\n                inputs=[\"regressor\", \"X_test\", \"y_test\"],\n                outputs=None,\n                name=\"evaluate_model_node\",\n            ),\n        ]\n    )\n</code></pre>"},{"location":"pages/tutorial/add_another_pipeline/#test-the-pipelines","title":"Test the pipelines","text":"<p>When you created your project with <code>kedro new</code>, one of the files generated was <code>src/&lt;package_name&gt;/pipeline_registry.py</code> which constructs a <code>__default__</code> pipeline that includes every pipeline in the project.</p> <p>This means that you do not need to manually instruct Kedro to run each pipeline, but can execute the default pipeline, which consists of the data processing and then data science pipeline in turn.</p> <pre><code>kedro run\n</code></pre> <p>You should see output similar to the following:</p> Click to expand <pre><code>                    INFO     Loading data from 'companies' (CSVDataset)...                   data_catalog.py:343\n                    INFO     Running node: preprocess_companies_node:                                node.py:327\n                             preprocess_companies([companies]) -&gt; [preprocessed_companies]\n                    INFO     Saving data to 'preprocessed_companies' (MemoryDataset)...      data_catalog.py:382\n                    INFO     Completed 1 out of 6 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'shuttles' (ExcelDataset)...                  data_catalog.py:343\n[08/09/22 16:56:15] INFO     Running node: preprocess_shuttles_node: preprocess_shuttles([shuttles]) node.py:327\n                             -&gt; [preprocessed_shuttles]\n                    INFO     Saving data to 'preprocessed_shuttles' (MemoryDataset)...       data_catalog.py:382\n                    INFO     Completed 2 out of 6 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'preprocessed_shuttles' (MemoryDataset)...    data_catalog.py:343\n                    INFO     Loading data from 'preprocessed_companies' (MemoryDataset)...   data_catalog.py:343\n                    INFO     Loading data from 'reviews' (CSVDataset)...                     data_catalog.py:343\n                    INFO     Running node: create_model_input_table_node:                            node.py:327\n                             create_model_input_table([preprocessed_shuttles,preprocessed_companies,\n                             reviews]) -&gt; [model_input_table]\n[08/09/22 16:56:18] INFO     Saving data to 'model_input_table' (MemoryDataset)...           data_catalog.py:382\n[08/09/22 16:56:19] INFO     Completed 3 out of 6 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'model_input_table' (MemoryDataset)...        data_catalog.py:343\n                    INFO     Loading data from 'params:model_options' (MemoryDataset)...     data_catalog.py:343\n                    INFO     Running node: split_data_node:                                          node.py:327\n                             split_data([model_input_table,params:model_options]) -&gt;\n                             [X_train,X_test,y_train,y_test]\n                    INFO     Saving data to 'X_train' (MemoryDataset)...                     data_catalog.py:382\n                    INFO     Saving data to 'X_test' (MemoryDataset)...                      data_catalog.py:382\n                    INFO     Saving data to 'y_train' (MemoryDataset)...                     data_catalog.py:382\n                    INFO     Saving data to 'y_test' (MemoryDataset)...                      data_catalog.py:382\n                    INFO     Completed 4 out of 6 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'X_train' (MemoryDataset)...                  data_catalog.py:343\n                    INFO     Loading data from 'y_train' (MemoryDataset)...                  data_catalog.py:343\n                    INFO     Running node: train_model_node: train_model([X_train,y_train]) -&gt;       node.py:327\n                             [regressor]\n[08/09/22 16:56:20] INFO     Saving data to 'regressor' (PickleDataset)...                   data_catalog.py:382\n                    INFO     Completed 5 out of 6 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'regressor' (PickleDataset)...                data_catalog.py:343\n                    INFO     Loading data from 'X_test' (MemoryDataset)...                   data_catalog.py:343\n                    INFO     Loading data from 'y_test' (MemoryDataset)...                   data_catalog.py:343\n                    INFO     Running node: evaluate_model_node:                                      node.py:327\n                             evaluate_model([regressor,X_test,y_test]) -&gt; None\n                    INFO     Model has a coefficient R^2 of 0.462 on test data.                      nodes.py:55\n                    INFO     Completed 6 out of 6 tasks                                  sequential_runner.py:85\n                    INFO     Pipeline execution completed successfully.                             runner.py:89\n</code></pre> <p>As you can see, the <code>data_processing</code> and <code>data_science</code> pipelines ran successfully, generated a model and evaluated it.</p>"},{"location":"pages/tutorial/add_another_pipeline/#slice-a-pipeline","title":"Slice a pipeline","text":"<p>There may be occasions when you want to run just part of the default pipeline. For example, you could skip <code>data_processing</code> execution and run only the <code>data_science</code> pipeline to tune the hyperparameters of the price prediction model.</p> <p>You can 'slice' the pipeline and specify just the portion you want to run by using the <code>--pipeline</code> option. For example, to only run the pipeline named <code>data_science</code> (as labelled automatically in <code>register_pipelines</code>), execute the following command:</p> <pre><code>kedro run --pipeline=data_science\n</code></pre> <p>There are a range of options to run sections of the default pipeline as described in the pipeline slicing documentation and the <code>kedro run</code> CLI documentation.</p>"},{"location":"pages/tutorial/add_another_pipeline/#modular-pipelines","title":"Modular pipelines","text":"<p>In many typical Kedro projects, a single (\u201cmain\u201d) pipeline increases in complexity as the project evolves. To keep your project fit for purpose, we recommend that you create modular pipelines, which are logically isolated and can be reused. You can instantiate a modular pipeline multiple times as a \"template\" pipeline that can run with different inputs/outputs/parameters.</p> <p>Modular pipelines are easier to develop, test and maintain. They are reusable within the same codebase but also portable across projects via micro-packaging as a scalable way to use Kedro pipelines.</p>"},{"location":"pages/tutorial/add_another_pipeline/#optional-extend-the-project-with-namespacing-and-a-modular-pipeline","title":"Optional: Extend the project with namespacing and a modular pipeline","text":"<p>This is optional code so is not provided in the spaceflights starter. If you want to see this in action, you need to copy and paste the code as instructed.</p> <p>First, add namespaces to the modelling component of the data science pipeline to instantiate it as a template with different parameters for an <code>active_modelling_pipeline</code> and a <code>candidate_modelling_pipeline</code> to test the model using different combinations of features.</p> <ol> <li>Update your catalog to add namespaces to the outputs of each instance. Replace the <code>regressor</code> key with the following two new dataset keys in the <code>conf/base/catalog.yml</code> file:</li> </ol> Click to expand <pre><code>active_modelling_pipeline.regressor:\n  type: pickle.PickleDataset\n  filepath: data/06_models/regressor_active.pickle\n  versioned: true\n\ncandidate_modelling_pipeline.regressor:\n  type: pickle.PickleDataset\n  filepath: data/06_models/regressor_candidate.pickle\n  versioned: true\n\n</code></pre> <ol> <li>Update the parameters file for the data science pipeline in <code>conf/base/parameters_data_science.yml</code> to replace the existing contents for <code>model_options</code> with the following for the two instances of the template pipeline:</li> </ol> <p> Click to expand <pre><code>active_modelling_pipeline:\n    model_options:\n      test_size: 0.2\n      random_state: 3\n      features:\n        - engines\n        - passenger_capacity\n        - crew\n        - d_check_complete\n        - moon_clearance_complete\n        - iata_approved\n        - company_rating\n        - review_scores_rating\n\ncandidate_modelling_pipeline:\n    model_options:\n      test_size: 0.2\n      random_state: 8\n      features:\n        - engines\n        - passenger_capacity\n        - crew\n        - review_scores_rating\n</code></pre> </p> <ol> <li>Replace the code in <code>pipelines/data_science/pipeline.py</code> with the snippet below:</li> </ol> <p> Click to expand <pre><code>from kedro.pipeline import Pipeline, node\nfrom kedro.pipeline.modular_pipeline import pipeline\n\nfrom .nodes import evaluate_model, split_data, train_model\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    pipeline_instance = pipeline(\n        [\n            node(\n                func=split_data,\n                inputs=[\"model_input_table\", \"params:model_options\"],\n                outputs=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n                name=\"split_data_node\",\n            ),\n            node(\n                func=train_model,\n                inputs=[\"X_train\", \"y_train\"],\n                outputs=\"regressor\",\n                name=\"train_model_node\",\n            ),\n            node(\n                func=evaluate_model,\n                inputs=[\"regressor\", \"X_test\", \"y_test\"],\n                outputs=None,\n                name=\"evaluate_model_node\",\n            ),\n        ]\n    )\n    ds_pipeline_1 = pipeline(\n        pipe=pipeline_instance,\n        inputs=\"model_input_table\",\n        namespace=\"active_modelling_pipeline\",\n    )\n    ds_pipeline_2 = pipeline(\n        pipe=pipeline_instance,\n        inputs=\"model_input_table\",\n        namespace=\"candidate_modelling_pipeline\",\n    )\n\n    return ds_pipeline_1 + ds_pipeline_2\n</code></pre> </p> <ol> <li>Execute <code>kedro run</code> from the terminal. You should see output as follows:</li> </ol> <p> Click to expand <pre><code>[11/02/22 10:41:08] INFO     Loading data from 'companies' (CSVDataset)...                                             data_catalog.py:343\n                    INFO     Running node: preprocess_companies_node: preprocess_companies([companies]) -&gt;                     node.py:327\n                             [preprocessed_companies]\n                    INFO     Saving data to 'preprocessed_companies' (ParquetDataset)...                               data_catalog.py:382\n                    INFO     Completed 1 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'shuttles' (ExcelDataset)...                                            data_catalog.py:343\n[11/02/22 10:41:13] INFO     Running node: preprocess_shuttles_node: preprocess_shuttles([shuttles]) -&gt;                        node.py:327\n                             [preprocessed_shuttles]\n                    INFO     Saving data to 'preprocessed_shuttles' (ParquetDataset)...                                data_catalog.py:382\n                    INFO     Completed 2 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'preprocessed_shuttles' (ParquetDataset)...                             data_catalog.py:343\n                    INFO     Loading data from 'preprocessed_companies' (ParquetDataset)...                            data_catalog.py:343\n                    INFO     Loading data from 'reviews' (CSVDataset)...                                               data_catalog.py:343\n                    INFO     Running node: create_model_input_table_node:                                                      node.py:327\n                             create_model_input_table([preprocessed_shuttles,preprocessed_companies,reviews]) -&gt;\n                             [model_input_table]\n^[[B[11/02/22 10:41:14] INFO     Saving data to 'model_input_table' (ParquetDataset)...                                    data_catalog.py:382\n[11/02/22 10:41:15] INFO     Completed 3 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'model_input_table' (ParquetDataset)...                                 data_catalog.py:343\n                    INFO     Loading data from 'params:active_modelling_pipeline.model_options' (MemoryDataset)...     data_catalog.py:343\n                    INFO     Running node: split_data_node:                                                                    node.py:327\n                             split_data([model_input_table,params:active_modelling_pipeline.model_options]) -&gt;\n                             [active_modelling_pipeline.X_train,active_modelling_pipeline.X_test,active_modelling_pipeline.y_t\n                             rain,active_modelling_pipeline.y_test]\n                    INFO     Saving data to 'active_modelling_pipeline.X_train' (MemoryDataset)...                     data_catalog.py:382\n                    INFO     Saving data to 'active_modelling_pipeline.X_test' (MemoryDataset)...                      data_catalog.py:382\n                    INFO     Saving data to 'active_modelling_pipeline.y_train' (MemoryDataset)...                     data_catalog.py:382\n                    INFO     Saving data to 'active_modelling_pipeline.y_test' (MemoryDataset)...                      data_catalog.py:382\n                    INFO     Completed 4 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'model_input_table' (ParquetDataset)...                                 data_catalog.py:343\n                    INFO     Loading data from 'params:candidate_modelling_pipeline.model_options' (MemoryDataset)...  data_catalog.py:343\n                    INFO     Running node: split_data_node:                                                                    node.py:327\n                             split_data([model_input_table,params:candidate_modelling_pipeline.model_options]) -&gt;\n                             [candidate_modelling_pipeline.X_train,candidate_modelling_pipeline.X_test,candidate_modelling_pip\n                             eline.y_train,candidate_modelling_pipeline.y_test]\n                    INFO     Saving data to 'candidate_modelling_pipeline.X_train' (MemoryDataset)...                  data_catalog.py:382\n                    INFO     Saving data to 'candidate_modelling_pipeline.X_test' (MemoryDataset)...                   data_catalog.py:382\n                    INFO     Saving data to 'candidate_modelling_pipeline.y_train' (MemoryDataset)...                  data_catalog.py:382\n                    INFO     Saving data to 'candidate_modelling_pipeline.y_test' (MemoryDataset)...                   data_catalog.py:382\n                    INFO     Completed 5 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'active_modelling_pipeline.X_train' (MemoryDataset)...                  data_catalog.py:343\n                    INFO     Loading data from 'active_modelling_pipeline.y_train' (MemoryDataset)...                  data_catalog.py:343\n                    INFO     Running node: train_model_node:                                                                   node.py:327\n                             train_model([active_modelling_pipeline.X_train,active_modelling_pipeline.y_train]) -&gt;\n                             [active_modelling_pipeline.regressor]\n                    INFO     Saving data to 'active_modelling_pipeline.regressor' (PickleDataset)...                   data_catalog.py:382\n                    INFO     Completed 6 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'candidate_modelling_pipeline.X_train' (MemoryDataset)...               data_catalog.py:343\n                    INFO     Loading data from 'candidate_modelling_pipeline.y_train' (MemoryDataset)...               data_catalog.py:343\n                    INFO     Running node: train_model_node:                                                                   node.py:327\n                             train_model([candidate_modelling_pipeline.X_train,candidate_modelling_pipeline.y_train]) -&gt;\n                             [candidate_modelling_pipeline.regressor]\n                    INFO     Saving data to 'candidate_modelling_pipeline.regressor' (PickleDataset)...                data_catalog.py:382\n                    INFO     Completed 7 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'active_modelling_pipeline.regressor' (PickleDataset)...                data_catalog.py:343\n                    INFO     Loading data from 'active_modelling_pipeline.X_test' (MemoryDataset)...                   data_catalog.py:343\n                    INFO     Loading data from 'active_modelling_pipeline.y_test' (MemoryDataset)...                   data_catalog.py:343\n                    INFO     Running node: evaluate_model_node:                                                                node.py:327\n                             evaluate_model([active_modelling_pipeline.regressor,active_modelling_pipeline.X_test,active_model\n                             ling_pipeline.y_test]) -&gt; None\n                    INFO     Model has a coefficient R^2 of 0.462 on test data.                                                nodes.py:60\n                    INFO     Completed 8 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'candidate_modelling_pipeline.regressor' (PickleDataset)...             data_catalog.py:343\n                    INFO     Loading data from 'candidate_modelling_pipeline.X_test' (MemoryDataset)...                data_catalog.py:343\n                    INFO     Loading data from 'candidate_modelling_pipeline.y_test' (MemoryDataset)...                data_catalog.py:343\n                    INFO     Running node: evaluate_model_node:                                                                node.py:327\n                             evaluate_model([candidate_modelling_pipeline.regressor,candidate_modelling_pipeline.X_test,candid\n                             ate_modelling_pipeline.y_test]) -&gt; None\n                    INFO     Model has a coefficient R^2 of 0.449 on test data.                                                nodes.py:60\n                    INFO     Completed 9 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Pipeline execution completed successfully.\n</code></pre> </p>"},{"location":"pages/tutorial/add_another_pipeline/#how-it-works-the-modular-pipeline-wrapper","title":"How it works: the modular <code>pipeline()</code> wrapper","text":"<p>The import you added to the code introduces the pipeline wrapper, which enables you to instantiate multiple instances of pipelines with static structure, but dynamic inputs/outputs/parameters:</p> <pre><code>from kedro.pipeline.modular_pipeline import pipeline\n</code></pre> <p>The <code>pipeline()</code> wrapper method takes the following arguments:</p> Keyword argument Description <code>pipe</code> The <code>Pipeline</code> object you want to wrap <code>inputs</code> Any overrides provided to this instance of the underlying wrapped <code>Pipeline</code> object <code>outputs</code> Any overrides provided to this instance of the underlying wrapped <code>Pipeline</code> object <code>parameters</code> Any overrides provided to this instance of the underlying wrapped <code>Pipeline</code> object <code>namespace</code> The namespace that will be encapsulated by this pipeline instance <p>You can see this snippet as part of the code you added to the example:</p> <p> Click to expand <pre><code>...\n\nds_pipeline_1 = pipeline(\n    pipe=pipeline_instance,\n    inputs=\"model_input_table\",\n    namespace=\"active_modelling_pipeline\",\n)\n\nds_pipeline_2 = pipeline(\n    pipe=pipeline_instance,\n    inputs=\"model_input_table\",\n    namespace=\"candidate_modelling_pipeline\",\n)\n</code></pre> </p> <p>The code instantiates the template_pipeline twice but passes in different parameters. The <code>pipeline_instance</code> variable is the template pipeline, and <code>ds_pipeline_1</code> and <code>ds_pipeline_2</code> are the two separately parameterised instantiations.</p>"},{"location":"pages/tutorial/add_another_pipeline/#how-do-namespaces-affect-parameters","title":"How do namespaces affect parameters?","text":"<p>All <code>inputs</code> and <code>outputs</code> within the nodes of the <code>ds_pipeline_1</code> have the <code>active_modelling_pipeline</code> prefix:</p> <ul> <li><code>params:model_options</code> turns into <code>active_modelling_pipeline.params:model_options</code></li> <li><code>X_train</code> turns into <code>active_modelling_pipeline.X_train</code></li> <li><code>X_test</code> turns into <code>active_modelling_pipeline.X_test</code>, and so on</li> </ul> <p>There are a separate set of parameters for <code>ds_pipeline_2</code> with the <code>candidate_modelling_pipeline</code> prefix:</p> <ul> <li><code>params:model_options</code> turns into <code>candidate_modelling_pipeline.params:model_options</code></li> <li><code>X_train</code> turns into <code>candidate_modelling_pipeline.X_train</code></li> <li><code>X_test</code> turns into <code>candidate_modelling_pipeline.X_test</code>, and so on</li> </ul> <p>However, <code>model_input_table</code> does not get parameterised as it needs to be shared between instances, so is frozen outside the scope of the namespace wrappers.</p> <p>This renders as follows using <code>kedro viz run</code> (hover over the datasets to see their full path) :</p> <p></p>"},{"location":"pages/tutorial/add_another_pipeline/#optional-kedro-runners","title":"Optional: Kedro runners","text":"<p>There are three different Kedro runners that can run the pipeline:</p> <ul> <li><code>SequentialRunner</code> - runs nodes sequentially; once a node has completed its task then the next one starts.</li> <li><code>ParallelRunner</code> - runs nodes in parallel; independent nodes are able to run at the same time, which is more efficient when there are independent branches in your pipeline and enables you to take advantage of multiple CPU cores.</li> <li><code>ThreadRunner</code> - runs nodes in parallel, similarly to <code>ParallelRunner</code>, but uses multithreading instead of multiprocessing.</li> </ul> <p>By default, Kedro uses a <code>SequentialRunner</code>, which is instantiated when you execute <code>kedro run</code> from the terminal. If you decide to use <code>ParallelRunner</code>, <code>ThreadRunner</code> or a custom runner, you can do so through the <code>--runner</code> flag as follows:</p> <pre><code>kedro run --runner=ParallelRunner\nkedro run --runner=ThreadRunner\nkedro run --runner=module.path.to.my.runner\n</code></pre> <p><code>ParallelRunner</code> performs task parallelisation via multiprocessing, while <code>ThreadRunner</code> is intended for use with remote execution engines such as Spark and {class}<code>Dask&lt;kedro-datasets:kedro_datasets.dask.ParquetDataset&gt;</code>.</p> <p>You can find out more about the runners Kedro provides, and how to create your own, in the pipeline documentation about runners.</p>"},{"location":"pages/tutorial/create_a_pipeline/","title":"Create a data processing pipeline","text":"<p>This section explains the following:</p> <ul> <li>How to create a Kedro node from a Python function</li> <li>How to construct a Kedro pipeline from a set of nodes</li> <li>How to persist, or save, datasets output from the pipeline by registering them in the data catalog</li> <li>How to run the pipeline</li> </ul>"},{"location":"pages/tutorial/create_a_pipeline/#introduction","title":"Introduction","text":"<p>The data processing pipeline prepares the data for model building by combining the datasets to create a model input table. The data processing pipeline is made up of the following:</p> <ul> <li>Two python files within <code>src/spaceflights/pipelines/data_processing</code><ul> <li><code>nodes.py</code> (for the node functions that form the data processing)</li> <li><code>pipeline.py</code> (to build the pipeline)</li> </ul> </li> <li>A yaml file: <code>conf/base/parameters_data_processing.yml</code> to define the parameters used when running the pipeline</li> <li><code>__init__.py</code> files in the required folders to ensure that Python can import the pipeline</li> </ul> <pre><code>Kedro provides the `kedro pipeline create` command to add the skeleton code for a new pipeline. If you are writing a project from scratch and want to add a new pipeline, run the following from the terminal: `kedro pipeline create &lt;pipeline_name&gt;`. You do **not** need to do this in the spaceflights example as it is already supplied by the starter project.\n</code></pre>"},{"location":"pages/tutorial/create_a_pipeline/#watch-the-video","title":"Watch the video","text":"<p>The hands-on video course walks through data exploration and data processing for the spaceflights data. There are several videos in the playlist that cover the topic starting with the following:</p> <pre><code>..  youtube:: bZD8N0yv3Fs\n    :width: 100%\n</code></pre>"},{"location":"pages/tutorial/create_a_pipeline/#data-preprocessing-node-functions","title":"Data preprocessing node functions","text":"<p>The first step is to preprocess two of the datasets, <code>companies.csv</code>, and <code>shuttles.xlsx</code>. The preprocessing code for the nodes is in <code>src/spaceflights/pipelines/data_processing/nodes.py</code> as a pair of functions (<code>preprocess_companies</code> and <code>preprocess_shuttles</code>). Each takes a raw DataFrame as input, converts the data in several columns to different types, and outputs a DataFrame containing the preprocessed data:</p> Click to expand <pre><code>import pandas as pd\n\n\ndef _is_true(x: pd.Series) -&gt; pd.Series:\n    return x == \"t\"\n\n\ndef _parse_percentage(x: pd.Series) -&gt; pd.Series:\n    x = x.str.replace(\"%\", \"\")\n    x = x.astype(float) / 100\n    return x\n\n\ndef _parse_money(x: pd.Series) -&gt; pd.Series:\n    x = x.str.replace(\"$\", \"\").str.replace(\",\", \"\")\n    x = x.astype(float)\n    return x\n\n\ndef preprocess_companies(companies: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Preprocesses the data for companies.\n\n    Args:\n        companies: Raw data.\n    Returns:\n        Preprocessed data, with `company_rating` converted to a float and\n        `iata_approved` converted to boolean.\n    \"\"\"\n    companies[\"iata_approved\"] = _is_true(companies[\"iata_approved\"])\n    companies[\"company_rating\"] = _parse_percentage(companies[\"company_rating\"])\n    return companies\n\n\ndef preprocess_shuttles(shuttles: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Preprocesses the data for shuttles.\n\n    Args:\n        shuttles: Raw data.\n    Returns:\n        Preprocessed data, with `price` converted to a float and `d_check_complete`,\n        `moon_clearance_complete` converted to boolean.\n    \"\"\"\n    shuttles[\"d_check_complete\"] = _is_true(shuttles[\"d_check_complete\"])\n    shuttles[\"moon_clearance_complete\"] = _is_true(shuttles[\"moon_clearance_complete\"])\n    shuttles[\"price\"] = _parse_money(shuttles[\"price\"])\n    return shuttles\n</code></pre>"},{"location":"pages/tutorial/create_a_pipeline/#the-data-processing-pipeline","title":"The data processing pipeline","text":"<p>Next, take a look at <code>src/spaceflights/pipelines/data_processing/pipeline.py</code> which constructs a node for each function defined above and creates a modular pipeline for data processing:</p> Click to expand <pre><code>from kedro.pipeline import Pipeline, node, pipeline\n\nfrom .nodes import preprocess_companies, preprocess_shuttles\n\n...\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        [\n            node(\n                func=preprocess_companies,\n                inputs=\"companies\",\n                outputs=\"preprocessed_companies\",\n                name=\"preprocess_companies_node\",\n            ),\n            node(\n                func=preprocess_shuttles,\n                inputs=\"shuttles\",\n                outputs=\"preprocessed_shuttles\",\n                name=\"preprocess_shuttles_node\",\n            ),\n            ...,\n        ]\n    )\n</code></pre> <p>Note that the <code>inputs</code> statements for <code>companies</code> and <code>shuttles</code> refer to the datasets defined in <code>conf/base/catalog.yml</code>. They are inputs to the <code>preprocess_companies</code> and <code>preprocess_shuttles</code> functions. Kedro uses the named node inputs (and outputs) to determine interdependencies between the nodes, and their execution order.</p>"},{"location":"pages/tutorial/create_a_pipeline/#test-the-example","title":"Test the example","text":"<p>Run the following command in your terminal window to test the node named <code>preprocess_companies_node</code>:</p> <pre><code>kedro run --nodes=preprocess_companies_node\n</code></pre> <p>You should see output similar to the below:</p> Click to expand <pre><code>[08/09/22 16:43:11] INFO     Loading data from 'companies' (CSVDataset)...                   data_catalog.py:343\n                    INFO     Running node: preprocess_companies_node:                                node.py:327\n                             preprocess_companies([companies]) -&gt; [preprocessed_companies]\n                    INFO     Saving data to 'preprocessed_companies' (MemoryDataset)...      data_catalog.py:382\n                    INFO     Completed 1 out of 1 tasks                                  sequential_runner.py:85\n                    INFO     Pipeline execution completed successfully.                             runner.py:89\n                    INFO     Loading data from 'preprocessed_companies' (MemoryDataset)...   data_catalog.py:343\n\n</code></pre> <p>You can run the <code>preprocess_shuttles</code> node similarly. To test both nodes together as the complete data processing pipeline:</p> <pre><code>kedro run\n</code></pre> <p>You can also run both nodes by naming each in turn, as follows:</p> <pre><code>kedro run --nodes=preprocess_companies_node,preprocess_shuttles_node\n</code></pre> <p>You should see output similar to the following:</p> Click to expand <pre><code>                    INFO     Loading data from 'companies' (CSVDataset)...                   data_catalog.py:343\n                    INFO     Running node: preprocess_companies_node:                                node.py:327\n                             preprocess_companies([companies]) -&gt; [preprocessed_companies]\n                    INFO     Saving data to 'preprocessed_companies' (MemoryDataset)...      data_catalog.py:382\n                    INFO     Completed 1 out of 2 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'shuttles' (ExcelDataset)...                  data_catalog.py:343\n[08/09/22 16:46:08] INFO     Running node: preprocess_shuttles_node: preprocess_shuttles([shuttles]) node.py:327\n                             -&gt; [preprocessed_shuttles]\n                    INFO     Saving data to 'preprocessed_shuttles' (MemoryDataset)...       data_catalog.py:382\n                    INFO     Completed 2 out of 2 tasks                                  sequential_runner.py:85\n                    INFO     Pipeline execution completed successfully.                             runner.py:89\n                    INFO     Loading data from 'preprocessed_companies' (MemoryDataset)...   data_catalog.py:343\n                    INFO     Loading data from 'preprocessed_shuttles' (MemoryDataset)...    data_catalog.py:343\n\n</code></pre>"},{"location":"pages/tutorial/create_a_pipeline/#preprocessed-data-registration","title":"Preprocessed data registration","text":"<p>Each of the nodes outputs a new dataset (<code>preprocessed_companies</code> and <code>preprocessed_shuttles</code>). Kedro saves these outputs in Parquet format {class}<code>pandas.ParquetDataset&lt;kedro-datasets:kedro_datasets.pandas.ParquetDataset&gt;</code> because they are registered within the Data Catalog as you can see in <code>conf/base/catalog.yml</code>:</p> Click to expand <pre><code>preprocessed_companies:\n  type: pandas.ParquetDataset\n  filepath: data/02_intermediate/preprocessed_companies.parquet\n\npreprocessed_shuttles:\n  type: pandas.ParquetDataset\n  filepath: data/02_intermediate/preprocessed_shuttles.parquet\n</code></pre> <p>If you remove these lines from <code>catalog.yml</code>, Kedro still runs the pipeline successfully and automatically stores the preprocessed data, in memory, as temporary Python objects of the {py:class}<code>~kedro.io.MemoryDataset</code> class. Once all nodes that depend on a temporary dataset have executed, Kedro clears the dataset and the Python garbage collector releases the memory.</p>"},{"location":"pages/tutorial/create_a_pipeline/#create-a-table-for-model-input","title":"Create a table for model input","text":"<p>The next step adds another node that joins together three datasets (<code>preprocessed_shuttles</code>, <code>preprocessed_companies</code>, and <code>reviews</code>) into a single model input table which is saved as <code>model_input_table</code>.</p> <p>The code for the <code>create_model_input_table()</code> function is in <code>src/spaceflights/pipelines/data_processing/nodes.py</code>:</p> Click to expand <pre><code>def create_model_input_table(\n    shuttles: pd.DataFrame, companies: pd.DataFrame, reviews: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Combines all data to create a model input table.\n\n    Args:\n        shuttles: Preprocessed data for shuttles.\n        companies: Preprocessed data for companies.\n        reviews: Raw data for reviews.\n    Returns:\n        model input table.\n\n    \"\"\"\n    rated_shuttles = shuttles.merge(reviews, left_on=\"id\", right_on=\"shuttle_id\")\n    rated_shuttles = rated_shuttles.drop(\"id\", axis=1)\n    model_input_table = rated_shuttles.merge(\n        companies, left_on=\"company_id\", right_on=\"id\"\n    )\n    model_input_table = model_input_table.dropna()\n    return model_input_table\n</code></pre> <p>The node is created in <code>src/kedro_tutorial/pipelines/data_processing/pipeline.py</code>:</p> Click to expand <pre><code>from kedro.pipeline import Pipeline, node, pipeline\n\nfrom .nodes import create_model_input_table, preprocess_companies, preprocess_shuttles\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        [\n            node(\n                func=preprocess_companies,\n                inputs=\"companies\",\n                outputs=\"preprocessed_companies\",\n                name=\"preprocess_companies_node\",\n            ),\n            node(\n                func=preprocess_shuttles,\n                inputs=\"shuttles\",\n                outputs=\"preprocessed_shuttles\",\n                name=\"preprocess_shuttles_node\",\n            ),\n            node(\n                func=create_model_input_table,\n                inputs=[\"preprocessed_shuttles\", \"preprocessed_companies\", \"reviews\"],\n                outputs=\"model_input_table\",\n                name=\"create_model_input_table_node\",\n            ),\n        ]\n    )\n</code></pre>"},{"location":"pages/tutorial/create_a_pipeline/#model-input-table-registration","title":"Model input table registration","text":"<p>The following entry in <code>conf/base/catalog.yml</code> saves the model input table dataset to file (in <code>data/03_primary</code>):</p> <pre><code>model_input_table:\n  type: pandas.ParquetDataset\n  filepath: data/03_primary/model_input_table.parquet\n</code></pre>"},{"location":"pages/tutorial/create_a_pipeline/#test-the-example-again","title":"Test the example again","text":"<p>To test the progress of the example:</p> <pre><code>kedro run\n</code></pre> <p>You should see output similar to the following:</p> Click to expand <pre><code>[08/09/22 17:01:10] INFO     Reached after_catalog_created hook                                     plugin.py:17\n                    INFO     Loading data from 'companies' (CSVDataset)...                   data_catalog.py:343\n                    INFO     Running node: preprocess_companies_node:                                node.py:327\n                             preprocess_companies([companies]) -&gt; [preprocessed_companies]\n                    INFO     Saving data to 'preprocessed_companies' (MemoryDataset)...      data_catalog.py:382\n                    INFO     Completed 1 out of 3 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'shuttles' (ExcelDataset)...                  data_catalog.py:343\n[08/09/22 17:01:25] INFO     Running node: preprocess_shuttles_node: preprocess_shuttles([shuttles]) node.py:327\n                             -&gt; [preprocessed_shuttles]\n\n                    INFO     Saving data to 'preprocessed_shuttles' (MemoryDataset)...       data_catalog.py:382\n                    INFO     Completed 2 out of 3 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'preprocessed_shuttles' (MemoryDataset)...    data_catalog.py:343\n                    INFO     Loading data from 'preprocessed_companies' (MemoryDataset)...   data_catalog.py:343\n                    INFO     Loading data from 'reviews' (CSVDataset)...                     data_catalog.py:343\n                    INFO     Running node: create_model_input_table_node:                            node.py:327\n                             create_model_input_table([preprocessed_shuttles,preprocessed_companies,\n                             reviews]) -&gt; [model_input_table]\n[08/09/22 17:01:28] INFO     Saving data to 'model_input_table' (MemoryDataset)...           data_catalog.py:382\n[08/09/22 17:01:29] INFO     Completed 3 out of 3 tasks                                  sequential_runner.py:85\n                    INFO     Pipeline execution completed successfully.                             runner.py:89\n                    INFO     Loading data from 'model_input_table' (MemoryDataset)...        data_catalog.py:343\n</code></pre>"},{"location":"pages/tutorial/create_a_pipeline/#visualise-the-project","title":"Visualise the project","text":"<p>This section introduces project visualisation using Kedro-Viz, which is a separate package from the standard Kedro installation. To install it in your virtual environment:</p> <pre><code>pip install kedro-viz\n</code></pre> <p>To start Kedro-Viz, enter the following in your terminal:</p> <pre><code>kedro viz run\n</code></pre> <p>This command automatically opens a browser tab to serve the visualisation at <code>http://127.0.0.1:4141/</code>. Explore the visualisation at leisure, and consult the {doc}<code>Kedro-Viz documentation&lt;kedro-viz:kedro-viz_visualisation&gt;</code> for more detail.</p> <p>To exit, close the browser tab. To regain control of the terminal, enter <code>^+c</code> on Mac or <code>Ctrl+c</code> on Windows or Linux machines.</p>"},{"location":"pages/tutorial/create_a_pipeline/#watch-the-video_1","title":"Watch the video","text":"<pre><code>..  youtube:: KWqSzbHgNW4\n    :width: 100%\n</code></pre>"},{"location":"pages/tutorial/create_a_pipeline/#checkpoint","title":"Checkpoint","text":"<p>This is an excellent place to take a breath and summarise what you have seen in the example so far.</p> <p></p> <p>Photo by Malte Helmhold on Unsplash</p> <ul> <li>How to create a new Kedro project from a starter and install its dependencies</li> <li>How to add three datasets to the project and set up the Kedro Data Catalog</li> <li>How to create a data processing pipeline with three nodes to transform and merge the input datasets and create a model input table</li> <li>How to persist the output from a pipeline by registering those datasets to the Data Catalog</li> <li>How to visualise the project</li> </ul> <p>The next step is to create the data science pipeline for spaceflight price prediction.</p>"},{"location":"pages/tutorial/package_a_project/","title":"Package an entire Kedro project","text":"<p>This section explains how to build project documentation, and how to bundle a Kedro project into a Python package.</p> <p>Kedro also has an advanced feature which supports packaging on a pipeline level allowing you share and reuse pipelines across projects! To read more about this please look at the section on micro-packaging.</p>"},{"location":"pages/tutorial/package_a_project/#add-documentation-to-a-kedro-project-if-you-have-not-selected-docs-tool","title":"Add documentation to a Kedro project if you have not selected <code>docs</code> tool","text":"<pre><code>These steps are for projects without the `docs` tool option. You can verify this by looking to see if you don't have a `docs` directory in your project.\n</code></pre> <p>There are several documentation frameworks for Python projects. This section describes how to use Sphinx.</p> <p>To install Sphinx, run the following:</p> <pre><code>pip install sphinx\n</code></pre>"},{"location":"pages/tutorial/package_a_project/#set-up-the-sphinx-project-files","title":"Set up the Sphinx project files","text":"<p>First, run the following command:</p> <pre><code>sphinx-quickstart docs\n</code></pre> <p>Sphinx will ask a series of configuration questions. The first is as follows:</p> <pre><code>You have two options for placing the build directory for Sphinx output.\nEither, you use a directory \"_build\" within the root path,\nor you separate \"source\" and \"build\" directories within the root path.\n\n&gt; Separate source and build directories (y/n)? [n]:\n</code></pre> <p>Select <code>y</code> to separate the build files from the source files, and enter any additional information that Sphinx requests such as the project name and the documentation language, which defaults to English.</p>"},{"location":"pages/tutorial/package_a_project/#build-html-documentation","title":"Build HTML documentation","text":"<pre><code>If you previously backed up the contents of `index.rst`, restore them before proceeding.\n</code></pre> <p>After the quickstart process is complete, you can build the documentation by navigating to the <code>docs</code> directory and running the following:</p> <pre><code>make html\n</code></pre> <p>Project documentation will be written to the <code>docs/build/html</code> directory.</p> <p>You may want to add project-specific Markdown documentation within the <code>docs/source</code> folder of your Kedro project. To be able to build it, follow the introduction instructions of MyST-Parser and update the <code>docs/source/index.rst</code> file to add the markdown files to the table of contents.</p>"},{"location":"pages/tutorial/package_a_project/#documentation-from-docstrings","title":"Documentation from docstrings","text":"<p>If you wish to add documentation built from <code>docstrings</code> within your project, you need to make some changes to the Sphinx configuration files found in the <code>docs/source</code> directory to use automatic documentation generation from code.</p> <p>In <code>conf.py</code>, add the following to ensure that the <code>sphinx.ext.autodoc</code> and <code>sphinx.ext.autosummary</code> extensions are specified, and <code>autosummary_generate</code> is enabled:</p> <pre><code>extensions = [\"sphinx.ext.autodoc\", \"sphinx.ext.autosummary\"]\nautosummary_generate = True\n</code></pre> <p>Finally, to ensure that you include the autodoc modules in the build, run the following command once from the <code>docs</code> folder:</p> <pre><code>sphinx-apidoc --module-first -o source ../src/&lt;package_name&gt;\n\n</code></pre> <p>This will generate a <code>docs/src/modules.rst</code> file, as well as other files containing references to any docstrings. To include those in your documentation, make sure <code>docs/src/index.rst</code> has a <code>modules</code> entry in the table of contents:</p> <pre><code>.. toctree::\n\n   modules\n</code></pre> <p>From the <code>docs</code> folder run the following:</p> <pre><code>pip install -e ../src\n</code></pre> <p>Finally, from the <code>docs folder</code>, run this command to build a full set of documentation that automatically includes docstrings:</p> <pre><code>make html\n</code></pre> <pre><code>Consult the Sphinx project documentation for [additional options to pass to `sphinx-build`](https://www.sphinx-doc.org/en/master/man/sphinx-build.html). To customise your documentation beyond the basic template, you'll need to adjust the [Sphinx configuration settings](https://www.sphinx-doc.org/en/master/usage/configuration.html) which are stored in `docs/source/conf.py` file.\n</code></pre>"},{"location":"pages/tutorial/package_a_project/#package-a-kedro-project","title":"Package a Kedro project","text":"<p>To package a project, run the following in your project root directory:</p> <pre><code>kedro package\n</code></pre> <p>Kedro builds the package into the <code>dist</code> folder of the project as a <code>.whl</code> file, which is a Python packaging format for binary distribution.</p> <p>The resulting <code>.whl</code> packages only contain the Python source code of the Kedro pipeline, not any of the <code>conf</code> and <code>data</code> subfolders. This means that you can distribute the project to run elsewhere, such as on a separate computer with different configuration information, dataset and logging locations.</p> <p>The project configuration is provided separately in a <code>tar.gz</code> file, also inside the <code>dist</code> folder. This compressed version of the config files excludes any files inside the <code>local</code> directory.</p>"},{"location":"pages/tutorial/package_a_project/#run-a-packaged-project","title":"Run a packaged project","text":"<p>To run a packaged project it must first be installed. To install the package from a <code>.whl</code> file, you need to have Python and <code>pip</code> installed on your machine, but you do not need to have Kedro installed.</p> <p>To install the project, run the following command:</p> <pre><code>pip install &lt;path-to-wheel-file&gt;\n</code></pre> <pre><code>Once the packaged project is installed, you will need to add:\n\n* a `conf` folder\n* a `data` folder if the pipeline loads/saves local data\n\nAlternatively, you can make use of the ``OmegaConfigLoader`` to run the configuration directly from the compressed .tar.gz configuration file by running\nkedro run --conf-source &lt;path-to-compressed-config&gt;.tar.gz\n</code></pre> <p>Once your project is installed, it can be run either from the command line or interactively using Python code.</p> <p>To do a basic run of your installed project from the command line, run <code>python -m &lt;package_name&gt;</code>. The packaged project also exposes a command line interface which you can use to modify how your project will be run. To see a list of options, use <code>python -m &lt;package_name&gt; --help</code> at the command line.</p> <p>To run your packaged project interactively using code, you can import <code>main</code> from the project:</p> <pre><code>from &lt;package_name&gt;.__main__ import main\n\nmain(\n    [\"--pipeline\", \"__default__\"]\n)  # or simply main() if you don't want to provide any arguments\n</code></pre> <p>This is equivalent to <code>python -m &lt;package_name&gt;</code> at the command line, and you can pass in all the arguments that correspond to the options described by <code>python -m &lt;package_name&gt; --help</code>.</p> <pre><code>If you run the packaged project in the interactive environment like IPython or Databricks you can also consume the output of the `main()`\nwhich returns the `session.run()` output.\n</code></pre> <pre><code>from spaceflights.__main__ import main\n\ndef run_kedro_pipeline():\n   result = main(pipeline_name=&lt;pipeline&gt;)\n   do_something_with(&lt;result&gt;)\n</code></pre>"},{"location":"pages/tutorial/package_a_project/#docker-airflow-and-other-deployment-targets","title":"Docker, Airflow and other deployment targets","text":"<p>There are various methods to deploy packaged pipelines via Kedro plugins:</p> <ul> <li>Kedro-Docker plugin for packaging and shipping Kedro projects within Docker containers.</li> <li>Kedro-Airflow to convert your Kedro project into an Airflow project.</li> <li>The Deployment guide touches on other deployment targets such as AWS Batch and Prefect, and there is a range of third-party plugins for deployment.</li> </ul>"},{"location":"pages/tutorial/set_up_data/","title":"Set up the data","text":"<p>This section shows how to add datasets to the project's <code>data</code> folder. It also reviews how those datasets are registered in Kedro's Data Catalog, which is the registry of all data sources available for use by the project.</p>"},{"location":"pages/tutorial/set_up_data/#project-datasets","title":"Project datasets","text":"<p>The spaceflights tutorial makes use of three fictional datasets of companies shuttling customers to the Moon and back. The data comes in two different formats: <code>.csv</code> and <code>.xlsx</code>:</p> <ul> <li><code>companies.csv</code> contains data about space travel companies, such as their location, fleet count and rating</li> <li><code>reviews.csv</code> is a set of reviews from customers for categories, such as comfort and price</li> <li><code>shuttles.xlsx</code> is a set of attributes for spacecraft across the fleet, such as their engine type and passenger capacity</li> </ul> <p>The spaceflights starter has already added the datasets to the <code>data/01_raw</code> folder of your project.</p>"},{"location":"pages/tutorial/set_up_data/#dataset-registration","title":"Dataset registration","text":"<p>The following information about a dataset must be registered before Kedro can load it:</p> <ul> <li>File location (path)</li> <li>Parameters for the given dataset</li> <li>Type of data</li> <li>Versioning</li> </ul> <p>Open <code>conf/base/catalog.yml</code> for the spaceflights project to inspect the contents. The two <code>csv</code> datasets are registered as follows:</p> Click to expand <pre><code>companies:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/companies.csv\n\nreviews:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/reviews.csv\n</code></pre> <p>Likewise for the <code>xlsx</code> dataset:</p> <p> Click to expand <pre><code>shuttles:\n  type: pandas.ExcelDataset\n  filepath: data/01_raw/shuttles.xlsx\n  load_args:\n    engine: openpyxl # Use modern Excel engine (the default since Kedro 0.18.0)\n</code></pre> </p> <p>The additional line, <code>load_args</code>, is passed to the excel file read method (<code>pd.read_excel</code>) as a keyword argument. Although not specified here, the equivalent output is <code>save_args</code> and the value would be passed to <code>pd.DataFrame.to_excel</code> method.</p>"},{"location":"pages/tutorial/set_up_data/#test-that-kedro-can-load-the-data","title":"Test that Kedro can load the data","text":"<p>Open a <code>kedro ipython</code> session in your terminal from the project root directory:</p> <pre><code>kedro ipython\n</code></pre> <p>Then type the following into the IPython prompt to test load some <code>csv</code> data:</p> <pre><code>companies = catalog.load(\"companies\")\ncompanies.head()\n</code></pre> <ul> <li>The first command creates a variable (<code>companies</code>), which is of type <code>pandas.DataFrame</code> and loads the dataset (also named <code>companies</code> as per top-level key in <code>catalog.yml</code>) from the underlying filepath <code>data/01_raw/companies.csv</code>.</li> <li>The <code>head</code> method from <code>pandas</code> displays the first five rows of the DataFrame.</li> </ul> <p> Click to expand <pre><code>INFO     Loading data from 'companies' (CSVDataset)\nOut[1]:\n      id company_rating       company_location  total_fleet_count iata_approved\n0  35029           100%                   Niue                4.0             f\n1  30292            67%               Anguilla                6.0             f\n2  19032            67%     Russian Federation                4.0             f\n3   8238            91%               Barbados               15.0             t\n4  30342            NaN  Sao Tome and Principe                2.0             t\n\n</code></pre> </p> <p>Similarly, to test that the <code>xlsx</code> data is loaded as expected:</p> <pre><code>shuttles = catalog.load(\"shuttles\")\nshuttles.head()\n</code></pre> <p>You should see output such as the following:</p> <p> Click to expand <pre><code>INFO     Loading data from 'shuttles' (ExcelDataset)\nOut[1]:\n      id       shuttle_location shuttle_type engine_type  ... d_check_complete  moon_clearance_complete     price company_id\n0  63561                   Niue      Type V5     Quantum  ...                f                        f  $1,325.0      35029\n1  36260               Anguilla      Type V5     Quantum  ...                t                        f  $1,780.0      30292\n2  57015     Russian Federation      Type V5     Quantum  ...                f                        f  $1,715.0      19032\n3  14035               Barbados      Type V5      Plasma  ...                f                        f  $4,770.0       8238\n4  10036  Sao Tome and Principe      Type V2      Plasma  ...                f                        f  $2,820.0      30342\n\n</code></pre> </p> <p>When you have finished, close <code>ipython</code> session with <code>exit()</code>.</p>"},{"location":"pages/tutorial/set_up_data/#further-information","title":"Further information","text":""},{"location":"pages/tutorial/set_up_data/#watch-the-video","title":"Watch the video","text":"<pre><code>..  youtube:: rl2cncGxyts\n    :width: 100%\n</code></pre>"},{"location":"pages/tutorial/set_up_data/#custom-data","title":"Custom data","text":"<p>{py:mod}<code>Kedro supports numerous datasets &lt;kedro-datasets:kedro_datasets&gt;</code> out of the box, but you can also add support for any proprietary data format or filesystem.</p> <p>You can find further information about how to add support for custom datasets in specific documentation covering advanced usage.</p>"},{"location":"pages/tutorial/set_up_data/#supported-data-locations","title":"Supported data locations","text":"<p>Kedro uses <code>fsspec</code> to read data from a variety of data stores including local file systems, network file systems, HDFS, and all of the widely-used cloud object stores.</p> <p></p>"},{"location":"pages/tutorial/spaceflights_tutorial/","title":"Next steps: Tutorial","text":"<p>In this tutorial, we construct nodes and pipelines for a price-prediction model to illustrate the steps of a typical Kedro workflow.</p> <p>The tutorial takes approximately 30 minutes to complete. You will work in the terminal and by inspecting project files in an IDE or text editor. There is no Jupyter notebook for the project.</p> <p>It is 2160, and the space tourism industry is booming. Globally, thousands of space shuttle companies take tourists to the Moon and back. You have been able to source data that lists the amenities offered in each space shuttle, customer reviews, and company information.</p> <p>Project: You want to construct a model that predicts the price for each trip to the Moon and the corresponding return flight.</p>"},{"location":"pages/tutorial/spaceflights_tutorial/#tutorial-steps","title":"Tutorial steps","text":"<ul> <li>Tutorial Template</li> <li>Set Up Data</li> <li>Create a Pipeline</li> <li>Add Another Pipeline</li> <li>Test a Project</li> <li>Package a Project</li> <li>Spaceflights Tutorial FAQs</li> </ul> <p>Photo by Ivan Diaz on Unsplash</p>"},{"location":"pages/tutorial/spaceflights_tutorial/#watch-the-video","title":"Watch the video","text":""},{"location":"pages/tutorial/spaceflights_tutorial/#get-help","title":"Get help","text":"<p>If you encounter an issue with the tutorial:</p> <ul> <li>Check the spaceflights tutorial FAQ to see if we have answered the question already.</li> <li>Use Kedro-Viz to visualise your project and better understand how the datasets, nodes, and pipelines fit together.</li> <li>Use the #questions channel on our Slack channel to ask the community for help.</li> <li>Search the searchable archive of Slack discussions.</li> </ul>"},{"location":"pages/tutorial/spaceflights_tutorial/#terminology","title":"Terminology","text":"<p>We explain any Kedro-specific terminology as we introduce it, and further information can be found in the glossary. Some additional terminology may not be familiar to some readers, such as the concepts below.</p>"},{"location":"pages/tutorial/spaceflights_tutorial/#project-root-directory","title":"Project root directory","text":"<p>Also known as the \"root directory,\" this is the parent folder for the entire project. It is the top-level folder that contains all other files and directories associated with the project.</p>"},{"location":"pages/tutorial/spaceflights_tutorial/#dependencies","title":"Dependencies","text":"<p>These are Python packages or libraries that an individual project depends upon to complete a task. For example, the Spaceflights tutorial project depends on the scikit-learn library.</p>"},{"location":"pages/tutorial/spaceflights_tutorial/#standard-development-workflow","title":"Standard development workflow","text":"<p>When you build a Kedro project, you will typically follow a standard development workflow:</p> <ol> <li> <p>Set up the project template</p> <ul> <li>Create a new project and install project dependencies.</li> <li>Configure credentials and any other sensitive/personal content, and logging.</li> </ul> </li> <li> <p>Set up the data</p> <ul> <li>Add data to the <code>data</code> folder.</li> <li>Reference all datasets for the project.</li> </ul> </li> <li> <p>Create the pipeline</p> <ul> <li>Construct nodes to make up the pipeline.</li> <li>Choose how to run the pipeline: sequentially or in parallel.</li> </ul> </li> <li> <p>Package the project</p> <ul> <li>Build the project documentation.</li> <li>Package the project for distribution.</li> </ul> </li> </ol>"},{"location":"pages/tutorial/spaceflights_tutorial_faqs/","title":"Spaceflights tutorial FAQs","text":"<pre><code>If you can't find the answer you need here, [ask the Kedro community for help](https://slack.kedro.org)!\n</code></pre>"},{"location":"pages/tutorial/spaceflights_tutorial_faqs/#how-do-i-resolve-these-common-errors","title":"How do I resolve these common errors?","text":""},{"location":"pages/tutorial/spaceflights_tutorial_faqs/#dataset-errors","title":"Dataset errors","text":""},{"location":"pages/tutorial/spaceflights_tutorial_faqs/#dataseterror-failed-while-loading-data-from-dataset","title":"DatasetError: Failed while loading data from dataset","text":"<p>You're testing whether Kedro can load the raw test data and see the following:</p> <pre><code>DatasetError: Failed while loading data from dataset\nCSVDataset(filepath=...).\n[Errno 2] No such file or directory: '.../companies.csv'\n</code></pre> <p>or a similar error for the <code>shuttles</code> or <code>reviews</code> data.</p> <p>Are the three sample data files stored in the <code>data/raw</code> folder?</p>"},{"location":"pages/tutorial/spaceflights_tutorial_faqs/#datasetnotfounderror-dataset-not-found-in-the-catalog","title":"DatasetNotFoundError: Dataset not found in the catalog","text":"<p>You see an error such as the following:</p> <pre><code>DatasetNotFoundError: Dataset 'companies' not found in the catalog\n</code></pre> <p>Has something changed in your <code>catalog.yml</code> from the version generated by the spaceflights starter? Take a look at the data specification to ensure it is valid.</p> <p>Call <code>exit()</code> within the IPython session and restart <code>kedro ipython</code> (or type <code>@kedro_reload</code> into the IPython console to reload Kedro into the session without restarting). Then try again.</p>"},{"location":"pages/tutorial/spaceflights_tutorial_faqs/#dataseterror-an-exception-occurred-when-parsing-config-for-dataset","title":"DatasetError: An exception occurred when parsing config for Dataset","text":"<p>Are you seeing a message saying that an exception occurred?</p> <pre><code>DatasetError: An exception occurred when parsing config for Dataset\n'data_processing.preprocessed_companies':\nObject 'ParquetDataset' cannot be loaded from 'kedro_datasets.pandas'. Please see the\ndocumentation on how to install relevant dependencies for kedro_datasets.pandas.ParquetDataset:\nhttps://docs.kedro.org/en/stable/kedro_project_setup/dependencies.html\n</code></pre> <p>The Kedro Data Catalog is missing dependencies needed to parse the data. Check that you have all the project dependencies to <code>requirements.txt</code> and then call <code>pip install -r requirements.txt</code> to install them.</p>"},{"location":"pages/tutorial/spaceflights_tutorial_faqs/#pipeline-run","title":"Pipeline run","text":"<p>To successfully run the pipeline, all required input datasets must already exist, otherwise you may get an error similar to this:</p> <pre><code>kedro run --pipeline=data_science\n\n2019-10-04 12:36:12,158 - kedro.io.data_catalog - INFO - Loading data from `model_input_table` (CSVDataset)...\n2019-10-04 12:36:12,158 - kedro.runner.sequential_runner - WARNING - There are 3 nodes that have not run.\nYou can resume the pipeline run with the following command:\nkedro run\nTraceback (most recent call last):\n  ...\n  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'data/03_primary/model_input_table.csv' does not exist: b'data/03_primary/model_input_table.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  ...\n    raise DatasetError(message) from exc\nkedro.io.core.DatasetError: Failed while loading data from dataset CSVDataset(filepath=data/03_primary/model_input_table.csv, save_args={'index': False}).\n[Errno 2] File b'data/03_primary/model_input_table.csv' does not exist: b'data/03_primary/model_input_table.csv'\n</code></pre>"},{"location":"pages/tutorial/test_a_project/","title":"Test a Kedro project","text":"<p>It is important to test our Kedro projects to validate and verify that our nodes and pipelines behave as we expect them to. In this section we look at some example tests for the spaceflights project.</p> <p>This section explains the following:</p> <ul> <li>How to test a Kedro node</li> <li>How to test a Kedro pipeline</li> <li>Testing best practices</li> </ul> <p>This section does not cover:</p> <ul> <li>Automating your tests - instead read our automated testing documentation.</li> <li>More advanced features of testing, including mocking and parameterising tests.</li> </ul>"},{"location":"pages/tutorial/test_a_project/#writing-tests-for-kedro-nodes-unit-testing","title":"Writing tests for Kedro nodes: Unit testing","text":"<p>Kedro expects node functions to be pure functions; a pure function is one whose output follows solely from its inputs, without any observable side effects. Testing these functions checks that a node will behave as expected - for a given set of input values, a node will produce the expected output. These tests are referred to as unit tests.</p> <p>Let us explore what this looks like in practice. Consider the node function <code>split_data</code> defined in the data science pipeline:</p> Click to expand <pre><code>def split_data(data: pd.DataFrame, parameters: dict[str, Any]) -&gt; Tuple:\n    \"\"\"Splits data into features and targets training and test sets.\n\n    Args:\n        data: Data containing features and target.\n        parameters: Parameters defined in parameters_data_science.yml.\n    Returns:\n        Split data.\n    \"\"\"\n    X = data[parameters[\"features\"]]\n    y = data[\"price\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=parameters[\"test_size\"], random_state=parameters[\"random_state\"]\n    )\n    return X_train, X_test, y_train, y_test\n</code></pre> <p>The function takes a pandas <code>DataFrame</code> and dictionary of parameters as input, and splits the input data into four different data objects as per the parameters provided. We recommend following pytest's anatomy of a test which breaks a test down into four  steps: arrange, act, assert, and cleanup. For this specific function, these steps will be:</p> <ol> <li>Arrange: Prepare the inputs <code>data</code> and <code>parameters</code>.</li> <li>Act: Make a call to <code>split_data</code> and capture the outputs with <code>X_train</code>, <code>X_test</code>, <code>Y_train</code>, and <code>Y_test</code>.</li> <li>Assert: Ensure that the length of the outputs are the same as the expected lengths</li> </ol> <p>The cleanup step becomes necessary in a test when any of the previous steps make modifications that may influence other tests - e.g. by modifying a file used as input for several tests. This is not the case for the example tests below, and so the cleanup step is omitted.</p> <p>Remember to import the function being tested and any necessary modules at the top of the file.</p> <p>When we put these steps together, we have the following test:</p> Click to expand <pre><code># NOTE: This example test is yet to be refactored.\n# A complete version is available under the testing best practices section.\n\nimport pandas as pd\nfrom spaceflights.pipelines.data_science.nodes import split_data\n\ndef test_split_data():\n    # Arrange\n    dummy_data = pd.DataFrame(\n        {\n            \"engines\": [1, 2, 3],\n            \"crew\": [4, 5, 6],\n            \"passenger_capacity\": [5, 6, 7],\n            \"price\": [120, 290, 30],\n        }\n    )\n\n    dummy_parameters = {\n        \"model_options\": {\n            \"test_size\": 0.2,\n            \"random_state\": 3,\n            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n        }\n    }\n\n    # Act\n    X_train, X_test, y_train, y_test = split_data(dummy_data, dummy_parameters[\"model_options\"])\n\n    # Assert\n    assert len(X_train) == 2\n    assert len(y_train) == 2\n    assert len(X_test) == 1\n    assert len(y_test) == 1\n</code></pre> <p>This test is an example of positive testing - it tests that a valid input produces the expected output. The inverse, testing that an invalid output will be appropriately rejected, is called negative testing and is equally as important.</p> <p>Using the same steps as above, we can write the following test to validate an error is thrown when price data is not available:</p> Click to expand <pre><code># NOTE: This example test is yet to be refactored.\n# A complete version is available under the testing best practices section.\n\nimport pandas as pd\nfrom spaceflights.pipelines.data_science.nodes import split_data\n\ndef test_split_data_missing_price():\n    # Arrange\n    dummy_data = pd.DataFrame(\n        {\n            \"engines\": [1, 2, 3],\n            \"crew\": [4, 5, 6],\n            \"passenger_capacity\": [5, 6, 7],\n            # Note the missing price data\n        }\n    )\n\n    dummy_parameters = {\n        \"model_options\": {\n            \"test_size\": 0.2,\n            \"random_state\": 3,\n            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n        }\n    }\n\n    with pytest.raises(KeyError) as e_info:\n        # Act\n        X_train, X_test, y_train, y_test = split_data(dummy_data, dummy_parameters[\"model_options\"])\n\n    # Assert\n    assert \"price\" in str(e_info.value) # checks that the error is about the missing price data\n</code></pre>"},{"location":"pages/tutorial/test_a_project/#writing-tests-for-kedro-pipelines-integration-testing","title":"Writing tests for Kedro pipelines: Integration testing","text":"<p>Writing tests for each node ensures each node will behave as expected when run individually. However, we must also consider how nodes in a pipeline interact with each other - this is called integration testing. Integration testing combines individual units as a group and checks whether they communicate, share data, and work together as expected. Let us look at this in practice.</p> <p>Consider the data science pipeline as a whole:</p> Click to expand <pre><code>from kedro.pipeline import Pipeline, node, pipeline\nfrom .nodes import evaluate_model, split_data, train_model\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        [\n            node(\n                func=split_data,\n                inputs=[\"model_input_table\", \"params:model_options\"],\n                outputs=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n                name=\"split_data_node\",\n            ),\n            node(\n                func=train_model,\n                inputs=[\"X_train\", \"y_train\"],\n                outputs=\"regressor\",\n                name=\"train_model_node\",\n            ),\n            node(\n                func=evaluate_model,\n                inputs=[\"regressor\", \"X_test\", \"y_test\"],\n                outputs=None,\n                name=\"evaluate_model_node\",\n            ),\n        ]\n    )\n</code></pre> <p>The pipeline takes a pandas <code>DataFrame</code> and dictionary of parameters as input, splits the data in accordance to the parameters, and uses it to train and evaluate a regression model. With an integration test, we can validate that this sequence of nodes runs as expected.</p> <p>From earlier in this tutorial we know a successful pipeline run will conclude with the message <code>Pipeline execution completed successfully.</code> being logged. To validate this is being logged in our tests we make use of pytest's <code>caplog</code> feature to capture logs generated during the execution.</p> <p>As we did with our unit tests, we break this down into several steps:</p> <ol> <li>Arrange: Prepare the runner and its inputs <code>pipeline</code> and <code>catalog</code>, and any additional test setup.</li> <li>Act: Run the pipeline.</li> <li>Assert: Ensure a successful run message was logged.</li> </ol> <p>When we put this together, we get the following test:</p> Click to expand <pre><code># NOTE: This example test is yet to be refactored.\n# A complete version is available under the testing best practices section.\n\nimport logging\nimport pandas as pd\nfrom kedro.io import DataCatalog\nfrom kedro.runner import SequentialRunner\nfrom spaceflights.pipelines.data_science import create_pipeline as create_ds_pipeline\n\ndef test_data_science_pipeline(caplog):    # Note: caplog is passed as an argument\n    # Arrange pipeline\n    pipeline = create_ds_pipeline()\n\n    # Arrange data catalog\n    catalog = DataCatalog()\n\n    dummy_data = pd.DataFrame(\n        {\n            \"engines\": [1, 2, 3],\n            \"crew\": [4, 5, 6],\n            \"passenger_capacity\": [5, 6, 7],\n            \"price\": [120, 290, 30],\n        }\n    )\n\n    duummy_parameters = {\n        \"model_options\": {\n            \"test_size\": 0.2,\n            \"random_state\": 3,\n            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n        }\n    }\n\n    catalog.add_feed_dict(\n        {\n            \"model_input_table\" : dummy_data,\n            \"params:model_options\": dummy_parameters[\"model_options\"],\n        }\n    )\n\n    # Arrange the log testing setup\n    caplog.set_level(logging.DEBUG, logger=\"kedro\") # Ensure all logs produced by Kedro are captured\n    successful_run_msg = \"Pipeline execution completed successfully.\"\n\n    # Act\n    SequentialRunner().run(pipeline, catalog)\n\n    # Assert\n    assert successful_run_msg in caplog.text\n\n</code></pre>"},{"location":"pages/tutorial/test_a_project/#testing-best-practices","title":"Testing best practices","text":""},{"location":"pages/tutorial/test_a_project/#where-to-write-your-tests","title":"Where to write your tests","text":"<p>We recommend creating a <code>tests</code> directory within the root directory of your project. The structure should mirror the directory structure of <code>/src/spaceflights</code>:</p> <pre><code>src\n\u2502   ...\n\u2514\u2500\u2500\u2500spaceflights\n\u2502   \u2514\u2500\u2500\u2500pipelines\n\u2502       \u2514\u2500\u2500\u2500data_science\n\u2502           \u2502   __init__.py\n\u2502           \u2502   nodes.py\n\u2502           \u2502   pipeline.py\n\u2502\ntests\n|   ...\n\u2514\u2500\u2500\u2500pipelines\n\u2502   \u2514\u2500\u2500\u2500data_science\n\u2502       \u2502   test_data_science_pipeline.py\n</code></pre>"},{"location":"pages/tutorial/test_a_project/#using-fixtures","title":"Using fixtures","text":"<p>In our tests, we can see that <code>dummy_data</code> and <code>dummy_parameters</code> have been defined three times with (mostly) the same values. Instead, we can define these outside of our tests as pytest fixtures:</p> Click to expand <pre><code>import pytest\n\n@pytest.fixture\ndef dummy_data():\n    return pd.DataFrame(\n        {\n            \"engines\": [1, 2, 3],\n            \"crew\": [4, 5, 6],\n            \"passenger_capacity\": [5, 6, 7],\n            \"price\": [120, 290, 30],\n        }\n    )\n\n@pytest.fixture\ndef dummy_parameters():\n    parameters = {\n        \"model_options\": {\n            \"test_size\": 0.2,\n            \"random_state\": 3,\n            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n        }\n    }\n    return parameters\n</code></pre> <p>We can then access these through the test arguments.</p> <pre><code>def test_split_data(dummy_data, dummy_parameters):\n        ...\n</code></pre>"},{"location":"pages/tutorial/test_a_project/#pipeline-slicing","title":"Pipeline slicing","text":"<p>In the test <code>test_data_science_pipeline</code> we test the data science pipeline, as currently defined, can be run successfully. However, as pipelines are not static, this test is not robust. Instead we should be specific with how we define the pipeline to be tested; we do this by using pipeline slicing to specify the pipeline's start and end:</p> <pre><code>def test_data_science_pipeline(self):\n    # Arrange pipeline\n    pipeline = create_pipeline().from_nodes(\"split_data_node\").to_nodes(\"evaluate_model_node\")\n    ...\n</code></pre> <p>This ensures that the test will still perform as designed, even with the addition of more nodes to the pipeline.</p> <p>After incorporating these testing practices, our test file <code>test_data_science_pipeline.py</code> becomes:</p> <pre><code># tests/pipelines/test_data_science_pipeline.py\n\nimport logging\nimport pandas as pd\nimport pytest\n\nfrom kedro.io import DataCatalog\nfrom kedro.runner import SequentialRunner\nfrom spaceflights.pipelines.data_science import create_pipeline as create_ds_pipeline\nfrom spaceflights.pipelines.data_science.nodes import split_data\n\n@pytest.fixture\ndef dummy_data():\n    return pd.DataFrame(\n        {\n            \"engines\": [1, 2, 3],\n            \"crew\": [4, 5, 6],\n            \"passenger_capacity\": [5, 6, 7],\n            \"price\": [120, 290, 30],\n        }\n    )\n\n@pytest.fixture\ndef dummy_parameters():\n    parameters = {\n        \"model_options\": {\n            \"test_size\": 0.2,\n            \"random_state\": 3,\n            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n        }\n    }\n    return parameters\n\n\ndef test_split_data(dummy_data, dummy_parameters):\n    X_train, X_test, y_train, y_test = split_data(\n        dummy_data, dummy_parameters[\"model_options\"]\n    )\n    assert len(X_train) == 2\n    assert len(y_train) == 2\n    assert len(X_test) == 1\n    assert len(y_test) == 1\n\ndef test_split_data_missing_price(dummy_data, dummy_parameters):\n    dummy_data_missing_price = dummy_data.drop(columns=\"price\")\n    with pytest.raises(KeyError) as e_info:\n        X_train, X_test, y_train, y_test = split_data(dummy_data_missing_price, dummy_parameters[\"model_options\"])\n\n    assert \"price\" in str(e_info.value)\n\ndef test_data_science_pipeline(caplog, dummy_data, dummy_parameters):\n    pipeline = (\n        create_ds_pipeline()\n        .from_nodes(\"split_data_node\")\n        .to_nodes(\"evaluate_model_node\")\n    )\n    catalog = DataCatalog()\n    catalog.add_feed_dict(\n        {\n            \"model_input_table\" : dummy_data,\n            \"params:model_options\": dummy_parameters[\"model_options\"],\n        }\n    )\n\n    caplog.set_level(logging.DEBUG, logger=\"kedro\")\n    successful_run_msg = \"Pipeline execution completed successfully.\"\n\n    SequentialRunner().run(pipeline, catalog)\n\n    assert successful_run_msg in caplog.text\n\n</code></pre>"},{"location":"pages/tutorial/test_a_project/#run-your-tests","title":"Run your tests","text":"<p>First, confirm that your project has been installed locally. This can be done by navigating to the project root and running the following command:</p> <pre><code>pip install -e .\n</code></pre> <p>This step allows pytest to accurately resolve the import statements in your test files.</p> <p>NOTE: The option <code>-e</code> installs an editable version of your project, allowing you to make changes to the project files without needing to re-install them each time.</p> <p>Ensure you have <code>pytest</code> installed. Please see our automated testing documentation for more information on getting set up with pytest.</p> <p>To run your tests, run <code>pytest</code> from within your project's root directory.</p> <pre><code>cd &lt;project_root&gt;\npytest tests/pipelines/test_data_science_pipeline.py\n</code></pre> <p>You should see the following output in your shell.</p> <pre><code>============================= test session starts ==============================\n...\ncollected 2 items\n\ntests/pipelines/test_data_science_pipeline.py ..                                                  [100%]\n\n============================== 2 passed in 4.38s ===============================\n</code></pre> <p>This output indicates that all tests ran without errors in the file <code>tests/pipelines/test_data_science_pipeline.py</code>.</p>"},{"location":"pages/tutorial/tutorial_template/","title":"Set up the spaceflights project","text":"<p>This section shows how to create a new project with <code>kedro new</code> using the Kedro spaceflights starter) and install project dependencies (with <code>pip install -r requirements.txt</code>).</p>"},{"location":"pages/tutorial/tutorial_template/#create-a-new-project","title":"Create a new project","text":"<p>Set up Kedro if you have not already done so.</p> <pre><code>We recommend that you use the same version of Kedro that was most recently used to test this tutorial (0.19.0). To check the version installed, type `kedro -V` in your terminal window.\n</code></pre> <p>Navigate to the folder you want to store the project. Type the following to generate the project from the Kedro spaceflights starter. The project will be populated with a complete set of working example code:</p> <pre><code>kedro new --starter=spaceflights-pandas\n</code></pre> <p>When prompted for a project name, you should accept the default choice (<code>Spaceflights</code>) as the rest of this tutorial assumes that project name.</p> <p>After Kedro has created the project, navigate to the project root directory:</p> <pre><code>cd spaceflights\n</code></pre>"},{"location":"pages/tutorial/tutorial_template/#install-project-dependencies","title":"Install project dependencies","text":"<p>Kedro projects have a <code>requirements.txt</code> file to specify their dependencies and enable sharable projects by ensuring consistency across Python packages and versions.</p> <p>The spaceflights project dependencies are stored in <code>requirements.txt</code>(you may find that the versions differ slightly depending on the version of Kedro):</p> <pre><code># code quality packages\nipython~=8.10; python_version &gt;= '3.8'\nruff==0.1.8\n\n# notebook tooling\njupyter~=1.0\njupyterlab_server&gt;=2.11.1\njupyterlab~=3.0\n\n# Pytest + useful extensions\npytest-cov~=3.0\npytest-mock&gt;=1.7.1, &lt;2.0\npytest~=7.2\n\n# Kedro dependencies and datasets to work with different data formats (including CSV, Excel, and Parquet)\nkedro~=0.19.0\nkedro-datasets[pandas-csvdataset, pandas-exceldataset, pandas-parquetdataset]&gt;=3.0\nkedro-telemetry&gt;=0.3.1\nkedro-viz~=6.0 # Visualise pipelines\n\n# For modeling in the data science pipeline\nscikit-learn~=1.0\n</code></pre>"},{"location":"pages/tutorial/tutorial_template/#install-the-dependencies","title":"Install the dependencies","text":"<p>To install all the project-specific dependencies, run the following from the project root directory:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"pages/tutorial/tutorial_template/#optional-logging-and-configuration","title":"Optional: logging and configuration","text":"<p>You might want to set up logging at this stage of the workflow, but we do not use it in this tutorial.</p> <p>You may also want to store credentials such as usernames and passwords if they are needed for specific data sources used by the project.</p> <p>To do this, add them to <code>conf/local/credentials.yml</code> (some examples are included in that file for illustration).</p>"},{"location":"pages/tutorial/tutorial_template/#configuration-best-practice-to-avoid-leaking-confidential-data","title":"Configuration best practice to avoid leaking confidential data","text":"<ul> <li>Do not commit data to version control.</li> <li>Do not commit notebook output cells (data can easily sneak into notebooks when you don't delete output cells).</li> <li>Do not commit credentials in <code>conf/</code>. Use only the <code>conf/local/</code> folder for sensitive information like access credentials.</li> </ul> <p>You can find additional information in the documentation on configuration.</p>"},{"location":"pages/visualisation/","title":"Visualisation with Kedro-Viz","text":"<p>Kedro-Viz is a key part of Kedro. It visualises the pipelines in a Kedro project by showing data, nodes, and the connections between them.</p> <p>The Kedro-Viz package needs to be installed separately as it is not part of the standard Kedro installation:</p> <pre><code>pip install kedro-viz\n</code></pre> <p>Consult the Kedro-Viz documentation to find out more about how to use the package.</p>"}]}